[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hey üëã",
    "section": "",
    "text": "I‚Äôm Taylor, a data scientist based in Halifax, Nova Scotia. This is my personal website, which I started to share my projects, learnings, code (mostly R and Python), and thoughts on various topics in data science.\nI work at Yelp as a Data Science Analyst where, among other things, I design and analyze online experiments. Before that, I worked for Ardea Outcomes, a contract research organization supporting pharmaceutical companies in clinical trials. My background is in Physics. I earned my MSc from Dalhousie University, and my BSc from the University of Prince Edward Island."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Predicting bike ridership: deploying the model\n\n\n\n\n\n\n\nR\n\n\nmachine learning\n\n\ntidymodels\n\n\nXGBoost\n\n\nGoogle Cloud Platform\n\n\nDocker\n\n\nShiny\n\n\nforecasting\n\n\nMLOps\n\n\n\n\nPart 3 of predicting bike ridership in Halifax, Nova Scotia. In this post, I deploy the machine learning model on Google Cloud Platform.\n\n\n\n\n\n\nMay 19, 2022\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nPredicting bike ridership: developing a model\n\n\n\n\n\n\n\nR\n\n\nmachine learning\n\n\ntidymodels\n\n\nrandom forest\n\n\nXGBoost\n\n\nsupport-vector machine\n\n\nforecasting\n\n\n\n\nPart 2 of predicting bike ridership in Halifax, Nova Scotia. In this post, I explore and tune different modeling approaches.\n\n\n\n\n\n\nApr 29, 2022\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nPredicting bike ridership: getting the data\n\n\n\n\n\n\n\nR\n\n\nAPI\n\n\n\n\nPart 1 of predicting bike ridership in Halifax, Nova Scotia. In this post, I retrieve and explore data from two open APIs.\n\n\n\n\n\n\nApr 27, 2022\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing US baby names over the years\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\ndata visualization\n\n\n\n\nTidyTuesday 2022 Week 12: Baby names.\n\n\n\n\n\n\nMar 26, 2022\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nCanada COVID-19 data in R: scheduling API queries\n\n\n\n\n\n\n\nR\n\n\nCOVID-19\n\n\nAPI\n\n\npackage development\n\n\nGitHub actions\n\n\ncron\n\n\n\n\nScheduling data retrieval and updating with GitHub Actions and cron.\n\n\n\n\n\n\nJan 22, 2022\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nCanada COVID-19 data in R: creating a package\n\n\n\n\n\n\n\nR\n\n\nCOVID-19\n\n\nAPI\n\n\npackage development\n\n\n\n\nCreating an R package that wraps the Canadian COVID-19 tracker API.\n\n\n\n\n\n\nDec 30, 2021\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nCanada COVID-19 data in R: exploring the API\n\n\n\n\n\n\n\nR\n\n\nCOVID-19\n\n\nAPI\n\n\n\n\nAn exploration of the Canadian COVID-19 tracker API.\n\n\n\n\n\n\nDec 28, 2021\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2021: Days 21-25\n\n\n\n\n\n\n\nAdvent of Code\n\n\nPython\n\n\nR\n\n\n\n\nMy solutions to the #AdventOfCode2021 coding challenges, days 21 through 25.\n\n\n\n\n\n\nDec 21, 2021\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2021: Days 16-20\n\n\n\n\n\n\n\nAdvent of Code\n\n\nPython\n\n\nR\n\n\n\n\nMy solutions to the #AdventOfCode2021 coding challenges, days 16 through 20.\n\n\n\n\n\n\nDec 16, 2021\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2021: Days 11-15\n\n\n\n\n\n\n\nAdvent of Code\n\n\nPython\n\n\nR\n\n\n\n\nMy solutions to the #AdventOfCode2021 coding challenges, days 11 through 15.\n\n\n\n\n\n\nDec 11, 2021\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2021: Days 6-10\n\n\n\n\n\n\n\nAdvent of Code\n\n\nPython\n\n\nR\n\n\n\n\nMy solutions to the #AdventOfCode2021 coding challenges, days 6 through 10.\n\n\n\n\n\n\nDec 6, 2021\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2021: Days 1-5\n\n\n\n\n\n\n\nAdvent of Code\n\n\nPython\n\n\nR\n\n\n\n\nMy solutions to the #AdventOfCode2021 coding challenges, days 1 through 5.\n\n\n\n\n\n\nDec 1, 2021\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 2021 Week 21\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\ntext mining\n\n\ndata visualization\n\n\n\n\n#TidyTuesday 2021-05-18: Ask a Manager Salary Survey.\n\n\n\n\n\n\nMay 18, 2021\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 2020 Week 45\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\ntidymodels\n\n\nmachine learning\n\n\nrandom forest\n\n\n\n\n#TidyTuesday 2020-11-03: Ikea Furniture.\n\n\n\n\n\n\nNov 8, 2020\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 2020 Week 28\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\ntidymodels\n\n\nmachine learning\n\n\nrandom forest\n\n\nlasso\n\n\n\n\n#TidyTuesday 2020-07-07: Coffee Ratings.\n\n\n\n\n\n\nJul 12, 2020\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nOrdinal regression in R: part 2\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\nordinal\n\n\nBayesian statistics\n\n\n\n\nA theoretical and applied walkthrough of ordinal regression. Part 2: the Bayesian approach with brms.\n\n\n\n\n\n\nMar 17, 2020\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\nOrdinal regression in R: part 1\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\nordinal\n\n\nfrequentist statistics\n\n\n\n\nA theoretical and applied walkthrough of ordinal regression. Part 1: the frequentist approach with ordinal.\n\n\n\n\n\n\nMar 15, 2020\n\n\nTaylor Dunn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/index.html",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/index.html",
    "title": "Ordinal regression in R: part 1",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(dunnr)\nlibrary(gt)\nlibrary(broom)\nlibrary(patchwork)\n\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()\n\nwine_red &lt;- \"#58181F\"\nupdate_geom_defaults(\"point\", list(color = wine_red))\nupdate_geom_defaults(\"line\", list(color = wine_red))\nThe purpose of this post is to learn more about ordinal regression models (a.k.a. cumulative link, proportional odds, ordered logit models, etc.) and practice their implementation in R. This is part 1, where I‚Äôll be taking the frequentist approach via the ordinal package. There are other options, like MASS::polr, but two features in particular drew me to ordinal: (1) it allows for random effects, and (2) it has broom::tidy methods available.\nParticularly, I‚Äôll be following along with"
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/index.html#setup",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/index.html#setup",
    "title": "Ordinal regression in R: part 1",
    "section": "Setup",
    "text": "Setup\nImport ordinal, and the included data set wine:\n\nlibrary(ordinal)\ndata(wine)\nwine &lt;- as_tibble(wine)\nglimpse(wine)\n\nRows: 72\nColumns: 6\n$ response &lt;dbl&gt; 36, 48, 47, 67, 77, 60, 83, 90, 17, 22, 14, 50, 30, 51, 90, 7‚Ä¶\n$ rating   &lt;ord&gt; 2, 3, 3, 4, 4, 4, 5, 5, 1, 2, 1, 3, 2, 3, 5, 4, 2, 3, 3, 2, 5‚Ä¶\n$ temp     &lt;fct&gt; cold, cold, cold, cold, warm, warm, warm, warm, cold, cold, c‚Ä¶\n$ contact  &lt;fct&gt; no, no, yes, yes, no, no, yes, yes, no, no, yes, yes, no, no,‚Ä¶\n$ bottle   &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5‚Ä¶\n$ judge    &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3‚Ä¶\n\n\nwine is a data set from Randall (1989) of wine bitterness ratings from multiple judges. The variables are as follows:\n\nOutcome:\n\nresponse: wine bitterness rating on a 0-100 scale\nrating: ordered factor with 5 levels (grouped version of response) with 1 = ‚Äúleast bitter‚Äù and 5 = ‚Äúmost bitter‚Äù\n\nTreatment factors:\n\ntemp: temperature during wine production (cold and warm)\ncontact: contact between juice and skins during wine production (no and yes)\n\nRandom effects\n\nbottle with 8 levels\njudge with 9 levels\n\n\nRelationship between response and rating:\n\nwine %&gt;%\n  ggplot(aes(y = rating, x = response)) +\n  geom_boxplot(width = 0.5) +\n  geom_jitter(alpha = 0.5)\n\n\n\n\nNote that there is no overlap between the levels.\nThere are 72 total observations with the following ratings distribution by treatment and random effects:\n\nwine %&gt;%\n  transmute(temp, contact, bottle, judge, rating = as.numeric(rating)) %&gt;%\n  pivot_wider(names_from = judge, values_from = rating) %&gt;%\n  gt() %&gt;%\n  tab_spanner(columns = `1`:`9`, label = \"judge\") %&gt;%\n  data_color(\n    columns = `1`:`9`,\n    colors = scales::col_numeric(\n      palette = c(\"white\", wine_red), domain = c(1, 5)\n    )\n  )\n\n\n\n\n\n\n\n\ntemp\ncontact\nbottle\njudge\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\ncold\nno\n1\n2\n1\n2\n3\n2\n3\n1\n2\n1\n\n\ncold\nno\n2\n3\n2\n3\n2\n3\n2\n1\n2\n2\n\n\ncold\nyes\n3\n3\n1\n3\n3\n4\n3\n2\n2\n3\n\n\ncold\nyes\n4\n4\n3\n2\n2\n3\n2\n2\n3\n2\n\n\nwarm\nno\n5\n4\n2\n5\n3\n3\n2\n2\n3\n3\n\n\nwarm\nno\n6\n4\n3\n5\n2\n3\n4\n3\n3\n2\n\n\nwarm\nyes\n7\n5\n5\n4\n5\n3\n5\n2\n3\n4\n\n\nwarm\nyes\n8\n5\n4\n4\n3\n3\n4\n3\n4\n4\n\n\n\n\n\n\n\nSo each bottle had a particular temp and contact (2 bottles for each of the 4 combinations), and each judge rated the bitterness each bottle.\nBefore modeling, can we see a clear effect of temp and contact?\n\nwine %&gt;%\n  count(contact, rating, temp) %&gt;%\n  mutate(temp = fct_rev(temp)) %&gt;%\n  ggplot(aes(x = temp, y = rating, color = temp)) +\n  geom_point(aes(group = temp, size = n)) +\n  facet_wrap(~contact, scales = \"free_x\",\n             labeller = labeller(contact = label_both)) +\n  scale_size(breaks = c(1, 2, 4, 6, 8)) +\n  add_facet_borders()\n\n\n\n\nAt a glance, it looks like the temp = warm and contact = yes is associated with higher ratings."
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/index.html#the-cumulative-link-model",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/index.html#the-cumulative-link-model",
    "title": "Ordinal regression in R: part 1",
    "section": "The cumulative link model",
    "text": "The cumulative link model\n\nTheory\nThe ordinal response \\(y_i\\) falls into response category \\(j\\) (out of \\(J\\) total) with probability \\(\\pi_{ij}\\). The cumulative probabilities are defined:\n\\[\nP(y_i \\leq j) = \\pi_{i1} + \\dots + \\pi_{ij}.\n\\]\nAs an oversimplification, suppose that each probability \\(\\pi_{ij}\\) is equal to the proportion of that response in the wine data. Then the cumulative ‚Äúprobability‚Äù can be visualized:\n\nwine_prop &lt;- wine %&gt;%\n  count(rating) %&gt;%\n  mutate(p = n / sum(n), cumsum_p = cumsum(p))\n\n(\n  ggplot(wine_prop, aes(x = rating, y = p)) +\n    geom_col(fill = wine_red) +\n    scale_y_continuous(labels = scales::percent, expand = c(0, 0)) +\n    labs(x = \"j\", y = \"proportion\")\n) +\n  (\n    ggplot(wine_prop, aes(x = as.integer(rating), y = cumsum_p)) +\n      geom_point(size = 2) +\n      geom_line(size = 1) +\n      labs(x = \"j\", y = \"cumulative proportion\")\n  ) +\n  (\n    ggplot(wine_prop,\n        aes(x = as.integer(rating), y = log(cumsum_p) - log(1 - cumsum_p))) +\n      geom_point(size = 2) +\n      geom_line(size = 1) +\n      labs(x = \"j\", y = \"logit(cumulative proportion)\")\n  )\n\n\n\n\nWe will explore other links, but first the most common, the logit link, which is depicted in the right-most panel of the above figure:\n\\[\n\\text{logit} (P(y_i \\leq j) = \\log \\frac{P(y_i \\leq j)}{1 - P(y_i \\leq j)}\n\\]\nNote that the above function is defined for all but the last category \\(j = J\\), because \\(1 - P(Y_i \\leq J) = 1 - 1 = 0\\).\nFor the wine data, where we have \\(J\\) = 5 rating categories, we will build up to the following mixed effects model:\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - u( \\text{judge}_i) \\\\\ni &= 1, \\dots n \\; \\; \\; \\; \\; \\; j = 1, \\dots, J - 1\n\\end{align}\n\\]\nwhere \\(\\theta_j\\) is called the threshold parameter, or cutpoint, of category \\(j\\). These thresholds can also be thought of as \\(J-1\\) = 4 intercepts. Note that the fixed effect parameters \\(\\beta_1\\) and \\(\\beta_2\\) are independent of \\(j\\), so each \\(\\beta\\) has the same effect for each of the \\(J-1\\) cumulative logits. The judge effects, which are also independent of \\(j\\), are assumed normal: \\(u(\\text{judge}_i) \\sim N(0, \\sigma_u^2)\\). We are using the logit link because it is the most popular for this kind of model (and the one I am familiar with), but there are other options we will briefly explore later.\nThe subtraction of terms in the above model is new to me. The main reason seems to be for familiar interpretation: the larger the value of any independent term \\(\\beta x\\), the smaller the thresholds \\(\\theta_j\\), and therefore a larger probability of the a response falling into a category at the upper end of the scale. This way, \\(\\beta\\) has the same direction of effect as in ordinary linear regression.\nWe are essentially modeling a ‚Äúchain‚Äù of logistic regressions where the binary response is ‚Äúless than or equal to a certain level‚Äù vs ‚Äúgreater than that level‚Äù. In this case, with \\(J\\) = 5, the thresholds \\(\\theta_j\\) are capturing the adjusted log-odds of observing:\n\n\\(j\\) = 1: log-odds of rating = 1 vs.¬†2-5\n\\(j\\) = 2: log-odds of rating = 1-2 vs.¬†3-5\n\\(j\\) = 3: log-odds of rating = 1-3 vs.¬†4-5\n\\(j\\) = 4: log-odds of rating = 1-4 vs.¬†5\n\n\n\nFitting\nNow with a surface-level understanding of what is being modeled, we will fit the data using ordinal::clm (cumulative link models) and ordinal::clmm (cumulative link mixed models), and logit links.\n\nFixed effects model\nFirst, fit a simple model, by maximum likelihood, with contact as the sole predictor:\n\\[\n\\text{logit}(p(y_i \\leq j)) = \\theta_j - \\beta_2 \\text{contact}_i\n\\]\n\nclm_rating_contact &lt;-\n  clm(\n    rating ~ contact,\n    data = wine, link = \"logit\"\n  )\nsummary(clm_rating_contact)\n\nformula: rating ~ contact\ndata:    wine\n\n link  threshold nobs logLik AIC    niter max.grad cond.H \n logit flexible  72   -99.96 209.91 5(0)  1.67e-07 1.7e+01\n\nCoefficients:\n           Estimate Std. Error z value Pr(&gt;|z|)   \ncontactyes   1.2070     0.4499   2.683   0.0073 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -2.13933    0.48981  -4.368\n2|3  0.04257    0.32063   0.133\n3|4  1.71449    0.38637   4.437\n4|5  2.97875    0.50207   5.933\n\n\nThe model gives us \\(K - 1 = 4\\) threshold coefficients, as expected. The \\(\\beta_2\\) coefficient estimate was statistically significant (by a Wald test), and tells us that contact = yes decreases the thresholds \\(\\theta_j\\) by \\(\\beta_2\\) = 1.21 (because of the subtraction of model terms), and therefore is associated with higher ratings.\nThe condition number of the Hessian for this model is 16.98. The ordinal primer says that larger values (like &gt; 1e4) might indicate that the model is ill-defined.\nIt is nicely illustrative to compare this model to 4 separate logistic regressions with a dichotomized response:\n\\[\n\\begin{align}\n\\text{logit} (p(y_i \\leq 1)) &= \\theta_1 + \\beta_2 \\text{contact}_i \\\\\n\\text{logit} (p(y_i \\leq 2)) &= \\theta_2 + \\beta_2 \\text{contact}_i \\\\\n\\text{logit} (p(y_i \\leq 3)) &= \\theta_3 + \\beta_2 \\text{contact}_i \\\\\n\\text{logit} (p(y_i \\leq 4)) &= \\theta_4 + \\beta_2 \\text{contact}_i\n\\end{align}\n\\]\n\nwine %&gt;%\n  crossing(j = 1:4) %&gt;%\n  # Create a binary (0 or 1) to indicate where rating &lt;= j\n  mutate(rating_leq_j = as.numeric(rating) &lt;= j) %&gt;%\n  group_by(j) %&gt;%\n  nest() %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    mod = map(\n      data,\n      ~glm(rating_leq_j ~ 1 + contact,\n           data = ., family = binomial(link = \"logit\")) %&gt;% broom::tidy()\n    )\n  ) %&gt;%\n  unnest(mod) %&gt;%\n  transmute(\n    j, term,\n    estimate_se = str_c(round(estimate, 2), \" (\", round(std.error, 2), \")\")\n  ) %&gt;%\n  pivot_wider(names_from = term, values_from = estimate_se) %&gt;%\n  left_join(\n    tidy(clm_rating_contact) %&gt;%\n      transmute(\n        j = as.integer(substr(term, 1, 1)),\n        term = if_else(!is.na(j), \"theta_j\", term),\n        estimate_se = str_c(round(estimate, 2), \" (\", round(std.error, 2), \")\")\n      ) %&gt;%\n      mutate(j = replace_na(j, 1)) %&gt;%\n      spread(term, estimate_se),\n    by = \"j\"\n  ) %&gt;%\n  ungroup() %&gt;%\n  gt() %&gt;%\n  tab_spanner(label = \"Logistic regression\",\n              columns = c(`(Intercept)`, contactyes.x)) %&gt;%\n  tab_spanner(label = \"CLM\",\n              columns = c(theta_j, contactyes.y)) %&gt;%\n  fmt_missing(columns = everything(), missing_text = \"\")\n\n\n\n\n\n\n\n\nj\nLogistic regression\nCLM\n\n\n(Intercept)\ncontactyes.x\ntheta_j\ncontactyes.y\n\n\n\n\n1\n-2.08 (0.53)\n-1.48 (1.14)\n-2.14 (0.49)\n1.21 (0.45)\n\n\n2\n0 (0.33)\n-1.1 (0.51)\n0.04 (0.32)\n\n\n\n3\n1.82 (0.48)\n-1.37 (0.59)\n1.71 (0.39)\n\n\n\n4\n2.83 (0.73)\n-1.01 (0.87)\n2.98 (0.5)\n\n\n\n\n\n\n\n\nThe intercepts from the ordinary logistic regression correspond closely to the threshold parameters \\(\\theta_j\\) from the cumulative link model. In the fixed effect of contact (\\(\\beta_2\\)), first note the sign difference, and second notice that the estimate from CLM is about the average of the 4 estimates from logistic regression. The advantage of the CLM is seen in the small standard error in the \\(\\beta_2\\) estimate.\nTo quote the primer:\n\nThe cumulative logit model can be seen as the model that combines these four ordinary logistic regression models into a single model and therefore makes better use of the information in the data.\n\nFor the second model, we add the \\(\\beta_1 \\text{temp}_i\\) term:\n\\[\n\\text{logit}(p(y_i \\leq j)) = \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i\n\\]\n\nclm_rating_contact_temp &lt;-\n  clm(\n    rating ~ contact + temp,\n    data = wine, link = \"logit\"\n  )\nsummary(clm_rating_contact_temp)\n\nformula: rating ~ contact + temp\ndata:    wine\n\n link  threshold nobs logLik AIC    niter max.grad cond.H \n logit flexible  72   -86.49 184.98 6(0)  4.01e-12 2.7e+01\n\nCoefficients:\n           Estimate Std. Error z value Pr(&gt;|z|)    \ncontactyes   1.5278     0.4766   3.205  0.00135 ** \ntempwarm     2.5031     0.5287   4.735 2.19e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -1.3444     0.5171  -2.600\n2|3   1.2508     0.4379   2.857\n3|4   3.4669     0.5978   5.800\n4|5   5.0064     0.7309   6.850\n\n\nBoth fixed effects (contact = yes and temp = warm) are strongly associated with higher probability of higher ratings. The summary function provides \\(p\\)-values from Wald tests, but more accurate likelihood ratio tests can be done via the drop1 function, which evaluates each fixed effect while controlling the other:\n\ndrop1(clm_rating_contact_temp, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\nrating ~ contact + temp\n        Df    AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;     184.98                     \ncontact  1 194.03 11.043 0.0008902 ***\ntemp     1 209.91 26.928 2.112e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOr the reverse via the add1() function, which evaluates each fixed effect while ignoring the other:\n\n# Fit the null model first\nclm_rating_null &lt;- clm(rating ~ 1, data = wine, link = \"logit\")\nadd1(clm_rating_null, scope = ~ contact + temp, test = \"Chisq\")\n\nSingle term additions\n\nModel:\nrating ~ 1\n        Df    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;     215.44                      \ncontact  1 209.91  7.5263   0.00608 ** \ntemp     1 194.03 23.4113 1.308e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSymmetric Wald confidence intervals can be extracted with confint or with broom::tidy:\n\ntidy(clm_rating_contact_temp, conf.int = TRUE, conf.type = \"Wald\") %&gt;%\n  ggplot(aes(y = term, x = estimate)) +\n  geom_point(size = 2) +\n  geom_linerange(size = 1, aes(xmin = conf.low, xmax = conf.high))\n\n\n\n\nIn these types of analyses, we are often interested in the odds ratios. For the two categorical fixed effects, which have two levels each, the odds ratios \\(y \\leq j\\) comparing the two levels are:\n\\[\n\\begin{align}\n\\text{OR} &= \\frac{\\gamma_j (\\text{temp} = \\text{warm})}{\\gamma_j (\\text{temp} = \\text{cold})} = \\frac{\\exp(\\theta_j - \\beta_1 - \\beta_2 \\text{contact})}{\\exp (\\theta_j - 0 - \\beta_2 \\text{contact}}) = \\exp(\\beta_1) \\\\\n\\text{OR} &= \\frac{\\gamma_j (\\text{contact} = \\text{yes})}{\\gamma_j (\\text{contact} = \\text{no})} = \\frac{\\exp(\\theta_j - \\beta_1 \\text{temp} - \\beta_2 )}{\\exp (\\theta_j - \\beta_1 \\text{temp} - 0)}) = \\exp(\\beta_2)\n\\end{align}\n\\]\nwhere we have introduced the shorthand \\(\\gamma_j = \\text{logit} (p(y \\leq j))\\). Compute those odds ratios, and their corresponding Wald 95% CIs:\n\ntidy(clm_rating_contact_temp, conf.int = T, conf.type = \"Wald\") %&gt;%\n  transmute(\n    term, across(c(estimate, conf.low, conf.high), exp)\n  ) %&gt;%\n  gt() %&gt;%\n  fmt_number(c(estimate, conf.low, conf.high), decimals = 2)\n\n\n\n\n\n\n\n\nterm\nestimate\nconf.low\nconf.high\n\n\n\n\n1|2\n0.26\n0.09\n0.72\n\n\n2|3\n3.49\n1.48\n8.24\n\n\n3|4\n32.04\n9.93\n103.39\n\n\n4|5\n149.37\n35.65\n625.75\n\n\ncontactyes\n4.61\n1.81\n11.73\n\n\ntempwarm\n12.22\n4.34\n34.44\n\n\n\n\n\n\n\nOne last thing to check: does the data support an interaction between \\(\\text{temp}_i\\) and \\(\\text{contact}_i\\)?\n\\[\n\\text{logit}(p(y_i \\leq j)) = \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - \\beta_3 \\text{temp}_i \\text{contact}_i\n\\]\n\nclm_rating_contact_temp_inter &lt;-\n  clm(\n    rating ~ contact * temp, data = wine, link = \"logit\"\n  )\n\n#drop1(clm_rating_contact_temp_inter, test = \"Chisq\") # this accomplishes the same thing as anova()\nanova(clm_rating_contact_temp, clm_rating_contact_temp_inter)\n\nLikelihood ratio tests of cumulative link models:\n \n                              formula:                link: threshold:\nclm_rating_contact_temp       rating ~ contact + temp logit flexible  \nclm_rating_contact_temp_inter rating ~ contact * temp logit flexible  \n\n                              no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)\nclm_rating_contact_temp            6 184.98 -86.492                      \nclm_rating_contact_temp_inter      7 186.83 -86.416  0.1514  1     0.6972\n\n\nNo, The interaction term contact:temp is not supported by the data.\n\nComparison to linear model\nConsider the following linear model which treats rating as continuous:\n\\[\ny_i = \\alpha + \\beta_1 \\text{temp}_i + \\beta_2 \\text{contact}_i + \\epsilon_i\n\\]\nwhere \\(\\epsilon_i \\sim N(0, \\sigma_{\\epsilon}^2)\\).\n\nlm_rating_contact_temp &lt;- lm(as.numeric(rating) ~ contact + temp, data = wine)\n\nTo compare this to a CLM, we must use the probit link:\n\nclm_rating_contact_temp_probit &lt;-\n  clm(\n    rating ~ contact + temp, data = wine, link = \"probit\"\n  )\ntidy(clm_rating_contact_temp_probit) %&gt;%\n  filter(coef.type == \"location\") %&gt;%\n  mutate(model = \"CLM\") %&gt;%\n  select(-coef.type) %&gt;%\n  bind_rows(\n    tidy(lm_rating_contact_temp) %&gt;%\n      filter(term != \"(Intercept)\") %&gt;%\n      # Need to divide by the residual SE here to get the right scale\n      mutate(estimate = estimate / summary(lm_rating_contact_temp)$sigma,\n             model = \"LM\")\n  ) %&gt;%\n  group_by(model) %&gt;%\n  gt() %&gt;%\n  fmt_number(c(estimate, std.error, statistic), decimals = 2) %&gt;%\n  fmt(p.value, fns = scales::pvalue)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nCLM\n\n\ncontactyes\n0.87\n0.27\n3.25\n0.001\n\n\ntempwarm\n1.50\n0.29\n5.14\n&lt;0.001\n\n\nLM\n\n\ncontactyes\n0.79\n0.20\n3.36\n0.001\n\n\ntempwarm\n1.38\n0.20\n5.87\n&lt;0.001\n\n\n\n\n\n\n\nThe relative estimates from the linear model are lower than those from the CLM (probit link), indicating that the assumptions of the linear model are not met. In particular, the distance between thresholds is not equidistant, as we can see from differences in the CLM coefficients:\n\ndiff(coef(clm_rating_contact_temp_probit)[1:4]) %&gt;% round(2)\n\n 2|3  3|4  4|5 \n1.51 1.31 0.90 \n\n\n\n\n\nMixed effects model\nNow that we have explored ordinal regression with just fixed effects, we will fit the following random effects model:\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - u( \\text{judge}_i) \\\\\ni &= 1, \\dots n \\; \\; \\; \\; \\; \\; j = 1, \\dots, J - 1\n\\end{align}\n\\]\nwhere the judge effects are independent of \\(j\\), and assumed normal: \\(u(\\text{judge}_i) \\sim N(0, \\sigma_u^2)\\).\nEach judge has 8 ratings each (two per combination of temp and contact). See if we can spot the judge variance in a plot of ratings:\n\nwine %&gt;%\n  count(judge, rating) %&gt;%\n  ggplot(aes(x = judge, y = rating)) +\n  geom_tile(aes(fill = n)) +\n  geom_text(aes(label = n), color = \"white\") +\n  scale_x_discrete(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Number of ratings by judge\")\n\n\n\n\nThere is definitely some judge-specific variability in the perception of bitterness of wine. judge 5, for instance, doesn‚Äôt stray far from rating = 3, while judge 7 didn‚Äôt consider any of the wines particularly bitter.\nFit the full model with ordinal::clmm and logit link:\n\nclmm_rating_contact_temp &lt;-\n  clmm(\n    rating ~ temp + contact + (1|judge),\n    data = wine, link = \"logit\"\n  )\n# This is an older function, which we need to run stats::profile later\nclmm2_rating_contact_temp &lt;-\n  clmm2(\n    rating ~ temp + contact, random = judge,\n    data = wine, link = \"logistic\"\n  )\nsummary(clmm_rating_contact_temp)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ temp + contact + (1 | judge)\ndata:    wine\n\n link  threshold nobs logLik AIC    niter    max.grad cond.H \n logit flexible  72   -81.57 177.13 332(999) 1.03e-05 2.8e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n judge  (Intercept) 1.279    1.131   \nNumber of groups:  judge 9 \n\nCoefficients:\n           Estimate Std. Error z value Pr(&gt;|z|)    \ntempwarm     3.0630     0.5954   5.145 2.68e-07 ***\ncontactyes   1.8349     0.5125   3.580 0.000344 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -1.6237     0.6824  -2.379\n2|3   1.5134     0.6038   2.507\n3|4   4.2285     0.8090   5.227\n4|5   6.0888     0.9725   6.261\n\n\nCompare model coefficients:\n\nbind_rows(\n  CLM = tidy(clm_rating_contact_temp),\n  CLMM = tidy(clmm_rating_contact_temp),\n  .id = \"model\"\n) %&gt;%\n  select(-coef.type) %&gt;%\n  group_by(model) %&gt;%\n  gt() %&gt;%\n  fmt_number(c(estimate, std.error, statistic), decimals = 2) %&gt;%\n  fmt(p.value, fns = scales::pvalue)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nCLM\n\n\n1|2\n‚àí1.34\n0.52\n‚àí2.60\n0.009\n\n\n2|3\n1.25\n0.44\n2.86\n0.004\n\n\n3|4\n3.47\n0.60\n5.80\n&lt;0.001\n\n\n4|5\n5.01\n0.73\n6.85\n&lt;0.001\n\n\ncontactyes\n1.53\n0.48\n3.21\n0.001\n\n\ntempwarm\n2.50\n0.53\n4.73\n&lt;0.001\n\n\nCLMM\n\n\n1|2\n‚àí1.62\n0.68\n‚àí2.38\n0.017\n\n\n2|3\n1.51\n0.60\n2.51\n0.012\n\n\n3|4\n4.23\n0.81\n5.23\n&lt;0.001\n\n\n4|5\n6.09\n0.97\n6.26\n&lt;0.001\n\n\ntempwarm\n3.06\n0.60\n5.14\n&lt;0.001\n\n\ncontactyes\n1.83\n0.51\n3.58\n&lt;0.001\n\n\n\n\n\n\n\nBoth fixed effect estimates \\(\\beta_1\\) and \\(\\beta_2\\) are higher in the CLMM. Use anova to compare the CLMM to the CLM:\n\nanova(clm_rating_contact_temp, clmm_rating_contact_temp)\n\nLikelihood ratio tests of cumulative link models:\n \n                         formula:                              link: threshold:\nclm_rating_contact_temp  rating ~ contact + temp               logit flexible  \nclmm_rating_contact_temp rating ~ temp + contact + (1 | judge) logit flexible  \n\n                         no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)   \nclm_rating_contact_temp       6 184.98 -86.492                         \nclmm_rating_contact_temp      7 177.13 -81.565   9.853  1   0.001696 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUnsurprisingly, the judge term makes a significant improvement to the fit. We can extract profile confidence intervals on the variance \\(\\sigma_u\\) using stats::profile:\n\nprofile(clmm2_rating_contact_temp,\n        range = c(0.1, 4), nSteps = 30, trace = 0) %&gt;%\n  confint()\n\n          2.5 %  97.5 %\nstDev 0.5014584 2.26678\n\n\nNote that these intervals are asymmetric (\\(\\sigma_u\\) = 1.28), unlike the less accurate Wald tests. We can produce ‚Äúbest guess‚Äù estimates for judge effects using conditional modes:\n\ntibble(\n  judge_effect = clmm_rating_contact_temp$ranef,\n  cond_var = clmm_rating_contact_temp$condVar\n) %&gt;%\n  mutate(\n    judge = fct_reorder(factor(1:n()), judge_effect),\n    conf.low = judge_effect - qnorm(0.975) * sqrt(cond_var),\n    conf.high = judge_effect + qnorm(0.975) * sqrt(cond_var)\n  ) %&gt;%\n  ggplot(aes(y = judge, x = judge_effect)) +\n  geom_point(size = 2) +\n  geom_linerange(size = 1, aes(xmin = conf.low, xmax = conf.high)) +\n  theme(panel.grid.major.x = element_line(color = \"grey\"))\n\n\n\n\n\nPredictions\nThere are different ways to extract predicted probabilities. First, and most obviously, with the predict function:\n\nwine %&gt;%\n  bind_cols(\n    pred =  predict(\n      # Have to use clmm2 for predict\n      clmm2_rating_contact_temp, newdata = wine\n    )\n  ) %&gt;%\n  # These are predicted probabilities for the average judge, so we can\n  #  exclude the judge variable\n  distinct(rating, temp, contact, pred) %&gt;%\n  arrange(temp, contact, rating)\n\n# A tibble: 15 √ó 4\n   rating temp  contact   pred\n   &lt;ord&gt;  &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt;\n 1 1      cold  no      0.165 \n 2 2      cold  no      0.655 \n 3 3      cold  no      0.166 \n 4 1      cold  yes     0.0305\n 5 2      cold  yes     0.390 \n 6 3      cold  yes     0.496 \n 7 4      cold  yes     0.0696\n 8 2      warm  no      0.166 \n 9 3      warm  no      0.587 \n10 4      warm  no      0.191 \n11 5      warm  no      0.0463\n12 2      warm  yes     0.0313\n13 3      warm  yes     0.306 \n14 4      warm  yes     0.428 \n15 5      warm  yes     0.233 \n\n\nThis only gives us predictions for rating, temp and contact values which exist in the data. There is no predicted probability for rating &gt; 3, temp cold and contact no, for example.\nAnother way is to pre-specify which values we want to predict:\n\nnd &lt;-\n  crossing(\n    temp = factor(c(\"cold\", \"warm\")),\n    contact = factor(c(\"no\", \"yes\")),\n    rating = factor(1:5, ordered = T)\n  )\nnd %&gt;%\n  bind_cols(pred = predict(clmm2_rating_contact_temp, nd)) %&gt;%\n  ggplot(aes(x = glue::glue(\"{temp}-{contact}\"), y = pred, fill = rating)) +\n  geom_col() +\n  scale_fill_td(palette = \"div5\") +\n  scale_y_continuous(expand = c(0, 0), labels = scales::percent) +\n  labs(x = \"temp-contact\", y = \"predicted probability\")\n\n\n\n\nWe can also get model-estimated cumulative probabilities by considering the model coefficients. For example, for a cold temp and contact, the cumulative probability of a bitterness rating \\(j\\) or less:\n\\[\nP(y_i \\leq j) = \\text{logit}^{-1} [\\theta_j - \\beta_2 \\text{contact}_i]\n\\]\nwhere we are considering the average judge (\\(u(\\text{judge}_i) = 0\\)). The inverse logit is \\(\\text{logit}^{-1}(x) = 1 / (1 + \\exp(-x))\\), and can be calculated with plogis as a shorthand (brms::inv_logit_scaled is another). We can subtract cumulative probabilities to get non-cumulative probabilities of a rating \\(j\\). For example, \\(j\\) = 3:\n\nplogis(clmm_rating_contact_temp$Theta[3] - clmm_rating_contact_temp$beta[2]) -\n  plogis(clmm_rating_contact_temp$Theta[2] - clmm_rating_contact_temp$beta[2])\n\ncontactyes \n 0.4960357 \n\n\nwhich matches the value calculated previously using predict.\n\n\nEstimated marginal means\nThe emmeans package provides functionality for estimating marginal mean effects of ordinal models. The package documentation also provides an example using ordinal and wine data here.\n\nlibrary(emmeans)\n\nIn the ‚ÄúModels supported by emmeans‚Äù document, we see the following:\n\n\n\n\n\n\n\n\n\n\nObject.class\nPackage\nGroup\nArguments/notes\n\n\n\n\n\nclm\nordinal\nO\nmode = c(\"latent\", \"linear.predictor\", \"cum.prob\", \"exc.prob\", \"prob\", \"mean.class\", \"scale\")\n\n\n\nclmm\nordinal\nO\nLike clm but no \"scale\" mode\n\n\n\n\n\n\n\n\n\n\n\n\nemmeans(clmm_rating_contact_temp,\n        specs = list(pairwise ~ temp, pairwise ~ contact), mode = \"latent\")\n\n$`emmeans of temp`\n temp emmean    SE  df asymp.LCL asymp.UCL\n cold  -1.63 0.547 Inf    -2.707    -0.562\n warm   1.43 0.532 Inf     0.387     2.470\n\nResults are averaged over the levels of: contact \nConfidence level used: 0.95 \n\n$`pairwise differences of temp`\n 1           estimate    SE  df z.ratio p.value\n cold - warm    -3.06 0.595 Inf  -5.145  &lt;.0001\n\nResults are averaged over the levels of: contact \n\n$`emmeans of contact`\n contact emmean    SE  df asymp.LCL asymp.UCL\n no      -1.020 0.522 Inf    -2.043   0.00274\n yes      0.815 0.513 Inf    -0.191   1.82072\n\nResults are averaged over the levels of: temp \nConfidence level used: 0.95 \n\n$`pairwise differences of contact`\n 1        estimate    SE  df z.ratio p.value\n no - yes    -1.83 0.513 Inf  -3.580  0.0003\n\nResults are averaged over the levels of: temp \n\n\nThe contrast estimates are in terms of the latent (underlying unobserved) bitterness rating.\nUsing mode = \"cum.prob\" and mode = \"exc.prob‚Äú, we can get cumulative probabilities and exceedance (1 - cumulative) probabilities. For example, the probability of a rating of at least 4 for different temp:\n\nemmeans(clmm_rating_contact_temp, ~ temp,\n        mode = \"exc.prob\", at = list(cut = \"3|4\"))\n\n temp exc.prob     SE  df asymp.LCL asymp.UCL\n cold    0.049 0.0304 Inf   -0.0107     0.109\n warm    0.450 0.1084 Inf    0.2371     0.662\n\nResults are averaged over the levels of: contact \nConfidence level used: 0.95 \n\n\nmode = \"prob\" gives us probability distributions of each rating, which have a nice auto plot functionality:\n\nemmeans(clmm_rating_contact_temp,\n        ~ rating | temp, mode = \"prob\") %&gt;%\n  plot() +\n  add_facet_borders()\n\n\n\n\n\n\nChoice of link function\nSo far, we have used the logit link (and briefly the probit link to compare estimates with linear regression). The links available to ordinal::clmm are logit, probit, cloglog, loglog, and cauchit.\nWe can fit the CLMM using all of these links and compare log-likelihoods:\n\nwine %&gt;%\n  nest(data = everything()) %&gt;%\n  crossing(\n    link = c(\"logit\", \"probit\", \"cloglog\", \"loglog\", \"cauchit\")\n  ) %&gt;%\n  mutate(\n    mod = map2(\n      data, link,\n      ~clmm(\n        rating ~ 1 + contact + temp + (1|judge),\n        data = .x, link = .y\n      )\n    )\n  ) %&gt;%\n  mutate(mod_summary = map(mod, glance)) %&gt;%\n  unnest(mod_summary) %&gt;%\n  select(link, logLik, AIC, BIC) %&gt;%\n  arrange(logLik) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nlink\nlogLik\nAIC\nBIC\n\n\n\n\ncauchit\n-86.83499\n187.6700\n203.6066\n\n\ncloglog\n-82.72936\n179.4587\n195.3954\n\n\nlogit\n-81.56541\n177.1308\n193.0675\n\n\nloglog\n-81.54137\n177.0827\n193.0194\n\n\nprobit\n-80.93061\n175.8612\n191.7979\n\n\n\n\n\n\n\nThe probit model appears to be the best description of the data.\nWe can also consider the effect of ‚Äúflexible‚Äù vs ‚Äúequidistant‚Äù thresholds:\n\nwine %&gt;%\n  nest(data = everything()) %&gt;%\n  crossing(\n    link = c(\"logit\", \"probit\", \"cloglog\", \"loglog\", \"cauchit\"),\n    threshold = c(\"flexible\", \"equidistant\")\n  ) %&gt;%\n  mutate(\n    mod = pmap(\n      list(data, link, threshold),\n      function(a, b, c) {\n        clmm(\n          rating ~ 1 + contact + temp + (1|judge),\n          data = a, link = b, threshold = c\n        )\n      }\n    )\n  ) %&gt;%\n  #mutate(mod_summary = map(mod, glance)) %&gt;%\n  mutate(\n    mod_summary = map(\n      mod,\n      # glance() on a clmm object returns a &lt;logLik&gt; variable type which\n      #  can't be bound together by unnest(), so need to convert it to numeric\n      ~glance(.x) %&gt;% mutate(logLik = as.numeric(logLik))\n    )\n  ) %&gt;%\n  unnest(mod_summary) %&gt;%\n  select(link, threshold, logLik, edf, AIC, BIC) %&gt;%\n  arrange(logLik) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nlink\nthreshold\nlogLik\nedf\nAIC\nBIC\n\n\n\n\ncauchit\nequidistant\n-87.75021\n5\n185.5004\n196.8837\n\n\ncauchit\nflexible\n-86.83499\n7\n187.6700\n203.6066\n\n\nloglog\nequidistant\n-84.36440\n5\n178.7288\n190.1121\n\n\ncloglog\nequidistant\n-83.32634\n5\n176.6527\n188.0360\n\n\nlogit\nequidistant\n-83.05497\n5\n176.1099\n187.4933\n\n\ncloglog\nflexible\n-82.72936\n7\n179.4587\n195.3954\n\n\nprobit\nequidistant\n-82.52622\n5\n175.0524\n186.4358\n\n\nlogit\nflexible\n-81.56541\n7\n177.1308\n193.0675\n\n\nloglog\nflexible\n-81.54137\n7\n177.0827\n193.0194\n\n\nprobit\nflexible\n-80.93061\n7\n175.8612\n191.7979\n\n\n\n\n\n\n\nNote the change in degrees of freedom, resulting in the equidistant probit model having the lowest BIC. In terms of log likelihood, however, flexible always outperform equidistant thresholds."
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/index.html#conclusion",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/index.html#conclusion",
    "title": "Ordinal regression in R: part 1",
    "section": "Conclusion",
    "text": "Conclusion\nThanks to detailed documentation, fitting cumulative link (mixed) models is very easy with ordinal. In this post, we first learned the theoretical basis for these models, then worked through examples using wine bitterness ratings from multiple judges.\nIn the next post, I‚Äôll explore the Bayesian approach to ordinal regression with the brms package."
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/index.html#reproducibility",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/index.html#reproducibility",
    "title": "Ordinal regression in R: part 1",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html",
    "href": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html",
    "title": "Ordinal regression in R: part 2",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(dunnr)\nlibrary(gt)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(patchwork)\n\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()\n\nwine_red &lt;- \"#58181F\"\nupdate_geom_defaults(\"point\", list(color = wine_red))\nupdate_geom_defaults(\"line\", list(color = wine_red))\nThis is part 2 of me learning ordinal regression in R. Previously, I took the frequentist approach with the ordinal package. Here, I‚Äôll use brms package to fit Bayesian mixed models via Stan.\nThough I won‚Äôt be reproducing their examples, B√ºrkner and Vuorre (2019) give a great tutorial of using brms for ordinal regression models. It also frames the cumulative model in the terms of a latent (not observable) continuous variable \\(\\tilde{y}\\), which has been categorized into the observed ordinal variable \\(y\\). I found this way of thinking very intuitive, and it helped make a lot of the concepts click.\nThis post also serves as practice in Bayesian inference, so I‚Äôll be comparing the results here to those from part 1, and explore different choices of prior distributions."
  },
  {
    "objectID": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#setup",
    "href": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#setup",
    "title": "Ordinal regression in R: part 2",
    "section": "Setup",
    "text": "Setup\nLoad brms, tidybayes and the wine data from Randall (1989) that was analyzed in part 1.\n\nlibrary(brms)\nlibrary(tidybayes)\n# Detect and set the number of cores for MCMC\noptions(mc.cores = parallel::detectCores())\n\nlibrary(ordinal)\ndata(wine)\nwine &lt;- as_tibble(wine)"
  },
  {
    "objectID": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#fitting-the-models",
    "href": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#fitting-the-models",
    "title": "Ordinal regression in R: part 2",
    "section": "Fitting the models",
    "text": "Fitting the models\nI will be fitting these models of wine bitterness ratings:\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - u( \\text{judge}_i) \\\\\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - u( \\text{judge}_i) \\\\\n\\end{align}\n\\]\nwhere \\(p(y_i \\leq j)\\) is the probability of a rating less than or equal to \\(j\\), \\(\\theta_j\\) are the thresholds for the \\(J-1 = 4\\) levels, \\(u(\\text{judge}_i)\\) are judge-specific random effects, and \\(\\beta_1\\) and \\(\\beta_2\\) are fixed effect coefficients for \\(\\text{temp}_i\\) and \\(\\text{contact}_i\\). (see part 1 for more details).\n\nf_rating_null &lt;- rating ~ 1 + (1|judge)\nf_rating_contact_temp &lt;- rating ~ 1 + contact + temp + (1|judge)\n\n\nNull model\nWe will start with the ‚Äúnull‚Äù model, with just thresholds and random effects. The default priors for this model are:\n\nget_prior(f_rating_null, data = wine, family = cumulative(\"logit\")) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\nstudent_t(3, 0, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\n\nIntercept\n1\n\n\n\n\n\n\ndefault\n\n\n\nIntercept\n2\n\n\n\n\n\n\ndefault\n\n\n\nIntercept\n3\n\n\n\n\n\n\ndefault\n\n\n\nIntercept\n4\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 2.5)\nsd\n\n\n\n\n\n0\n\ndefault\n\n\n\nsd\n\njudge\n\n\n\n\n\ndefault\n\n\n\nsd\nIntercept\njudge\n\n\n\n\n\ndefault\n\n\n\n\n\n\n\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - u( \\text{judge}_i) \\\\\n\\theta_j &\\sim \\text{Student-}t(3, 0, 2.5) \\\\\nu(\\text{judge}_i) &\\sim \\text{Normal}(0, \\sigma_u) \\\\\n\\sigma_u &\\sim \\text{Student-} t(3, 0, 2.5)\n\\end{align}\n\\]\nFit the model:\n\nbrm_rating_null &lt;-\n  brm(\n    f_rating_null,\n    data = wine,\n    family = cumulative(\"logit\"),\n    sample_prior = TRUE,\n    file = \"brm-rating-null\"\n  )\n\nVisualize the two priors (on thresholds/Intercepts, and on SD of judge effects) with brms:prior_samples():\n\nprior_draws(brm_rating_null) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"term\", values_to = \"value\") %&gt;%\n  mutate(samples = \"model prior samples\") %&gt;%\n  ggplot(aes(x = value, y = samples, fill = samples)) +\n  geom_violin() +\n  # Also visualize some random samples from the t-distribution for comparison\n  geom_violin(\n    data = tibble(\n      value = rstudent_t(n = 4000, df = 3, mu = 0, sigma = 2.5),\n      samples = \"student_t(3, 0, 2.5)\", term = \"Intercept\"\n    )\n  ) +\n  geom_violin(\n    data = tibble(\n      value = abs(rstudent_t(n = 4000, df = 3, mu = 0, sigma = 2.5)),\n      samples = \"abs(student_t(3, 0, 2.5))\", term = \"sd_judge\"\n    )\n  ) +\n  labs(y = NULL) +\n  facet_wrap(~term, scales = \"free_x\") +\n  coord_cartesian(xlim = c(-15, 15)) +\n  theme(legend.position = \"none\") +\n  add_facet_borders()\n\n\n\n\nNote that, since a standard deviation can‚Äôt be negative, brms automatically takes the absolute value of the default student_t(3, 0, 2.5) prior.\nThese priors aren‚Äôt terrible. For instance, intercepts (the thresholds \\(\\theta_j\\)) between -10 and +10 correspond to the following probabilities:\n\ntibble(theta = seq(-10, 10)) %&gt;%\n  mutate(p = inv_logit_scaled(theta)) %&gt;%\n  ggplot(aes(x = theta, y = p)) +\n  geom_line(size = 1)\n\n\n\n\nAny prior values outside of this range would essentially zero cumulative probability for a level \\(\\leq j\\).\nNow that we‚Äôve thought about our (default) prior assumptions, investigate the chains:\n\nplot(brm_rating_null, ask = FALSE,\n     # Only plot specified variables\n     variable = \"^b_|^sd_\", regex = TRUE)\n\n\n\n\nThe intercept trace plots look good to me. There are some spikes in sd_judge__Intercept, but not enough to be concerning.\nPrint the model estimates:\n\nbrm_rating_null\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: rating ~ 1 + (1 | judge) \n   Data: wine (Number of observations: 72) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~judge (Number of levels: 9) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.73      0.41     0.07     1.64 1.00     1339     1614\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]    -2.79      0.54    -3.93    -1.81 1.00     2240     2206\nIntercept[2]    -0.56      0.36    -1.30     0.12 1.00     2777     2750\nIntercept[3]     1.10      0.38     0.39     1.87 1.00     2986     2927\nIntercept[4]     2.42      0.49     1.54     3.47 1.00     3761     2958\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe Rhat values are also a good sign of model convergence.\nCompare the null Bayesian model estimates to the frequentist estimates:\n\n# Can't figure out how to extract random effect SDs from a clmm model, use clmm2\nclmm2_rating_null &lt;-\n  clmm2(\n    rating ~ 1, random = judge,\n    data = wine, link = \"logistic\", Hess = TRUE\n  )\n# Unfortunately, clmm2 doesn't have a broom::tidy() function\nsummary(clmm2_rating_null) %&gt;%\n  coef() %&gt;%\n  as_tibble() %&gt;%\n  mutate(term = str_c(\"b_Intercept[\", 1:4, \"]\")) %&gt;%\n  bind_rows(\n    tibble(\n      Estimate = as.numeric(clmm2_rating_null$stDev),\n      term = \"sd_judge__Intercept\"\n    )\n  ) %&gt;%\n  janitor::clean_names() %&gt;%\n  left_join(\n    broom.mixed::tidyMCMC(brm_rating_null, conf.int = TRUE),\n    by = \"term\"\n  ) %&gt;%\n  relocate(term) %&gt;%\n  mutate(\n    pr_z = scales::pvalue(pr_z),\n    across(where(is.numeric), ~round(., 2))\n  ) %&gt;%\n  gt() %&gt;%\n  tab_spanner(\n    label = \"ordinal::clmm\",\n    columns = c(estimate.x, std_error, z_value, pr_z)\n  ) %&gt;%\n  tab_spanner(\n    label = \"brms::brm\",\n    columns = c(estimate.y, std.error, conf.low, conf.high)\n  ) %&gt;%\n  sub_missing(columns = everything(), missing_text = \"\")\n\n\n\n\n\n\n\n\nterm\nordinal::clmm\nbrms::brm\n\n\nestimate.x\nstd_error\nz_value\npr_z\nestimate.y\nstd.error\nconf.low\nconf.high\n\n\n\n\nb_Intercept[1]\n-2.72\n0.52\n-5.25\n&lt;0.001\n-2.76\n0.54\n-3.93\n-1.81\n\n\nb_Intercept[2]\n-0.54\n0.32\n-1.72\n0.085\n-0.55\n0.36\n-1.30\n0.12\n\n\nb_Intercept[3]\n1.10\n0.34\n3.24\n0.001\n1.10\n0.38\n0.39\n1.87\n\n\nb_Intercept[4]\n2.35\n0.46\n5.13\n&lt;0.001\n2.40\n0.49\n1.54\n3.47\n\n\nsd_judge__Intercept\n0.57\n\n\n\n0.69\n0.41\n0.07\n1.64\n\n\n\n\n\n\n\nFrequentist estimates are pretty close to the Bayesian estimates with naive priors.\n\nChoice of priors\nSo what are reasonable priors for this data and model? My go-to resource for this kind of thing is this page from the stan wiki, but under the ‚ÄúPrior for cutpoints in ordered logit or probit regression‚Äù, they have a couple suggestions like ‚Äúuniform priors typically should be ok‚Äù, but also say ‚ÄúNeed to flesh out this section with examples‚Äù, so not a lot of help there.\nI‚Äôll consider the following priors on the the thresholds:\n\\[\n\\begin{align}\n\\theta_j &\\sim \\text{Student-}t(3, 0, 5.0) \\\\\n\\theta_j &\\sim \\text{Student-}t(3, 0, 2.5) \\\\\n\\theta_j &\\sim \\text{Student-}t(3, 0, 1.0) \\\\\n\\theta_j &\\sim \\text{Normal}(0, 5.0) \\\\\n\\theta_j &\\sim \\text{Normal}(0, 2.5) \\\\\n\\theta_j &\\sim \\text{Normal}(0, 1.0) \\\\\n\\end{align}\n\\]\nand simulate some corresponding cumulative probabilities:\n\nbind_rows(\n  tibble(sigma = c(1, 2.5, 5), dist = \"Normal\") %&gt;%\n    mutate(\n      prior = str_c(\"Normal(0, \", sigma, \")\"),\n      samples = map(sigma, ~rnorm(2000, mean = 0, sd = .x))\n    ),\n  tibble(sigma = c(1, 2.5, 5), dist = \"Student-t\") %&gt;%\n    mutate(prior = str_c(\"Student-t(3, 0, \", sigma, \")\"),\n           samples = map(sigma, ~rstudent_t(2000, df = 3, mu = 0, sigma = .x)))\n  ) %&gt;%\n  #mutate(prior = fct_reorder2(prior, sigma, dist)) %&gt;%\n  unnest(samples) %&gt;%\n  mutate(p = inv_logit_scaled(samples)) %&gt;%\n  ggplot(aes(x = p)) +\n  geom_histogram(fill = wine_red, binwidth = 0.1) +\n  facet_wrap(~prior) +\n  scale_y_continuous(expand = c(0, 0)) +\n  add_facet_borders()\n\n\n\n\nThe \\(\\text{Normal}(0, 5)\\) and \\(\\text{Student-}t(3, 0, 5)\\) priors place most of the samples at the extremes (cumulative probabilities of 0 and 100%). The \\(\\text{Normal}(0, 2.5)\\) and \\(\\text{Student-}t(3, 0, 2.5)\\) are sensible default priors because they are fairly uniform in the probability space, although they do have slight peaks at the extremes. The \\(\\text{Normal}(0, 1)\\) and \\(\\text{Student-}t(3, 0, 1)\\) prior distributions are bell-shaped around \\(p = 0.5\\), and might be appropriate if we have reason to believe that there will be no extreme probabilities.\nAs for the scale parameter describing the variance in judge-specific random effects, I‚Äôve seen the half-Cauchy distribution recommended:\n\ntibble(scale = c(1, 2.5, 5)) %&gt;%\n  crossing(x = seq(0, 10, 0.1)) %&gt;%\n  mutate(\n    prior = str_c(\"Half-Cauchy(0, \", scale, \")\"),\n    dens = dcauchy(x, location = 0, scale = scale)\n  ) %&gt;%\n  ggplot(aes(x, y = dens)) +\n  geom_line(aes(color = prior), size = 1) +\n  theme(legend.position = c(0.6, 0.6), axis.ticks.y = element_blank(),\n        axis.text.y = element_blank())\n\n\n\n\nThis distribution is fairly conservative, with a long right tail that allows for large values.\nI don‚Äôt think there is anything wrong with the default priors, but just to show how it is done, I‚Äôll fit the null model with the following:\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - u( \\text{judge}_i) \\\\\n\\theta_j &\\sim \\text{Normal}(0, 1.5) \\\\\nu(\\text{judge}_i) &\\sim \\text{Normal}(0, \\sigma_u) \\\\\n\\sigma_u &\\sim \\text{Half-Cauchy}(0, 2.5)\n\\end{align}\n\\]\n\nprior_rating_null &lt;- c(\n  prior(normal(0, 1.5), class = Intercept),\n  prior(cauchy(0, 2.5), class = sd)\n)\nbrm_rating_null_alt_prior &lt;-\n  brm(\n    f_rating_null,\n    prior = prior_rating_null ,\n    data = wine,\n    family = cumulative(\"logit\"),\n    file = \"brm-rating-null-alt-prior\"\n  )\nplot(brm_rating_null_alt_prior, ask = FALSE,\n     variable = \"^b_|^sd_\", regex = TRUE)\n\n\n\n\nAs expected, these priors don‚Äôt noticeably improve the model convergence (which was already good). Likewise, the model estimates changed only slightly:\n\ntidyMCMC(brm_rating_null) %&gt;%\n  mutate(priors = \"default priors\") %&gt;%\n  bind_rows(\n    tidyMCMC(brm_rating_null_alt_prior) %&gt;%\n      mutate(priors = \"alternative priors\")\n  ) %&gt;%\n  filter(!str_starts(term, \"r_|prior_|disc\")) %&gt;%\n  transmute(\n    term,\n    estimate_se = str_c(round(estimate, 2), \" (\", round(std.error, 2), \")\"),\n    priors\n  ) %&gt;%\n  pivot_wider(names_from = priors, values_from = estimate_se) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nterm\ndefault priors\nalternative priors\n\n\n\n\nb_Intercept[1]\n-2.76 (0.54)\n-2.63 (0.48)\n\n\nb_Intercept[2]\n-0.55 (0.36)\n-0.53 (0.33)\n\n\nb_Intercept[3]\n1.1 (0.38)\n1.04 (0.33)\n\n\nb_Intercept[4]\n2.4 (0.49)\n2.28 (0.43)\n\n\nsd_judge__Intercept\n0.69 (0.41)\n0.64 (0.39)\n\n\n\n\n\n\n\n\n\n\nFixed effects\nWe now add the ‚Äútreatment‚Äù effects of temp and contact:\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - u( \\text{judge}_i) \\\\\n\\end{align}\n\\]\nThis introduces two new priors we can specify:\n\nget_prior(f_rating_contact_temp, data = wine, family = cumulative(\"logit\")) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\ncontactyes\n\n\n\n\n\n\ndefault\n\n\n\nb\ntempwarm\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 2.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\n\nIntercept\n1\n\n\n\n\n\n\ndefault\n\n\n\nIntercept\n2\n\n\n\n\n\n\ndefault\n\n\n\nIntercept\n3\n\n\n\n\n\n\ndefault\n\n\n\nIntercept\n4\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 2.5)\nsd\n\n\n\n\n\n0\n\ndefault\n\n\n\nsd\n\njudge\n\n\n\n\n\ndefault\n\n\n\nsd\nIntercept\njudge\n\n\n\n\n\ndefault\n\n\n\n\n\n\n\nWe know from part 1 that contactyes (\\(\\beta_1\\)) and tempwarm (\\(\\beta_2\\)) are associated with higher ratings, but we shouldn‚Äôt be biasing our priors by using the same data we are modeling. Instead, use a weakly regularizing normal distributions centered at 0:\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - u( \\text{judge}_i) \\\\\n\\beta_1 &\\sim \\text{Normal}(0, 5) \\\\\n\\beta_2 &\\sim \\text{Normal}(0, 5) \\\\\n\\theta_j &\\sim \\text{Normal}(0, 1.5) \\\\\nu(\\text{judge}_i) &\\sim \\text{Normal}(0, \\sigma_u) \\\\\n\\sigma_u &\\sim \\text{Half-Cauchy}(0, 2.5)\n\\end{align}\n\\]\n\nprior_rating_contact_temp &lt;-\n  c(prior_rating_null,\n    prior(normal(0, 5), class = b))\nbrm_rating_contact_temp &lt;-\n  brm(\n    f_rating_contact_temp,\n    prior = prior_rating_contact_temp,\n    data = wine,\n    family = cumulative(\"logit\"),\n    file = \"brm-rating-contact-temp-weak-prior\"\n  )\n# Also fit using the default priors\nbrm_rating_contact_temp_default_prior &lt;-\n  brm(\n    f_rating_contact_temp,\n    data = wine,\n    family = cumulative(\"logit\"),\n    sample_prior = TRUE,\n    file = \"brm-rating-contact-temp-default-prior\"\n  )\nbrm_rating_contact_temp\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: rating ~ 1 + contact + temp + (1 | judge) \n   Data: wine (Number of observations: 72) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~judge (Number of levels: 9) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.14      0.45     0.46     2.20 1.00     1553     2226\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]    -1.40      0.57    -2.55    -0.31 1.00     3148     3209\nIntercept[2]     1.30      0.53     0.26     2.33 1.00     3604     3207\nIntercept[3]     3.66      0.67     2.40     5.02 1.00     2992     2650\nIntercept[4]     5.34      0.78     3.88     6.91 1.00     2974     3058\ncontactyes       1.61      0.47     0.69     2.54 1.00     4020     2752\ntempwarm         2.69      0.51     1.72     3.71 1.00     3161     2589\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCompare these estimates to those from clmm and with default priors:\n\nclmm2_rating_contact_temp &lt;-\n  clmm2(\n    rating ~ contact + temp, random = judge,\n    data = wine, link = \"logistic\", Hess = TRUE\n  )\ntab_brm_clmm_rating_contact_temp &lt;- tidyMCMC(brm_rating_contact_temp) %&gt;%\n  mutate(model = \"brm weak priors\") %&gt;%\n  bind_rows(\n    tidyMCMC(brm_rating_contact_temp_default_prior) %&gt;%\n      mutate(model = \"brm default priors\")\n  ) %&gt;%\n  filter(!str_detect(term, \"r_judge|lp__|prior|disc\")) %&gt;%\n  transmute(\n    term, model,\n    estimate_se = str_c(round(estimate, 2), \" (\", round(std.error, 2), \")\")\n  ) %&gt;%\n  bind_rows(\n    summary(clmm2_rating_contact_temp) %&gt;%\n      coef() %&gt;%\n      as_tibble() %&gt;%\n      mutate(term = c(str_c(\"b_Intercept[\", 1:4, \"]\"),\n                      \"b_contactyes\", \"b_tempwarm\")) %&gt;%\n      bind_rows(\n        tibble(\n          Estimate = as.numeric(clmm2_rating_contact_temp$stDev),\n          term = \"sd_judge__Intercept\"\n        )\n      ) %&gt;%\n      janitor::clean_names() %&gt;%\n      transmute(\n        model = \"clmm\", term,\n        estimate_se = ifelse(\n          !is.na(std_error),\n          str_c(round(estimate, 2), \" (\", round(std_error, 2), \")\"),\n          round(estimate, 2)\n        )\n      )\n  ) %&gt;%\n  pivot_wider(names_from = model, values_from = estimate_se)\ngt(tab_brm_clmm_rating_contact_temp)\n\n\n\n\n\n\n\n\nterm\nbrm weak priors\nbrm default priors\nclmm\n\n\n\n\nb_Intercept[1]\n-1.39 (0.57)\n-1.7 (0.78)\n-1.62 (0.68)\n\n\nb_Intercept[2]\n1.3 (0.53)\n1.45 (0.65)\n1.51 (0.6)\n\n\nb_Intercept[3]\n3.65 (0.67)\n4.17 (0.83)\n4.23 (0.81)\n\n\nb_Intercept[4]\n5.32 (0.78)\n6.06 (0.97)\n6.09 (0.97)\n\n\nb_contactyes\n1.6 (0.47)\n1.85 (0.52)\n1.83 (0.51)\n\n\nb_tempwarm\n2.67 (0.51)\n3.1 (0.6)\n3.06 (0.6)\n\n\nsd_judge__Intercept\n1.08 (0.45)\n1.36 (0.56)\n1.13\n\n\n\n\n\n\n\nThe estimates from the Bayesian regression with default priors are very close to the frequentist estimates.\nBut is the model actually a better fit to the data? We can compare them with leave-one-out cross-validation (LOOCV) based on the posterior likelihoods:\n\nloo(brm_rating_contact_temp, brm_rating_contact_temp_default_prior)\n\nOutput of model 'brm_rating_contact_temp':\n\nComputed from 4000 by 72 log-likelihood matrix\n\n         Estimate  SE\nelpd_loo    -85.2 4.3\np_loo        10.3 0.9\nlooic       170.4 8.5\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'brm_rating_contact_temp_default_prior':\n\nComputed from 4000 by 72 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo    -85.3  5.3\np_loo        12.7  1.3\nlooic       170.5 10.6\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n                                      elpd_diff se_diff\nbrm_rating_contact_temp                0.0       0.0   \nbrm_rating_contact_temp_default_prior -0.1       1.1   \n\n\nThe output here is the expected log predicted density (elpd_loo), the estimated number of effective parameters (p_loo), and the LOOCV information criteria (looic). Lower values of looic indicate better model fit. These are essentially equal, so the weak priors have not made a large difference to the model fit, as seen in the low elpd_diff relative to se_diff."
  },
  {
    "objectID": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#aside-adjacent-category-models",
    "href": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#aside-adjacent-category-models",
    "title": "Ordinal regression in R: part 2",
    "section": "Aside: adjacent-category models",
    "text": "Aside: adjacent-category models\nHere are what B√ºrkner and Vuorre (2019) had to say about the adjacent-category class of ordinal models.\n\nPredicts the decision between two adjacent categories \\(k\\) and \\(k+1\\).\nLatent variables \\(\\tilde{Y}_k\\) with thresholds \\(\\tau_k\\) and cumulative distribution function \\(F\\).\nIf \\(\\tilde{Y}_k &lt; \\tau_k\\), we choose category \\(k\\); \\(k+1\\) otherwise.\nIt is difficult to think of a natural process leading to them; chosen for its mathematical convenience rather than quality of interpretation.\n\nMathematically:\n\\[\n\\text{Pr}(Y = k | Y \\in \\{k, k+1\\}) = F(\\tau_k).\n\\]\nSuppose the latent variable \\(\\tilde{Y}_2\\) is standard normally distributed with distribution function \\(\\Phi\\), and \\(\\tau_2\\) = 1. Then the probability of choosing \\(Y\\) = 2 over \\(Y\\) = 3 would be written as:\n\\[\n\\text{Pr}(Y = 2 | Y \\in \\{2, 3\\}) = \\Phi(\\tau_2) = \\Phi(1) = 0.84.\n\\]\nTry fitting the null wine rating model with the acat family in brms:\n\nbrm_rating_contact_temp_default_prior_acat &lt;-\n  brm(\n    f_rating_contact_temp,\n    family = acat(link = \"logit\"),\n    data = wine,\n    file = \"brm-rating-contact-temp-default-prior-acat\"\n  )\n\nHow do these estimates compare to the cumulative model?\n\ntidyMCMC(brm_rating_contact_temp_default_prior) %&gt;%\n  mutate(model = \"cumulative\") %&gt;%\n  bind_rows(\n    tidyMCMC(brm_rating_contact_temp_default_prior_acat) %&gt;%\n      mutate(model = \"adjacent-category\")\n  ) %&gt;%\n  filter(!str_detect(term, \"r_judge|lp__|prior|disc\")) %&gt;%\n  transmute(\n    term, model,\n    estimate_se = str_c(round(estimate, 2), \" (\", round(std.error, 2), \")\")\n  ) %&gt;%\n  pivot_wider(names_from = model, values_from = estimate_se) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nterm\ncumulative\nadjacent-category\n\n\n\n\nb_Intercept[1]\n-1.7 (0.78)\n-1.28 (0.67)\n\n\nb_Intercept[2]\n1.45 (0.65)\n1.19 (0.57)\n\n\nb_Intercept[3]\n4.17 (0.83)\n3.36 (0.8)\n\n\nb_Intercept[4]\n6.06 (0.97)\n4.02 (1.03)\n\n\nb_contactyes\n1.85 (0.52)\n1.32 (0.4)\n\n\nb_tempwarm\n3.1 (0.6)\n2.3 (0.53)\n\n\nsd_judge__Intercept\n1.36 (0.56)\n1 (0.43)\n\n\n\n\n\n\n\nbrms has a convenience function conditional_effects() for quickly plotting effect estimates. For example, the effect of contact:\n\nce_rating_contact_temp_default_prior_acat &lt;-\n  conditional_effects(brm_rating_contact_temp_default_prior_acat,\n                      categorical = TRUE, re_formula = NA, ask = FALSE)\nce_rating_contact_temp_default_prior &lt;-\n  conditional_effects(brm_rating_contact_temp_default_prior,\n                      categorical = TRUE, re_formula = NA, ask = FALSE)\nce_rating_contact_temp_default_prior_acat$`contact:cats__` %&gt;%\n  mutate(model = \"adjacent-category\") %&gt;%\n  bind_rows(\n    ce_rating_contact_temp_default_prior$`contact:cats__` %&gt;%\n      mutate(model = \"cumulative\")\n  ) %&gt;%\n  ggplot(aes(x = contact, y = estimate__, color = effect2__)) +\n  geom_point(position = position_dodge(1), size = 3) +\n  geom_linerange(aes(ymin = lower__, ymax = upper__),\n                 position = position_dodge(1), size = 1) +\n  facet_wrap(~model) +\n  scale_color_viridis_d() +\n  labs(y = \"Estimated probabilities\", color = \"rating\") +\n  dunnr::add_facet_borders()"
  },
  {
    "objectID": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#category-specific-effects",
    "href": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#category-specific-effects",
    "title": "Ordinal regression in R: part 2",
    "section": "Category-specific effects",
    "text": "Category-specific effects\nIn all of the models specified so far, all fixed effects were presumed to affect all response categories equally. For example, the effect of temp = warm had a mean effect of \\(\\beta_1\\) = 2.67 on the thresholds \\(\\theta_j\\), for all \\(j = 1, 2, 3, 4\\).\nThis may not be an appropriate assumption. For example, temp warm might have little relation to the highest rating, but it may strongly predict ratings of 3 relative to 1 or 2.\nIf this is a possibility, then we can model the predictor as having a category-specific effect by estimating \\(K-1\\) coefficients for it. The reason we‚Äôve introduced the adjacent-category model is that it is straightforward to incorporate these effects (sequential models work as well). Cumulative models, however, can lead to negative probabilities, and so should be avoided when using category-specific effects.\nFit the adjacent-category model with category-specific effects on temp:\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - \\beta_{1j} \\text{temp}_i - \\beta_2 \\text{contact}_i - u( \\text{judge}_i) \\\\\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - \\beta_{1j} \\text{temp}_i - u( \\text{judge}_i) \\\\\n\\end{align}\n\\]\n\nf_rating_cs_temp &lt;- rating ~ 1 + cs(temp) + (1|judge)\nbrm_rating_cs_temp_default_prior_acat &lt;-\n  brm(\n    f_rating_cs_temp,\n    family = acat(link = \"logit\"),\n    data = wine,\n    file = \"brm-rating-cs-temp-default-prior-acat\"\n  )\n\nThere were many divergent transitions, which is clear from the ugly trace plots:\n\nplot(brm_rating_cs_temp_default_prior_acat, ask = FALSE,\n     variable = \"^b_|^sd_\", regex = TRUE)\n\n\n\n\nMost of the divergence is coming from estimating the lowest coefficient \\(\\beta_{11}\\) (bcs_tempwarm[1]). I will try some regularizing priors (previously defined) and increasing the adapt_delta argument:\n\nbrm_rating_cs_temp_weak_prior_acat &lt;-\n  brm(\n    f_rating_cs_temp,\n    prior = prior_rating_contact_temp,\n    family = acat(link = \"logit\"),\n    data = wine,\n    file = \"brm-rating-cs-temp-weak-prior-acat\",\n    control = list(adapt_delta = 0.9)\n  )\nplot(brm_rating_cs_temp_weak_prior_acat, ask = FALSE,\n     variable = \"^b_|^sd_\", regex = TRUE)\n\n\n\n\nThis makes a huge difference. Now compare this to a model without category-specific effects:\n\nce_rating_cs_temp_weak_prior_acat &lt;-\n  conditional_effects(brm_rating_cs_temp_weak_prior_acat,\n                      categorical = TRUE, re_formula = NA, ask = FALSE)\nbrm_rating_temp_weak_prior_acat &lt;-\n  brm(\n    rating ~ 1 + temp + (1|judge),\n    prior = prior_rating_contact_temp,\n    family = acat(link = \"logit\"),\n    data = wine,\n    file = \"brm-rating-temp-weak-prior-acat\"\n  )\nce_rating_temp_weak_prior_acat &lt;-\n  conditional_effects(brm_rating_temp_weak_prior_acat,\n                      categorical = TRUE, re_formula = NA, ask = FALSE)\nce_rating_temp_weak_prior_acat$`temp:cats__` %&gt;%\n  mutate(model = \"constant effects\") %&gt;%\n  bind_rows(\n    ce_rating_cs_temp_weak_prior_acat$`temp:cats__` %&gt;%\n      mutate(model = \"category-specific effects\")\n  ) %&gt;%\n  ggplot(aes(x = temp, y = estimate__, color = effect2__)) +\n  geom_point(position = position_dodge(1), size = 3) +\n  geom_linerange(aes(ymin = lower__, ymax = upper__),\n                 position = position_dodge(1), size = 1) +\n  facet_wrap(~model) +\n  scale_color_viridis_d() +\n  labs(y = \"Estimated probabilities\", color = \"rating\") +\n  dunnr::add_facet_borders()\n\n\n\n\nOr, put the probabilities for each model side-by-side, along with the empirical probabilities:\n\nce_rating_temp_weak_prior_acat$`temp:cats__` %&gt;%\n  mutate(model = \"constant effects\") %&gt;%\n  bind_rows(\n    ce_rating_cs_temp_weak_prior_acat$`temp:cats__` %&gt;%\n      mutate(model = \"category-specific effects\")\n  ) %&gt;%\n  ggplot(aes(x = effect2__, y = estimate__, color = model)) +\n  geom_point(position = position_dodge(0.5), size = 3) +\n  geom_linerange(aes(ymin = lower__, ymax = upper__),\n                 position = position_dodge(0.5), size = 1) +\n  geom_point(\n    data = wine %&gt;%\n      group_by(temp, rating) %&gt;%\n      tally() %&gt;%\n      group_by(temp) %&gt;%\n      mutate(p = n / sum(n), model = \"empirical\"),\n    aes(x = rating, y = p, color = model), size = 3\n  ) +\n  facet_wrap(~temp, ncol = 1) +\n  labs(y = \"Estimated probabilities\", x = \"Rating\", color = \"model\") +\n  dunnr::add_facet_borders()\n\n\n\n\nThe category-specific effects have not made a notable difference to the estimated probabilities. The constant effects model may even be better, meaning that it is a valid assumption for these data, and category-specific effects are excessive."
  },
  {
    "objectID": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#conclusion",
    "href": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#conclusion",
    "title": "Ordinal regression in R: part 2",
    "section": "Conclusion",
    "text": "Conclusion\nThe brms package is a great tool for fitting Bayesian models in Stan. Though it requires a more thoughtful approach (what are my priors?) and longer computations (Markov chain Monte Carlo can be slow), I find Bayesian inference far more intuitive than frequentist null hypothesis significance testing.\nFor instance, consider the cumulative link regression with contact and temp:\n\ntab_brm_clmm_rating_contact_temp %&gt;%\n  select(-`brm default priors`) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nterm\nbrm weak priors\nclmm\n\n\n\n\nb_Intercept[1]\n-1.39 (0.57)\n-1.62 (0.68)\n\n\nb_Intercept[2]\n1.3 (0.53)\n1.51 (0.6)\n\n\nb_Intercept[3]\n3.65 (0.67)\n4.23 (0.81)\n\n\nb_Intercept[4]\n5.32 (0.78)\n6.09 (0.97)\n\n\nb_contactyes\n1.6 (0.47)\n1.83 (0.51)\n\n\nb_tempwarm\n2.67 (0.51)\n3.06 (0.6)\n\n\nsd_judge__Intercept\n1.08 (0.45)\n1.13\n\n\n\n\n\n\n\nWith the Bayesian model, not only do we have the full posterior to work with, we can make conclusions like:\n\nUnder weakly regularizing priors, temperature probably affects wine rating \\(\\beta_{\\text{temp}}\\) = 2.67, 95% credible interval = 1.72, 3.71.\n\nVersus the frequentist model:\n\nThere is evidence against the null hypothesis that \\(\\beta_{\\text{temp}}\\) = 0, \\(p\\) &lt; 0.001. The point estimate from the model was 3.06, 95% confidence interval = 1.9, 4.23.\n\nWith frequentist inference, we find the probability of the data assuming the null hypothesis is true, \\(P(\\text{data}|H_0)\\). With Bayesian inference, we find the probability of a hypothesis given the data, \\(P(H|\\text{data})\\), which means we don‚Äôt even have to consider the ‚Äúnull world.‚Äù Much more intuitive, in my opinion."
  },
  {
    "objectID": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#reproducibility",
    "href": "posts/2020-03-17-ordinal-regression-in-r-part-2/index.html#reproducibility",
    "title": "Ordinal regression in R: part 2",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2020-07-12-tidytuesday-2020-week-28/index.html",
    "href": "posts/2020-07-12-tidytuesday-2020-week-28/index.html",
    "title": "TidyTuesday 2020 Week 28",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(gt)\nlibrary(rmarkdown)\nlibrary(patchwork)\nlibrary(ggtext)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\n# A coffee themed palette I found here: https://colorpalettes.net/color-palette-4281/\ncoffee_pal &lt;- c(\"#a0583c\", \"#c08267\", \"#ccb9b1\", \"#616063\", \"#212123\")\noptions(ggplot2.discrete.color = coffee_pal,\n        ggplot2.discrete.fill = coffee_pal)"
  },
  {
    "objectID": "posts/2020-07-12-tidytuesday-2020-week-28/index.html#load-the-data",
    "href": "posts/2020-07-12-tidytuesday-2020-week-28/index.html#load-the-data",
    "title": "TidyTuesday 2020 Week 28",
    "section": "Load the data",
    "text": "Load the data\n\ntt &lt;- tidytuesdayR::tt_load(\"2020-07-07\")\n\nThese data were scraped by James LeDoux in 2018 from the Coffee Quality Institute (see the original data here)."
  },
  {
    "objectID": "posts/2020-07-12-tidytuesday-2020-week-28/index.html#data-exploration",
    "href": "posts/2020-07-12-tidytuesday-2020-week-28/index.html#data-exploration",
    "title": "TidyTuesday 2020 Week 28",
    "section": "Data exploration",
    "text": "Data exploration\n\ncoffee &lt;- tt$coffee_ratings\n\nFor my own convenience, I‚Äôve copied data dictionary below:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntotal_cup_points\ndouble\nTotal rating/points (0 - 100 scale)\n\n\nspecies\ncharacter\nSpecies of coffee bean (arabica or robusta)\n\n\nowner\ncharacter\nOwner of the farm\n\n\ncountry_of_origin\ncharacter\nWhere the bean came from\n\n\nfarm_name\ncharacter\nName of the farm\n\n\nlot_number\ncharacter\nLot number of the beans tested\n\n\nmill\ncharacter\nMill where the beans were processed\n\n\nico_number\ncharacter\nInternational Coffee Organization number\n\n\ncompany\ncharacter\nCompany name\n\n\naltitude\ncharacter\nAltitude - this is a messy column - I‚Äôve left it for some cleaning\n\n\nregion\ncharacter\nRegion where bean came from\n\n\nproducer\ncharacter\nProducer of the roasted bean\n\n\nnumber_of_bags\ndouble\nNumber of bags tested\n\n\nbag_weight\ncharacter\nBag weight tested\n\n\nin_country_partner\ncharacter\nPartner for the country\n\n\nharvest_year\ncharacter\nWhen the beans were harvested (year)\n\n\ngrading_date\ncharacter\nWhen the beans were graded\n\n\nowner_1\ncharacter\nWho owns the beans\n\n\nvariety\ncharacter\nVariety of the beans\n\n\nprocessing_method\ncharacter\nMethod for processing\n\n\naroma\ndouble\nAroma grade\n\n\nflavor\ndouble\nFlavor grade\n\n\naftertaste\ndouble\nAftertaste grade\n\n\nacidity\ndouble\nAcidity grade\n\n\nbody\ndouble\nBody grade\n\n\nbalance\ndouble\nBalance grade\n\n\nuniformity\ndouble\nUniformity grade\n\n\nclean_cup\ndouble\nClean cup grade\n\n\nsweetness\ndouble\nSweetness grade\n\n\ncupper_points\ndouble\nCupper Points\n\n\nmoisture\ndouble\nMoisture Grade\n\n\ncategory_one_defects\ndouble\nCategory one defects (count)\n\n\nquakers\ndouble\nquakers\n\n\ncolor\ncharacter\nColor of bean\n\n\ncategory_two_defects\ndouble\nCategory two defects (count)\n\n\nexpiration\ncharacter\nExpiration date of the beans\n\n\ncertification_body\ncharacter\nWho certified it\n\n\ncertification_address\ncharacter\nCertification body address\n\n\ncertification_contact\ncharacter\nCertification contact\n\n\nunit_of_measurement\ncharacter\nUnit of measurement\n\n\naltitude_low_meters\ndouble\nAltitude low meters\n\n\naltitude_high_meters\ndouble\nAltitude high meters\n\n\naltitude_mean_meters\ndouble\nAltitude mean meters\n\n\n\nA lot of variables to consider here. Summarize them with skimr:\n\nskimr::skim(coffee)\n\n\nData summary\n\n\nName\ncoffee\n\n\nNumber of rows\n1339\n\n\nNumber of columns\n43\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n24\n\n\nnumeric\n19\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nspecies\n0\n1.00\n7\n7\n0\n2\n0\n\n\nowner\n7\n0.99\n3\n50\n0\n315\n0\n\n\ncountry_of_origin\n1\n1.00\n4\n28\n0\n36\n0\n\n\nfarm_name\n359\n0.73\n1\n73\n0\n571\n0\n\n\nlot_number\n1063\n0.21\n1\n71\n0\n227\n0\n\n\nmill\n315\n0.76\n1\n77\n0\n460\n0\n\n\nico_number\n151\n0.89\n1\n40\n0\n847\n0\n\n\ncompany\n209\n0.84\n3\n73\n0\n281\n0\n\n\naltitude\n226\n0.83\n1\n41\n0\n396\n0\n\n\nregion\n59\n0.96\n2\n76\n0\n356\n0\n\n\nproducer\n231\n0.83\n1\n100\n0\n691\n0\n\n\nbag_weight\n0\n1.00\n1\n8\n0\n56\n0\n\n\nin_country_partner\n0\n1.00\n7\n85\n0\n27\n0\n\n\nharvest_year\n47\n0.96\n3\n24\n0\n46\n0\n\n\ngrading_date\n0\n1.00\n13\n20\n0\n567\n0\n\n\nowner_1\n7\n0.99\n3\n50\n0\n319\n0\n\n\nvariety\n226\n0.83\n4\n21\n0\n29\n0\n\n\nprocessing_method\n170\n0.87\n5\n25\n0\n5\n0\n\n\ncolor\n218\n0.84\n4\n12\n0\n4\n0\n\n\nexpiration\n0\n1.00\n13\n20\n0\n566\n0\n\n\ncertification_body\n0\n1.00\n7\n85\n0\n26\n0\n\n\ncertification_address\n0\n1.00\n40\n40\n0\n32\n0\n\n\ncertification_contact\n0\n1.00\n40\n40\n0\n29\n0\n\n\nunit_of_measurement\n0\n1.00\n1\n2\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntotal_cup_points\n0\n1.00\n82.09\n3.50\n0\n81.08\n82.50\n83.67\n90.58\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nnumber_of_bags\n0\n1.00\n154.18\n129.99\n0\n14.00\n175.00\n275.00\n1062.00\n‚ñá‚ñá‚ñÅ‚ñÅ‚ñÅ\n\n\naroma\n0\n1.00\n7.57\n0.38\n0\n7.42\n7.58\n7.75\n8.75\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nflavor\n0\n1.00\n7.52\n0.40\n0\n7.33\n7.58\n7.75\n8.83\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\naftertaste\n0\n1.00\n7.40\n0.40\n0\n7.25\n7.42\n7.58\n8.67\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nacidity\n0\n1.00\n7.54\n0.38\n0\n7.33\n7.58\n7.75\n8.75\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nbody\n0\n1.00\n7.52\n0.37\n0\n7.33\n7.50\n7.67\n8.58\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nbalance\n0\n1.00\n7.52\n0.41\n0\n7.33\n7.50\n7.75\n8.75\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nuniformity\n0\n1.00\n9.83\n0.55\n0\n10.00\n10.00\n10.00\n10.00\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nclean_cup\n0\n1.00\n9.84\n0.76\n0\n10.00\n10.00\n10.00\n10.00\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nsweetness\n0\n1.00\n9.86\n0.62\n0\n10.00\n10.00\n10.00\n10.00\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\ncupper_points\n0\n1.00\n7.50\n0.47\n0\n7.25\n7.50\n7.75\n10.00\n‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ\n\n\nmoisture\n0\n1.00\n0.09\n0.05\n0\n0.09\n0.11\n0.12\n0.28\n‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÅ\n\n\ncategory_one_defects\n0\n1.00\n0.48\n2.55\n0\n0.00\n0.00\n0.00\n63.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nquakers\n1\n1.00\n0.17\n0.83\n0\n0.00\n0.00\n0.00\n11.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ncategory_two_defects\n0\n1.00\n3.56\n5.31\n0\n0.00\n2.00\n4.00\n55.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\naltitude_low_meters\n230\n0.83\n1750.71\n8669.44\n1\n1100.00\n1310.64\n1600.00\n190164.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\naltitude_high_meters\n230\n0.83\n1799.35\n8668.81\n1\n1100.00\n1350.00\n1650.00\n190164.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\naltitude_mean_meters\n230\n0.83\n1775.03\n8668.63\n1\n1100.00\n1310.64\n1600.00\n190164.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\nRatings\nThe key outcome variable is total_cup_points, which is a quality rating 0-100\n\np &lt;- coffee %&gt;%\n  ggplot(aes(x = total_cup_points)) +\n  geom_boxplot(y = 0, fill = coffee_pal[1], outlier.shape = NA) +\n  geom_jitter(aes(y = 1), color = coffee_pal[2],\n              alpha = 0.3, height = 0.3, width = 0) +\n  remove_axis(\"y\")\np\n\n\n\n\nA very obvious outlier at total_cup_points = 0\n\ncoffee %&gt;%\n  filter(total_cup_points == min(total_cup_points)) %&gt;%\n  glimpse()\n\nRows: 1\nColumns: 43\n$ total_cup_points      &lt;dbl&gt; 0\n$ species               &lt;chr&gt; \"Arabica\"\n$ owner                 &lt;chr&gt; \"bismarck castro\"\n$ country_of_origin     &lt;chr&gt; \"Honduras\"\n$ farm_name             &lt;chr&gt; \"los hicaques\"\n$ lot_number            &lt;chr&gt; \"103\"\n$ mill                  &lt;chr&gt; \"cigrah s.a de c.v.\"\n$ ico_number            &lt;chr&gt; \"13-111-053\"\n$ company               &lt;chr&gt; \"cigrah s.a de c.v\"\n$ altitude              &lt;chr&gt; \"1400\"\n$ region                &lt;chr&gt; \"comayagua\"\n$ producer              &lt;chr&gt; \"Reinerio Zepeda\"\n$ number_of_bags        &lt;dbl&gt; 275\n$ bag_weight            &lt;chr&gt; \"69 kg\"\n$ in_country_partner    &lt;chr&gt; \"Instituto Hondure√±o del Caf√©\"\n$ harvest_year          &lt;chr&gt; \"2017\"\n$ grading_date          &lt;chr&gt; \"April 28th, 2017\"\n$ owner_1               &lt;chr&gt; \"Bismarck Castro\"\n$ variety               &lt;chr&gt; \"Caturra\"\n$ processing_method     &lt;chr&gt; NA\n$ aroma                 &lt;dbl&gt; 0\n$ flavor                &lt;dbl&gt; 0\n$ aftertaste            &lt;dbl&gt; 0\n$ acidity               &lt;dbl&gt; 0\n$ body                  &lt;dbl&gt; 0\n$ balance               &lt;dbl&gt; 0\n$ uniformity            &lt;dbl&gt; 0\n$ clean_cup             &lt;dbl&gt; 0\n$ sweetness             &lt;dbl&gt; 0\n$ cupper_points         &lt;dbl&gt; 0\n$ moisture              &lt;dbl&gt; 0.12\n$ category_one_defects  &lt;dbl&gt; 0\n$ quakers               &lt;dbl&gt; 0\n$ color                 &lt;chr&gt; \"Green\"\n$ category_two_defects  &lt;dbl&gt; 2\n$ expiration            &lt;chr&gt; \"April 28th, 2018\"\n$ certification_body    &lt;chr&gt; \"Instituto Hondure√±o del Caf√©\"\n$ certification_address &lt;chr&gt; \"b4660a57e9f8cc613ae5b8f02bfce8634c763ab4\"\n$ certification_contact &lt;chr&gt; \"7f521ca403540f81ec99daec7da19c2788393880\"\n$ unit_of_measurement   &lt;chr&gt; \"m\"\n$ altitude_low_meters   &lt;dbl&gt; 1400\n$ altitude_high_meters  &lt;dbl&gt; 1400\n$ altitude_mean_meters  &lt;dbl&gt; 1400\n\n\nAll of the gradings (aroma, flavor, etc.) are also 0. Remove it:\n\ncoffee &lt;- coffee %&gt;% filter(total_cup_points &gt; 0)\np %+% coffee\n\n\n\n\nShow the distribution of the other numerical gradings:\n\ncoffee %&gt;%\n  select(aroma:moisture) %&gt;%\n  pivot_longer(cols = everything()) %&gt;%\n  ggplot(aes(x = value)) +\n  geom_boxplot(y = 0, fill = coffee_pal[1], outlier.shape = NA) +\n  geom_jitter(aes(y = 1), color = coffee_pal[2],\n              alpha = 0.3, height = 0.3, width = 0) +\n  remove_axis(\"y\") +\n  facet_wrap(~name, ncol = 3)\n\n\n\n\nNone of the values are missing, and they all seem to range from 0 to 10, except for moisture:\n\ncoffee %&gt;%\n  ggplot(aes(x = moisture)) +\n  geom_boxplot(y = 0, fill = coffee_pal[1], outlier.shape = NA) +\n  geom_jitter(aes(y = 1), color = coffee_pal[2],\n              alpha = 0.3, height = 0.3, width = 0) +\n  remove_axis(\"y\")\n\n\n\n\nWhat is the relationship between the individual gradings and the overall total_cup_points? There are 10 gradings with scores 0-10, so I assume adding them together gives total_cup_points, which ranges 0-100:\n\ncoffee %&gt;%\n  rowwise() %&gt;%\n  transmute(\n    total_cup_points,\n    sum_gradings = sum(c_across(aroma:cupper_points))\n  ) %&gt;%\n  ggplot(aes(x = total_cup_points, y = sum_gradings)) +\n  geom_point(color = coffee_pal[1], size = 2) +\n  geom_abline(size = 1)\n\n\n\n\nSome very slight deviations, but yes my assumption is correct. A second assumption: there will be mostly positive associations among the individual gradings (e.g.¬†a coffee with high flavor will have high body on average). Compute and plot the pairwise correlations with corrr:\n\ncoffee %&gt;%\n  select(aroma:cupper_points) %&gt;%\n  corrr::correlate(method = \"pearson\", use = \"everything\") %&gt;%\n  corrr::rplot() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.7))\n\n\n\n\nYes, lots of high correlations. Interestingly, there are three variables in particular (uniformity, clean_cup and sweetness) which correlate moderately with eachother, and weakly with the others. These happen to be the gradings that are almost always 10:\n\ncoffee %&gt;%\n  select(uniformity, clean_cup, sweetness) %&gt;%\n  pivot_longer(everything()) %&gt;%\n  group_by(name) %&gt;%\n  mutate(\n    name = glue::glue(\n      \"{name} ({scales::percent(mean(value == 10))} values = 10.0)\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = value, y = 1)) +\n  geom_jitter(alpha = 0.3, width = 0, color = coffee_pal[3]) +\n  facet_wrap(~name, ncol = 1) +\n  remove_axis(\"y\") +\n  scale_x_continuous(breaks = 0:10)\n\n\n\n\n\n\nCategorical variables\nThere are two species, though Arabica makes up the large majority:\n\ncoffee %&gt;% count(species)\n\n# A tibble: 2 √ó 2\n  species     n\n  &lt;chr&gt;   &lt;int&gt;\n1 Arabica  1310\n2 Robusta    28\n\n\nInteresting that Arabica makes up 98% of the data, but ~60% of the coffee produced worldwide, so seems to be over-represented here.\nThere are 36 countries of origin (1 value missing):\n\ncoffee %&gt;%\n  count(country_of_origin, sort = TRUE) %&gt;%\n  mutate(\n    country_of_origin = country_of_origin %&gt;%\n      fct_explicit_na() %&gt;%\n      fct_reorder(n)\n  ) %&gt;%\n  ggplot(aes(y = country_of_origin, x = n)) +\n  geom_col(fill = coffee_pal[1]) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  labs(y = NULL)\n\n\n\n\nFor the most frequent countries, show the distribution of overall ratings:\n\ncoffee %&gt;%\n  mutate(\n    country_of_origin  = country_of_origin %&gt;%\n      fct_explicit_na() %&gt;%\n      fct_lump(n = 10) %&gt;%\n      fct_reorder(total_cup_points)\n  ) %&gt;%\n  ggplot(aes(y = country_of_origin, x = total_cup_points)) +\n  geom_boxplot(fill = coffee_pal[3]) +\n  labs(y = NULL)\n\n\n\n\nEthiopian coffee has a very clear lead in terms of ratings, while Colombia has very consistently high ratings.\n\ncoffee %&gt;%\n  mutate(\n    variety = variety %&gt;%\n      fct_explicit_na() %&gt;%\n      fct_lump_min(10, other_level = \"Other (n&lt;10)\")\n  ) %&gt;%\n  count(variety) %&gt;%\n  mutate(variety = fct_reorder(variety, n)) %&gt;%\n  ggplot(aes(y = variety, x = n)) +\n  geom_col(fill = coffee_pal[2]) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  labs(y = NULL)\n\n\n\n\nThere are two unknown values for variety: ‚ÄúOther‚Äù and NA (Missing). These could have different meanings (e.g.¬†an NA value could be a common variety but is just missing) but I will combine both:\n\ncoffee &lt;- coffee %&gt;%\n  mutate(variety = ifelse(variety == \"Other\", NA_character_, variety))\n\nin_country_partner has a manageable number of unique values (27)\n\nd &lt;- coffee %&gt;%\n  mutate(\n    in_country_partner = fct_lump(in_country_partner, 10)\n  ) %&gt;%\n  add_count(in_country_partner) %&gt;%\n  mutate(in_country_partner = fct_reorder(in_country_partner, n))\np1 &lt;- d %&gt;%\n  distinct(in_country_partner, n) %&gt;%\n  ggplot(aes(y = in_country_partner, x = n)) +\n  geom_col(fill = coffee_pal[1]) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  labs(y = NULL)\np2 &lt;- d %&gt;%\n  ggplot(aes(y = in_country_partner, x = total_cup_points)) +\n  geom_boxplot(fill = coffee_pal[2]) +\n  labs(y = NULL) +\n  theme(axis.text.y = element_blank())\np1 | p2\n\n\n\n\nThe coffee bean color is presumably before roasting:\n\nd &lt;- coffee %&gt;%\n  add_count(color) %&gt;%\n  mutate(color = fct_reorder(color, n))\np1 &lt;- d %&gt;%\n  distinct(color, n) %&gt;%\n  ggplot(aes(y = color, x = n)) +\n  geom_col(fill = coffee_pal[1]) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  labs(y = NULL)\np2 &lt;- d %&gt;%\n  ggplot(aes(y = color, x = total_cup_points)) +\n  geom_boxplot(fill = coffee_pal[2]) +\n  labs(y = NULL) +\n  theme(axis.text.y = element_blank())\np1 | p2\n\n\n\n\nharvest_year could use some data processing:\n\nd %&gt;%\n  count(harvest_year, sort = T) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nFor values like ‚Äú2013/2014‚Äù and ‚Äú2010-2011, I‚Äôll extract the first year.\n\ncoffee &lt;- coffee %&gt;%\n  mutate(\n    harvest_year_num = harvest_year %&gt;%\n      str_extract(\"\\\\d{4}\") %&gt;%\n      as.numeric()\n  )\ncoffee %&gt;%\n  count(harvest_year, harvest_year_num, sort = T) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nIt doesn‚Äôt work with values like ‚Äú08/09‚Äù because they are not 4 digits, but those values are very low frequency.\nThe 5 processing_methods:\n\nd &lt;- coffee %&gt;%\n  add_count(processing_method) %&gt;%\n  mutate(processing_method = fct_reorder(processing_method, n))\np1 &lt;- d %&gt;%\n  distinct(processing_method, n) %&gt;%\n  ggplot(aes(y = processing_method, x = n)) +\n  geom_col(fill = coffee_pal[1]) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  labs(y = NULL)\np2 &lt;- d %&gt;%\n  ggplot(aes(y = processing_method, x = total_cup_points)) +\n  geom_boxplot(fill = coffee_pal[2]) +\n  labs(y = NULL) +\n  theme(axis.text.y = element_blank())\np1 | p2\n\n\n\n\nThere are two date variables that I think would be interesting to compare: grading_date and expiration. Parse them as date objects and compute the difference in days:\n\ncoffee &lt;- coffee %&gt;%\n  mutate(\n    # Convert both to date objects\n    expiration = lubridate::mdy(expiration),\n    grading_date = lubridate::mdy(grading_date)\n  )\ncoffee %&gt;%\n  mutate(\n    days_from_expiration = expiration - grading_date\n  ) %&gt;%\n  count(days_from_expiration)\n\n# A tibble: 1 √ó 2\n  days_from_expiration     n\n  &lt;drtn&gt;               &lt;int&gt;\n1 365 days              1338\n\n\nEvery single grading was (supposedly) done 365 days before expiration. Maybe it is standard procedure that gradings be done exactly one year before expiration. Not sure, but this unfortunately makes it an uninteresting variable for exploration/modeling.\n\n\nDefects\nThere are two defect variables (category_one_defects and category_two_defects) and the quakers variable, which are immature/unripe beans.\n\nd &lt;- coffee %&gt;%\n  mutate(\n    across(\n      c(category_one_defects, category_two_defects, quakers),\n      # Group counts above 5 together\n      ~cut(., breaks = c(0:5, 100), include.lowest = TRUE, right = FALSE,\n           labels = c(0:4, \"5+\"))\n    )\n  ) %&gt;%\n  select(where(is.factor), total_cup_points) %&gt;%\n  pivot_longer(cols = -total_cup_points,\n               names_to = \"defect\", values_to = \"n_defects\") %&gt;%\n  filter(!is.na(n_defects))\n\np1 &lt;- d %&gt;%\n  filter(defect == \"category_one_defects\") %&gt;%\n  ggplot(aes(y = n_defects)) +\n  geom_bar(fill = coffee_pal[1]) +\n  scale_x_continuous(NULL, expand = expansion(c(0, 0.05)))\np2 &lt;- d %&gt;% \n  filter(defect == \"category_one_defects\") %&gt;%\n  ggplot(aes(y = n_defects, x = total_cup_points)) +\n  geom_boxplot(fill = coffee_pal[3]) +\n  labs(x = NULL, y = NULL)\n\n\n(\n  (p1 + labs(y = \"Category 1\") | p2)\n) /\n  (\n    (p1 %+% filter(d, defect == \"category_two_defects\") +\n       labs(y = \"Category 2\") |\n       p2 %+% filter(d, defect == \"category_two_defects\"))\n  ) /\n  (\n    (p1 %+% filter(d, defect == \"quakers\") +\n       labs(y = \"Quakers\", x = \"Count\") |\n       (p2 %+% filter(d, defect == \"quakers\") +\n          labs(x = \"Total cup points\")))\n  )\n\n\n\n\nLooks to be a slight decline in ratings with increasing category 1 and 2 defects. Not much of an effect with number of quakers.\n\n\nAltitude\nThe altitude variable is messy, but looks like it was cleaned via the altitude_*_variables:\n\ncoffee %&gt;%\n  count(\n    altitude, altitude_mean_meters, altitude_low_meters, altitude_high_meters,\n    unit_of_measurement, sort = T\n  ) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nThere are some unit conversions from feet to meters, for example:\n\ncoffee %&gt;%\n  filter(unit_of_measurement == \"ft\", !is.na(altitude)) %&gt;%\n  count(altitude, unit_of_measurement, altitude_mean_meters, sort = T) %&gt;%\n  # Re-calculate the altitude in feet to see if it matches\n  mutate(feet_manual = altitude_mean_meters * 3.28) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nAnd re-calculating the altitude in feet, everything looks to be correctly converted.\nLook for any obvious outliers:\n\ncoffee %&gt;%\n  filter(!is.na(altitude_mean_meters)) %&gt;%\n  ggplot(aes(x = altitude_mean_meters)) +\n  geom_boxplot(y = 0, fill = coffee_pal[2]) +\n  scale_x_log10() +\n  remove_axis(\"y\")\n\n\n\n\nYes, some pretty clear ones. Look at values larger than 3000:\n\ncoffee %&gt;%\n  select(country_of_origin, altitude, unit_of_measurement,\n         altitude_mean_meters) %&gt;%\n  arrange(desc(altitude_mean_meters)) %&gt;%\n  filter(altitude_mean_meters &gt; 2000) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\ncountry_of_origin\naltitude\nunit_of_measurement\naltitude_mean_meters\n\n\n\n\nGuatemala\n190164\nm\n190164\n\n\nGuatemala\n1901.64\nm\n190164\n\n\nNicaragua\n1100.00 mosl\nm\n110000\n\n\nBrazil\n11000 metros\nm\n11000\n\n\nMyanmar\n4287\nm\n4287\n\n\nMyanmar\n4001\nm\n4001\n\n\nColombia\n1800 meters (5900\nm\n3850\n\n\nMyanmar\n3845\nm\n3845\n\n\nMyanmar\n3825\nm\n3825\n\n\nMyanmar\n3800\nm\n3800\n\n\nIndonesia\n3500\nm\n3500\n\n\nGuatemala\n3280\nm\n3280\n\n\nGuatemala\n3280\nm\n3280\n\n\nGuatemala\n3280\nm\n3280\n\n\nIndia\n3170\nm\n3170\n\n\nIndia\n3140\nm\n3140\n\n\nIndia\n3000'\nm\n3000\n\n\nUnited States\n3000'\nm\n3000\n\n\nColombia\n2.560 msnm\nm\n2560\n\n\nColombia\n2560\nm\n2560\n\n\nColombia\n2527\nm\n2527\n\n\nMalawi\n2500m\nm\n2500\n\n\nTanzania, United Republic Of\n2285\nm\n2285\n\n\nColombia\n2136 msnm\nm\n2136\n\n\nColombia\n2136 msnm\nm\n2136\n\n\nUnited States\nmeters above sea level: 2.112\nm\n2112\n\n\nGuatemala\n2100\nm\n2100\n\n\nUnited States\nmeters above sea level: 2.080\nm\n2080\n\n\nEthiopia\n1950-2200\nm\n2075\n\n\nEthiopia\n1950-2200\nm\n2075\n\n\nEthiopia\n1950-2200\nm\n2075\n\n\nUnited States\nmeters above sea level: 2.019\nm\n2019\n\n\n\n\n\n\n\nSome helpful frames of reference:\n\nThe elevation of Everest is 8849m.\nThe highest point in Guatemala is Tajumulco volcano at 4220m.\nOne of the Myanmar coffee producers claims here that their beans are grown at 4000-7000 ft -&gt; 1200-2200m\nCoffee elevations by country:\n\nBrazil: 1300-5300 ft -&gt; 400-1600 meters\nGuatemala: 3900-6200 ft -&gt; 1200-1900 meters\nColombia: 2600-6200 ft -&gt; 800-1900 meters\n\n\nBesides the obvious errors (e.g.¬†190164 meters), my guess is that many of these measurements are still in feet and need to be converted to meters.\nA couple decimal values were incorrectly processed:\n\ncoffee %&gt;%\n  filter(str_detect(altitude, regex(\"sea\", ignore_case = T))) %&gt;%\n  select(matches(\"altitu\")) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\naltitude\naltitude_low_meters\naltitude_high_meters\naltitude_mean_meters\n\n\n\n\nmeters above sea level: 1.872\n1872\n1872\n1872\n\n\nmeters above sea level: 1.943\n1943\n1943\n1943\n\n\nmeters above sea level: 2.080\n2080\n2080\n2080\n\n\nmeters above sea level: 2.019\n2019\n2019\n2019\n\n\nmeters above sea level: 2.112\n2112\n2112\n2112\n\n\nmeters above sea level: 1.941\n1941\n1941\n1941\n\n\n800 meters above sea level\n800\n800\n800\n\n\n1600 + meters above sea level\n1600\n1600\n1600\n\n\n1400 meter above sea level\n1400\n1400\n1400\n\n\n\n\n\n\n\nLook at some of the lowest values:\n\ncoffee %&gt;%\n  select(country_of_origin, altitude, unit_of_measurement,\n         altitude_mean_meters, producer, country_of_origin) %&gt;%\n  arrange(desc(altitude_mean_meters)) %&gt;%\n  filter(altitude_mean_meters &lt;= 10) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\ncountry_of_origin\naltitude\nunit_of_measurement\naltitude_mean_meters\nproducer\n\n\n\n\nKenya\n-1\nm\n1\nNA\n\n\nBrazil\n1\nm\n1\nIpanema Agr√≠cola SA\n\n\nBrazil\n1\nm\n1\nIpanema Agr√≠cola SA\n\n\nBrazil\n1\nm\n1\nIpanema Agr√≠cola SA\n\n\nBrazil\n1\nm\n1\nIpanema Agr√≠cola SA\n\n\nBrazil\n1\nm\n1\nIpanema Agr√≠cola SA\n\n\nBrazil\n1\nm\n1\nIpanema Agr√≠cola SA\n\n\nBrazil\n1\nm\n1\nIpanema Agr√≠cola SA\n\n\nBrazil\n1\nm\n1\nIpanema Agr√≠cola SA\n\n\nBrazil\n1\nm\n1\nIpanema Agr√≠cola SA\n\n\nBrazil\n1\nm\n1\nIpanema Agr√≠cola SA\n\n\nBrazil\n1\nm\n1\nIpanema Agr√≠cola SA\n\n\n\n\n\n\n\nThese are almost certainly incorrect, but there are so few values, I won‚Äôt worry about them.\nMake a value of ‚Äúfixed‚Äù mean altitude:\n\ncoffee &lt;- coffee %&gt;%\n  mutate(\n    altitude_mean_meters_fixed = case_when(\n      altitude == \"1800 meters (5900\" ~ 1800,\n      altitude_mean_meters == 190164 ~ 1901,\n      altitude_mean_meters == 110000 ~ 1100,\n      str_detect(altitude, \"^meters above\") ~ altitude_mean_meters / 1000.0,\n      # Assume anything above 3000 needs to be converted from feet\n      altitude_mean_meters &gt; 3000 ~ 0.3048 * altitude_mean_meters,\n      TRUE ~ altitude_mean_meters\n    )\n  )\ncoffee %&gt;%\n  filter(!is.na(altitude_mean_meters_fixed)) %&gt;%\n  ggplot(aes(x = altitude_mean_meters_fixed)) +\n  geom_boxplot(y = 0, fill = coffee_pal[2]) +\n  remove_axis(\"y\")\n\n\n\n\nThat is looking a bit better. Plot the relationship between the uni-dimensional ratings and mean altitude:\n\ncoffee %&gt;%\n  select(aroma:cupper_points, altitude_mean_meters_fixed) %&gt;%\n  filter(!is.na(altitude_mean_meters_fixed)) %&gt;%\n  pivot_longer(cols = -altitude_mean_meters_fixed,\n               names_to = \"grading\", values_to = \"score\") %&gt;%\n  ggplot(aes(x = altitude_mean_meters_fixed, y = score)) +\n  geom_point(alpha = 0.3, color = coffee_pal[2]) +\n  geom_smooth(method = \"loess\", formula = \"y ~ x\", color = coffee_pal[3]) +\n  facet_wrap(~grading, ncol = 2, scales = \"free_y\") +\n  scale_y_continuous(breaks = seq(0, 10, 2))\n\n\n\n\nThe ratings are so clustered around the 6-9 range, it is hard to see much of a relationship but there does seem to be a small bump in ratings around 1500-2000 meters for a few of the variables. Can we see it in total scores?\n\ncoffee %&gt;%\n  filter(!is.na(altitude_mean_meters_fixed)) %&gt;%\n  ggplot(aes(x = altitude_mean_meters_fixed, y = total_cup_points)) +\n  geom_point(alpha = 0.3, color = coffee_pal[2]) +\n  geom_smooth(method = \"loess\", formula = \"y ~ x\", color = coffee_pal[3]) +\n  scale_y_continuous(breaks = seq(0, 100, 10))\n\n\n\n\nYes, there looks to be the range of altitudes with the highest ratings on average."
  },
  {
    "objectID": "posts/2020-07-12-tidytuesday-2020-week-28/index.html#model",
    "href": "posts/2020-07-12-tidytuesday-2020-week-28/index.html#model",
    "title": "TidyTuesday 2020 Week 28",
    "section": "Model",
    "text": "Model\nI want to attempt to predict scores for one of the individual gradings (not the 0-100 total score, or the three that are mostly 10s), which I will choose randomly:\n\nset.seed(74)\nsample(\n  c(\"aroma\", \"flavor\", \"aftertaste\", \"acidity\", \"body\",\n    \"balance\", \"cupper_points\"),\n  size = 1\n)\n\n[1] \"flavor\"\n\n\nflavor it is. I‚Äôll attempt to predict it with the following:\n\nCategorical: species, country_of_origin, processing_method, color, in_country_partner, variety\nNumerical: aroma, flavor, aftertaste, acidity, body, balance, uniformity, clean_cup, sweetness, cupper_points, moisture, category_one_defects, category_two_defects, quakers, altitude_mean_meters_fixed\n\nLoad tidymodels, split the data 3:1 into training and testing (stratified by the outcome flavor), and define the resampling strategy:\n\n# Some minor pre-processing\ncoffee &lt;- coffee %&gt;%\n  mutate(\n    variety = fct_explicit_na(variety),\n    across(where(is.character), factor)\n  )\n\nlibrary(tidymodels)\nset.seed(42)\ncoffee_split &lt;- initial_split(coffee, prop = 3/4, strata = flavor)\ncoffee_train &lt;- training(coffee_split)\ncoffee_test &lt;- testing(coffee_split)\n\ncoffee_resamples &lt;- vfold_cv(coffee_train, v = 5, strata = flavor)\n\nNow define the recipe:\n\nImpute the mode for missing categorical values\nLump together categorical values with &lt;5% frequency\nImpute the mean for missing numerical values\nStandardize all numerical predictors (important for lasso regularization)\nGiven its non-linear relationship, use splines in the altitude_mean_meters_fixed predictor\n\n\ncoffee_rec &lt;-\n  recipe(\n    flavor ~\n      species + country_of_origin + processing_method + color +\n      in_country_partner + variety + aroma + aftertaste + acidity + body +\n      balance + uniformity + clean_cup + sweetness + cupper_points + moisture +\n      category_one_defects + category_two_defects + quakers +\n      altitude_mean_meters_fixed,\n    data = coffee_train\n  ) %&gt;%\n  # Where missing, impute categorical variables with the most common value\n  step_impute_mode(all_nominal_predictors()) %&gt;%\n  # Some of these categorical variables have too many levels, group levels\n  #  with &lt;5% frequency \n  step_other(country_of_origin, variety, in_country_partner, processing_method,\n             threshold = 0.05) %&gt;%\n  # These two numerical predictors have some missing value, so impute with mean\n  step_impute_mean(quakers, altitude_mean_meters_fixed) %&gt;%\n  # Normalize (0 mean, 1 SD) all numerical predictors\n  step_normalize(all_numeric_predictors()) %&gt;%\n  # Use splines in the altitude variable to capture it's non-linearity.\n  step_ns(altitude_mean_meters_fixed, deg_free = 4) %&gt;%\n  # Finally, create the dummy variables (note that this step must come *after*\n  #  normalizing numerical variables)\n  step_dummy(all_nominal_predictors())\ncoffee_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nMode imputation for all_nominal_predictors()\nCollapsing factor levels for country_of_origin, variety, in_country_partner,...\nMean imputation for quakers, altitude_mean_meters_fixed\nCentering and scaling for all_numeric_predictors()\nNatural splines on altitude_mean_meters_fixed\nDummy variables from all_nominal_predictors()\n\n\nBefore applying the recipe, we can explore the processing steps with the recipes::prep() and recipes::bake() functions applied to the training data:\n\ncoffee_baked &lt;- bake(prep(coffee_rec), new_data = NULL)\ncoffee_baked %&gt;% paged_table()\n\n\n\n  \n\n\n\nAnd before fitting any models, register parallel computing to speed up the tuning process:\n\n# Speed up the tuning with parallel processing\nn_cores &lt;- parallel::detectCores(logical = FALSE)\nlibrary(doParallel)\ncl &lt;- makePSOCKcluster(n_cores - 1)\nregisterDoParallel(cl)\n\n\nLinear regression\nStart simple, with just a linear regression model:\n\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\nlm_workflow &lt;- workflow() %&gt;%\n  add_recipe(coffee_rec) %&gt;%\n  add_model(lm_spec)\nlm_fit_train &lt;- lm_workflow %&gt;%\n  fit(data = coffee_train)\nlm_fit &lt;- last_fit(lm_fit_train, coffee_split)\n\ncollect_metrics(lm_fit)\n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.144 Preprocessor1_Model1\n2 rsq     standard       0.819 Preprocessor1_Model1\n\n\nShow the actual vs predicted flavor in the testing data:\n\ncollect_predictions(lm_fit) %&gt;%\n  ggplot(aes(x = flavor, y = .pred)) +\n  geom_point(color = coffee_pal[1], alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, size = 1.5)\n\n\n\n\nLinear regression does great, which makes me think that the more complicated models to follow will be overkill.\n\n\nLasso regression\nWe will tune (i.e.¬†allow the penalty term \\(\\lambda\\) to vary) a lasso regression model to see if it can outperform a basic linear regression:\n\nlasso_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n# Define the lambda values to try when tuning\nlasso_lambda_grid &lt;- grid_regular(penalty(), levels = 50)\n\nlasso_workflow &lt;- workflow() %&gt;%\n  add_recipe(coffee_rec) %&gt;%\n  add_model(lasso_spec)\n\nlibrary(tictoc) # A convenient package for timing\ntic()\nlasso_tune &lt;-\n  tune_grid(\n    lasso_workflow,\n    resamples = coffee_resamples,\n    grid = lasso_lambda_grid\n  )\ntoc()\n\n1.69 sec elapsed\n\n\n\nshow_best(lasso_tune, metric = \"rmse\")\n\n# A tibble: 5 √ó 7\n   penalty .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.00356  rmse    standard   0.149     5 0.00417 Preprocessor1_Model38\n2 0.00222  rmse    standard   0.149     5 0.00417 Preprocessor1_Model37\n3 0.00569  rmse    standard   0.149     5 0.00427 Preprocessor1_Model39\n4 0.00139  rmse    standard   0.149     5 0.00416 Preprocessor1_Model36\n5 0.000869 rmse    standard   0.149     5 0.00414 Preprocessor1_Model35\n\n\nLasso regression was un-needed in this case, as it did not perform differently than basic linear regression. We can see this by looking at the behaviour of the metrics with \\(\\lambda\\):\n\ncollect_metrics(lasso_tune) %&gt;%\n  ggplot(aes(x = penalty, y = mean)) +\n  geom_line(size = 1, color = coffee_pal[1]) +\n  geom_point(color = coffee_pal[1]) +\n  geom_ribbon(aes(ymin = mean - std_err, ymax = mean + std_err),\n              alpha = 0.5, fill = coffee_pal[3]) +\n  facet_wrap(~.metric, ncol = 1, scales = \"free_y\") +\n  scale_x_log10()\n\n\n\n\nLower levels of regularization gave better metrics. Regardless, finalize the workflow.\n\nlasso_best_workflow &lt;- lasso_workflow %&gt;%\n  finalize_workflow(select_best(lasso_tune, metric = \"rmse\"))\nlasso_fit &lt;- last_fit(lasso_best_workflow, coffee_split)\n\ncollect_metrics(lasso_fit)\n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.143 Preprocessor1_Model1\n2 rsq     standard       0.822 Preprocessor1_Model1\n\n\nAlso look at variable importance:\n\nlibrary(vip)\nlasso_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  vi(\n    # This step seems to be necessary for glmnet objects\n    lambda =  select_best(lasso_tune, metric = \"rmse\")$penalty\n  ) %&gt;%\n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = NULL) +\n  theme(legend.position = c(0.3, 0.3))\n\n\n\n\n\n\nRandom forest\nLastly, try a random forest model:\n\nranger_spec &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  # The importance argument allows us to compute variable importance afterwards\n  set_engine(\"ranger\", importance = \"permutation\")\n\nranger_workflow &lt;- workflow() %&gt;%\n  add_recipe(coffee_rec) %&gt;%\n  add_model(ranger_spec)\n\nset.seed(12)\n\ntic()\nranger_tune &lt;-\n  tune_grid(ranger_workflow, resamples = coffee_resamples, grid = 11)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntoc()\n\n22.39 sec elapsed\n\n\nTuning results:\n\nshow_best(ranger_tune, metric = \"rmse\")\n\n# A tibble: 5 √ó 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    20    17 rmse    standard   0.148     5 0.00543 Preprocessor1_Model08\n2    17    22 rmse    standard   0.149     5 0.00555 Preprocessor1_Model02\n3    26    13 rmse    standard   0.149     5 0.00522 Preprocessor1_Model10\n4     6     7 rmse    standard   0.149     5 0.00591 Preprocessor1_Model04\n5    30    11 rmse    standard   0.150     5 0.00509 Preprocessor1_Model01\n\n\nAnd plotting those same results:\n\nautoplot(ranger_tune) +\n  add_facet_borders()\n\n\n\n\nWe see that (besdies that first point) the tuning process did not amount to significantly improved metrics. Choose the best and fit to the test data:\n\nranger_best &lt;- ranger_workflow %&gt;%\n  finalize_workflow(select_best(ranger_tune, metric = \"rmse\"))\nranger_fit &lt;- last_fit(ranger_best, coffee_split)\ncollect_metrics(ranger_fit)\n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.137 Preprocessor1_Model1\n2 rsq     standard       0.839 Preprocessor1_Model1\n\n\nCheck out variable importance:\n\nranger_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  vi() %&gt;%\n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;%\n  ggplot(aes(x = Importance, y = Variable)) +\n  geom_col() +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = NULL) +\n  theme(legend.position = c(0.3, 0.3))\n\n\n\n\nEven fewer variables were deemed important in this model compared to lasso."
  },
  {
    "objectID": "posts/2020-07-12-tidytuesday-2020-week-28/index.html#conclusion",
    "href": "posts/2020-07-12-tidytuesday-2020-week-28/index.html#conclusion",
    "title": "TidyTuesday 2020 Week 28",
    "section": "Conclusion",
    "text": "Conclusion\nThe random forest model was very slightly better in predicting coffee flavor, which I‚Äôll summarize with a plot:\n\nd &lt;- bind_rows(\n  mutate(collect_metrics(lm_fit), fit = \"linear\"),\n  mutate(collect_metrics(lasso_fit), fit = \"lasso\"),\n  mutate(collect_metrics(ranger_fit), fit = \"random forest\")\n) %&gt;%\n  group_by(fit) %&gt;%\n  summarise(\n    fit_metrics = glue::glue(\n      \"RMSE = {round(.estimate[.metric == 'rmse'], 3)}\\n\",\n      \"R2 = {round(.estimate[.metric == 'rsq'], 3)}\"\n    ),\n    .groups = \"drop\"\n  ) %&gt;%\n  left_join(\n    bind_rows(\n      mutate(collect_predictions(lm_fit), fit = \"linear\"),\n      mutate(collect_predictions(lasso_fit), fit = \"lasso\"),\n      mutate(collect_predictions(ranger_fit), fit = \"random forest\")\n    ),\n    by = \"fit\"\n  ) %&gt;%\n  mutate(fit = factor(fit, levels = c(\"linear\", \"lasso\", \"random forest\")))\n\n\nd %&gt;%\n  ggplot(aes(x = flavor, y = .pred)) +\n  geom_point(color = coffee_pal[1], alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, size = 1.5, lty = 2) +\n  geom_text(data = . %&gt;% distinct(fit, fit_metrics),\n            aes(label = fit_metrics, x = 6.5, y = 8.3), hjust = 0) +\n  facet_wrap(~fit, nrow = 1) +\n  add_facet_borders() +\n  labs(y = \"Predicted flavor score\", x = \"Flavor score\",\n       title = \"Model performance in predicting coffee flavor ratings\",\n       caption = paste0(\"data from the Coffee Quality Database | \",\n                        \"plot by @TDunn12 for #TidyTuesday\"))"
  },
  {
    "objectID": "posts/2020-07-12-tidytuesday-2020-week-28/index.html#reproducibility",
    "href": "posts/2020-07-12-tidytuesday-2020-week-28/index.html#reproducibility",
    "title": "TidyTuesday 2020 Week 28",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2020-11-08-tidytuesday-2020-week-45/index.html",
    "href": "posts/2020-11-08-tidytuesday-2020-week-45/index.html",
    "title": "TidyTuesday 2020 Week 45",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(gt)\nlibrary(rmarkdown)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td_grey())\nset_geom_fonts()\nset_palette()\nikea_colors &lt;- c(\"#0051BA\", \"#FFDA1A\")"
  },
  {
    "objectID": "posts/2020-11-08-tidytuesday-2020-week-45/index.html#load-the-data",
    "href": "posts/2020-11-08-tidytuesday-2020-week-45/index.html#load-the-data",
    "title": "TidyTuesday 2020 Week 45",
    "section": "Load the data",
    "text": "Load the data\n\ntt &lt;- tt_load(\"2020-11-03\")\n\nThe data this week comes from the Kaggle, and was the subject of a FiveThirtyEight article. See the Kaggle page for a data dictionary."
  },
  {
    "objectID": "posts/2020-11-08-tidytuesday-2020-week-45/index.html#data-exploration",
    "href": "posts/2020-11-08-tidytuesday-2020-week-45/index.html#data-exploration",
    "title": "TidyTuesday 2020 Week 45",
    "section": "Data exploration",
    "text": "Data exploration\n\nikea &lt;- tt$ikea %&gt;% rename(row_number = `...1`)\nglimpse(ikea)\n\nRows: 3,694\nColumns: 14\n$ row_number        &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15‚Ä¶\n$ item_id           &lt;dbl&gt; 90420332, 368814, 9333523, 80155205, 30180504, 10122‚Ä¶\n$ name              &lt;chr&gt; \"FREKVENS\", \"NORDVIKEN\", \"NORDVIKEN / NORDVIKEN\", \"S‚Ä¶\n$ category          &lt;chr&gt; \"Bar furniture\", \"Bar furniture\", \"Bar furniture\", \"‚Ä¶\n$ price             &lt;dbl&gt; 265, 995, 2095, 69, 225, 345, 129, 195, 129, 2176, 1‚Ä¶\n$ old_price         &lt;chr&gt; \"No old price\", \"No old price\", \"No old price\", \"No ‚Ä¶\n$ sellable_online   &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TR‚Ä¶\n$ link              &lt;chr&gt; \"https://www.ikea.com/sa/en/p/frekvens-bar-table-in-‚Ä¶\n$ other_colors      &lt;chr&gt; \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No‚Ä¶\n$ short_description &lt;chr&gt; \"        Bar table, in/outdoor,          51x51 cm\", ‚Ä¶\n$ designer          &lt;chr&gt; \"Nicholai Wiig Hansen\", \"Francis Cayouette\", \"Franci‚Ä¶\n$ depth             &lt;dbl&gt; NA, NA, NA, 50, 60, 45, 44, 50, 44, NA, 44, 45, 47, ‚Ä¶\n$ height            &lt;dbl&gt; 99, 105, NA, 100, 43, 91, 95, NA, 95, NA, 103, 102, ‚Ä¶\n$ width             &lt;dbl&gt; 51, 80, NA, 60, 74, 40, 50, 50, 50, NA, 52, 40, 46, ‚Ä¶\n\n\nAre there duplicate names?\n\nikea %&gt;%\n  count(name, sort = T)\n\n# A tibble: 607 √ó 2\n   name        n\n   &lt;chr&gt;   &lt;int&gt;\n 1 BEST√Ö     173\n 2 PAX       111\n 3 GR√ñNLID    83\n 4 BEKANT     74\n 5 TROFAST    74\n 6 IVAR       69\n 7 VIMLE      63\n 8 EKET       61\n 9 PLATSA     57\n10 LIDHULT    52\n# ‚Ä¶ with 597 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nYes, quite a bit. 400 out of 3694 items have multiple entries. Take a look at one example with two entries, the ‚ÄúTOBIAS‚Äù:\n\nikea %&gt;%\n  filter(name == \"TOBIAS\") %&gt;%\n  mutate(across(everything(), as.character)) %&gt;%\n  pivot_longer(cols = -row_number, names_to = \"variable\") %&gt;%\n  pivot_wider(names_from = row_number, values_from = value) %&gt;%\n  gt() %&gt;%\n  tab_spanner(label = \"row number\", columns = -variable)\n\n\n\n\n\n\n\n\nvariable\nrow number\n\n\n1167\n1501\n\n\n\n\nitem_id\n80349671\n60334722\n\n\nname\nTOBIAS\nTOBIAS\n\n\ncategory\nChairs\nChairs\n\n\nprice\n345\n345\n\n\nold_price\nNo old price\nNo old price\n\n\nsellable_online\nTRUE\nTRUE\n\n\nlink\nhttps://www.ikea.com/sa/en/p/tobias-chair-transparent-chrome-plated-80349671/\nhttps://www.ikea.com/sa/en/p/tobias-chair-blue-chrome-plated-60334722/\n\n\nother_colors\nNo\nNo\n\n\nshort_description\nChair\nChair\n\n\ndesigner\nCarl √ñjerstam\nCarl √ñjerstam\n\n\ndepth\n56\n56\n\n\nheight\n82\n82\n\n\nwidth\n55\n55\n\n\n\n\n\n\n\nIn the data, these are basically identical. Looking at the actual web page, I see that these are two different colors of the same item. Seems to me that it should have been a single entry with other_colors = ‚ÄúYes‚Äù.\nLikewise, the item_id variable is not unique ‚Äì there are 2962 distinct values for 3694 items. It turns out this has to do with a single entry consisting of multiple items, for example this table and chairs:\n\nikea %&gt;%\n  filter(item_id == 49011766) %&gt;%\n  mutate(across(everything(), as.character)) %&gt;%\n  pivot_longer(cols = -row_number, names_to = \"variable\") %&gt;%\n  pivot_wider(names_from = row_number, values_from = value) %&gt;%\n  gt() %&gt;%\n  tab_spanner(label = \"row number\", columns = -variable)\n\n\n\n\n\n\n\n\nvariable\nrow number\n\n\n1121\n2631\n\n\n\n\nitem_id\n49011766\n49011766\n\n\nname\nMELLTORP / ADDE\nMELLTORP / ADDE\n\n\ncategory\nChairs\nTables & desks\n\n\nprice\n179\n179\n\n\nold_price\nSR 205\nSR 205\n\n\nsellable_online\nTRUE\nTRUE\n\n\nlink\nhttps://www.ikea.com/sa/en/p/melltorp-adde-table-and-2-chairs-white-s49011766/\nhttps://www.ikea.com/sa/en/p/melltorp-adde-table-and-2-chairs-white-s49011766/\n\n\nother_colors\nNo\nNo\n\n\nshort_description\nTable and 2 chairs, 75 cm\nTable and 2 chairs, 75 cm\n\n\ndesigner\nLisa Norinder/Marcus Arvonen\nLisa Norinder/Marcus Arvonen\n\n\ndepth\nNA\nNA\n\n\nheight\n72\n72\n\n\nwidth\n75\n75\n\n\n\n\n\n\n\nBecause this entry consists of chairs and a table, it falls under the ‚ÄúChairs‚Äù and ‚ÄúTables & desks‚Äù categories, despite being identical.\nThe category variable has the following distribution:\n\nikea %&gt;%\n  count(category, sort = T) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nThe price variable is the cost in Saudi Riyals at the time of data extraction:\n\nikea %&gt;%\n  ggplot(aes(x = price)) +\n  geom_boxplot(aes(y = 0), outlier.shape = NA) +\n  geom_jitter(aes(y = 1), alpha = 0.2) +\n  scale_x_log10(breaks = c(1, 10, 100, 1000, 10000)) +\n  dunnr::remove_axis(\"y\")\n\n\n\n\nOut of curiosity, look at the top and bottom 3 items by price (and convert it to Canadian dollars for my own reference):\n\nslice_max(ikea, price, n = 3) %&gt;% mutate(price_group = \"Most expensive\") %&gt;%\n  bind_rows(\n    slice_min(ikea, price, n = 3) %&gt;% mutate(price_group = \"Least expensive\")\n  ) %&gt;%\n  arrange(price) %&gt;%\n  transmute(\n    price_group, name, category, short_description, price,\n    # Exchange rate as of writing this\n    price_cad = round(price * 0.34),\n    # Put the URL into a clickable link\n    link = map(link,\n               ~gt::html(as.character(htmltools::a(href = .x, \"Link\")))),\n  ) %&gt;%\n  group_by(price_group) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nname\ncategory\nshort_description\nprice\nprice_cad\nlink\n\n\n\n\nLeast expensive\n\n\nGUBBARP\nBookcases & shelving units\nKnob, 21 mm\n3\n1\nLink\n\n\nGUBBARP\nCabinets & cupboards\nKnob, 21 mm\n3\n1\nLink\n\n\nGUBBARP\nTV & media furniture\nKnob, 21 mm\n3\n1\nLink\n\n\nMost expensive\n\n\nGR√ñNLID\nSofas & armchairs\nU-shaped sofa, 6 seat\n8900\n3026\nLink\n\n\nLIDHULT\nBeds\nCorner sofa-bed, 6-seat\n9585\n3259\nLink\n\n\nLIDHULT\nSofas & armchairs\nCorner sofa-bed, 6-seat\n9585\n3259\nLink\n\n\n\n\n\n\n\nThe most expensive items are 6-seat sofas, and the least expensive are drawer knobs.\nThe old_price variable is, according to the data dictionary, the price before discount:\n\nikea %&gt;%\n  count(old_price, sort = T) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nMost of the time, there is ‚ÄúNo old price‚Äù, which I‚Äôll replace with NA. Every other value is a price in Saudia Riyals, which I will convert to numeric. Before that, we need to consider a handful of items that are priced in packs:\n\nikea %&gt;%\n  filter(str_detect(old_price, \"pack\")) %&gt;%\n  select(name, category, price, old_price, link)\n\n# A tibble: 10 √ó 5\n   name      category                   price old_price     link                \n   &lt;chr&gt;     &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;               \n 1 BRYNILEN  Beds                          30 SR 50/4 pack  https://www.ikea.co‚Ä¶\n 2 BRENN√ÖSEN Beds                          30 SR 50/4 pack  https://www.ikea.co‚Ä¶\n 3 BURFJORD  Beds                          40 SR 50/4 pack  https://www.ikea.co‚Ä¶\n 4 B√ÖTSFJORD Beds                          30 SR 50/4 pack  https://www.ikea.co‚Ä¶\n 5 BJORLI    Beds                          40 SR 50/4 pack  https://www.ikea.co‚Ä¶\n 6 SK√ÖDIS    Bookcases & shelving units     6 SR 10/4 pack  https://www.ikea.co‚Ä¶\n 7 SJ√ÑLLAND  Outdoor furniture            356 SR 445/2 pack https://www.ikea.co‚Ä¶\n 8 NORSBORG  Sofas & armchairs             80 SR 100/4 pack https://www.ikea.co‚Ä¶\n 9 NORSBORG  Sofas & armchairs            140 SR 175/2 pack https://www.ikea.co‚Ä¶\n10 NORSBORG  Sofas & armchairs             80 SR 100/4 pack https://www.ikea.co‚Ä¶\n\n\nLooking at a few of the links for these items, and from the fact that the price is always less than old_price, both of the prices should be for the same number of items in a pack, so I shouldn‚Äôt need to adjust the prices by unit.\n\nikea &lt;- ikea %&gt;%\n  mutate(\n    old_price = readr::parse_number(old_price, na = \"No old price\"),\n    # Also compute the percentage discount\n    perc_discount = ifelse(is.na(old_price), 0.0,\n                           (old_price - price) / old_price)\n  )\n\nNow compare price and old_price for the 654 items with both (I split up items into quartiles of price to separate the data a bit):\n\nikea %&gt;%\n  filter(!is.na(old_price)) %&gt;%\n  # For ease of visualization, separate the items by their price range\n  mutate(\n    price_range = cut(\n      price,\n      breaks = quantile(price, c(0, 0.25, 0.5, 0.75, 1.0)), include.lowest = T,\n      labels = c(\"1st quartile\", \"2nd quartile\", \"3rd quartile\", \"4th quartile\")\n    )\n  ) %&gt;%\n  select(row_number, price_range, price, old_price, perc_discount) %&gt;%\n  pivot_longer(cols = c(old_price, price)) %&gt;%\n  ggplot(aes(x = name, y = value)) +\n  geom_point() +\n  geom_line(aes(group = row_number, color = perc_discount), alpha = 0.4) +\n  scale_y_log10(\"Price in SAR\") +\n  facet_wrap(~price_range, ncol = 2, scales = \"free_y\") +\n  scale_color_gradient2(\n    \"Percent discount\",\n    low = td_colors$div5[1], mid = td_colors$div5[3], high = td_colors$div5[5],\n    labels = scales::percent_format(1), midpoint = 0.25\n  )\n\n\n\n\nIt looks like there may be a relationship between an item‚Äôs price and its discount, which we can look at directly:\n\nikea %&gt;%\n  filter(!is.na(old_price)) %&gt;%\n  ggplot(aes(x = old_price, y = perc_discount)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", formula = \"y ~ x\") +\n  scale_x_log10() +\n  scale_y_continuous(labels = scales::percent_format(1))\n\n\n\n\nYes, the more expensive items are less likely discounted by more than 20%. Cheaper items (old_price &lt; 100) are most often discounted by 40%.\nThe sellable_online variable is logical:\n\nikea %&gt;% count(sellable_online)\n\n# A tibble: 2 √ó 2\n  sellable_online     n\n  &lt;lgl&gt;           &lt;int&gt;\n1 FALSE              28\n2 TRUE             3666\n\n\nThe 28 items which are not sellable online:\n\nikea %&gt;%\n  filter(!sellable_online) %&gt;%\n  select(name, category, price, short_description) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nThe other_colors variable is ‚ÄúYes‚Äù or ‚ÄúNo‚Äù, which I‚Äôll convert to a logical:\n\nikea &lt;- ikea %&gt;% mutate(other_colors = other_colors == \"Yes\")\nikea %&gt;% count(other_colors)\n\n# A tibble: 2 √ó 2\n  other_colors     n\n  &lt;lgl&gt;        &lt;int&gt;\n1 FALSE         2182\n2 TRUE          1512\n\n\nIs there any relationship between price and other_colors? Try a simple t-test:\n\nt.test(price ~ other_colors, data = ikea)\n\n\n    Welch Two Sample t-test\n\ndata:  price by other_colors\nt = -4.9283, df = 2944.3, p-value = 8.754e-07\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -324.0042 -139.5679\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           983.3355           1215.1216 \n\n\nYes, items that do not come in other colors are less expensive on average.\n\nikea %&gt;%\n  ggplot(aes(y = other_colors, x = price)) +\n  geom_jitter(alpha = 0.2) +\n  scale_x_log10()\n\n\n\n\nDoesn‚Äôt really pass the eye test though ‚Äì the distributions are near identical. The magic of large sample sizes.\n\nShort description\nThere are 1706 unique short_descriptions with the following counts:\n\n# The short_description has a lot of white space issues, which we can\n#  trim and squish\nikea &lt;- ikea %&gt;%\n  mutate(\n    short_description = short_description %&gt;% str_trim() %&gt;% str_squish()\n  )\n\nikea %&gt;%\n  count(short_description, sort = T) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nThe first notable pattern is that the description often includes dimensions in centimeters, but it doesn‚Äôt seem to align with the dimension variables (heigth, width, depth). As an example, consider this item:\n\nikea %&gt;%\n  filter(row_number == 1156) %&gt;%\n  glimpse()\n\nRows: 1\nColumns: 15\n$ row_number        &lt;dbl&gt; 1156\n$ item_id           &lt;dbl&gt; 99014376\n$ name              &lt;chr&gt; \"MELLTORP / ADDE\"\n$ category          &lt;chr&gt; \"Chairs\"\n$ price             &lt;dbl&gt; 359\n$ old_price         &lt;dbl&gt; NA\n$ sellable_online   &lt;lgl&gt; TRUE\n$ link              &lt;chr&gt; \"https://www.ikea.com/sa/en/p/melltorp-adde-table-an‚Ä¶\n$ other_colors      &lt;lgl&gt; FALSE\n$ short_description &lt;chr&gt; \"Table and 4 chairs, 125 cm\"\n$ designer          &lt;chr&gt; \"Marcus Arvonen/Lisa Norinder\"\n$ depth             &lt;dbl&gt; NA\n$ height            &lt;dbl&gt; 72\n$ width             &lt;dbl&gt; 75\n$ perc_discount     &lt;dbl&gt; 0\n\n\nshort_description has ‚Äú125 cm‚Äù, but height = 72 and width = 75. Looking at the web page for the item, I can see that the 125 cm is the length of the table, which probably should have been imputed for the missing depth variable here.\nI should be able to pull the dimensions out of the short_description with a regex:\n\n# This regex looks for measurements in cm, mm and inches (\") after a comma and\n#  before the end of the string\n# The complicated part is (,)(?!.*,) which is a negative lookahead that ensures\n#  I only match the last comma of the text\ndim_regex &lt;- \"(,)(?!.*,) (.*?) (cm|mm|\\\")$\"\nikea &lt;- ikea %&gt;%\n  mutate(\n    short_description_dim = str_extract(short_description, dim_regex) %&gt;%\n      # Remove the comma\n      str_remove(\", \"),\n    # With the dimensions extracted, remove from the full short_description\n    short_description = str_remove(short_description, dim_regex)\n  )\n\nHere is how that affected the example item:\n\nikea %&gt;%\n  filter(row_number == 1156) %&gt;%\n  select(short_description, short_description_dim)\n\n# A tibble: 1 √ó 2\n  short_description  short_description_dim\n  &lt;chr&gt;              &lt;chr&gt;                \n1 Table and 4 chairs 125 cm               \n\n\nFor the remaining text in short_description, we might be able to text mine some useful information to better categorize items. For instance, consider the category = ‚ÄúChairs‚Äù:\n\nikea_chairs &lt;- ikea %&gt;% filter(category == \"Chairs\")\nikea_chairs %&gt;%\n  select(name, price, short_description) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nMost of these are just single chairs or stools. But others are a table and multiple chairs:\n\nikea_chairs %&gt;%\n  filter(str_detect(short_description, \"\\\\d+\")) %&gt;%\n  select(price, short_description)\n\n# A tibble: 156 √ó 2\n   price short_description                 \n   &lt;dbl&gt; &lt;chr&gt;                             \n 1   179 Table and 2 chairs                \n 2   359 Table and 4 chairs                \n 3   549 Table and 4 chairs                \n 4   495 Storage bench w towel rail/4 hooks\n 5   589 Table and 2 chairs                \n 6   285 Table and 4 chairs                \n 7   145 Stepladder, 3 steps               \n 8   289 Table and 2 chairs                \n 9   579 Table and 4 chairs                \n10   889 Table and 2 chairs                \n# ‚Ä¶ with 146 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nI think a way to operationalize these descriptions is to extract the number of chairs/stools and indicate if there is a table:\n\nchairs_regex &lt;- \"\\\\d (.*?)chair\"\nstools_regex &lt;- \"\\\\d (.*?)stool\"\nikea &lt;- ikea %&gt;%\n  mutate(\n    # Count chairs\n    chairs = str_extract(short_description, chairs_regex) %&gt;%\n      readr::parse_number(),\n    chairs = ifelse(\n      # If not multiple chairs\n      is.na(chairs),\n      # There is 0 or 1 chair\n      as.numeric(str_detect(short_description,\n                            regex(\"chair\", ignore_case = TRUE))), chairs\n    ),\n    # Count stools\n    stools = str_extract(short_description, stools_regex) %&gt;%\n      readr::parse_number(),\n    stools = ifelse(\n      # If not multiple stools\n      is.na(stools),\n      # There is 0 or 1 stools\n      as.numeric(str_detect(short_description,\n                            regex(\"stool\", ignore_case = TRUE))), stools\n    ),\n    # Just look for 1 or 0 tables\n    tables = as.integer(str_detect(short_description,\n                                   regex(\"table\", ignore_case = TRUE)))\n  )\nikea %&gt;%\n  filter(chairs &gt; 0 | stools &gt; 0 | tables &gt; 0) %&gt;%\n  count(short_description, chairs, stools, tables, sort = T)\n\n# A tibble: 185 √ó 5\n   short_description          chairs stools tables     n\n   &lt;chr&gt;                       &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt; &lt;int&gt;\n 1 Table and 4 chairs              4      0      1   166\n 2 Table                           0      0      1   132\n 3 Table and 6 chairs              6      0      1    78\n 4 Chair                           1      0      0    59\n 5 Table and 2 chairs              2      0      1    34\n 6 Armchair                        1      0      0    30\n 7 Bar stool with backrest         0      1      0    25\n 8 Coffee table                    0      0      1    25\n 9 Table top                       0      0      1    22\n10 Bar table and 2 bar stools      0      2      1    20\n# ‚Ä¶ with 175 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nIt‚Äôs not perfect feature engineering ‚Äì the ‚ÄúChair pad‚Äù item (16 occurrences) isn‚Äôt actually a chair, for instance ‚Äì but these variables should be helpful in predicting prices.\n\nikea %&gt;%\n  filter(stools &gt; 0 | chairs &gt; 0) %&gt;%\n  # No items have both chairs and stools, so split them into groups\n  mutate(chair_stool = ifelse(stools &gt; 0, \"stools\", \"chairs\"),\n         chair_stool_count = chairs + stools) %&gt;%\n  ggplot(aes(x = factor(chair_stool_count, levels = 1:8), y = price,\n             color = factor(tables))) +\n  geom_jitter(alpha = 0.5, height = 0, width = 0.2) +\n  facet_wrap(~chair_stool) +\n  dunnr::add_facet_borders() +\n  labs(x = \"chair/stool count\", color = \"tables\") +\n  theme(legend.position = c(0.9, 0.8)) +\n  scale_y_log10() +\n  scale_color_manual(values = ikea_colors) +\n  dunnr::theme_td_grey()\n\n\n\n\nOur new variables are, unsurprisingly, very predictive of price. We can estimate the effects with a simple linear regression model:\n\nlm(\n  log(price) ~ chair_stool_count + tables,\n  data = ikea %&gt;%\n    filter(stools &gt; 0 | chairs &gt; 0) %&gt;%\n    mutate(chair_stool_count = chairs + stools)\n) %&gt;%\n  gtsummary::tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nchair_stool_count\n0.41\n0.34, 0.48\n&lt;0.001\n\n\ntables\n0.91\n0.66, 1.2\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nBoth the number of stools/chairs and the presence of a table are highly significant predictors of item price.\nWe can also extract some useful variables for sofas. Here are the short_descriptions under the category ‚ÄúSofas & armchairs‚Äù:\n\nikea_sofas &lt;- ikea %&gt;%\n  filter(category == \"Sofas & armchairs\")\nikea_sofas %&gt;%\n  count(short_description, sort = T) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nThe number of seats can be extracted with some more regex:\n\n# Find the sofas and sectionals\nsofa_sectional_idx &lt;-\n  str_detect(ikea_sofas$short_description,\n             regex(\"sofa|section\", ignore_case = TRUE)) &\n  # Exclude sofa covers and legs\n  str_detect(ikea_sofas$short_description,\n             regex(\"cover|leg\", ignore_case = TRUE), negate = TRUE)\n\nikea_sofas &lt;- ikea_sofas %&gt;%\n  mutate(\n    sofa_seats = ifelse(\n      sofa_sectional_idx,\n      # Extract words or numbers before \"-seat\" or \" seat\"\n      str_extract(short_description,\n                  \"(\\\\w+|\\\\d)(-| )seat\"),\n      NA_character_\n    ),\n    # If the description doesn't say the number of seats, then assume one seat\n    sofa_seats = ifelse(\n      sofa_sectional_idx & is.na(sofa_seats), \"1-seat\", sofa_seats\n    )\n  )\n\nikea_sofas %&gt;%\n  count(sofa_seats, short_description, sort = T) %&gt;%\n  filter(!is.na(sofa_seats))\n\n# A tibble: 47 √ó 3\n   sofa_seats short_description         n\n   &lt;chr&gt;      &lt;chr&gt;                 &lt;int&gt;\n 1 3-seat     3-seat sofa              30\n 2 2-seat     2-seat sofa              18\n 3 4-seat     4-seat sofa              16\n 4 3-seat     3-seat sofa-bed          12\n 5 4-seat     Corner sofa, 4-seat      12\n 6 5-seat     Corner sofa, 5-seat      12\n 7 1-seat     Corner section            7\n 8 3-seat     3-seat section            7\n 9 1-seat     Chaise longue section     6\n10 2-seat     2-seat section            6\n# ‚Ä¶ with 37 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nNow to convert this to a numeric variable, replace word representations (‚Äútwo‚Äù, ‚Äúthree‚Äù) and use the readr::parse_number() function:\n\nikea_sofas &lt;- ikea_sofas %&gt;%\n  mutate(\n    sofa_seats = sofa_seats %&gt;%\n      tolower() %&gt;%\n      str_replace(\"two\", \"2\")  %&gt;%\n      str_replace(\"three\", \"3\") %&gt;%\n      readr::parse_number() %&gt;%\n      replace_na(0)\n  )\n\nAnd use logicals to indicate sofa beds and sectionals/modulars:\n\nikea_sofas &lt;- ikea_sofas %&gt;%\n  mutate(\n    sofa_bed = sofa_sectional_idx &\n      str_detect(short_description, regex(\"sofa( |-)bed\", ignore_case = T)),\n    sofa_sectional = sofa_sectional_idx &\n      str_detect(short_description, regex(\"modular|section\", ignore_case = T))\n  )\n\nPlot the prices to see if there is an obvious relationship with these new variables:\n\nikea_sofas &lt;- ikea_sofas %&gt;%\n  mutate(\n    sofa_type = case_when(\n      sofa_bed & sofa_sectional ~ \"Bed + sectional\",\n      sofa_bed ~ \"Bed\",\n      sofa_sectional ~ \"Sectional\",\n      sofa_seats &gt; 0 ~ \"Normal\",\n      TRUE ~ \"Not a sofa\"\n    )\n  )\nikea_sofas %&gt;%\n  filter(sofa_type != \"Not a sofa\") %&gt;%\n  ggplot(aes(x = sofa_seats, y = log(price))) +\n  geom_jitter(color = ikea_colors[1], height = 0, width = 0.11) +\n  geom_smooth(color = ikea_colors[2], method = \"lm\", formula = \"y ~ x\") +\n  facet_wrap(~sofa_type, ncol = 2) +\n  theme_td_grey(gridlines = \"xy\")\n\n\n\n\n\nlm(\n  log(price) ~ sofa_seats * sofa_type,\n  data = ikea_sofas %&gt;% filter(sofa_type != \"Not a sofa\") \n) %&gt;%\n  gtsummary::tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nsofa_seats\n0.39\n0.29, 0.50\n&lt;0.001\n\n\nsofa_type\n\n\n\n\n\nBed\n‚Äî\n‚Äî\n\n\n\nBed + sectional\n0.53\n-0.61, 1.7\n0.4\n\n\nNormal\n0.03\n-0.37, 0.42\n0.9\n\n\nSectional\n-0.12\n-0.56, 0.32\n0.6\n\n\nsofa_seats * sofa_type\n\n\n\n\n\nsofa_seats * Bed + sectional\n-0.03\n-0.46, 0.41\n&gt;0.9\n\n\nsofa_seats * Normal\n-0.04\n-0.17, 0.09\n0.5\n\n\nsofa_seats * Sectional\n-0.01\n-0.21, 0.18\n&gt;0.9\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nThe sofa_type variable makes little difference, but the sofa_seats variable is definitely an important predictor of price in this subset of the data. Incorporate these variables into the larger data set:\n\n\nRepeated code\n\n\n# Find the sofas and sectionals\nsofa_sectional_idx &lt;-\n  ikea$category == \"Sofas & armchairs\" &\n  str_detect(ikea$short_description,\n             regex(\"sofa|section\", ignore_case = TRUE)) &\n  # Exclude sofa covers and legs\n  str_detect(ikea$short_description,\n             regex(\"cover|leg\", ignore_case = TRUE), negate = TRUE)\n\nikea &lt;- ikea %&gt;%\n  mutate(\n    sofa_seats = ifelse(\n      sofa_sectional_idx,\n      # Extract words or numbers before \"-seat\" or \" seat\"\n      str_extract(short_description,\n                  \"(\\\\w+|\\\\d)(-| )seat\"),\n      NA_character_\n    ),\n    # If the description doesn't say the number of seats, then assume one seat\n    sofa_seats = ifelse(\n      sofa_sectional_idx & is.na(sofa_seats), \"1-seat\", sofa_seats\n    ),\n    sofa_seats = sofa_seats %&gt;%\n      tolower() %&gt;%\n      str_replace(\"two\", \"2\")  %&gt;%\n      str_replace(\"three\", \"3\") %&gt;%\n      readr::parse_number() %&gt;%\n      replace_na(0),\n    sofa_bed = sofa_sectional_idx &\n      str_detect(short_description, regex(\"sofa( |-)bed\", ignore_case = T)),\n    sofa_sectional = sofa_sectional_idx &\n      str_detect(short_description, regex(\"modular|section\", ignore_case = T)),\n    sofa_type = case_when(\n      sofa_bed & sofa_sectional ~ \"Bed + sectional\",\n      sofa_bed ~ \"Bed\",\n      sofa_sectional ~ \"Sectional\",\n      sofa_seats &gt; 0 ~ \"Normal\",\n      TRUE ~ \"Not a sofa\"\n    )\n  )\n\n\n\n\nDesigner\nCheck out the designer variable:\n\nikea %&gt;%\n  count(designer, sort = TRUE) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nThere are a lot of unique designers unsurprisingly. One odd thing I noticed is that some of the designer values are not designer names, but rather additional descriptions of the items. Check the 3 longest designer values, for example:\n\nikea %&gt;%\n  slice_max(nchar(designer), n = 3, with_ties = FALSE) %&gt;%\n  select(name, short_description, designer, link) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nLooking at the actual webpages for these items, it is clear that the designer variable was just incorrectly scraped from the website in these cases. Here is the distribution of string length:\n\nikea %&gt;%\n  ggplot(aes(x = nchar(designer), y = 0)) +\n  geom_jitter(alpha = 0.1) +\n  labs() +\n  scale_x_log10(limits = c(1, 2e3)) +\n  dunnr::remove_axis(\"y\")\n\n\n\n\nI‚Äôm sure there is some valuable information in these descriptions, but it would take a long time to extract that information. Instead I‚Äôll use the fact that those designer values seem to begin with an ‚Äúarticle number‚Äù like 104.114.40:\n\nikea %&gt;%\n  mutate(\n    article_number = str_detect(designer, regex(\"^\\\\d+\"))\n  ) %&gt;%\n  ggplot(aes(x = nchar(designer), y = article_number, color = article_number)) +\n  geom_jitter(alpha = 0.5, position = position_jitter(seed = 1), shape = 21,\n              fill = \"white\") +\n  scale_x_log10() +\n  theme(legend.position = \"none\") +\n  #geom_label(\n  ggrepel::geom_text_repel(\n    data = . %&gt;% filter(article_number, nchar(designer) &lt; 50),\n    aes(label = designer),\n    position = position_jitter(seed = 1), size = 3\n  ) +\n  scale_color_manual(values = rev(ikea_colors))\n\n\n\n\nThere were a couple cases of designer having the article number, but not being a long piece of text. These are the labelled points in the plot above (704.510.65 and 304.510.67). I‚Äôll mark all of these designer values with article numbers as NA. Also of interest to me: does the price differ significantly when the designer is IKEA vs an individual?\n\nikea &lt;- ikea %&gt;%\n  mutate(\n    designer_group = case_when(\n      str_detect(designer, regex(\"^\\\\d+\")) ~ NA_character_,\n      designer == \"IKEA of Sweden\" ~ \"IKEA\",\n      str_detect(designer, \"IKEA of Sweden\") ~ \"IKEA + individual(s)\",\n      str_detect(designer, \"IKEA\") ~ \"oops\",\n      TRUE ~ \"Individual(s)\"\n    ) %&gt;%\n      factor()\n  )\nikea %&gt;%\n  filter(!is.na(designer_group)) %&gt;%\n  ggplot(aes(y = designer_group, x = price)) +\n  geom_boxplot(outlier.shape = NA, width = 0.2) +\n  geom_jitter(alpha = 0.1) +\n  scale_x_log10()\n\n\n\n\nIn general, items designed by IKEA alone are cheaper than items designed by one or more individuals, and items designed in collaboration with IKEA are the most expensive.\n\n\nSize\nThe depth, height and width variables are all in centimeters. Summarize the values:\n\nikea %&gt;%\n  select(row_number, depth, height, width) %&gt;%\n  pivot_longer(cols = -row_number, names_to = \"dimension\") %&gt;%\n  group_by(dimension) %&gt;%\n  summarise(\n    p_missing = mean(is.na(value)) %&gt;% scales::percent(),\n    median = median(value, na.rm = TRUE),\n    min = min(value, na.rm = TRUE), max = max(value, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\ndimension\np_missing\nmedian\nmin\nmax\n\n\n\n\ndepth\n40%\n47\n1\n257\n\n\nheight\n27%\n83\n1\n700\n\n\nwidth\n16%\n80\n1\n420\n\n\n\n\n\n\n\nQuite a few items are missing measurements and some of them are certainly wrong. A height of 700cm seems‚Ä¶ unlikely. Looking at the link (https://www.ikea.com/sa/en/p/hilver-leg-cone-shaped-bamboo-80278273/), I see that it is actually 700mm, so this was an error in the data scraping. Take a look at the distribution of these values:\n\nikea %&gt;%\n  select(row_number, depth, height, width) %&gt;%\n  pivot_longer(cols = -row_number, names_to = \"dimension\") %&gt;%\n  filter(!is.na(value)) %&gt;%\n  ggplot(aes(y = dimension, x = value)) +\n  geom_jitter(alpha = 0.4, color = ikea_colors[1], width = 0)\n\n\n\n\nThat is definitely the biggest outlier across all dimensions. There may be more incorrect values (e.g.¬†maybe some of the smaller values are in meters), but I‚Äôll just divide that one value of 700 by 10 (to convert it to centimeters) and be done with it for now.\n\nikea &lt;- ikea %&gt;%\n  mutate(height = ifelse(height == 700, height / 10, height))\n\nNow, for the values that we have, there should be an obvious relationship between price and size:\n\nikea %&gt;%\n  select(row_number, depth, height, width, price) %&gt;%\n  pivot_longer(cols = -c(row_number, price), names_to = \"dimension\") %&gt;%\n  filter(!is.na(value)) %&gt;%\n  ggplot(aes(x = value, y = log(price))) +\n  geom_point(alpha = 0.5, color = ikea_colors[1]) +\n  geom_smooth(method = \"loess\", formula = \"y ~ x\", color = ikea_colors[2]) +\n  facet_wrap(~dimension, scales = \"free_x\") +\n  theme_td_grey(gridlines = \"xy\")\n\n\n\n\nIn all three cases, there is a positive non-linear relationship. The relationship between the three dimensions:\n\nikea %&gt;%\n  select(row_number, depth, height, width) %&gt;%\n  pivot_longer(cols = -row_number, names_to = \"dimension\") %&gt;%\n  filter(!is.na(value)) %&gt;%\n  mutate(dimension = factor(dimension)) %&gt;%\n  left_join(., ., by = \"row_number\") %&gt;%\n  # By converting the factor levels to an integer, the pairwise comparisons\n  #  won't be duplicated\n  filter(as.integer(dimension.x) &gt; as.integer(dimension.y)) %&gt;%\n  mutate(dimension_comparison = str_c(dimension.x, \" vs \", dimension.y)) %&gt;%\n  ggplot(aes(x = value.x, y = value.y)) +\n  geom_point(alpha = 0.5, color = ikea_colors[1]) +\n  geom_smooth(method = \"loess\", formula = \"y ~ x\", color = ikea_colors[2]) +\n  facet_wrap(~dimension_comparison) +\n  theme_td_grey(gridlines = \"xy\")\n\n\n\n\nIt is a very non-linear relationship, but generally monotonic. There is an interesting spike in depth around height \\(\\approx\\) 100cm (in the left-most plot):\n\nikea %&gt;%\n  filter(depth &gt; 150, height &lt; 110, height &gt; 90) %&gt;%\n  select(name, category, depth, height, width) %&gt;%\n  paged_table()\n\n\n\n  \n\n\n\nMostly sofas and beds, which makes sense."
  },
  {
    "objectID": "posts/2020-11-08-tidytuesday-2020-week-45/index.html#model",
    "href": "posts/2020-11-08-tidytuesday-2020-week-45/index.html#model",
    "title": "TidyTuesday 2020 Week 45",
    "section": "Model",
    "text": "Model\nI‚Äôll attempt to predict item price with\n\ncategory, sellable_online, other_colors, depth, height, width from the original data, and\nperc_discount, chairs, stools, tables, sofa_seats, sofa_bed, sofa_sectional and designer_group from feature engineering.\n\n\nPre-processing\nConvert the categorical variables to factors, and add log price as the outcome variable:\n\nikea &lt;- ikea %&gt;%\n  mutate(across(where(is.character), factor),\n         log_price = log10(price))\n\nLoad tidymodels and define the data split (75% training to 25% testing) and resampling:\n\nlibrary(tidymodels)\nlibrary(textrecipes) # For step_clean_levels()\n\nset.seed(4)\nikea_split &lt;- initial_split(ikea, prop = 3/4,\n                            # Stratify by price\n                            strata = log_price)\nikea_train &lt;- training(ikea_split)\nikea_test &lt;- testing(ikea_split)\n\n# Use bootstrap resamples\nikea_bootstrap &lt;- bootstraps(ikea_train, times = 25, strata = log_price)\n\nDefine the pre-processing recipe for the modelling pipeline:\n\nikea_rec &lt;-\n  recipe(\n    log_price ~ category + sellable_online + other_colors +\n      depth + height + width +\n      perc_discount + chairs + stools + tables +\n      sofa_seats + sofa_bed + sofa_sectional + designer_group,\n    data = ikea_train\n  ) %&gt;%\n  # The designer_group variable has missing values due to a scraping error,\n  #  which we will simply impute with the most common value (the mode)\n  step_impute_mode(designer_group) %&gt;%\n  # Too many categories for this model, so group low frequency levels\n  step_other(category, threshold = 0.05) %&gt;%\n  # Impute dimension values with nearest neighbors (default k = 5)\n  step_impute_knn(depth, height, width) %&gt;%\n  # Before turning into dummy variables, clean the factor levels\n  textrecipes::step_clean_levels(all_nominal_predictors()) %&gt;%\n  # Turn our two nominal predictors (category and designer_group) to dummy vars\n  step_dummy(all_nominal_predictors()) %&gt;%\n  # Remove variables with just a single value, if any\n  step_zv(all_predictors())\nikea_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         14\n\nOperations:\n\nMode imputation for designer_group\nCollapsing factor levels for category\nK-nearest neighbor imputation for depth, height, width\nCleaning factor levels for all_nominal_predictors()\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()\n\n\nBefore applying the recipe, we can explore the processing steps with the recipes::prep() and recipes::bake() functions applied to the training data:\n\nikea_baked &lt;- bake(prep(ikea_rec), new_data = NULL)\nikea_baked %&gt;% paged_table()\n\n\n\n  \n\n\n\nCheck out the designer_group dummy variables:\n\nikea_baked %&gt;%\n  select(matches(\"designer_group\")) %&gt;%\n  count(across(everything()))\n\n# A tibble: 3 √ó 3\n  designer_group_ikea_individual_s designer_group_individual_s     n\n                             &lt;dbl&gt;                       &lt;dbl&gt; &lt;int&gt;\n1                                0                           0   633\n2                                0                           1  1637\n3                                1                           0   500\n\n\nNote that the 107 NA designer_group values were imputed with the most common value (the mode = ‚ÄúIndividual(s)‚Äù):\n\nikea_train %&gt;% count(designer_group)\n\n# A tibble: 4 √ó 2\n  designer_group           n\n  &lt;fct&gt;                &lt;int&gt;\n1 IKEA                   633\n2 IKEA + individual(s)   500\n3 Individual(s)         1530\n4 &lt;NA&gt;                   107\n\n\nThe category variable was reduced to the following levels by collapsing low-frequency (&lt;5%) levels into category_other:\n\nikea_baked %&gt;%\n  select(matches(\"category\")) %&gt;%\n  count(across(everything())) %&gt;%\n  pivot_longer(cols = -n, names_to = \"var\") %&gt;%\n  filter(value == 1) %&gt;%\n  transmute(var, n, p = scales::percent(n / sum(n))) %&gt;%\n  arrange(desc(n)) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nvar\nn\np\n\n\n\n\ncategory_tables_desks\n456\n17.46%\n\n\ncategory_bookcases_shelving_units\n412\n15.78%\n\n\ncategory_other\n357\n13.67%\n\n\ncategory_chairs\n352\n13.48%\n\n\ncategory_sofas_armchairs\n329\n12.60%\n\n\ncategory_cabinets_cupboards\n214\n8.20%\n\n\ncategory_wardrobes\n178\n6.82%\n\n\ncategory_outdoor_furniture\n173\n6.63%\n\n\ncategory_tv_media_furniture\n140\n5.36%\n\n\n\n\n\n\n\nThe dimension variables height, depth and width were imputed by nearest neighbors (using the default \\(k\\) = 5). Summarise the values of each:\n\nikea_baked %&gt;%\n  select(height, depth, width) %&gt;%\n  mutate(idx = 1:n(), data = \"ikea_baked\") %&gt;%\n  bind_rows(\n    ikea_train %&gt;%\n      select(height, depth, width) %&gt;%\n      mutate(idx = 1:n(), data = \"ikea_train\")\n  ) %&gt;%\n  pivot_longer(cols = c(height, depth, width), names_to = \"dim\") %&gt;%\n  group_by(data, dim) %&gt;%\n  summarise(\n    n = as.character(sum(!is.na(value))),\n    p_missing = scales::percent(mean(is.na(value))),\n    mean_val = as.character(round(mean(value, na.rm = TRUE), 1)),\n    sd_val = as.character(round(sd(value, na.rm = TRUE), 1)),\n    .groups = \"drop\"\n  ) %&gt;%\n  pivot_longer(cols = c(n, p_missing, mean_val, sd_val), names_to = \"stat\") %&gt;%\n  pivot_wider(names_from = dim, values_from = value) %&gt;%\n  group_by(data) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nstat\ndepth\nheight\nwidth\n\n\n\n\nikea_baked\n\n\nn\n2770\n2770\n2770\n\n\np_missing\n0%\n0%\n0%\n\n\nmean_val\n55.5\n95.6\n106.4\n\n\nsd_val\n26.6\n54.8\n70.3\n\n\nikea_train\n\n\nn\n1660\n2026\n2314\n\n\np_missing\n40%\n27%\n16%\n\n\nmean_val\n54.3\n100.4\n104.4\n\n\nsd_val\n29.7\n60.3\n71.6\n\n\n\n\n\n\n\nThe zero variance filter did not remove any variables, which we can see by printing the prep() step:\n\nprep(ikea_rec)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         14\n\nTraining data contained 2770 data points and 1403 incomplete rows. \n\nOperations:\n\nMode imputation for designer_group [trained]\nCollapsing factor levels for category [trained]\nK-nearest neighbor imputation for depth, height, width [trained]\nCleaning factor levels for category, designer_group [trained]\nDummy variables from category, designer_group [trained]\nZero variance filter removed &lt;none&gt; [trained]\n\n\n\n\nLinear regression\nI‚Äôll start with a simple linear regression, which I expect will be outperformed by a more flexible method.1\n\nlm_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"lm\")\n\nlm_workflow &lt;- workflow() %&gt;%\n  add_recipe(ikea_rec) %&gt;%\n  add_model(lm_spec)\n\nlm_fit_train &lt;- lm_workflow %&gt;%\n  fit(data = ikea_train)\n\nPrediction metrics on the training and testing data:\n\nikea_train_pred &lt;- augment(lm_fit_train, new_data = ikea_train)\nmetrics(ikea_train_pred, truth = log_price, estimate = .pred)\n\n# A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.411\n2 rsq     standard       0.596\n3 mae     standard       0.302\n\nlm_fit &lt;- last_fit(lm_fit_train, ikea_split)\ncollect_metrics(lm_fit)\n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.412 Preprocessor1_Model1\n2 rsq     standard       0.589 Preprocessor1_Model1\n\n\nShow the actual vs predicted log price in the testing data:\n\ncollect_predictions(lm_fit) %&gt;%\n  ggplot(aes(x = log_price, y = .pred)) +\n  geom_point(color = ikea_colors[1], alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = ikea_colors[2], size = 1.5)\n\n\n\n\nNot as bad as I was expecting, but we can definitely improve on this:\n\n\nRandom forest\nFollowing along with Julia Silge‚Äôs approach, I will attempt to tune a random forest model via ranger:\n\nranger_spec &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"ranger\")\n\nranger_workflow &lt;- workflow() %&gt;%\n  add_recipe(ikea_rec) %&gt;%\n  add_model(ranger_spec)\n\n# Speed up the tuning with parallel processing\nn_cores &lt;- parallel::detectCores(logical = FALSE)\nlibrary(doParallel)\ncl &lt;- makePSOCKcluster(n_cores - 1)\nregisterDoParallel(cl)\n\nset.seed(12)\n\nlibrary(tictoc) # A convenient package for timing\ntic()\nranger_tune &lt;-\n  tune_grid(ranger_workflow, resamples = ikea_bootstrap, grid = 11,\n            # This argument is required when running in parallel\n            control = control_grid(pkgs = c(\"textrecipes\")))\ntoc()\n\n226.84 sec elapsed\n\n\nThe 5 best fits to the training data by RMSE:\n\nshow_best(ranger_tune, metric = \"rmse\")\n\n# A tibble: 5 √ó 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    22     3 rmse    standard   0.305    25 0.00225 Preprocessor1_Model08\n2     6     8 rmse    standard   0.309    25 0.00205 Preprocessor1_Model11\n3    19    15 rmse    standard   0.309    25 0.00210 Preprocessor1_Model09\n4    12    18 rmse    standard   0.310    25 0.00212 Preprocessor1_Model01\n5    15    20 rmse    standard   0.311    25 0.00211 Preprocessor1_Model07\n\n\nAs expected, we get better performance with the more flexible random forest models compared to linear regression. Choose the best model from the tuning by RMSE:\n\nranger_best &lt;- ranger_workflow %&gt;%\n  finalize_workflow(select_best(ranger_tune, metric = \"rmse\"))\nranger_best\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: rand_forest()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_impute_mode()\n‚Ä¢ step_other()\n‚Ä¢ step_impute_knn()\n‚Ä¢ step_clean_levels()\n‚Ä¢ step_dummy()\n‚Ä¢ step_zv()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 22\n  trees = 1000\n  min_n = 3\n\nComputational engine: ranger \n\n\nThe last step is the last_fit() to the training data and evaluating on testing data:\n\nranger_fit &lt;- last_fit(ranger_best, ikea_split)\n\nThe metrics and predictions to the testing data:\n\ncollect_metrics(ranger_fit)\n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.263 Preprocessor1_Model1\n2 rsq     standard       0.833 Preprocessor1_Model1\n\ncollect_predictions(ranger_fit) %&gt;%\n  ggplot(aes(log_price, .pred)) +\n  geom_point(color = ikea_colors[1], alpha = 0.5) +\n  geom_abline(color = ikea_colors[2], size = 1.5)\n\n\n\n\nAs an example of applying this model, here is a random item:\n\nset.seed(293)\nikea_sample &lt;- ikea %&gt;% slice_sample(n = 1)\n\nikea_sample %&gt;% glimpse()\n\nRows: 1\nColumns: 25\n$ row_number            &lt;dbl&gt; 180\n$ item_id               &lt;dbl&gt; 19129932\n$ name                  &lt;fct&gt; BRIMNES\n$ category              &lt;fct&gt; \"Beds\"\n$ price                 &lt;dbl&gt; 1795\n$ old_price             &lt;dbl&gt; NA\n$ sellable_online       &lt;lgl&gt; TRUE\n$ link                  &lt;fct&gt; https://www.ikea.com/sa/en/p/brimnes-day-bed-w-2‚Ä¶\n$ other_colors          &lt;lgl&gt; TRUE\n$ short_description     &lt;fct&gt; \"Day-bed w 2 drawers/2 mattresses\"\n$ designer              &lt;fct&gt; \"IKEA of Sweden/K Hagberg/M Hagberg\"\n$ depth                 &lt;dbl&gt; 53\n$ height                &lt;dbl&gt; 21\n$ width                 &lt;dbl&gt; 86\n$ perc_discount         &lt;dbl&gt; 0\n$ short_description_dim &lt;fct&gt; 80x200 cm\n$ chairs                &lt;dbl&gt; 0\n$ stools                &lt;dbl&gt; 0\n$ tables                &lt;int&gt; 0\n$ sofa_seats            &lt;dbl&gt; 0\n$ sofa_bed              &lt;lgl&gt; FALSE\n$ sofa_sectional        &lt;lgl&gt; FALSE\n$ sofa_type             &lt;fct&gt; Not a sofa\n$ designer_group        &lt;fct&gt; IKEA + individual(s)\n$ log_price             &lt;dbl&gt; 3.254064\n\n\nThe true price (on the log scale) is 3.25. Our model predicts:\n\npredict(ranger_fit$.workflow[[1]], ikea_sample)\n\n# A tibble: 1 √ó 1\n  .pred\n  &lt;dbl&gt;\n1  3.16\n\n\nWhich is pretty close.\nLastly, I am interested to see the variable importance of this model, particularly for the engineered features:\n\nlibrary(vip)\n\n# Overwrite the previous random forest specification\nranger_spec_imp &lt;- ranger_spec %&gt;%\n  # Specify the best tuning parameters\n  finalize_model(select_best(ranger_tune, metric = \"rmse\")) %&gt;%\n  # Alter the engine arguments to compute variable importance\n  set_engine(\"ranger\", importance = \"permutation\")\n\nranger_fit_imp &lt;- workflow() %&gt;%\n  add_recipe(ikea_rec) %&gt;%\n  add_model(ranger_spec_imp) %&gt;%\n  fit(ikea_train) %&gt;%\n  extract_fit_parsnip()\n\nvip(ranger_fit_imp, num_features = 20,\n    aesthetics = list(fill = ikea_colors[1])) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme(axis.text.y = element_text(face = \"bold\"))\n\n\n\n\nAs with Julie Silge‚Äôs model, the width/depth/height variables are the most important predictors. The chairs feature, which took some fancy regex to engineer, turned out to be worthwhile as it was the 4th most important predictor here. designer_group ended up being more important than category."
  },
  {
    "objectID": "posts/2020-11-08-tidytuesday-2020-week-45/index.html#reproducibility",
    "href": "posts/2020-11-08-tidytuesday-2020-week-45/index.html#reproducibility",
    "title": "TidyTuesday 2020 Week 45",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2020-11-08-tidytuesday-2020-week-45/index.html#footnotes",
    "href": "posts/2020-11-08-tidytuesday-2020-week-45/index.html#footnotes",
    "title": "TidyTuesday 2020 Week 45",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee ISLR section 2.2.2 (James et al. 2013) on the bias-variance trade-off for an accessible explanation of this idea.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2021-05-18-tidytuesday-2021-week-21/index.html",
    "href": "posts/2021-05-18-tidytuesday-2021-week-21/index.html",
    "title": "TidyTuesday 2021 Week 21",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(lubridate)\nlibrary(gt)\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2021-05-18-tidytuesday-2021-week-21/index.html#load-the-data",
    "href": "posts/2021-05-18-tidytuesday-2021-week-21/index.html#load-the-data",
    "title": "TidyTuesday 2021 Week 21",
    "section": "Load the data",
    "text": "Load the data\n\ntt &lt;- tt_load(\"2021-05-18\")\n\nThe data this week comes from the Ask a Manager Survey. From the README:\n\nThe salary survey a few weeks ago got a huge response ‚Äî 24,000+ people shared their salaries and other info, which is a lot of raw data to sift through. Reader Elisabeth Engl kindly took the raw data and analyzed some of the trends in it and here‚Äôs what she found. (She asked me to note that she did this as a fun project to share some insights from the survey, rather than as a paid engagement.)\nThis data does not reflect the general population; it reflects Ask a Manager readers who self-selected to respond, which is a very different group (as you can see just from the demographic breakdown below, which is very white and very female)."
  },
  {
    "objectID": "posts/2021-05-18-tidytuesday-2021-week-21/index.html#explore-the-data",
    "href": "posts/2021-05-18-tidytuesday-2021-week-21/index.html#explore-the-data",
    "title": "TidyTuesday 2021 Week 21",
    "section": "Explore the data",
    "text": "Explore the data\nIt comes with just the single dataframe:\n\nsurvey &lt;- tt$survey\nglimpse(survey)\n\nRows: 26,232\nColumns: 18\n$ timestamp                                &lt;chr&gt; \"4/27/2021 11:02:10\", \"4/27/2‚Ä¶\n$ how_old_are_you                          &lt;chr&gt; \"25-34\", \"25-34\", \"25-34\", \"2‚Ä¶\n$ industry                                 &lt;chr&gt; \"Education (Higher Education)‚Ä¶\n$ job_title                                &lt;chr&gt; \"Research and Instruction Lib‚Ä¶\n$ additional_context_on_job_title          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, \"‚Ä¶\n$ annual_salary                            &lt;dbl&gt; 55000, 54600, 34000, 62000, 6‚Ä¶\n$ other_monetary_comp                      &lt;dbl&gt; 0, 4000, NA, 3000, 7000, NA, ‚Ä¶\n$ currency                                 &lt;chr&gt; \"USD\", \"GBP\", \"USD\", \"USD\", \"‚Ä¶\n$ currency_other                           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ additional_context_on_income             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ country                                  &lt;chr&gt; \"United States\", \"United King‚Ä¶\n$ state                                    &lt;chr&gt; \"Massachusetts\", NA, \"Tenness‚Ä¶\n$ city                                     &lt;chr&gt; \"Boston\", \"Cambridge\", \"Chatt‚Ä¶\n$ overall_years_of_professional_experience &lt;chr&gt; \"5-7 years\", \"8 - 10 years\", ‚Ä¶\n$ years_of_experience_in_field             &lt;chr&gt; \"5-7 years\", \"5-7 years\", \"2 ‚Ä¶\n$ highest_level_of_education_completed     &lt;chr&gt; \"Master's degree\", \"College d‚Ä¶\n$ gender                                   &lt;chr&gt; \"Woman\", \"Non-binary\", \"Woman‚Ä¶\n$ race                                     &lt;chr&gt; \"White\", \"White\", \"White\", \"W‚Ä¶\n\n\nLooks like each row corresponds to a single survey. 26232 survey responses is really impressive. Here is how those were gathered over time:\n\n# Convert character timestamp to a datetime object so we can plot it\nsurvey &lt;- survey %&gt;%\n  mutate(\n    timestamp = lubridate::parse_date_time(timestamp, orders = \"mdy HMS\"),\n    survey_date = as.Date(timestamp)\n  )\nsurvey %&gt;%\n  ggplot(aes(x = survey_date)) +\n  geom_bar(fill = td_colors$nice$day9_yellow, alpha = 0.6) +\n  scale_y_continuous(expand = expansion(c(0, 0.07))) +\n  scale_x_date(breaks = seq.Date(as.Date(\"2021-05-01\"),\n                                 as.Date(\"2021-06-01\"), by = \"week\"))\n\n\n\n\nOver 10,000 responses in one day!\nHere are the age ranges:\n\nsurvey %&gt;%\n  count(how_old_are_you) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nhow_old_are_you\nn\n\n\n\n\n18-24\n1015\n\n\n25-34\n11748\n\n\n35-44\n9398\n\n\n45-54\n3042\n\n\n55-64\n931\n\n\n65 or over\n88\n\n\nunder 18\n10\n\n\n\n\n\n\n\nConvert to an ordered factor:\n\nsurvey &lt;- survey %&gt;%\n  mutate(\n    age_range = fct_reorder(\n      how_old_are_you,\n      # This will order the factor by the first number in how_old_are_you\n      parse_number(how_old_are_you)\n    ) %&gt;%\n      fct_relevel(\"under 18\")\n  )\nsurvey %&gt;%\n  ggplot(aes(y = age_range)) +\n  geom_bar(aes(fill = age_range), show.legend = FALSE) +\n  scale_fill_viridis_d() +\n  scale_x_continuous(expand = expansion(c(0, 0.05)))\n\n\n\n\nOut of curiosity, these are the 10 survey respondents listed as ‚Äúunder 18‚Äù:\n\nsurvey %&gt;%\n  filter(age_range == \"under 18\") %&gt;%\n  select(industry, job_title, annual_salary, country,\n         overall_years_of_professional_experience) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nindustry\njob_title\nannual_salary\ncountry\noverall_years_of_professional_experience\n\n\n\n\nEducation (Primary/Secondary)\nTeacher\n45000\nUSA\n21 - 30 years\n\n\nHealth care\nUndergraduate Research Assistant\n17000\nUnited States\n2 - 4 years\n\n\nRetail\nWalmart cashier\n32552\nCanada\n2 - 4 years\n\n\nHealth care\nIntern\n29120\nU.S.\n2 - 4 years\n\n\nNonprofits\nFinance Director\n118000\nUnited States\n31 - 40 years\n\n\nHealth care\nDoctor\n220000\nUnited States\n2 - 4 years\n\n\nLeisure, Sport & Tourism\nLifeguard, Swim Instructor\n34320\nUnited States\n1 year or less\n\n\nLeisure, Sport & Tourism\nHead Lifegaurd\n31200\nUnited States\n2 - 4 years\n\n\nLaw\nRecords manager\n106000\nUSA\n41 years or more\n\n\nRetail\nTechnology Manager\n100200\nUSA\n21 - 30 years\n\n\n\n\n\n\n\nSome pretty impressive teenagers here. I think my favorite are the ones with more years of experience than years alive.\nAs for the industry variable, I know from checking out the survey itself that there are 26 options, +1 ‚ÄúOther‚Äù option to enter free-text. Do the most frequent 26 industry correspond to those preset options?\n\nsurvey %&gt;%\n  count(industry, sort = T) %&gt;%\n  head(30) %&gt;%\n  mutate(rank = 1:n()) %&gt;% relocate(rank) %&gt;%\n  gt() %&gt;%\n  tab_options(container.height = 300, container.overflow.y = TRUE)\n\n\n\n\n\n\n\n\nrank\nindustry\nn\n\n\n\n\n1\nComputing or Tech\n4360\n\n\n2\nEducation (Higher Education)\n2359\n\n\n3\nNonprofits\n2331\n\n\n4\nGovernment and Public Administration\n1821\n\n\n5\nHealth care\n1739\n\n\n6\nAccounting, Banking & Finance\n1688\n\n\n7\nEngineering or Manufacturing\n1519\n\n\n8\nMarketing, Advertising & PR\n1064\n\n\n9\nLaw\n1050\n\n\n10\nEducation (Primary/Secondary)\n805\n\n\n11\nBusiness or Consulting\n778\n\n\n12\nMedia & Digital\n732\n\n\n13\nInsurance\n500\n\n\n14\nRetail\n450\n\n\n15\nRecruitment or HR\n429\n\n\n16\nProperty or Construction\n350\n\n\n17\nArt & Design\n327\n\n\n18\nUtilities & Telecommunications\n324\n\n\n19\nTransport or Logistics\n270\n\n\n20\nSales\n262\n\n\n21\nSocial Work\n260\n\n\n22\nEntertainment\n234\n\n\n23\nHospitality & Events\n234\n\n\n24\nAgriculture or Forestry\n130\n\n\n25\nLeisure, Sport & Tourism\n96\n\n\n26\nPublishing\n59\n\n\n27\nNA\n59\n\n\n28\nLibrary\n54\n\n\n29\nLibraries\n51\n\n\n30\nBiotech\n48\n\n\n\n\n\n\n\nNot quite. industry = ‚ÄúPublishing‚Äù is the 26th most frequent despite not being a preset option, for example. Make a factor out of this variable, and group anything infrequent (&lt;100 occurrences) into ‚ÄúOther‚Äù:\n\nsurvey &lt;- survey %&gt;%\n  mutate(\n    industry_factor = fct_infreq(industry) %&gt;%\n      fct_lump_min(min = 100)\n  )\nsurvey %&gt;%\n  ggplot(aes(y = industry_factor)) +\n  geom_bar(fill = td_colors$nice$indigo_blue) +\n  scale_x_continuous(expand = c(0, 0))\n\n\n\n\nThe ‚ÄúOther‚Äù category accounts for a lot of responses.\nThe job_title variable is entirely free-text so will be difficult to use for any analysis/visualization, but here are the most common responses:\n\nsurvey %&gt;%\n  count(job_title, sort = T) %&gt;%\n  head(50) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nHow about job titles related to data or statistics or machine learning?\n\nsurvey %&gt;%\n  filter(\n    str_detect(job_title,\n               regex(\"data|statistic|machine learn\", ignore_case = TRUE))\n  ) %&gt;%\n  count(job_title, sort = T) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nI might check this out later ‚Äì quite a few responses.\nIn terms of location, we have country, state, and city, but I‚Äôll just be looking at just the country level for now:\n\nsurvey %&gt;%\n  count(country, sort = T) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThis is in need of some data cleaning obviously‚Äì a lot of variations on ‚ÄúUnited States‚Äù, for example. I‚Äôll also consolidate England, Scotland, Northern Ireland and Wales into the UK.\n\nsurvey &lt;- survey %&gt;%\n  mutate(\n    country_clean = str_remove_all(tolower(country), \"\\\\.\"),\n    country_clean = case_when(\n      str_detect(country_clean,\n                 \"states|^us|america|d state|^u s|sates\") ~ \"USA\",\n      str_detect(country_clean,\n                 \"kingdom|^uk|england|scotland|wales|northern i|britain\") ~ \"UK\",\n      str_detect(country_clean,\n                 \"canada|canda|canadw|csnada\") ~ \"Canada\",\n      str_detect(country_clean, \"austral\") ~ \"Australia\",\n      str_detect(country_clean, \"brazil|brasil\") ~ \"Brazil\",\n      str_detect(country_clean, \"zealand|nz\") ~ \"New Zealand\",\n      # If less than three letters, put in all caps\n      nchar(country_clean) &lt;= 3 ~ toupper(country_clean),\n      # Otherwise, title case\n      TRUE ~ str_to_title(country_clean)\n    )\n  )\nsurvey %&gt;%\n  count(country, country_clean, sort = T) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nI think that did a pretty reasonable job of consolidating spellings/misspellings of the most common countries. Visualize the counts, lumping any countries with &lt;20 responses into ‚ÄúOther‚Äù:\n\nsurvey %&gt;%\n  mutate(\n    country_fact = fct_infreq(country_clean) %&gt;%\n      fct_lump_min(min = 20)\n  ) %&gt;%\n  ggplot(aes(y = country_fact)) +\n  geom_bar(fill = td_colors$nice$strong_red) +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = NULL)\n\n\n\n\nUnsurprisingly for a mostly US-based blog, a large majority of respondents are based in America.\nThankfully, the currency option was a dropdown selection in the survey, so less data cleaning will be needed:\n\nsurvey %&gt;%\n  count(currency, sort = TRUE) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\ncurrency\nn\n\n\n\n\nUSD\n21852\n\n\nCAD\n1564\n\n\nGBP\n1521\n\n\nEUR\n585\n\n\nAUD/NZD\n469\n\n\nOther\n133\n\n\nCHF\n35\n\n\nSEK\n34\n\n\nJPY\n22\n\n\nZAR\n13\n\n\nHKD\n4\n\n\n\n\n\n\n\nCheck out the other currency:\n\nsurvey %&gt;%\n  filter(currency == \"Other\") %&gt;%\n  count(currency_other, sort = TRUE) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThese have few enough responses that they probably aren‚Äôt worth cleaning. Just make currency a factor in order of frequency:\n\nsurvey &lt;- survey %&gt;%\n  mutate(currency = fct_infreq(currency))\n\nThese are the country-currency combinations for USA, Canada and the UK:\n\nsurvey %&gt;%\n  filter(country_clean %in% c(\"USA\", \"Canada\", \"UK\")) %&gt;% \n  count(country_clean, currency, sort = TRUE) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nannual_salary and other_monetary_comp are the key variables of this data, but they must be interpreted with currency in mind. First, look for improper (negative, non-numeric) responses:\n\nsurvey %&gt;%\n  filter(\n    # Missing annual salary\n    is.na(annual_salary) | is.na(as.numeric(annual_salary)) |\n      # Zero or negative annual salary\n      (as.numeric(annual_salary) &lt;= 0) |\n      # Negative other compensation\n      (as.numeric(other_monetary_comp) &lt; 0)\n  ) %&gt;%\n  select(industry,  job_title, annual_salary, other_monetary_comp,\n         additional_context_on_income) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nSo 10 cases of annual_salary = 0. In 2 of those cases, the other_monetary_comp makes up the difference, so I‚Äôll create a total_salary variable adding those two:\n\nsurvey &lt;- survey %&gt;%\n  mutate(\n    total_salary = ifelse(\n      is.na(other_monetary_comp),\n      as.numeric(annual_salary),\n      as.numeric(annual_salary) + as.numeric(other_monetary_comp)\n    )\n  ) %&gt;%\n  # Remove those 8 cases of 0 income\n  filter(total_salary &gt; 0)\n\nVisualize income on a log scale for the most common currencies:\n\np &lt;- survey %&gt;%\n  filter(currency %in% c(\"USD\", \"CAD\", \"GBP\", \"EUR\")) %&gt;%\n  ggplot(aes(x = total_salary, y = currency)) +\n  geom_violin(aes(fill = currency), show.legend = FALSE) +\n  scale_x_log10(labels = scales::label_dollar()) +\n  facet_wrap(~currency, scales = \"free\") +\n  remove_axis(\"y\") +\n  add_facet_borders()\np\n\n\n\n\nEven on a log scale, there are a lot of outliers in each currency. If we trim the data to only have the inner 95%:\n\np %+%\n  (\n    survey %&gt;%\n      filter(currency %in% c(\"USD\", \"CAD\", \"GBP\", \"EUR\")) %&gt;%\n      group_by(currency) %&gt;%\n      filter(\n        total_salary &lt; quantile(total_salary, 0.975),\n        total_salary &gt; quantile(total_salary, 0.025)\n      )\n  )\n\n\n\n\nThese are much more reasonable salary ranges.\nThere are two variables related to years of experience:\n\nsurvey %&gt;%\n  count(overall_years_of_professional_experience) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\nsurvey %&gt;%\n  count(years_of_experience_in_field) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThe same 8 levels were used. Like with the age variable, we can parse these numbers and put them into ordered factors:\n\nsurvey &lt;- survey %&gt;%\n  mutate(\n    across(\n      c(years_of_experience_in_field,\n        overall_years_of_professional_experience),\n      ~fct_reorder(., parse_number(.))\n    )\n  )\nsurvey %&gt;%\n  count(years_of_experience_in_field,\n        overall_years_of_professional_experience) %&gt;%\n  ggplot(aes(x = years_of_experience_in_field,\n             y = overall_years_of_professional_experience)) +\n  geom_tile(aes(fill = n), show.legend = FALSE) +\n  geom_text(aes(label = n), color = \"white\") +\n  scale_x_discrete(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  theme(axis.text.x = element_text(angle = 25, vjust = 0.8))\n\n\n\n\nIn each case except ‚Äú41 years or more‚Äù, the diagonal of overall_years_of_professional_experience = years_of_experience_in_field is the most frequent co-occurence.\nThe education options:\n\nsurvey %&gt;%\n  count(highest_level_of_education_completed, sort = T) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nWe‚Äôll put it in a factor manually ordered by the most years to complete (on average):\n\nsurvey &lt;- survey %&gt;%\n  mutate(\n    highest_level_of_education_completed = factor(\n      highest_level_of_education_completed,\n      levels = c(\"High School\", \"Some college\", \"College degree\",\n                 \"Master's degree\", \"Professional degree (MD, JD, etc.)\", \"PhD\")\n    )\n  )\nsurvey %&gt;%\n  mutate(\n    highest_level_of_education_completed = fct_explicit_na(\n      highest_level_of_education_completed\n    )\n  ) %&gt;%\n  ggplot(aes(y = highest_level_of_education_completed)) +\n  geom_bar(aes(fill = highest_level_of_education_completed),\n           show.legend = FALSE) +\n  scale_fill_viridis_d() +\n  scale_x_continuous(expand = expansion(c(0, 0.1)))\n\n\n\n\nThe gender options:\n\nsurvey %&gt;%\n  count(gender)\n\n# A tibble: 6 √ó 2\n  gender                            n\n  &lt;chr&gt;                         &lt;int&gt;\n1 Man                            4740\n2 Non-binary                      713\n3 Other or prefer not to answer   267\n4 Prefer not to answer              1\n5 Woman                         20355\n6 &lt;NA&gt;                            148\n\n\nWe can put the ‚ÄúPrefer not to answer‚Äù response into the appropriate category, and make a factor:\n\nsurvey &lt;- survey %&gt;%\n  mutate(gender = ifelse(gender == \"Prefer not to answer\",\n                         \"Other or prefer not to answer\", gender),\n         gender = fct_infreq(gender))\nsurvey %&gt;%\n  mutate(gender = fct_explicit_na(gender)) %&gt;%\n  ggplot(aes(y = gender)) +\n  geom_bar(aes(fill = gender), show.legend = FALSE) +\n  scale_x_continuous(expand = c(0, 0))\n\n\n\n\nA lot more women respondents than men.\nThe race variable was a ‚ÄúChoose all that apply‚Äù question:\n\nsurvey %&gt;%\n  count(race, sort = T) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThe responses are split by commas, but the ‚ÄúHispanic, Latino, or Spanish origin‚Äù option has commas itself. We can approach this two ways. (1) Remove the extra commas, tokenize the answers, and store it in a column vector:\n\nsurvey &lt;- survey %&gt;%\n  mutate(\n    race = str_replace(\n      race,\n      \"Hispanic, Latino, or Spanish\",\n      \"Hispanic Latino or Spanish\"\n    ),\n    race_split = str_split(\n      race,\n      pattern = \", \"\n    )\n  )\nsurvey %&gt;%\n  select(race, race_split) %&gt;%\n  head(30) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThen to get race counts, we unnest the race_split column:\n\nsurvey %&gt;%\n  mutate(n_respondents = n()) %&gt;%\n  unnest(race_split) %&gt;%\n  count(race_split, n_respondents) %&gt;%\n  mutate(\n    p = n / n_respondents,\n    race = fct_reorder(race_split, n) %&gt;% fct_explicit_na()\n  ) %&gt;%\n  ggplot(aes(y = race, x = p)) +\n  geom_text(aes(label = scales::percent(p, accuracy = 0.1)), nudge_x = 0.05) +\n  geom_col(fill = td_colors$nice$soft_orange) +\n  scale_x_continuous(\"Percentage of respondents\",\n                     labels = scales::percent,\n                     expand = c(0, 0), limits = c(0, 1))\n\n\n\n\nOr (2) manually create indicator variables for each response:\n\nsurvey &lt;- survey %&gt;%\n  mutate(\n    white = str_detect(race, \"White\"),\n    asian = str_detect(race, \"Asian\"),\n    black = str_detect(race, \"Black\"),\n    hispanic_latino_spanish = str_detect(race, \"Hispanic\"),\n    middle_eastern_northern_african = str_detect(race, \"Middle Eastern\"),\n    native_american_alaska = str_detect(race, \"Native\"),\n    other_na = str_detect(race, \"Another option\")\n  ) %&gt;%\n  # Need to fix NA values\n  mutate(\n    across(white:other_na, ~replace_na(., FALSE))\n  )\nsurvey %&gt;%\n  # Add an indicator if NA race\n  mutate(missing = is.na(race)) %&gt;%\n  summarise(across(white:missing, ~mean(.))) %&gt;%\n  pivot_longer(everything(), names_to = \"race\", values_to = \"p\") %&gt;%\n  mutate(race = fct_reorder(race, p)) %&gt;%\n  ggplot(aes(y = race, x = p)) +\n  geom_text(aes(label = scales::percent(p, accuracy = 0.1)), nudge_x = 0.05) +\n  geom_col(fill = td_colors$nice$soft_orange) +\n  scale_x_continuous(\"Percentage of respondents\",\n                     labels = scales::percent,\n                     expand = c(0, 0), limits = c(0, 1))\n\n\n\n\nSame percentages, just different labels on the race variable because of how the data were wrangled."
  },
  {
    "objectID": "posts/2021-05-18-tidytuesday-2021-week-21/index.html#reproducibility",
    "href": "posts/2021-05-18-tidytuesday-2021-week-21/index.html#reproducibility",
    "title": "TidyTuesday 2021 Week 21",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html",
    "href": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html",
    "title": "Advent of Code 2021: Days 1-5",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(gt)\nThe Advent of Code has begun for 2021, and I decided to participate this year to work on my programming and problem solving skills in R and, when I have the time, I‚Äôll try to translate the solutions to Python. Load the reticulate package and activate my virtual Python environment:\nlibrary(reticulate)\nuse_virtualenv(\"r-reticulate\")\nI‚Äôll also be ‚Äúcompeting‚Äù in a private leaderboard started by Tan Ho. I don‚Äôt expect to rank highly here because the puzzles are released at 1AM my time (and scores are based on time from release) but it‚Äôll be a good source of motivation throughout the month. There are 25 days of challenges, so my current plan is to split up the posts into 5-day chunks."
  },
  {
    "objectID": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#day-1-sonar-sweep",
    "href": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#day-1-sonar-sweep",
    "title": "Advent of Code 2021: Days 1-5",
    "section": "Day 1: Sonar Sweep",
    "text": "Day 1: Sonar Sweep\n\nPart 1\n\nThe first order of business is to figure out how quickly the depth increases, just so you know what you‚Äôre dealing with - you never know if the keys will get carried into deeper water by an ocean current or a fish or something. To do this, count the number of times a depth measurement increases from the previous measurement. (There is no measurement before the first measurement.)\n\nImport the measurements:\n\nday1 &lt;- read_lines(\"day01-input.txt\") %&gt;%\n  as.integer()\nhead(day1)\n\n[1] 191 185 188 189 204 213\n\n\nThe tidyverse solution to this problem is to use the dplyr::lag()/lead() function to refer to previous/next values. For example, for a vector of values 1-10 (in random order), I can show the cases where the value has increased like this:\n\nd &lt;- sample(1:10)\nbind_cols(\n  value = d,\n  increased = d &gt; dplyr::lag(d)\n)\n\n# A tibble: 10 √ó 2\n   value increased\n   &lt;int&gt; &lt;lgl&gt;    \n 1    10 NA       \n 2     9 FALSE    \n 3     8 FALSE    \n 4     2 FALSE    \n 5     3 TRUE     \n 6     7 TRUE     \n 7     1 FALSE    \n 8     5 TRUE     \n 9     6 TRUE     \n10     4 FALSE    \n\n\nExcluding the NA value, which occurs due to being the first element, sum() up the cases of larger measurements:\n\nsum(lag(day1) &lt; day1, na.rm = TRUE)\n\n[1] 1709\n\n\nFor the Python solution, I will use the numpy.diff function to calculate the difference between consecutive values:\n\nimport numpy as np\n\n# Reference an object from the R session with r.obj\n(np.diff(r.day1) &gt; 0)\n\narray([False,  True,  True, ...,  True,  True,  True])\n\n\nThen chain the .sum() function to add up the True values:\n\n(np.diff(r.day1) &gt; 0).sum()\n\n1709\n\n\nNote that this method is also possible in base R, and is a bit simpler than the tidyverse solution:\n\nsum(diff(day1) &gt; 0)\n\n[1] 1709\n\n\n\n\nPart 2\n\nYour goal now is to count the number of times the sum of measurements in this sliding window increases from the previous sum. So, compare A with B, then compare B with C, then C with D, and so on. Stop when there aren‚Äôt enough measurements left to create a new three-measurement sum.\n\nHere, I will use both lag and lead to compute the sum of the window:\n\nd_sum3 &lt;- lag(d) + d + lead(d)\nbind_cols(\n  value = d,\n  sum3 = d_sum3,\n  increased = lag(d_sum3) &lt; d_sum3\n)\n\n# A tibble: 10 √ó 3\n   value  sum3 increased\n   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    \n 1    10    NA NA       \n 2     9    27 NA       \n 3     8    19 FALSE    \n 4     2    13 FALSE    \n 5     3    12 FALSE    \n 6     7    11 FALSE    \n 7     1    13 TRUE     \n 8     5    12 FALSE    \n 9     6    15 TRUE     \n10     4    NA NA       \n\n\nNow sum the number of increases in the day 1 data:\n\nday1_sum3 &lt;- lag(day1) + day1 + lead(day1)\nsum(day1_sum3 &gt; lag(day1_sum3), na.rm = TRUE)\n\n[1] 1761\n\n\nIn Python, the np.convolve function allows computation in sliding windows:\n\nnp.convolve(r.day1, np.ones(3, dtype = int))\n\narray([  191,   376,   564, ..., 31566, 21051, 10526])\n\n\nAbove, we provided the np.ones(3, dtype = int) array which is simply [1, 1, 1] and works as the convolution operator that slides along the r.day1 array. Note that the first two elements are not correct, however, because the boundaries (with fewer than 3 values) were returned. Fix this with the mode argument:\n\nnp.convolve(r.day1, np.ones(3, dtype = int), mode = 'valid')\n\narray([  564,   562,   581, ..., 31536, 31551, 31566])\n\n(np.diff(np.convolve(r.day1, np.ones(3, dtype = int), mode = 'valid')) &gt; 0) \\\n  .sum()\n\n1761"
  },
  {
    "objectID": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#day-2-dive",
    "href": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#day-2-dive",
    "title": "Advent of Code 2021: Days 1-5",
    "section": "Day 2: Dive!",
    "text": "Day 2: Dive!\n\nPart 1\n\nYour horizontal position and depth both start at 0. The steps above would then modify them as follows:\n\n\n\nforward 5 adds 5 to your horizontal position, a total of 5.\ndown 5 adds 5 to your depth, resulting in a value of 5.\nforward 8 adds 8 to your horizontal position, a total of 13.\nup 3 decreases your depth by 3, resulting in a value of 2.\ndown 8 adds 8 to your depth, resulting in a value of 10.\nforward 2 adds 2 to your horizontal position, a total of 15.\n\n\n\nCalculate the horizontal position and depth you would have after following the planned course. What do you get if you multiply your final horizontal position by your final depth?\n\nImport the steps:\n\nday2 &lt;- read_lines(\"day02-input.txt\")\nhead(day2)\n\n[1] \"forward 7\" \"down 2\"    \"forward 7\" \"down 6\"    \"forward 1\" \"forward 7\"\n\n\nPut it in a tibble, and tidyr::separate the instruction and the amount:\n\nd_day2 &lt;- tibble(step = day2) %&gt;%\n  separate(step, into = c(\"instruction\", \"amount\"), sep = \" \", convert = TRUE)\nhead(d_day2)\n\n# A tibble: 6 √ó 2\n  instruction amount\n  &lt;chr&gt;        &lt;int&gt;\n1 forward          7\n2 down             2\n3 forward          7\n4 down             6\n5 forward          1\n6 forward          7\n\n\nThen summarize the horizontal position and depth, and multiply the result:\n\nd_day2 %&gt;%\n  summarise(\n    horizontal_position = sum(amount[instruction == \"forward\"]),\n    # Depth is inverse, so down - up\n    depth = sum(amount[instruction == \"down\"]) -\n      sum(amount[instruction == \"up\"]),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(product = horizontal_position * depth)\n\n# A tibble: 1 √ó 3\n  horizontal_position depth product\n                &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n1                1990  1000 1990000\n\n\nFor the Python solution, I‚Äôll use pandas:\n\nimport pandas as pd\n\nday2_df = pd.DataFrame(r.day2, dtype = str, columns = ['step']) \\\n  .step.str.split(' ', expand = True) \\\n  .rename(columns = {0: 'instruction', 1: 'amount'}) \\\n  .astype({'amount': 'int32'})\nday2_df\n\n    instruction  amount\n0       forward       7\n1          down       2\n2       forward       7\n3          down       6\n4       forward       1\n..          ...     ...\n995     forward       9\n996        down       3\n997        down       7\n998        down       5\n999     forward       7\n\n[1000 rows x 2 columns]\n\n\nThen it is easy enough to sum up the different columns:\n\nday2_df[day2_df.instruction == 'forward'].amount.sum()\n\n1990\n\nday2_df[day2_df.instruction == 'down'].amount.sum() - \\\n  day2_df[day2_df.instruction == 'up'].amount.sum()\n\n1000\n\n\nHere is another way with the groupby and aggregate functions:\n\nday2_df_sum = day2_df \\\n  .groupby('instruction', as_index = True) \\\n  .aggregate('sum')\n  \nday2_df_sum.loc['forward'].amount\nday2_df_sum.loc['down'].amount - day2_df_sum.loc['up'].amount\n\n\n\nPart 2\n\nIn addition to horizontal position and depth, you‚Äôll also need to track a third value, aim, which also starts at 0. The commands also mean something entirely different than you first thought:\n\n\n\ndown X increases your aim by X units.\nup X decreases your aim by X units.\nforward X does two things:\nIt increases your horizontal position by X units.\nIt increases your depth by your aim multiplied by X.\n\n\n\nUsing this new interpretation of the commands, calculate the horizontal position and depth you would have after following the planned course. What do you get if you multiply your final horizontal position by your final depth?\n\nFirst, I‚Äôll use cumsum() to add a running total of the aim variable from the ‚Äúdown‚Äù and ‚Äúup‚Äù instructions:\n\nd_day2 &lt;- d_day2 %&gt;%\n  mutate(\n    # Have to use a placeholder variable so it has the same length as the\n    #  \"aim\" variable below\n    aim_placeholder = case_when(\n      instruction == \"down\" ~ amount,\n      instruction == \"up\" ~ -amount,\n      TRUE ~ 0L\n    ),\n    aim = cumsum(aim_placeholder)\n  ) %&gt;%\n  select(-aim_placeholder)\nhead(d_day2, 9)\n\n# A tibble: 9 √ó 3\n  instruction amount   aim\n  &lt;chr&gt;        &lt;int&gt; &lt;int&gt;\n1 forward          7     0\n2 down             2     2\n3 forward          7     2\n4 down             6     8\n5 forward          1     8\n6 forward          7     8\n7 down             3    11\n8 up               5     6\n9 forward          7     6\n\n\nNow with the running total of aim, I can compute horizontal position and depth:\n\nd_day2 %&gt;%\n  summarise(\n    horizontal_position = sum(amount[instruction == \"forward\"]),\n    depth = sum(\n      # Depth is aim multiplied by forward amount\n      aim[instruction == \"forward\"] * amount[instruction == \"forward\"]\n    ),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(product = horizontal_position * depth)\n\n# A tibble: 1 √ó 3\n  horizontal_position  depth    product\n                &lt;int&gt;  &lt;int&gt;      &lt;int&gt;\n1                1990 992674 1975421260\n\n\nIn Python, I will assign a new aim column, and use the np.select() function to conditionally sum the values:\n\nday2_df = day2_df \\\n  .assign(\n    aim = np.select(\n      [day2_df.instruction == 'down',\n       day2_df.instruction == 'up',\n       day2_df.instruction == 'forward'],\n      [day2_df.amount, -day2_df.amount, 0]\n    )\n  )\nday2_df.aim = day2_df.aim.cumsum()\n\nThe aggregate function can only operate on single columns, so need to make a new depth column first by multiplying aim with amount (for instruction = ‚Äòforward‚Äô):\n\nday2_df = day2_df \\\n  .assign(\n    depth = np.where(\n      day2_df.instruction == 'forward', day2_df.aim * day2_df.amount, 0\n    ),\n    horizontal_position = np.where(\n      day2_df.instruction == 'forward', day2_df.amount, 0\n    )\n  )\nday2_df \n\n    instruction  amount   aim  depth  horizontal_position\n0       forward       7     0      0                    7\n1          down       2     2      0                    0\n2       forward       7     2     14                    7\n3          down       6     8      0                    0\n4       forward       1     8      8                    1\n..          ...     ...   ...    ...                  ...\n995     forward       9   985   8865                    9\n996        down       3   988      0                    0\n997        down       7   995      0                    0\n998        down       5  1000      0                    0\n999     forward       7  1000   7000                    7\n\n[1000 rows x 5 columns]\n\n\nI‚Äôve also added the horizontal_position variable, so that I can compute the sums with a simple aggregate:\n\nday2_df[['depth', 'horizontal_position']].aggregate('sum')\n\ndepth                  992674\nhorizontal_position      1990\ndtype: int64"
  },
  {
    "objectID": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#day-3-binary-diagnostic",
    "href": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#day-3-binary-diagnostic",
    "title": "Advent of Code 2021: Days 1-5",
    "section": "Day 3: Binary Diagnostic",
    "text": "Day 3: Binary Diagnostic\n\nPart 1\n\nThe diagnostic report (your puzzle input) consists of a list of binary numbers which, when decoded properly, can tell you many useful things about the conditions of the submarine. The first parameter to check is the power consumption.\n\n\nYou need to use the binary numbers in the diagnostic report to generate two new binary numbers (called the gamma rate and the epsilon rate). The power consumption can then be found by multiplying the gamma rate by the epsilon rate.\n\n\nEach bit in the gamma rate can be determined by finding the most common bit in the corresponding position of all numbers in the diagnostic report.\n\n\nThe epsilon rate is calculated in a similar way; rather than use the most common bit, the least common bit from each position is used.\n\n\nUse the binary numbers in your diagnostic report to calculate the gamma rate and epsilon rate, then multiply them together. What is the power consumption of the submarine?\n\nImport the binary numbers:\n\nday3 &lt;- read_lines(\"day03-input.txt\")\nhead(day3)\n\n[1] \"001000010101\" \"010010111110\" \"001010110111\" \"001001011101\" \"001001010011\"\n[6] \"001111100111\"\n\n\nEach bit needs to be considered separately, so use strsplit like this:\n\nstrsplit(day3[1:2], split = \"\")\n\n[[1]]\n [1] \"0\" \"0\" \"1\" \"0\" \"0\" \"0\" \"0\" \"1\" \"0\" \"1\" \"0\" \"1\"\n\n[[2]]\n [1] \"0\" \"1\" \"0\" \"0\" \"1\" \"0\" \"1\" \"1\" \"1\" \"1\" \"1\" \"0\"\n\n\nSplit every binary number and put it into a tibble of integers:\n\nday3_split &lt;- strsplit(day3, split = \"\")\nday3_df &lt;- matrix(unlist(day3_split), ncol = 12, byrow = TRUE) %&gt;%\n  as_tibble(.name_repair = \"unique\") %&gt;%\n  mutate(across(everything(), as.integer))\n\nhead(day3_df)\n\n# A tibble: 6 √ó 12\n   ...1  ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 ...10 ...11 ...12\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     0     0     1     0     0     0     0     1     0     1     0     1\n2     0     1     0     0     1     0     1     1     1     1     1     0\n3     0     0     1     0     1     0     1     1     0     1     1     1\n4     0     0     1     0     0     1     0     1     1     1     0     1\n5     0     0     1     0     0     1     0     1     0     0     1     1\n6     0     0     1     1     1     1     1     0     0     1     1     1\n\n\nBefore computing the solution, and there any bits with an equal number of 0s and 1s?\n\nday3_df %&gt;%\n  summarise(across(everything(), mean)) %&gt;%\n  pivot_longer(everything(), names_to = \"bit\", values_to = \"prop\")\n\n# A tibble: 12 √ó 2\n   bit    prop\n   &lt;chr&gt; &lt;dbl&gt;\n 1 ...1  0.489\n 2 ...2  0.509\n 3 ...3  0.487\n 4 ...4  0.504\n 5 ...5  0.498\n 6 ...6  0.498\n 7 ...7  0.493\n 8 ...8  0.52 \n 9 ...9  0.493\n10 ...10 0.534\n11 ...11 0.486\n12 ...12 0.493\n\n\nNo, none the bits have prop = 0.500. Compute the gamma and epsilon rates:\n\n# There isn't a function available in base R to compute the mode of a vector,\n#  so define one here that takes the most frequent by default (freq_rank = 1)\nvector_mode &lt;- function(x, freq_rank = 1) {\n  # Frequency table\n  table(x) %&gt;%\n    # Sort it by count\n    sort(decreasing = TRUE) %&gt;%\n    # Get the labels of the counts\n    names() %&gt;%\n    pluck(freq_rank)\n}\n\nday3_rates &lt;- day3_df %&gt;%\n  pivot_longer(everything(), names_to = \"bit\", values_to = \"value\") %&gt;%\n  mutate(bit = as.integer(str_remove(bit, \"...\"))) %&gt;%\n  group_by(bit) %&gt;%\n  summarise(\n    gamma = vector_mode(value, freq_rank = 1),\n    epsilon = vector_mode(value, freq_rank = 2),\n    .groups = \"drop\"\n  ) %&gt;%\n  summarise(\n    # Collapse the most/least frequent values into a single string\n    across(c(gamma, epsilon), str_c, collapse = \"\")\n  )\nday3_rates\n\n# A tibble: 1 √ó 2\n  gamma        epsilon     \n  &lt;chr&gt;        &lt;chr&gt;       \n1 010100010100 101011101011\n\n\nWe now have the binary representations, which we convert using strtoi:\n\nday3_rates %&gt;%\n  mutate(across(c(gamma, epsilon), strtoi, base = 2),\n         prod = gamma * epsilon)\n\n# A tibble: 1 √ó 3\n  gamma epsilon    prod\n  &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1  1300    2795 3633500\n\n\nTo put this into a pandas DataFrame, use list comprehension to split the strings into characters:\n\nday3_df = pd.DataFrame([list(number) for number in r.day3]).astype('int32')\nday3_df\n\n     0   1   2   3   4   5   6   7   8   9   10  11\n0     0   0   1   0   0   0   0   1   0   1   0   1\n1     0   1   0   0   1   0   1   1   1   1   1   0\n2     0   0   1   0   1   0   1   1   0   1   1   1\n3     0   0   1   0   0   1   0   1   1   1   0   1\n4     0   0   1   0   0   1   0   1   0   0   1   1\n..   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n995   0   0   1   0   1   1   1   1   1   0   1   0\n996   1   0   0   1   1   0   0   1   0   0   1   1\n997   1   0   0   1   1   1   1   1   1   0   1   1\n998   0   0   0   0   1   0   0   1   0   0   1   1\n999   1   0   0   1   1   1   1   0   1   1   0   0\n\n[1000 rows x 12 columns]\n\n\nThen get gamma and epsilon rates:\n\ngamma = day3_df.aggregate('mode')\n# For epsilon rate, just swap the numbers\nepsilon = gamma.replace([0, 1], [1, 0])\n\n# Concatenate the bits into a single string\ngamma = gamma.apply(lambda row: ''.join(row.values.astype(str)), axis = 1)[0]\nepsilon = epsilon.apply(lambda row: ''.join(row.values.astype(str)), axis = 1)[0]\ngamma; epsilon\n\n'010100010100'\n'101011101011'\n\n\nFinally, use int() with base = 2 to convert to decimal:\n\nint(gamma, 2) * int(epsilon, 2)\n\n3633500\n\n\n\n\nPart 2\n\nNext, you should verify the life support rating, which can be determined by multiplying the oxygen generator rating by the CO2 scrubber rating.\n\n\nBoth the oxygen generator rating and the CO2 scrubber rating are values that can be found in your diagnostic report - finding them is the tricky part. Both values are located using a similar process that involves filtering out values until only one remains. Before searching for either rating value, start with the full list of binary numbers from your diagnostic report and consider just the first bit of those numbers. Then:\n\n\n\nKeep only numbers selected by the bit criteria for the type of rating value for which you are searching. Discard numbers which do not match the bit criteria.\nIf you only have one number left, stop; this is the rating value for which you are searching.\nOtherwise, repeat the process, considering the next bit to the right.\n\n\n\nThe bit criteria depends on which type of rating value you want to find:\n\nTo find oxygen generator rating, determine the most common value (0 or 1) in the current bit position, and keep only numbers with that bit in that position. If 0 and 1 are equally common, keep values with a 1 in the position being considered.\nTo find CO2 scrubber rating, determine the least common value (0 or 1) in the current bit position, and keep only numbers with that bit in that position. If 0 and 1 are equally common, keep values with a 0 in the position being considered.\n\n\nBefore doing anything, I need to alter my vector_mode function to deal with ties:\n\nvector_mode_part2 &lt;- function(x, freq_rank = 1) {\n  freq_table &lt;- table(x) %&gt;%\n    sort(decreasing = TRUE)\n  \n  # If there is a tie\n  if (freq_table[\"0\"] == freq_table[\"1\"]) {\n    # And we're looking for the most frequent (oxygen rating)\n    if (freq_rank == 1) {\n      # Then return 1\n      return(1) \n    } else {\n      # Otherwise return 0 (CO2 rating)\n      return(0)\n    }\n  # Otherwise, return the value from the table as usual\n  } else {\n    freq_table %&gt;%\n      names() %&gt;%\n      pluck(freq_rank) %&gt;%\n      as.integer()\n  }\n}\n\nThis definitely isn‚Äôt the most efficient way to implement the bit criteria, but an easy solution is to just filter bit-by-bit.\n\noxygen_rating &lt;- day3_df\nfor (bit in names(day3_df)) {\n  # If 1 number (row) remains, we have found the single oxygen rating\n  if (nrow(oxygen_rating) == 1) break\n  \n  most_freq &lt;- vector_mode_part2(oxygen_rating[[bit]])\n  oxygen_rating &lt;- oxygen_rating %&gt;%\n    filter(!!sym(bit) == most_freq)\n}\noxygen_rating\n\n# A tibble: 1 √ó 12\n   ...1  ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 ...10 ...11 ...12\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     0     1     0     1     0     0     1     0     1     1     1     1\n\n\n\nco2_rating &lt;- day3_df\nfor (bit in names(day3_df)) {\n  if (nrow(co2_rating) == 1) break\n  \n  least_freq &lt;- vector_mode_part2(co2_rating[[bit]], 2)\n  co2_rating &lt;- co2_rating %&gt;%\n    filter(!!sym(bit) == least_freq)\n}\nco2_rating\n\n# A tibble: 1 √ó 12\n   ...1  ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 ...10 ...11 ...12\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1     1     0     1     0     1     1     0     0     1     0     1\n\n\nConvert the binary representations and compute the product:\n\ntibble(\n  oxygen_rating = oxygen_rating %&gt;% str_c(collapse = \"\"),\n  co2_rating = co2_rating %&gt;% str_c(collapse = \"\")\n) %&gt;%\n  mutate(across(c(oxygen_rating, co2_rating), strtoi, base = 2),\n         prod = oxygen_rating * co2_rating)\n\n# A tibble: 1 √ó 3\n  oxygen_rating co2_rating    prod\n          &lt;int&gt;      &lt;int&gt;   &lt;int&gt;\n1          1327       3429 4550283\n\n\nIt is simple enough to reproduce those loops in Python:\n\noxygen_rating = day3_df\nfor bit in day3_df:\n  if len(oxygen_rating) == 1:\n    break\n  \n  bit_value_counts = oxygen_rating[bit].value_counts()\n  if bit_value_counts[1] &gt;= bit_value_counts[0]:\n    oxygen_rating = oxygen_rating[oxygen_rating[bit] == 1]\n  else:\n    oxygen_rating = oxygen_rating[oxygen_rating[bit] == 0]\n    \nco2_rating = day3_df\nfor bit in day3_df:\n  if len(co2_rating) == 1:\n    break\n  \n  bit_value_counts = co2_rating[bit].value_counts()\n  # In cases where there are no 0s or no 1s, need to fill with 0 \n  bit_value_counts = bit_value_counts.reindex([0, 1], fill_value = 0)\n  \n  if bit_value_counts[0] &lt;= bit_value_counts[1]:\n    co2_rating = co2_rating[co2_rating[bit] == 0]\n  else:\n    co2_rating = co2_rating[co2_rating[bit] == 1]\n   \n# In part 1, I used apply, here I'll use aggregate along axis = 1\nco2_rating = co2_rating.astype(str).aggregate(''.join, axis = 1).values[0]\noxygen_rating = oxygen_rating.astype(str).aggregate(''.join, axis = 1).values[0]\n\nint(co2_rating, 2) * int(oxygen_rating, 2)\n\n4550283"
  },
  {
    "objectID": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#day-4-giant-squid",
    "href": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#day-4-giant-squid",
    "title": "Advent of Code 2021: Days 1-5",
    "section": "Day 4: Giant Squid",
    "text": "Day 4: Giant Squid\n\nPart 1\n\nBingo is played on a set of boards each consisting of a 5x5 grid of numbers. Numbers are chosen at random, and the chosen number is marked on all boards on which it appears. (Numbers may not appear on all boards.) If all numbers in any row or any column of a board are marked, that board wins. (Diagonals don‚Äôt count.)\n\n\nThe submarine has a bingo subsystem to help passengers (currently, you and the giant squid) pass the time. It automatically generates a random order in which to draw numbers and a random set of boards (your puzzle input).\n\n\nThe score of the winning board can now be calculated. Start by finding the sum of all unmarked numbers on that board; in this case, the sum is 188. Then, multiply that sum by the number that was just called when the board won, 24, to get the final score, 188 * 24 = 4512.\n\n\nTo guarantee victory against the giant squid, figure out which board will win first. What will your final score be if you choose that board?\n\nImport the bingo input:\n\nday4 &lt;- read_lines(\"day04-input.txt\")\nprint(str_trunc(head(day4, 8), 50))\n\n[1] \"94,21,58,16,4,1,44,6,17,48,20,92,55,36,40,63,62...\"\n[2] \"\"                                                  \n[3] \"49 74 83 34 40\"                                    \n[4] \"87 16 57 75  3\"                                    \n[5] \"68 94 77 78 89\"                                    \n[6] \"56 38 29 26 60\"                                    \n[7] \"41 42 45 19  1\"                                    \n[8] \"\"                                                  \n\n\nThe data needs to be split up into the called numbers (at the top) and the boards. To do this, I‚Äôll use this trick with cumsum that I found on Stack Overflow:\n\nday4_split &lt;- split(\n  day4[day4 != \"\"],\n  cumsum(day4 == \"\")[day4 != \"\"]\n)\n\ncalled_numbers &lt;- day4_split[[1]]\nbingo_boards &lt;- day4_split[2:length(day4_split)]\nstr_trunc(called_numbers, 50); bingo_boards[1]\n\n[1] \"94,21,58,16,4,1,44,6,17,48,20,92,55,36,40,63,62...\"\n\n\n$`1`\n[1] \"49 74 83 34 40\" \"87 16 57 75  3\" \"68 94 77 78 89\" \"56 38 29 26 60\"\n[5] \"41 42 45 19  1\"\n\n\nNow I need to convert the called_numbers to a numeric vector, and the bingo_boards to numeric matrices:\n\ncalled_numbers &lt;- strsplit(called_numbers, \",\")[[1]] %&gt;% as.integer()\n\nbingo_boards &lt;- bingo_boards %&gt;%\n  map(\n    ~ {\n      .x %&gt;%\n        # str_squish replaces the double spaces before single digits numbers\n        #  with single spaces, so that we can properly strsplit by \" \"\n        str_squish() %&gt;%\n        str_split(\" \") %&gt;%\n        map(as.integer) %&gt;%\n        unlist() %&gt;%\n        matrix(nrow = 5, byrow = TRUE)\n    }\n  )\n\nhead(called_numbers); bingo_boards[1]\n\n[1] 94 21 58 16  4  1\n\n\n$`1`\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   49   74   83   34   40\n[2,]   87   16   57   75    3\n[3,]   68   94   77   78   89\n[4,]   56   38   29   26   60\n[5,]   41   42   45   19    1\n\n\nHere is my iteration strategy for identifying and marking called numbers (not evaluated, just a demonstration with one board and one number):\n\nbingo_board1 &lt;- bingo_boards[[1]]\ncalled_number1 &lt;- 49 # suppose 49 was called\n# Replace any 49s with -1\nbingo_board1[bingo_board1 == called_number1] &lt;- -1\n\n# Look for and row or column sums that = -5 (all values = -1)\nrow_sums1 &lt;- rowSums(bingo_board1)\ncol_sums1 &lt;- colSums(bingo_board1)\n\n# If we have bingo\nif (-5 %in% c(row_sums1, col_sums1)) {\n  # Compute the sum of the uncalled (non-negative) numbers\n  uncalled_sum &lt;- sum(bingo_board1[bingo_board1 &gt; 0])\n  # Return the product as the answer to the puzzle\n  called_number1 * uncalled_sum\n}\n\nNow put it into a loop over all numbers and boards:\n\nbingo_boards_part1 &lt;- bingo_boards\n\nfor (called_number in called_numbers) {\n  bingo_boards_part1 &lt;- map(\n    bingo_boards_part1,\n    ~{\n      .x[.x == called_number] &lt;- -1\n      .x\n    }\n  )\n  \n  # Find any winning boards\n  bingo_board_winner &lt;- map_lgl(\n    bingo_boards_part1,\n    ~{-5 %in% c(rowSums(.x), colSums(.x))}\n  )\n  \n  if (sum(bingo_board_winner) &gt; 0) {\n    bingo_board_final &lt;- bingo_boards_part1[bingo_board_winner] \n    break\n  }\n}\nbingo_board_final; called_number\n\n$`19`\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   93   -1   26   35   39\n[2,]   91   -1   85   69   -1\n[3,]   -1   -1   27   57   10\n[4,]   -1   -1   30   73   22\n[5,]   -1   -1   -1   -1    9\n\n\n[1] 7\n\n\nThen the solution is:\n\nsum(bingo_board_final[[1]][bingo_board_final[[1]] &gt; 0]) * called_number\n\n[1] 4662\n\n\nFor the Python solution, I‚Äôll practice my list comprehension to compile the bingo boards:\n\ncalled_numbers = [int(s) for s in r.day4[0].split(',')]\n\n# Find the indices of the '' characters separating the bingo boards\nbingo_boards_sep = [i for i,j in enumerate(r.day4) if j == '']\n# Compile a list of bingo boards\nbingo_boards = [r.day4[(i+1):(i+6)] for i in bingo_boards_sep]\n# For each row of each board, split the string into multiple values\nbingo_boards = [[board_row.split() for board_row in board] for board in bingo_boards]\n\nThat last line is a bit of a mess ‚Äì it is a nested list comprehension loop which iterates over boards and then iterates over rows of each board to split the string into single values ‚Äì but converting it all to numeric arrays is now simple:\n\nbingo_boards = [np.array(board).astype(int) for board in bingo_boards]\nbingo_boards[0]\n\narray([[49, 74, 83, 34, 40],\n       [87, 16, 57, 75,  3],\n       [68, 94, 77, 78, 89],\n       [56, 38, 29, 26, 60],\n       [41, 42, 45, 19,  1]])\n\n\nNow I can re-create the same loop from the R solution:\n\n# In Python, you use deepcopy() to make copies of nested structures like this\nimport copy\nbingo_boards_part1 = copy.deepcopy(bingo_boards)\n\nfor called_number in called_numbers:\n  # For each board, mark the called numbers as -1\n  for i,b in enumerate(bingo_boards_part1):\n    bingo_boards_part1[i][bingo_boards_part1[i] == called_number] = -1\n  \n  # Find winning boards\n  winners = [-5 in np.concatenate([board.sum(axis = 0), board.sum(axis = 1)]) \\\n             for board in bingo_boards_part1]\n  \n  if True in winners:\n    bingo_board_final = bingo_boards_part1[winners.index(True)]\n    break\n\nbingo_board_final[bingo_board_final &gt; 0].sum() * called_number\n\n4662\n\n\n\n\nPart 2\n\nOn the other hand, it might be wise to try a different strategy: let the giant squid win.\n\n\nYou aren‚Äôt sure how many bingo boards a giant squid could play at once, so rather than waste time counting its arms, the safe thing to do is to figure out which board will win last and choose that one. That way, no matter which boards it picks, it will win for sure.\n\n\nFigure out which board will win last. Once it wins, what would its final score be?\n\nSimple enough to alter the loop to iteratively remove winning boards until one remains:\n\nbingo_boards_part2 &lt;- bingo_boards\n\nfor (called_number in called_numbers) {\n  bingo_boards_part2 &lt;- map(\n    bingo_boards_part2,\n    ~{\n      .x[.x == called_number] &lt;- -1\n      .x\n    }\n  )\n  \n  # Find any winning boards\n  bingo_board_winner &lt;- map_lgl(\n    bingo_boards_part2,\n    ~{-5 %in% c(rowSums(.x), colSums(.x))}\n  )\n  \n  # If more than one board remains, remove winners\n  if (length(bingo_boards_part2) &gt; 1) {\n    bingo_boards_part2 &lt;- bingo_boards_part2[!bingo_board_winner]\n  } else {\n    # Otherwise, continue until the last board wins\n    if (sum(bingo_board_winner) &gt; 0) {\n      bingo_board_final &lt;- bingo_boards_part2[bingo_board_winner] \n      break\n    }\n  }\n}\nbingo_board_final; called_number\n\n$`14`\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   -1   -1   -1   -1   -1\n[2,]   -1   -1   -1   -1    0\n[3,]   10   38   -1   -1   25\n[4,]   -1   11   -1   -1   -1\n[5,]   -1   -1   67   -1   -1\n\n\n[1] 80\n\n\nAnd the product:\n\nsum(bingo_board_final[[1]][bingo_board_final[[1]] &gt; 0]) * called_number\n\n[1] 12080\n\n\nPython:\n\nbingo_boards_part2 = copy.deepcopy(bingo_boards)\n\nfor called_number in called_numbers:\n  for i,b in enumerate(bingo_boards_part2):\n    bingo_boards_part2[i][bingo_boards_part2[i] == called_number] = -1\n  \n  winners = [-5 in np.concatenate([board.sum(axis = 0), board.sum(axis = 1)]) \\\n             for board in bingo_boards_part2]\n  \n  # If more than one board remains, remove winners\n  if len(bingo_boards_part2) &gt; 1:\n    bingo_boards_part2 = [b for i,b in \\\n                          enumerate(bingo_boards_part2) if not winners[i]]\n  else:\n    if True in winners:\n      bingo_board_final = bingo_boards_part2[winners.index(True)]\n      break\n\nbingo_board_final[bingo_board_final &gt; 0].sum() * called_number\n\n12080"
  },
  {
    "objectID": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#day-5-hydrothermal-venture",
    "href": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#day-5-hydrothermal-venture",
    "title": "Advent of Code 2021: Days 1-5",
    "section": "Day 5: Hydrothermal Venture",
    "text": "Day 5: Hydrothermal Venture\n\nPart 1\n\nYou come across a field of hydrothermal vents on the ocean floor! These vents constantly produce large, opaque clouds, so it would be best to avoid them if possible. They tend to form in lines; the submarine helpfully produces a list of nearby lines of vents (your puzzle input) for you to review.\n\n\nEach line of vents is given as a line segment in the format x1,y1 -&gt; x2,y2 where x1,y1 are the coordinates of one end the line segment and x2,y2 are the coordinates of the other end. These line segments include the points at both ends. For now, only consider horizontal and vertical lines: lines where either x1 = x2 or y1 = y2.\n\n\nTo avoid the most dangerous areas, you need to determine the number of points where at least two lines overlap. Consider only horizontal and vertical lines. At how many points do at least two lines overlap?\n\nImport the lines:\n\nday5 &lt;- read_lines(\"day05-input.txt\")\nhead(day5)\n\n[1] \"223,805 -&gt; 223,548\" \"609,164 -&gt; 609,503\" \"461,552 -&gt; 796,552\"\n[4] \"207,361 -&gt; 207,34\"  \"503,879 -&gt; 503,946\" \"937,52 -&gt; 937,268\" \n\n\nI‚Äôll use a series of separates to get the numeric coordinates\n\nday5_df &lt;- tibble(x = day5) %&gt;%\n  separate(x, into = c(\"x1_y1\", \"x2_y2\"), sep = \" -&gt; \") %&gt;%\n  separate(x1_y1, into = c(\"x1\", \"y1\"), sep = \",\", convert = TRUE) %&gt;%\n  separate(x2_y2, into = c(\"x2\", \"y2\"), sep = \",\", convert = TRUE)\nhead(day5_df)\n\n# A tibble: 6 √ó 4\n     x1    y1    x2    y2\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1   223   805   223   548\n2   609   164   609   503\n3   461   552   796   552\n4   207   361   207    34\n5   503   879   503   946\n6   937    52   937   268\n\n\nGet the straight lines by looking for x1 == x2 or y1 == y2, then use crossing to get all the points touched by each line:\n\n# Only straight lines\nday5_straight &lt;- day5_df %&gt;%\n  filter((x1 == x2) | (y1 == y2)) %&gt;%\n  rowwise() %&gt;%\n  mutate(xy = list(crossing(x = x1:x2, y = y1:y2))) %&gt;%\n  ungroup()\n# As an example, show the first few points crossed by the first line\nday5_straight %&gt;%\n  slice(1) %&gt;%\n  unnest(xy) %&gt;%\n  head()\n\n# A tibble: 6 √ó 6\n     x1    y1    x2    y2     x     y\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1   223   805   223   548   223   548\n2   223   805   223   548   223   549\n3   223   805   223   548   223   550\n4   223   805   223   548   223   551\n5   223   805   223   548   223   552\n6   223   805   223   548   223   553\n\n\nNow to find the dangerous points, just need to look for any combinations of x and y that occur more than once:\n\nday5_straight %&gt;%\n  unnest(xy) %&gt;%\n  count(x, y) %&gt;%\n  summarise(dangerous_points = sum(n &gt; 1))\n\n# A tibble: 1 √ó 1\n  dangerous_points\n             &lt;int&gt;\n1             7142\n\n\nCreate the same data frame in a pandas DataFrame:\n\nday5_df = [[coord.split(',') for coord in line.split(' -&gt; ')] \\\n            for line in r.day5]\n# \"Flatten\" the lists so that each element has the four coordinates\nday5_df = [xy[0] + xy[1] for xy in day5_df]\n\nday5_df = pd.DataFrame(day5_df, columns = ['x1', 'y1', 'x2', 'y2']).astype(int)\nday5_df.head()\n\n    x1   y1   x2   y2\n0  223  805  223  548\n1  609  164  609  503\n2  461  552  796  552\n3  207  361  207   34\n4  503  879  503  946\n\n\nFind the straight lines with query:\n\nday5_straight = day5_df.query('(x1 == x2) | (y1 == y2)')\n\nI‚Äôm going to brute force a solution here with a for loop and a grid of values:\n\nocean_floor = np.zeros((1000, 1000))\n\nfor index, row in day5_straight.iterrows():\n  # Need to fix the range() step if going \"backwards\"\n  x_step = 1 if row.x1 &lt;= row.x2 else -1\n  y_step = 1 if row.y1 &lt;= row.y2 else -1\n  \n  for x in range(row.x1, row.x2 + x_step, x_step):\n    for y in range(row.y1, row.y2 + y_step, y_step):\n      ocean_floor[x, y] += 1\n      \nnp.count_nonzero(ocean_floor &gt; 1)\n\n7142\n\n\n\n\nPart 2\n\nUnfortunately, considering only horizontal and vertical lines doesn‚Äôt give you the full picture; you need to also consider diagonal lines. Because of the limits of the hydrothermal vent mapping system, the lines in your list will only ever be horizontal, vertical, or a diagonal line at exactly 45 degrees. Consider all of the lines. At how many points do at least two lines overlap?\n\nFind the diagonal line points:\n\nday5_diag &lt;- day5_df %&gt;%\n  filter(x1 != x2, y1 != y2) %&gt;%\n  rowwise() %&gt;%\n  mutate(x = list(x1:x2), y = list(y1:y2)) %&gt;%\n  ungroup()\n\nCombine the straight and diagonal lines and add up the points:\n\nbind_rows(\n  day5_straight %&gt;%  unnest(xy),\n  day5_diag %&gt;% unnest(c(x, y))\n) %&gt;%\n  count(x, y) %&gt;%\n  summarise(dangerous_points = sum(n &gt; 1))\n\n# A tibble: 1 √ó 1\n  dangerous_points\n             &lt;int&gt;\n1            20012\n\n\nFor the Python brute force solution, I can continue adding to the existing ocean_floor grid from part 1:\n\nday5_diag = day5_df.query('(x1 != x2) & (y1 != y2)')\n\nfor index, row in day5_diag.iterrows():\n  x_step = 1 if row.x1 &lt;= row.x2 else -1\n  y_step = 1 if row.y1 &lt;= row.y2 else -1\n  \n  for x, y in zip(range(row.x1, row.x2 + x_step, x_step),\n                  range(row.y1, row.y2 + y_step, y_step)):\n    ocean_floor[x, y] += 1\n      \nnp.count_nonzero(ocean_floor &gt; 1)\n\n20012"
  },
  {
    "objectID": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#stats",
    "href": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#stats",
    "title": "Advent of Code 2021: Days 1-5",
    "section": "Stats",
    "text": "Stats\nHere are my personal stats so far:\n\ntibble::tribble(\n  ~Part, ~Day, ~Time, ~Rank, ~Score,\n  1, 5, \"00:50:24\", 6542, 0,\n  2, 5, \"00:57:17\", 4865, 0,\n  1, 4, \"10:52:11\", 33771, 0,\n  2, 4, \"11:07:58\", 30829, 0,\n  1, 3, \"10:43:14\", 64952, 0,\n  2, 3, \"11:43:13\", 45788, 0,\n  1, 2, \"11:48:16\", 74444, 0,\n  2, 2, \"12:21:09\", 72356, 0,\n  1, 1, \"13:02:23\", 72332, 0,\n  2, 1, \"13:23:44\", 63804, 0\n) %&gt;%\n  pivot_wider(names_from = Part, values_from = c(Time, Rank, Score),\n              names_glue = \"Part {Part}_{.value}\") %&gt;%\n  mutate(\n    `Time between parts` = as.numeric(hms(`Part 2_Time`) - hms(`Part 1_Time`),\n                                      \"minutes\") %&gt;% round(1)\n  ) %&gt;%\n  gt() %&gt;%\n  tab_spanner_delim(delim = \"_\", split = \"first\")\n\n\n\n\n\n\n\n\nDay\nTime\nRank\nScore\nTime between parts\n\n\nPart 1\nPart 2\nPart 1\nPart 2\nPart 1\nPart 2\n\n\n\n\n5\n00:50:24\n00:57:17\n6542\n4865\n0\n0\n6.9\n\n\n4\n10:52:11\n11:07:58\n33771\n30829\n0\n0\n15.8\n\n\n3\n10:43:14\n11:43:13\n64952\n45788\n0\n0\n60.0\n\n\n2\n11:48:16\n12:21:09\n74444\n72356\n0\n0\n32.9\n\n\n1\n13:02:23\n13:23:44\n72332\n63804\n0\n0\n21.4\n\n\n\n\n\n\n\nExcept for day 5 (when I stayed up late because it was the weekend), I‚Äôve been completing the puzzles around lunch time on my break from work.\nThese 0 scores come from the global leaderboard, which only gives points to the first 100 users to finish, which I definitely won‚Äôt be doing. A better benchmark is the private leaderboard:\n\nlibrary(httr)\n\nleaderboard &lt;- httr::GET(\n  url = \"https://adventofcode.com/2021/leaderboard/private/view/1032765.json\",\n  httr::set_cookies(session = Sys.getenv(\"AOC_COOKIE\"))\n) %&gt;%\n  content() %&gt;%\n  as_tibble() %&gt;%\n  unnest_wider(members) %&gt;%\n  arrange(desc(local_score)) %&gt;%\n  transmute(\n    Rank = 1:n(), Name = name, Score = local_score, Stars = stars\n  )\n\n\nleaderboard %&gt;%\n  gt() %&gt;%\n  text_transform(\n    locations = cells_body(columns = Stars),\n    fn = function(stars_col) {\n      map_chr(stars_col,\n              ~ html(rep(fontawesome::fa('star', fill = 'gold'),\n                         times = as.integer(.x))))\n    }\n  ) %&gt;%\n  cols_align(\"left\") %&gt;%\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_body(\n      rows = (Name == \"taylordunn\")\n    )\n  ) %&gt;%\n  tab_options(container.height = 500)\n\n\n\n\n\n\n\n\nRank\nName\nScore\nStars\n\n\n\n\n1\nEmil Hvitfeldt\n1191\n         \n\n\n2\nColin Rundel\n1183\n         \n\n\n3\ntrang1618\n1170\n         \n\n\n4\n@_TanHo\n1154\n         \n\n\n5\nmkiang\n1126\n         \n\n\n6\ngpecci\n1112\n         \n\n\n7\nDavid Robinson\n1112\n         \n\n\n8\nJaros≈Çaw Nirski\n1110\n         \n\n\n9\ndhimmel\n1105\n         \n\n\n10\nhrushikeshrv\n1032\n         \n\n\n11\npritikadasgupta\n1028\n         \n\n\n12\nIldik√≥ Czeller\n1026\n         \n\n\n13\nSherry Zhang\n1023\n         \n\n\n14\njohn-b-edwards\n1014\n         \n\n\n15\n@_mnar99\n1012\n         \n\n\n16\nMelinda Tang\n984\n         \n\n\n17\nJosh Gray\n952\n         \n\n\n18\nJonathan Spring\n952\n         \n\n\n19\nDerek Holliday\n946\n         \n\n\n20\nJacqueline Nolis\n905\n       \n\n\n21\nAnna Fergusson\n901\n         \n\n\n22\nAndrew Argeros\n894\n         \n\n\n23\nZach Bogart üíô\n891\n         \n\n\n24\nexunckly\n876\n         \n\n\n25\nRiinu Pius\n869\n         \n\n\n26\nHannesOberreiter\n863\n         \n\n\n27\nTokhir Dadaev\n856\n         \n\n\n28\nashbaldry\n835\n         \n\n\n29\ncramosu\n831\n         \n\n\n30\nTom Jemmett\n795\n         \n\n\n31\nfabio machado\n791\n         \n\n\n32\n@Mid1995Sed\n766\n         \n\n\n33\nkarawoo\n761\n         \n\n\n34\nFlavien Petit\n756\n         \n\n\n35\nFarhan Reynaldo\n751\n         \n\n\n36\npatelis\n746\n         \n\n\n37\nArun Chavan\n746\n         \n\n\n38\n@woodspock\n740\n         \n\n\n39\ndelabj\n737\n         \n\n\n40\nMetaMoraleMundo\n715\n      \n\n\n41\nCalum You\n712\n         \n\n\n42\nDaniel Coulton\n708\n         \n\n\n43\nmbjoseph\n684\n         \n\n\n44\nAlex N\n681\n       \n\n\n45\nKT421\n680\n         \n\n\n46\njordi figueras puig\n680\n         \n\n\n47\npi55p00r\n671\n         \n\n\n48\nscalgary\n659\n         \n\n\n49\nErez Shomron\n654\n         \n\n\n50\nJim Leach\n649\n         \n\n\n51\nrywhale\n641\n         \n\n\n52\nMatt Onimus\n635\n         \n\n\n53\ncollinberke\n632\n       \n\n\n54\nGhislain Nono Gueye\n628\n       \n\n\n55\nldnam\n612\n     \n\n\n56\ntaylordunn\n606\n         \n\n\n57\njaapwalhout\n603\n         \n\n\n58\nJeffrey Brabec\n589\n        \n\n\n59\ndirkschumacher\n574\n         \n\n\n60\nAlbertRapp\n561\n         \n\n\n61\nNathan Moore\n556\n       \n\n\n62\nmfiorina\n553\n         \n\n\n63\nMiha Gazvoda\n553\n         \n\n\n64\nNerwosolek\n545\n        \n\n\n65\nTylerGrantSmith\n542\n         \n\n\n66\nDarrin Speegle\n540\n         \n\n\n67\nduju211\n525\n         \n\n\n68\nblongworth\n514\n        \n\n\n69\nSydney\n510\n       \n\n\n70\nKelly N. Bodwin\n508\n         \n\n\n71\nDavid Schoch\n503\n     \n\n\n72\nlong39ng\n489\n         \n\n\n73\nCarlssonLeo\n486\n       \n\n\n74\nA-Farina\n468\n         \n\n\n75\ncathblatter\n463\n       \n\n\n76\nScott-Gee\n445\n     \n\n\n77\nJulian Tagell\n434\n    \n\n\n78\nJosiah Parry\n419\n      \n\n\n79\nthedivtagguy\n400\n     \n\n\n80\njwinget\n397\n         \n\n\n81\nandrew-tungate-cms\n385\n     \n\n\n82\n@mfarkhann\n382\n       \n\n\n83\n@Maatspencer\n372\n       \n\n\n84\n@KentWeyrauch\n368\n       \n\n\n85\nAndrew Tungate\n365\n         \n\n\n86\nEmryn Hofmann\n359\n      \n\n\n87\ncolumbaspexit\n346\n       \n\n\n88\nALBERT\n343\n   \n\n\n89\nMaya Gans\n338\n      \n\n\n90\nAlan Feder\n314\n     \n\n\n91\nJenna Jordan\n306\n       \n\n\n92\nKevin Kent\n305\n      \n\n\n93\nolmgeorg\n296\n     \n\n\n94\nWendy Christensen\n291\n  \n\n\n95\nEric Ekholm\n287\n     \n\n\n96\nDaniel Gemara\n276\n   \n\n\n97\nAmitLevinson\n268\n   \n\n\n98\nquickcoffee\n258\n     \n\n\n99\ncynthiahqy\n235\n       \n\n\n100\nAndrew Fraser\n226\n  \n\n\n101\njennifer-furman\n223\n   \n\n\n102\nsoto solo\n222\n  \n\n\n103\nantdurrant\n211\n  \n\n\n104\nAdrian Perez\n196\n   \n\n\n105\nBilly Fryer\n186\n    \n\n\n106\nApril\n181\n \n\n\n107\nLukas Gr√∂ninger\n156\n   \n\n\n108\nJose Pliego San Martin\n113\n \n\n\n109\naleighbrown\n106\n \n\n\n110\nKyle Ligon\n105\n    \n\n\n111\nBruno Mioto\n85\n  \n\n\n112\nDuncan Gates\n68\n \n\n\n113\n@jdknguyen\n30\n  \n\n\n114\nMatthew Wankiewicz\n17\n\n\n\n115\nchapmandu2\n0\n\n\n\n116\nNA\n0\n\n\n\n117\nWiktor Jacaszek\n0\n\n\n\n118\njacquietran\n0\n\n\n\n119\nTony ElHabr\n0\n\n\n\n120\nRizky Luthfianto\n0\n\n\n\n121\nCaioBrighenti\n0\n\n\n\n\n\n\n\n\nCurrently at rank 56, so about middle of the pack."
  },
  {
    "objectID": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#reproducibility",
    "href": "posts/2021-12-01-advent-of-code-2021-days-1-5/index.html#reproducibility",
    "title": "Advent of Code 2021: Days 1-5",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\npython:         C:/Users/tdunn/Documents/.virtualenvs/r-reticulate/Scripts/python.exe\nlibpython:      C:/Users/tdunn/AppData/Local/r-reticulate/r-reticulate/pyenv/pyenv-win/versions/3.9.13/python39.dll\npythonhome:     C:/Users/tdunn/Documents/.virtualenvs/r-reticulate\nversion:        3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          C:/Users/tdunn/Documents/.virtualenvs/r-reticulate/Lib/site-packages/numpy\nnumpy_version:  1.23.3\n\nNOTE: Python version was forced by use_python function\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html",
    "href": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html",
    "title": "Advent of Code 2021: Days 6-10",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(lubridate)"
  },
  {
    "objectID": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#day-6-lanternfish",
    "href": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#day-6-lanternfish",
    "title": "Advent of Code 2021: Days 6-10",
    "section": "Day 6: Lanternfish",
    "text": "Day 6: Lanternfish\n\nday6 &lt;- read_lines(\"day06-input.txt\")\nstr_trunc(day6, 70)\n\n[1] \"5,1,4,1,5,1,1,5,4,4,4,4,5,1,2,2,1,3,4,1,1,5,1,5,2,2,2,2,1,4,2,4,3,3...\"\n\n\n\nPart 1\n\nA massive school of glowing lanternfish swims past. They must spawn quickly to reach such large numbers - maybe exponentially quickly? You should model their growth rate to be sure. Although you know nothing about this specific species of lanternfish, you make some guesses about their attributes. Surely, each lanternfish creates a new lanternfish once every 7 days. However, this process isn‚Äôt necessarily synchronized between every lanternfish - one lanternfish might have 2 days left until it creates another lanternfish, while another might have 4. So, you can model each fish as a single number that represents the number of days until it creates a new lanternfish. Furthermore, you reason, a new lanternfish would surely need slightly longer before it‚Äôs capable of producing more lanternfish: two more days for its first cycle.\n\n\nEach day, a 0 becomes a 6 and adds a new 8 to the end of the list, while each other number decreases by 1 if it was present at the start of the day. Find a way to simulate lanternfish. How many lanternfish would there be after 80 days?\n\nThis is simple enough to do with a vector and a loop over 80 days:\n\nfish_timers &lt;- strsplit(day6, \",\") %&gt;% unlist() %&gt;% as.numeric()\n\nfor (day in seq(1:80)) {\n  # Decrease each timer by one\n  fish_timers &lt;- map_dbl(fish_timers, ~{.x - 1})\n  \n  # Look for elapsed timers\n  elapsed_timers &lt;- fish_timers &lt; 0\n  # Reset them to 6\n  fish_timers[elapsed_timers] &lt;- 6\n  # Add new fish with timers starting at 8\n  fish_timers &lt;- c(fish_timers, rep(8, sum(elapsed_timers)))\n}\n\n# Count the number of fish after 80 days\nlength(fish_timers)\n\n[1] 355386\n\n\nFor Python, I‚Äôll use a list:\n\nfish_timers = [int(x) for x in r.day6.split(',')]\n\nfor day in range(80):\n  fish_timers = [f - 1 for f in fish_timers]\n  \n  n_new_fish = fish_timers.count(-1)\n  fish_timers = [6 if f == -1 else f for f in fish_timers]\n  \n  if n_new_fish &gt; 0:\n    fish_timers.extend([8] * n_new_fish)\n\nlen(fish_timers) \n\n355386\n\n\n\n\nPart 2\n\nSuppose the lanternfish live forever and have unlimited food and space. Would they take over the entire ocean? How many lanternfish would there be after 256 days?\n\nFor this part, the list of fish would quickly become too large to store in memory after 256 days. Instead, find the number of fish at each timer value:\n\nfish_timers_df &lt;- tibble(\n  timer = strsplit(day6, \",\") %&gt;% unlist() %&gt;% as.numeric()\n) %&gt;%\n  count(timer, name = \"n_fish\") %&gt;%\n  # Need to fill in the missing days with 0 fish\n  bind_rows(tibble(timer = c(0, 6, 7, 8), n_fish = c(0, 0, 0, 0))) %&gt;%\n  arrange(timer)\nfish_timers_df\n\n# A tibble: 9 √ó 2\n  timer n_fish\n  &lt;dbl&gt;  &lt;dbl&gt;\n1     0      0\n2     1     96\n3     2     54\n4     3     48\n5     4     51\n6     5     51\n7     6      0\n8     7      0\n9     8      0\n\n\nNow it is a matter of looping through the days and updating the counts:\n\nfor (day in 1:256) {\n  # Decrease the timer values by 1\n  fish_timers_df &lt;- fish_timers_df %&gt;% mutate(timer = timer - 1)\n  # Count new fish\n  new_fish &lt;- fish_timers_df$n_fish[fish_timers_df$timer == -1]\n  \n  # Add new fish with timers at 8 \n  fish_timers_df &lt;- fish_timers_df %&gt;%\n    bind_rows(tibble(timer = 8, n_fish = new_fish))\n \n  # Reset elapsed timers to 6\n  fish_timers_df$n_fish[fish_timers_df$timer == 6] &lt;-\n    fish_timers_df$n_fish[fish_timers_df$timer == 6] +\n    new_fish\n  \n  # Remove the -1 values now that they've been accounted for\n  fish_timers_df &lt;- fish_timers_df %&gt;% filter(timer != -1)\n}\n\nfish_timers_df\n\n# A tibble: 9 √ó 2\n  timer       n_fish\n  &lt;dbl&gt;        &lt;dbl&gt;\n1     0 141979186737\n2     1 163480438824\n3     2 172999320549\n4     3 189419502198\n5     4 211569179655\n6     5 220260612873\n7     6 255636828738\n8     7 118721109675\n9     8 139349146560\n\n\nThat‚Äôs a lot of fish. To get the answer, I need to format the sum so that it is not printed in scientific notation:\n\nformat(sum(fish_timers_df$n_fish), scientific = FALSE)\n\n[1] \"1613415325809\"\n\n\nPython:\n\nfish_timers_df = pd.DataFrame({'timer': [int(x) for x in r.day6.split(',')]})\nfish_timers_df = fish_timers_df.value_counts('timer') \\\n  .rename_axis('n_fish') \\\n  .reindex(range(9), fill_value = 0)\n  \nfor day in range(256):\n  n_new_fish = fish_timers_df[0]\n  \n  for i in range(1, len(fish_timers_df)):\n    fish_timers_df[i-1] = fish_timers_df[i]\n  \n  fish_timers_df[6] += n_new_fish\n  fish_timers_df[8] = n_new_fish\n  \nfish_timers_df.sum()\n\n1613415325809"
  },
  {
    "objectID": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#day-7-the-treachery-of-whales",
    "href": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#day-7-the-treachery-of-whales",
    "title": "Advent of Code 2021: Days 6-10",
    "section": "Day 7: The Treachery of Whales",
    "text": "Day 7: The Treachery of Whales\n\nday7 &lt;- read_lines(\"day07-input.txt\")\nstr_trunc(day7, 70)\n\n[1] \"1101,1,29,67,1102,0,1,65,1008,65,35,66,1005,66,28,1,67,65,20,4,0,10...\"\n\n\n\nPart 1\n\nA giant whale has decided your submarine is its next meal, and it‚Äôs much faster than you are. There‚Äôs nowhere to run!\nSuddenly, a swarm of crabs (each in its own tiny submarine - it‚Äôs too deep for them otherwise) zooms in to rescue you! They seem to be preparing to blast a hole in the ocean floor; sensors indicate a massive underground cave system just beyond where they‚Äôre aiming!\nThe crab submarines all need to be aligned before they‚Äôll have enough power to blast a large enough hole for your submarine to get through. However, it doesn‚Äôt look like they‚Äôll be aligned before the whale catches you! Maybe you can help?\nThere‚Äôs one major catch - crab submarines can only move horizontally.\nYou quickly make a list of the horizontal position of each crab (your puzzle input). Crab submarines have limited fuel, so you need to find a way to make all of their horizontal positions match while requiring them to spend as little fuel as possible.\nEach change of 1 step in horizontal position of a single crab costs 1 fuel. Determine the horizontal position that the crabs can align to using the least fuel possible. How much fuel must they spend to align to that position?\n\nThe amount of fuel spent by a crab \\(i\\) at position \\(x_i\\) moving to position \\(x_0\\) can be represented as the absolute deviation \\(|x_i - x_0|\\). We seek the position \\(x_0\\) that minimizes the sum of absolute deviations for all \\(N\\) crabs:\n\\[\n\\sum_i^N |x_i - x_0|\n\\]\nMy mathematical proof skills are rusty (it has been about 8 years since I took Mathematical Reasoning at UPEI), but Google tells me that the median minimizes the sum of absolute deviations. The median position of the input is:\n\ncrab_positions &lt;- strsplit(day7, \",\") %&gt;% unlist() %&gt;% as.numeric()\nmedian(crab_positions)\n\n[1] 331\n\n\nAnd so the total fuel taken for each crab to move to this position is:\n\nsum(abs(crab_positions - median(crab_positions)))\n\n[1] 349769\n\n\nPython:\n\ncrab_positions = [int(x) for x in r.day7.split(',')]\nsum([np.abs(x - np.median(crab_positions)) for x in crab_positions])\n\n349769.0\n\n\n\n\nPart 2\n\nThe crabs don‚Äôt seem interested in your proposed solution. Perhaps you misunderstand crab engineering?\nAs it turns out, crab submarine engines don‚Äôt burn fuel at a constant rate. Instead, each change of 1 step in horizontal position costs 1 more unit of fuel than the last: the first step costs 1, the second step costs 2, the third step costs 3, and so on.\nDetermine the horizontal position that the crabs can align to using the least fuel possible so they can make you an escape route! How much fuel must they spend to align to that position?\n\nMy first thought for calculating this fuel was the cumulative sum. For example, moving 7 steps would cost fuel equal to the last value of this cumsum():\n\ncumsum(1:7)\n\n[1]  1  3  6 10 15 21 28\n\n\nBut then I figured there is probably a useful formula for this sum, and another lazy Google search tells me that these are called triangular numbers with the simple formula:\n\\[\n\\sum_{i=1}^n k = \\frac{n (n + 1)}{2}.\n\\]\nAnd so we want to find \\(x_0\\) that minimizes:\n\\[\n\\sum_i^N \\frac{|x_i - x_0|(|x_i - x_0| + 1)}{2}.\n\\]\nI‚Äôm not aware of a simple solution (like the median in part 1) to this optimization problem. The brute force way is to loop over values of \\(x_0\\), ranging from \\(\\text{min}(x_i)\\) to \\(\\text{max}(x_i)\\):\n\nfuel_spent &lt;- tibble(\n  x0 = seq(min(crab_positions), max(crab_positions))\n) %&gt;%\n  mutate(\n    crab_dist = map(x0, ~abs(crab_positions - .x)),\n    fuel_spent = map_dbl(crab_dist, ~sum(.x * (.x + 1) / 2))\n  ) %&gt;%\n  arrange(fuel_spent)\nhead(fuel_spent)\n\n# A tibble: 6 √ó 3\n     x0 crab_dist     fuel_spent\n  &lt;int&gt; &lt;list&gt;             &lt;dbl&gt;\n1   479 &lt;dbl [1,000]&gt;   99540554\n2   480 &lt;dbl [1,000]&gt;   99540639\n3   478 &lt;dbl [1,000]&gt;   99541469\n4   481 &lt;dbl [1,000]&gt;   99541724\n5   477 &lt;dbl [1,000]&gt;   99543385\n6   482 &lt;dbl [1,000]&gt;   99543809\n\n\nWe find that \\(x_0\\) = 479 minimizes the fuel with 99540554 spent.\nUse a pandas.Series to do the same:\n\nfuel_spent = pd.Series(dtype = np.float64)\nfor x0 in range(min(crab_positions), max(crab_positions)):\n  crab_dist = [np.abs(x - x0) for x in crab_positions]\n  fuel_spent.at[x0] = sum([d * (d + 1) / 2 for d in crab_dist])\n  \nfuel_spent.sort_values().head()\n\n479    99540554.0\n480    99540639.0\n478    99541469.0\n481    99541724.0\n477    99543385.0\ndtype: float64"
  },
  {
    "objectID": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#day-8-seven-segment-search",
    "href": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#day-8-seven-segment-search",
    "title": "Advent of Code 2021: Days 6-10",
    "section": "Day 8: Seven Segment Search",
    "text": "Day 8: Seven Segment Search\n\nday8 &lt;- read_lines(\"day08-input.txt\")\nhead(day8)\n\n[1] \"cbgefad agc fdega cgdf ecdgfa efgca gaefbd edagbc cg ecafb | cdgafbe cfdg cg gac\"   \n[2] \"gcbdfe cgefb bgadcfe de cfabeg cbgade dbfe ced acfdg egdcf | de bedf ecdgf ecd\"     \n[3] \"dcafgb eagdfb bagdefc abg ba afbc bdgec cdgab gaedfc gadfc | abg ba agfdc gab\"      \n[4] \"agfbe fc fgbadec bdgcef gcdbe fbcead ebgcf cfe gdfc dcbega | edfabc efgab cdfg ecf\" \n[5] \"fcdeg cfbedg dcfeba fge fcdga ebdg cfebd edgcabf efbagc ge | cbdgef begcaf ge dbcfe\"\n[6] \"fceadbg ecbg cab cb bdfcea agfce cfaedg gadbf bfagc gceabf | cba cab bc bgce\"       \n\n\n\nPart 1\n\nYou barely reach the safety of the cave when the whale smashes into the cave mouth, collapsing it. Sensors indicate another exit to this cave at a much greater depth, so you have no choice but to press on.\nAs your submarine slowly makes its way through the cave system, you notice that the four-digit seven-segment displays in your submarine are malfunctioning; they must have been damaged during the escape. You‚Äôll be in a lot of trouble without them, so you‚Äôd better figure out what‚Äôs wrong.\nEach digit of a seven-segment display is rendered by turning on or off any of seven segments named a through g:\n\n  0:      1:      2:      3:      4:     5:      6:      7:      8:      9:\n aaaa    ....    aaaa    aaaa    ....   aaaa    aaaa    aaaa    aaaa    aaaa\nb    c  .    c  .    c  .    c  b    c b    .  b    .  .    c  b    c  b    c\nb    c  .    c  .    c  .    c  b    c b    .  b    .  .    c  b    c  b    c\n ....    ....    dddd    dddd    dddd  dddd    dddd    ....    dddd    dddd\ne    f  .    f  e    .  .    f  .    f .    f  e    f  .    f  e    f  .    f  \ne    f  .    f  e    .  .    f  .    f .    f  e    f  .    f  e    f  .    f\n gggg    ....    gggg    gggg    ....   gggg    gggg    ....    gggg    gggg\n\nSo, to render a 1, only segments c and f would be turned on; the rest would be off. To render a 7, only segments a, c, and f would be turned on.\nThe problem is that the signals which control the segments have been mixed up on each display. The submarine is still trying to display numbers by producing output on signal wires a through g, but those wires are connected to segments randomly. Worse, the wire/segment connections are mixed up separately for each four-digit display! (All of the digits within a display use the same connections, though.)\nFor each display, you watch the changing signals for a while, make a note of all ten unique signal patterns you see, and then write down a single four digit output value (your puzzle input). Using the signal patterns, you should be able to work out which pattern corresponds to which digit.\nEach entry consists of ten unique signal patterns, a | delimiter, and finally the four digit output value. Within an entry, the same wire/segment connections are used (but you don‚Äôt know what the connections actually are). The unique signal patterns correspond to the ten different ways the submarine tries to render a digit using the current wire/segment connections.\nIn the output values, how many times do digits 1, 4, 7, or 8 appear?\n\nFirst, for my own reference, compile the number of segments used by each digit:\n\ndigit_segments_count &lt;- tribble(\n  ~digit, ~n_segments,\n  0, 6,\n  1, 2,\n  2, 5,\n  3, 5,\n  4, 4,\n  5, 5,\n  6, 6,\n  7, 3,\n  8, 7,\n  9, 6\n)\ndigit_segments_count %&gt;%\n  group_by(n_segments) %&gt;%\n  summarise(digits = str_c(digit, collapse = \",\"), .groups = \"drop\")\n\n# A tibble: 6 √ó 2\n  n_segments digits\n       &lt;dbl&gt; &lt;chr&gt; \n1          2 1     \n2          3 7     \n3          4 4     \n4          5 2,3,5 \n5          6 0,6,9 \n6          7 8     \n\n\nI see why part 1 is asking about just the digits 1, 4, 7, and 8 ‚Äì they consist of a unique number of segments (2, 4, 3, and 7, respectively).\nSplit the input into the the signal patterns (ten unique values) and four digit output values:\n\nday8_split &lt;- strsplit(day8, \" \\\\| \") %&gt;%\n  map(strsplit, \" \")\n\nday8_df &lt;-\n  tibble(\n    signal_patterns = map(day8_split, 1),\n    output = map(day8_split, 2)\n  )\nday8_df\n\n# A tibble: 200 √ó 2\n   signal_patterns output   \n   &lt;list&gt;          &lt;list&gt;   \n 1 &lt;chr [10]&gt;      &lt;chr [4]&gt;\n 2 &lt;chr [10]&gt;      &lt;chr [4]&gt;\n 3 &lt;chr [10]&gt;      &lt;chr [4]&gt;\n 4 &lt;chr [10]&gt;      &lt;chr [4]&gt;\n 5 &lt;chr [10]&gt;      &lt;chr [4]&gt;\n 6 &lt;chr [10]&gt;      &lt;chr [4]&gt;\n 7 &lt;chr [10]&gt;      &lt;chr [4]&gt;\n 8 &lt;chr [10]&gt;      &lt;chr [4]&gt;\n 9 &lt;chr [10]&gt;      &lt;chr [4]&gt;\n10 &lt;chr [10]&gt;      &lt;chr [4]&gt;\n# ‚Ä¶ with 190 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nConsider a single row of this data:\n\nsignals1 &lt;- day8_df %&gt;% slice(2) %&gt;%\n  unnest(signal_patterns) %&gt;% pull(signal_patterns)\noutput1 &lt;- day8_df %&gt;% slice(2) %&gt;% unnest(output) %&gt;% pull(output)\nsignals1; output1\n\n [1] \"gcbdfe\"  \"cgefb\"   \"bgadcfe\" \"de\"      \"cfabeg\"  \"cbgade\"  \"dbfe\"   \n [8] \"ced\"     \"acfdg\"   \"egdcf\"  \n\n\n[1] \"de\"    \"bedf\"  \"ecdgf\" \"ecd\"  \n\n\nWe actually don‚Äôt need the 10 signal_patterns for part 1 ‚Äì we just need to find the string length of the output values:\n\nnchar(output1)\n\n[1] 2 4 5 3\n\n\nFrom these lengths, I know that the first digit is 1, the second digit is 4, the third digit is one of {2, 3, 5}, and the fourth digit is 7. We want to count the occurrences of 1, 4, 7 and 8, so this row contributes 3 to that count. Apply this logic to the full input:\n\npart1_count &lt;- day8_df %&gt;%\n  mutate(\n    n_1_4_7_8 = map_int(\n      output,\n      ~sum(nchar(.x) %in% c(2, 3, 4, 7))\n    )\n  )\npart1_count\n\n# A tibble: 200 √ó 3\n   signal_patterns output    n_1_4_7_8\n   &lt;list&gt;          &lt;list&gt;        &lt;int&gt;\n 1 &lt;chr [10]&gt;      &lt;chr [4]&gt;         4\n 2 &lt;chr [10]&gt;      &lt;chr [4]&gt;         3\n 3 &lt;chr [10]&gt;      &lt;chr [4]&gt;         3\n 4 &lt;chr [10]&gt;      &lt;chr [4]&gt;         2\n 5 &lt;chr [10]&gt;      &lt;chr [4]&gt;         1\n 6 &lt;chr [10]&gt;      &lt;chr [4]&gt;         4\n 7 &lt;chr [10]&gt;      &lt;chr [4]&gt;         3\n 8 &lt;chr [10]&gt;      &lt;chr [4]&gt;         2\n 9 &lt;chr [10]&gt;      &lt;chr [4]&gt;         2\n10 &lt;chr [10]&gt;      &lt;chr [4]&gt;         4\n# ‚Ä¶ with 190 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\nsum(part1_count$n_1_4_7_8)\n\n[1] 514\n\n\n\n\nPart 2\n\nFor each entry, determine all of the wire/segment connections and decode the four-digit output values. What do you get if you add up all of the output values?\n\nConsider again the segment patterns for each digit:\n  0:      1:      2:      3:      4:     5:      6:      7:      8:      9:\n aaaa    ....    aaaa    aaaa    ....   aaaa    aaaa    aaaa    aaaa    aaaa\nb    c  .    c  .    c  .    c  b    c b    .  b    .  .    c  b    c  b    c\nb    c  .    c  .    c  .    c  b    c b    .  b    .  .    c  b    c  b    c\n ....    ....    dddd    dddd    dddd  dddd    dddd    ....    dddd    dddd\ne    f  .    f  e    .  .    f  .    f .    f  e    f  .    f  e    f  .    f  \ne    f  .    f  e    .  .    f  .    f .    f  e    f  .    f  e    f  .    f\n gggg    ....    gggg    gggg    ....   gggg    gggg    ....    gggg    gggg\n\ndigit_segments &lt;- c(\"abcefg\", \"cf\", \"acdeg\", \"acdfg\", \"bcdf\",\n                    \"abdfg\", \"abdefg\", \"acf\", \"abcdefg\", \"abcdfg\")\n\nWe know digits 1, 4, 7 and 8 are uniquely identified by their number of segments. We also know that the digit 1 has two overlapping segments (c and f) with the digit 0. This can be derived with intersect on the letters:\n\ncompute_overlap &lt;- function(segments1, segments2) {\n  length(\n    intersect(strsplit(segments1, \"\") %&gt;% unlist(),\n              strsplit(segments2, \"\") %&gt;% unlist())\n  )\n}\ncompute_overlap(digit_segments[1], digit_segments[2])\n\n[1] 2\n\n\nCompute the amount of overlap between each pair of digits:\n\ndigit_overlap &lt;- \n  crossing(\n    tibble(digit1 = 0:9, segments1 = digit_segments),\n    tibble(digit2 = 0:9, segments2 = digit_segments)\n  ) %&gt;%\n  filter(digit1 != digit2) %&gt;%\n  mutate(\n    n_segment_overlap = map2_int(\n      segments1, segments2,\n      compute_overlap\n    )\n  )\ndigit_overlap %&gt;%\n  select(digit1, digit2, n_segment_overlap) %&gt;%\n  pivot_wider(names_from = digit2, values_from = n_segment_overlap) %&gt;%\n  relocate(`0`, .before = `1`) %&gt;% # Need to correct order of columns\n  gt(rowname_col = \"digit1\") %&gt;%\n  sub_missing(everything(), missing_text = \"\")\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n\n2\n4\n4\n3\n4\n5\n3\n6\n5\n\n\n1\n2\n\n1\n2\n2\n1\n1\n2\n2\n2\n\n\n2\n4\n1\n\n4\n2\n3\n4\n2\n5\n4\n\n\n3\n4\n2\n4\n\n3\n4\n4\n3\n5\n5\n\n\n4\n3\n2\n2\n3\n\n3\n3\n2\n4\n4\n\n\n5\n4\n1\n3\n4\n3\n\n5\n2\n5\n5\n\n\n6\n5\n1\n4\n4\n3\n5\n\n2\n6\n5\n\n\n7\n3\n2\n2\n3\n2\n2\n2\n\n3\n3\n\n\n8\n6\n2\n5\n5\n4\n5\n6\n3\n\n6\n\n\n9\n5\n2\n4\n5\n4\n5\n5\n3\n6\n\n\n\n\n\n\n\n\nUsing these relationships, it is simply a matter of elimination to determine which pattern of segments corresponds to which digit:\n\nThe digit 1 has 2 segments.\nThe digit 4 has 4 segments.\nThe digit 7 has 3 segments.\nThe digit 8 has 7 segments.\nThe digits 2, 3 and 5 have 5 segments.\n\nThe digit 3 shares 2 segments with 1.\nThe digit 2 shares 2 segments with 4.\nThe digit 5 remains.\n\nThe digits 0, 6 and 9 have 6 segments.\n\nThe digit 6 shares 2 segments with 7.\nThe digit 9 shares 4 segments with 4.\nThe digit 0 remains.\n\n\nWrite a function that follows this logic, and returns a list mapping segment patterns to digits:\n\nget_digit_map &lt;- function(signals) {\n  digit_map &lt;- list()\n  \n  d1 &lt;- signals[nchar(signals) == 2]\n  digit_map[d1] &lt;- 1\n  \n  d4 &lt;- signals[nchar(signals) == 4]\n  digit_map[d4] &lt;- 4\n  \n  d7 &lt;- signals[nchar(signals) == 3]\n  digit_map[d7] &lt;- 7\n  \n  d8 &lt;- signals[nchar(signals) == 7]\n  digit_map[d8] &lt;- 8\n  \n  # Remove the four digits identified so far\n  signals &lt;- setdiff(signals, names(digit_map))\n  \n  d3 &lt;- signals[nchar(signals) == 5 & map_int(signals, compute_overlap, d1) == 2]\n  digit_map[d3] &lt;- 3\n  signals &lt;- setdiff(signals, d3)\n  \n  d2 &lt;- signals[nchar(signals) == 5 & map_int(signals, compute_overlap, d4) == 2]\n  digit_map[d2] &lt;- 2\n  signals &lt;- setdiff(signals, d2)\n  \n  d5 &lt;- signals[nchar(signals) == 5]\n  digit_map[d5] &lt;- 5\n  signals &lt;- setdiff(signals, d5)\n  \n  d6 &lt;- signals[nchar(signals) == 6 & map_int(signals, compute_overlap, d7) == 2]\n  digit_map[d6] &lt;- 6\n  signals &lt;- setdiff(signals, d6)\n  \n  d9 &lt;- signals[nchar(signals) == 6 & map_int(signals, compute_overlap, d4) == 4]\n  digit_map[d9] &lt;- 9\n  signals &lt;- setdiff(signals, d9)\n  \n  # The last digit is 0\n  digit_map[signals] &lt;- 0\n  \n  return(digit_map)\n}\n\nOne last step before applying this function is the sort the segments alphabetically, because the output does not necessarily match the patterns in the signals, e.g.¬†‚Äúbedf‚Äù may appear as ‚Äúdbfe‚Äù in the output.\n\nsort_segment &lt;- function(segment) {\n  strsplit(segment, \"\")[[1]] %&gt;%\n    sort() %&gt;%\n    paste0(collapse = \"\")\n}\n\nday8_df &lt;- day8_df %&gt;%\n  mutate(\n    signal_patterns = map(\n      signal_patterns, ~map_chr(.x, sort_segment)\n    ),\n    output = map(\n      output, ~map_chr(.x, sort_segment)\n    )\n  )\nday8_df$signal_patterns[[5]]\n\n [1] \"cdefg\"   \"bcdefg\"  \"abcdef\"  \"efg\"     \"acdfg\"   \"bdeg\"    \"bcdef\"  \n [8] \"abcdefg\" \"abcefg\"  \"eg\"     \n\n\nNow find the digit mapping for each set of signals:\n\nday8_df &lt;- day8_df %&gt;%\n  mutate(digit_map = map(signal_patterns, get_digit_map))\nday8_df$digit_map[[5]] %&gt;% glimpse()\n\nList of 10\n $ eg     : num 1\n $ bdeg   : num 4\n $ efg    : num 7\n $ abcdefg: num 8\n $ cdefg  : num 3\n $ acdfg  : num 2\n $ bcdef  : num 5\n $ abcdef : num 6\n $ bcdefg : num 9\n $ abcefg : num 0\n\n\nNow use each digit_map to determine the 4 digit output codes:\n\nday8_df &lt;- day8_df %&gt;%\n  mutate(\n    output_decoded = map2_int(\n      output, digit_map,\n      ~as.integer(paste0(.y[.x], collapse = \"\"))\n    )\n  )\nday8_df\n\n# A tibble: 200 √ó 4\n   signal_patterns output    digit_map         output_decoded\n   &lt;list&gt;          &lt;list&gt;    &lt;list&gt;                     &lt;int&gt;\n 1 &lt;chr [10]&gt;      &lt;chr [4]&gt; &lt;named list [10]&gt;           8417\n 2 &lt;chr [10]&gt;      &lt;chr [4]&gt; &lt;named list [10]&gt;           1437\n 3 &lt;chr [10]&gt;      &lt;chr [4]&gt; &lt;named list [10]&gt;           7157\n 4 &lt;chr [10]&gt;      &lt;chr [4]&gt; &lt;named list [10]&gt;            247\n 5 &lt;chr [10]&gt;      &lt;chr [4]&gt; &lt;named list [10]&gt;           9015\n 6 &lt;chr [10]&gt;      &lt;chr [4]&gt; &lt;named list [10]&gt;           7714\n 7 &lt;chr [10]&gt;      &lt;chr [4]&gt; &lt;named list [10]&gt;           7764\n 8 &lt;chr [10]&gt;      &lt;chr [4]&gt; &lt;named list [10]&gt;           1425\n 9 &lt;chr [10]&gt;      &lt;chr [4]&gt; &lt;named list [10]&gt;           9677\n10 &lt;chr [10]&gt;      &lt;chr [4]&gt; &lt;named list [10]&gt;           4784\n# ‚Ä¶ with 190 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nAnd finally, the sum:\n\nsum(day8_df$output_decoded)\n\n[1] 1012272"
  },
  {
    "objectID": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#day-9-smoke-basin",
    "href": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#day-9-smoke-basin",
    "title": "Advent of Code 2021: Days 6-10",
    "section": "Day 9: Smoke Basin",
    "text": "Day 9: Smoke Basin\n\nday9 &lt;- read_lines(\"day09-input.txt\")\nhead(day9) %&gt;% str_trunc(70)\n\n[1] \"4210129998765678999876598999987654567899891099876534567899989543458...\"\n[2] \"4321298987674567998965467898996543478998789987654323478998775412367...\"\n[3] \"5734987794543678987654356987987656567897678998943214589987654323456...\"\n[4] \"7649876543212347898765567896598767678976567899876323678998775454868...\"\n[5] \"8756989652103456999898678966459878989785456989965456789999976565699...\"\n[6] \"9897898773212367895939989656346989799654347778976587997896989876989...\"\n\n\n\nPart 1\n\nThese caves seem to be lava tubes. Parts are even still volcanically active; small hydrothermal vents release smoke into the caves that slowly settles like rain.\nIf you can model how the smoke flows through the caves, you might be able to avoid it and be that much safer. The submarine generates a heightmap of the floor of the nearby caves for you (your puzzle input). Smoke flows to the lowest point of the area it‚Äôs in. Each number corresponds to the height of a particular location, where 9 is the highest and 0 is the lowest a location can be.\nYour first goal is to find the low points - the locations that are lower than any of its adjacent locations. Most locations have four adjacent locations (up, down, left, and right); locations on the edge or corner of the map have three or two adjacent locations, respectively. (Diagonal locations do not count as adjacent.)\nThe risk level of a low point is 1 plus its height. Find all of the low points on your heightmap. What is the sum of the risk levels of all low points on your heightmap?\n\nI‚Äôll use a base R matrix to represent the height map of the cave floor:\n\n# Split the columns and convert to integers\nheight_map &lt;- day9 %&gt;% map(~unlist(strsplit(.x, \"\")) %&gt;% as.integer)\n# Convert it to a matrix\nheight_map &lt;- matrix(unlist(height_map), nrow = length(height_map), byrow = TRUE)\n\nheight_map[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    4    2    1    0    1    2    9    9    9     8\n [2,]    4    3    2    1    2    9    8    9    8     7\n [3,]    5    7    3    4    9    8    7    7    9     4\n [4,]    7    6    4    9    8    7    6    5    4     3\n [5,]    8    7    5    6    9    8    9    6    5     2\n [6,]    9    8    9    7    8    9    8    7    7     3\n [7,]    5    9    9    8    9    9    9    7    6     5\n [8,]    4    5    6    9    8    9    9    8    7     5\n [9,]    3    4    5    6    7    8    9    9    7     6\n[10,]    2    3    4    8    9    9    9    9    8     7\n\n\nI‚Äôll also define a helper function that finds neighboring points (while still within the bounds of the matrix):\n\nget_neighbor_coords &lt;- function(row, col, max_row, max_col) {\n  neighbor_coords &lt;- cbind(row + c(-1, 1, 0, 0),\n                           col + c(0, 0, -1, 1))\n  \n  in_bounds &lt;- (neighbor_coords[,1] &lt;= max_row) & (neighbor_coords[,1] &gt; 0) &\n    (neighbor_coords[,2] &lt;= max_col) & (neighbor_coords[,2] &gt; 0)\n  neighbor_coords[in_bounds, , drop = FALSE]\n}\n\nLoop over all points and determine if it is a low point:\n\nlow_point_heights &lt;- c()\nfor (row in 1:nrow(height_map)) {\n  for (col in 1:ncol(height_map)) {\n    height &lt;- height_map[row, col]\n    \n    # Get adjacent points\n    neighbor_coords &lt;- get_neighbor_coords(row, col,\n                                           nrow(height_map), ncol(height_map))\n    neighbor_heights &lt;- height_map[neighbor_coords]\n    \n    # Is it the lowest point?\n    if (all(height &lt; neighbor_heights)) {\n      low_point_heights &lt;- c(low_point_heights, height)\n    } \n  }\n}\nlow_point_heights\n\n  [1] 0 0 0 0 1 3 1 3 1 0 2 2 2 0 1 1 0 0 2 1 0 2 0 0 0 1 3 0 6 2 2 1 1 0 2 1 1\n [38] 0 3 1 0 3 0 1 1 0 0 0 4 1 5 0 0 3 2 0 0 2 0 3 1 0 2 0 1 2 0 0 0 2 2 1 1 5\n [75] 2 1 0 2 0 1 1 4 1 2 2 1 0 2 1 1 1 0 0 0 2 0 0 2 1 0 0 1 0 4 0 0 0 0 1 5 1\n[112] 4 2 2 0 4 2 2 0 1 0 1 0 0 0 1 1 0 3 3 1 1 3 3 0 1 0 2 1 1 0 0 3 0 0 0 1 0\n[149] 3 2 1 0 0 1 1 0 0 0 0 0 2 0 0 1 0 0 3 1 0 0 1 3 0 1 0 0 0 0 1 0 2 2 1 1 4\n[186] 0 0 3 1 1 6 5 1 0 1 0 0 0\n\n\nThen add 1 to get the risk level and sum it up:\n\nsum(low_point_heights + 1)\n\n[1] 417\n\n\nIn Python, represent the height map with a 2d numpy.array:\n\nheight_map = np.array([[height for height in row] for row in r.day9],\n                      dtype = int)\nheight_map[0:9, 0:9]\n\narray([[4, 2, 1, 0, 1, 2, 9, 9, 9],\n       [4, 3, 2, 1, 2, 9, 8, 9, 8],\n       [5, 7, 3, 4, 9, 8, 7, 7, 9],\n       [7, 6, 4, 9, 8, 7, 6, 5, 4],\n       [8, 7, 5, 6, 9, 8, 9, 6, 5],\n       [9, 8, 9, 7, 8, 9, 8, 7, 7],\n       [5, 9, 9, 8, 9, 9, 9, 7, 6],\n       [4, 5, 6, 9, 8, 9, 9, 8, 7],\n       [3, 4, 5, 6, 7, 8, 9, 9, 7]])\n\n\nThen apply the same logic to get the sum of risk levels:\n\ndef get_neighbor_coords(row, col, max_row, max_col):\n  neighbor_coords = [[row + d for d in [-1, 1, 0, 0]],\n                     [col + d for d in [0, 0, -1, 1]]]\n  neighbor_coords = np.array(neighbor_coords).T\n  \n  in_bounds = (neighbor_coords[:, 0] &gt;= 0) & \\\n    (neighbor_coords[:, 0] &lt; max_row) & \\\n    (neighbor_coords[:, 1] &gt;= 0) & \\\n    (neighbor_coords[:, 1] &lt; max_col)\n    \n  return(neighbor_coords[in_bounds, :])\n\nlow_point_heights = []\n\nfor row in range(height_map.shape[0]):\n  for col in range(height_map.shape[1]):\n    height = height_map[row, col]\n    \n    neighbor_coords = get_neighbor_coords(row, col,\n                                          height_map.shape[0],\n                                          height_map.shape[1])\n    neighbor_heights = [height_map[nc[0], nc[1]] for nc in neighbor_coords]\n    \n    if all([height &lt; nh for nh in neighbor_heights]):\n      low_point_heights.append(height)\n    \nsum([h + 1 for h in low_point_heights])\n\n417\n\n\n\n\nPart 2\n\nNext, you need to find the largest basins so you know what areas are most important to avoid.\nA basin is all locations that eventually flow downward to a single low point. Therefore, every low point has a basin, although some basins are very small. Locations of height 9 do not count as being in any basin, and all other locations will always be part of exactly one basin.\nThe size of a basin is the number of locations within the basin, including the low point. The example above has four basins.\nWhat do you get if you multiply together the sizes of the three largest basins?\n\nMy strategy for part 2 is:\n\nLoop over each point.\n\nIf that point has a height &lt;9, start a new basin.\nRewrite that point with 9 (to prevent overcounting).\nFor each of that points neighbors:\n\nIf that point has a height &lt;9, add it to the basin.\nRe-write that point with height 9.\nAdd the points neighbors to a new list.\n\nRepeat the above loop with new neighbors until no neighbors remain.\nCompute the number of points in the basin.\n\n\nBasically, I‚Äôm finding points that are not 9, spreading out from those points, filling the basins with 9s, and counting those points along the way to get the basin size.\n\nbasin_size &lt;- c()\n\nfor (row in 1:nrow(height_map)) {\n  for (col in 1:ncol(height_map)) {\n    height &lt;- height_map[row, col]\n    if (height &lt; 9) {\n      basin_points &lt;- c(height)\n      height_map[row, col] &lt;- 9\n      \n      neighbor_coords &lt;- get_neighbor_coords(row, col,\n                                             nrow(height_map), ncol(height_map)) \n      \n      while (nrow(neighbor_coords) &gt; 0) {\n        new_neighbor_coords &lt;- matrix(nrow = 0, ncol = 2)\n        for (neighbor in 1:nrow(neighbor_coords)) {\n          nc &lt;- neighbor_coords[neighbor, , drop = FALSE]\n          height &lt;- height_map[nc]\n          if (height &lt; 9) {\n            basin_points &lt;- c(basin_points, height)\n            height_map[nc] &lt;- 9\n            \n            new_neighbor_coords &lt;- rbind(\n              new_neighbor_coords,\n              get_neighbor_coords(nc[1], nc[2],\n                                  nrow(height_map), ncol(height_map))\n            )\n          }\n        }\n        neighbor_coords &lt;- new_neighbor_coords\n      }\n      basin_size &lt;- c(basin_size, length(basin_points))\n    }\n  }\n}\n\nbasin_size\n\n  [1]  26  96  34  37  64   2  58  52   2  53  32  54  55  60   4   6  68  39\n [19]  14  69  30  22   5  21  33  64  38  96  45  21  15  29  16  89   3   7\n [37]  55  39   7  32   3  85  74  41  60  23  18  84   4  32  15  97   8   8\n [55]  50  15  24  66  20  68  45  84  42  67  68  70  10  16  49  63   4  19\n [73]   2  31  15  51   9  22  52  36   3  28  13   2  79  18  12   9  68   2\n [91]  83   4  39  95  75 115  41   7  79   2   4  27  85  26  25  87  16   8\n[109]   5  62  48   8  22  59  39   2  25  61  89  25  69  31  22  40   7  73\n[127]   8   4  38  50  23  57  30  18  41  92   4  69   2  48  86  20  52  94\n[145]  29  36   7  30  37  36  75  84  91  24  26   4  16 103  93  74  44   5\n[163]  31   6  51   5  15  83  93  73  18  94  22  61  72   8   8  53  53  45\n[181]  40  55   6  48  33  32  20   9   3  18  14  20  19   7   4   3   4   2\n\n\nThe product of the three largest basins:\n\nsort(basin_size, decreasing = TRUE)[1:3] %&gt;% prod()\n\n[1] 1148965\n\n\nAnd in Python:\n\nbasin_size = []\n\nfor row in range(height_map.shape[0]):\n  for col in range(height_map.shape[1]):\n    height = height_map[row, col]\n    \n    if height &lt; 9:\n      basin_points = [height]\n      height_map[row, col] = 9\n      \n      neighbor_coords = get_neighbor_coords(row, col,\n                                            height_map.shape[0],\n                                            height_map.shape[1])\n      while len(neighbor_coords) &gt; 0:\n        new_neighbor_coords = np.empty((0, 2), dtype = int)\n        \n        for nc in neighbor_coords:\n          height = height_map[nc[0], nc[1]]\n          if height &lt; 9:\n            basin_points.append(height)\n            height_map[nc[0], nc[1]] = 9\n            \n            new_neighbor_coords = np.append(\n              new_neighbor_coords,\n              get_neighbor_coords(nc[0], nc[1],\n                                  height_map.shape[0], height_map.shape[1]),\n              axis = 0\n            )\n            \n        neighbor_coords = new_neighbor_coords \n        \n      basin_size.append(len(basin_points))\n\nbasin_size.sort(reverse = True)\nnp.prod(basin_size[0:3])\n\n1148965"
  },
  {
    "objectID": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#day-10-syntax-scoring",
    "href": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#day-10-syntax-scoring",
    "title": "Advent of Code 2021: Days 6-10",
    "section": "Day 10: Syntax Scoring",
    "text": "Day 10: Syntax Scoring\n\nday10 &lt;- read_lines(\"day10-input.txt\")\nhead(day10) %&gt;% str_trunc(70)\n\n[1] \"(&lt;{([{([[[&lt;{(&lt;(){}&gt;({}()))}[(&lt;[]&lt;&gt;&gt;{&lt;&gt;})[{{}()}&lt;{}[]&gt;]]&gt;&lt;[[&lt;{}{}&gt;([...\"\n[2] \"&lt;[&lt;&lt;{&lt;[(([[[[&lt;[][]&gt;][(&lt;&gt;()){&lt;&gt;()}]]][&lt;{&lt;&lt;&gt;{}&gt;(()&lt;&gt;)}&gt;&lt;([&lt;&gt;{}]{()()}...\"\n[3] \"&lt;(({(&lt;{&lt;({[&lt;[[[]()][&lt;&gt;[]]]{{{}()}({}[])}&gt;]{[{&lt;&lt;&gt;[]&gt;[()[]]}][[[{}[]]...\"\n[4] \"{{[{[[[{{{{{[([]())(&lt;&gt;())](&lt;[]&lt;&gt;&gt;{[]{}})}([[(){}]&lt;&lt;&gt;()&gt;]&lt;&lt;()()&gt;&lt;{}{...\"\n[5] \"({([({([[({([{[]&lt;&gt;}]&lt;&lt;&lt;&gt;{}&gt;[&lt;&gt;[]]&gt;)})({{{[()[]]&lt;()&gt;}(([]&lt;&gt;)&lt;[]()&gt;&gt;}...\"\n[6] \"&lt;(((&lt;&lt;&lt;{([[[&lt;{()&lt;&gt;}{{}()}&gt;[({}())([][])]]{{(()())(&lt;&gt;{})}(&lt;{}&gt;&lt;[]{}&gt;...\"\n\n\n\nPart 1\n\nYou ask the submarine to determine the best route out of the deep-sea cave, but it only replies: Syntax error in navigation subsystem on line: all of them All of them?! The damage is worse than you thought. You bring up a copy of the navigation subsystem (your puzzle input).\nThe navigation subsystem syntax is made of several lines containing chunks. There are one or more chunks on each line, and chunks contain zero or more other chunks. Adjacent chunks are not separated by any delimiter; if one chunk stops, the next chunk (if any) can immediately start. Every chunk must open and close with one of four legal pairs of matching characters:\nIf a chunk opens with (, it must close with ). If a chunk opens with [, it must close with ]. If a chunk opens with {, it must close with }. If a chunk opens with &lt;, it must close with &gt;.\nSo, () is a legal chunk that contains no other chunks, as is []. More complex but valid chunks include ([]), {()()()}, &lt;([{}])&gt;, [&lt;&gt;({}){}[([])&lt;&gt;]], and even (((((((((()))))))))).\nSome lines are incomplete, but others are corrupted. Find and discard the corrupted lines first.\nA corrupted line is one where a chunk closes with the wrong character - that is, where the characters it opens and closes with do not form one of the four legal pairs listed above.\nExamples of corrupted chunks include (], {()()()&gt;, (((()))}, and &lt;([]){()}[{}]). Such a chunk can appear anywhere within a line, and its presence causes the whole line to be considered corrupted.\nTo calculate the syntax error score for a line, take the first illegal character on the line and look it up in the following table:\n): 3 points. ]: 57 points. }: 1197 points. &gt;: 25137 points.\nFind the first illegal character in each corrupted line of the navigation subsystem. What is the total syntax error score for those errors?\n\nI‚Äôll make heavy use of stringr and regular expressions to solve this problem. First, define a function that iterates over a line and determines if it is corrupt:\n\n# Regex representations of the character pairs (requires \\\\ to escape)\npairs &lt;- c(\"\\\\(\\\\)\" = \"\", \"\\\\[\\\\]\" = \"\", \"\\\\{\\\\}\" = \"\", \"\\\\&lt;\\\\&gt;\" = \"\")\n\nremove_pairs &lt;- function(line) {\n  repeat {\n    new_line &lt;- line %&gt;% str_replace_all(pairs)\n    if (new_line == line) return(new_line)\n    else line &lt;- new_line\n  }\n}\n\nline &lt;- \"([{&lt;&gt;}])[](&lt;&gt;)\"\nline_corrupt &lt;- \"([{&lt;&gt;]])\"\n\nremove_pairs(line)\n\n[1] \"\"\n\nremove_pairs(line_corrupt)\n\n[1] \"([{]])\"\n\n\nThen for corrupt lines, I need to find the location of the wrong closing characters:\n\n# Regex representations of mismatched pairs \"[&gt;\", \"(&gt;\", \"{&gt;\", \"[}\", etc.\nmismatched_pairs &lt;- c(\"[\\\\(\\\\[\\\\{]\\\\&gt;\",\n                      \"[\\\\(\\\\[\\\\&lt;]\\\\}\",\n                      \"[\\\\(\\\\{\\\\&lt;]\\\\]\",\n                      \"[\\\\[\\\\{\\\\&lt;]\\\\)\")\n\nfind_corrupt_loc &lt;- function(line_reduced) {\n  map_dfr(\n    mismatched_pairs,\n    ~remove_pairs(line_reduced) %&gt;% str_locate(.x) %&gt;% as.data.frame()\n  ) %&gt;%\n    drop_na()\n}\n\ncorrupt_loc &lt;- find_corrupt_loc(remove_pairs(line_corrupt))\ncorrupt_loc\n\n  start end\n1     3   4\n\n\nThen I use the end location (the smallest value to get the first occurrence) to determine which character is incorrect:\n\nremove_pairs(line_corrupt) %&gt;%\n  str_sub(start = min(corrupt_loc$end), end = min(corrupt_loc$end))\n\n[1] \"]\"\n\n\nApply this to the full input:\n\nday10_df &lt;- tibble(line = day10) %&gt;%\n  mutate(\n    line_reduced = map_chr(line, remove_pairs),\n    corrupt_loc = map(line_reduced, find_corrupt_loc)\n  )\n\ncorrupt_chars &lt;- day10_df %&gt;%\n  mutate(line_num = 1:n()) %&gt;%\n  # Filter down to just corrupt lines\n  filter(map_int(corrupt_loc, nrow) &gt; 0) %&gt;%\n  transmute(\n    line_num,\n    corrupt_char = map2_chr(\n      line_reduced, corrupt_loc,\n      ~str_sub(.x, start = min(.y$end), end = min(.y$end))\n    )\n  )\ncorrupt_chars\n\n# A tibble: 47 √ó 2\n   line_num corrupt_char\n      &lt;int&gt; &lt;chr&gt;       \n 1        1 }           \n 2        5 &gt;           \n 3        9 }           \n 4       12 }           \n 5       14 }           \n 6       16 ]           \n 7       17 }           \n 8       18 &gt;           \n 9       21 ]           \n10       22 ]           \n# ‚Ä¶ with 37 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nFinally, add up the points of the characters:\n\ncorrupt_chars %&gt;%\n  mutate(score = recode(corrupt_char,\n                        `)` = 3, `]` = 57, `}` = 1197, `&gt;` = 25137)) %&gt;%\n  pull(score) %&gt;% sum()\n\n[1] 469755\n\n\n\n\nPart 2\n\nNow, discard the corrupted lines. The remaining lines are incomplete.\nIncomplete lines don‚Äôt have any incorrect characters - instead, they‚Äôre missing some closing characters at the end of the line. To repair the navigation subsystem, you just need to figure out the sequence of closing characters that complete all open chunks in the line.\nYou can only use closing characters ((, ], }, or &gt;), and you must add them in the correct order so that only legal pairs are formed and all chunks end up closed.\nThe score is determined by considering the completion string character-by-character. Start with a total score of 0. Then, for each character, multiply the total score by 5 and then increase the total score by the point value given for the character in the following table:\n): 1 point. ]: 2 points. }: 3 points. &gt;: 4 points.\nAutocomplete tools are an odd bunch: the winner is found by sorting all of the scores and then taking the middle score. (There will always be an odd number of scores to consider.) In this example, the middle score is 288957 because there are the same number of scores smaller and larger than it.\nFind the completion string for each incomplete line, score the completion strings, and sort the scores. What is the middle score?\n\nFrom part 1, we have these incomplete (and non-corrupt) lines:\n\nday10_incomplete &lt;- day10_df %&gt;%\n  filter(map_int(corrupt_loc, nrow) == 0) %&gt;%\n  select(-corrupt_loc)\nglimpse(day10_incomplete)\n\nRows: 47\nColumns: 2\n$ line         &lt;chr&gt; \"&lt;[&lt;&lt;{&lt;[(([[[[&lt;[][]&gt;][(&lt;&gt;()){&lt;&gt;()}]]][&lt;{&lt;&lt;&gt;{}&gt;(()&lt;&gt;)}&gt;&lt;([‚Ä¶\n$ line_reduced &lt;chr&gt; \"&lt;[&lt;&lt;{&lt;[({{&lt;{\", \"&lt;(({(&lt;{&lt;((\", \"{{[{[[[{{((&lt;&lt;({\", \"&lt;(((&lt;&lt;&lt;‚Ä¶\n\n\nAs expected, we have an odd number of lines so we can get a middle score. Because I‚Äôve already reduced the lines down to line_reduced without matching pairs, I can simply reverse the order, and count the scores. For example:\n\nreversed_chars &lt;- remove_pairs(\"[({(&lt;(())[]&gt;[[{[]{&lt;()&lt;&gt;&gt;\") %&gt;%\n  strsplit(\"\") %&gt;%\n  unlist() %&gt;%\n  rev()\n \ntotal_score &lt;- 0\nfor (char in reversed_chars) {\n  total_score &lt;- total_score * 5\n  # No need to replace the opening with closing characters\n  total_score &lt;- total_score + switch(char, `(` = 1, `[` = 2, `{` = 3, `&lt;` = 4)\n}\ntotal_score\n\n[1] 288957\n\n\n\nday10_incomplete &lt;- day10_incomplete %&gt;%\n  mutate(\n    total_score = map_dbl(\n      line_reduced,\n      ~{\n        reversed_chars &lt;- str_split(.x, \"\") %&gt;% unlist() %&gt;% rev()\n        \n        total_score &lt;- 0\n        for (char in reversed_chars) {\n          total_score &lt;- total_score * 5\n          total_score &lt;- total_score + switch(char, `(` = 1, `[` = 2, `{` = 3, `&lt;` = 4)\n        }\n        total_score\n      }\n    )\n  )\nsort(day10_incomplete$total_score)[(nrow(day10_incomplete) + 1) / 2]\n\n[1] 2762335572"
  },
  {
    "objectID": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#stats",
    "href": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#stats",
    "title": "Advent of Code 2021: Days 6-10",
    "section": "Stats",
    "text": "Stats\nMy personal stats for this period:\n\ntibble::tribble(\n  ~Part, ~Day, ~Time, ~Rank,\n  1, 10, \"11:11:48\", 34029,\n  2, 10, \"11:25:31\", 32045,\n  1, 9, \"14:58:48\", 42395,\n  2, 9, \"16:04:08\", 33905,\n  1, 8, \"16:37:13\", 49102,\n  2, 8, \"16:55:20\", 34954,\n  1, 7, \"11:23:20\", 45816,\n  2, 7, \"11:42:48\", 43886,\n  1, 6, \"10:38:42\", 41671,\n  2, 6, \"10:58:31\", 34999\n) %&gt;%\n  pivot_wider(names_from = Part, values_from = c(Time, Rank),\n              names_glue = \"Part {Part}_{.value}\") %&gt;%\n  mutate(\n    `Time between parts` = as.numeric(hms(`Part 2_Time`) - hms(`Part 1_Time`),\n                                      \"minutes\") %&gt;% round(1)\n  ) %&gt;%\n  gt() %&gt;%\n  tab_spanner_delim(delim = \"_\", split = \"first\") %&gt;%\n  sub_missing(columns = \"Time between parts\", missing_text = \"\")\n\n\n\n\n\n\n\n\nDay\nTime\nRank\nTime between parts\n\n\nPart 1\nPart 2\nPart 1\nPart 2\n\n\n\n\n10\n11:11:48\n11:25:31\n34029\n32045\n13.7\n\n\n9\n14:58:48\n16:04:08\n42395\n33905\n65.3\n\n\n8\n16:37:13\n16:55:20\n49102\n34954\n18.1\n\n\n7\n11:23:20\n11:42:48\n45816\n43886\n19.5\n\n\n6\n10:38:42\n10:58:31\n41671\n34999\n19.8"
  },
  {
    "objectID": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#reproducibility",
    "href": "posts/2021-12-06-advent-of-code-2021-days-6-10/index.html#reproducibility",
    "title": "Advent of Code 2021: Days 6-10",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\npython:         C:/Users/tdunn/Documents/.virtualenvs/r-reticulate/Scripts/python.exe\nlibpython:      C:/Users/tdunn/AppData/Local/r-reticulate/r-reticulate/pyenv/pyenv-win/versions/3.9.13/python39.dll\npythonhome:     C:/Users/tdunn/Documents/.virtualenvs/r-reticulate\nversion:        3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          C:/Users/tdunn/Documents/.virtualenvs/r-reticulate/Lib/site-packages/numpy\nnumpy_version:  1.23.3\n\nNOTE: Python version was forced by use_python function\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html",
    "href": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html",
    "title": "Advent of Code 2021: Days 11-15",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(tictoc)\nlibrary(lubridate)"
  },
  {
    "objectID": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#day-11-dumbo-octopus",
    "href": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#day-11-dumbo-octopus",
    "title": "Advent of Code 2021: Days 11-15",
    "section": "Day 11: Dumbo Octopus",
    "text": "Day 11: Dumbo Octopus\n\nday11 &lt;- read_lines(\"day11-input.txt\")\nday11\n\n [1] \"4781623888\" \"1784156114\" \"3265645122\" \"4371551414\" \"3377154886\"\n [6] \"7882314455\" \"6421348681\" \"7175424287\" \"5488242184\" \"2448568261\"\n\n\n\nPart 1\n\nYou enter a large cavern full of rare bioluminescent dumbo octopuses! They seem to not like the Christmas lights on your submarine, so you turn them off for now.\nThere are 100 octopuses arranged neatly in a 10 by 10 grid. Each octopus slowly gains energy over time and flashes brightly for a moment when its energy is full. Although your lights are off, maybe you could navigate through the cave without disturbing the octopuses if you could predict when the flashes of light will happen.\nEach octopus has an energy level - your submarine can remotely measure the energy level of each octopus (your puzzle input).\nThe energy level of each octopus is a value between 0 and 9. You can model the energy levels and flashes of light in steps. During a single step, the following occurs:\n\nFirst, the energy level of each octopus increases by 1.\nThen, any octopus with an energy level greater than 9 flashes. This increases the energy level of all adjacent octopuses by 1, including octopuses that are diagonally adjacent. If this causes an octopus to have an energy level greater than 9, it also flashes. This process continues as long as new octopuses keep having their energy level increased beyond 9. (An octopus can only flash at most once per step.)\nFinally, any octopus that flashed during this step has its energy level set to 0, as it used all of its energy to flash.\n\nAdjacent flashes can cause an octopus to flash on a step even if it begins that step with very little energy. Given the starting energy levels of the dumbo octopuses in your cavern, simulate 100 steps. How many total flashes are there after 100 steps?\n\nPut the input into a 10x10 matrix:\n\nday11_mat &lt;- day11 %&gt;%\n  strsplit(\"\") %&gt;%\n  unlist() %&gt;%\n  as.integer() %&gt;%\n  matrix(nrow = 10, byrow = TRUE)\n\nAnd define a function that returns a point and its neighbors:\n\n# The 9 neighbors of a point (including diagonals and the point itself)\nd_coords &lt;- cbind(c(0, 0, 0, -1, 1, -1, -1, 1, 1),\n                  c(0, -1, 1, 0, 0, -1, 1, -1, 1))\n\nget_neighbors &lt;- function(row_col, max_row = 10, max_col = 10) {\n  neighbors &lt;- apply(d_coords, 1, function(d) d + row_col) %&gt;% t()\n  in_bounds &lt;- (neighbors[,1] &lt;= max_row) & (neighbors[,1] &gt; 0) &\n    (neighbors[,2] &lt;= max_col) & (neighbors[,2] &gt; 0)\n  neighbors[in_bounds, , drop = FALSE]\n}\n\nNow loop over the 100 steps and count the flashes:\n\nday11_part1 &lt;- day11_mat\nn_flashes &lt;- 0\n\nfor (step in 1:100) {\n  day11_part1 &lt;- day11_part1 + 1\n  # This matrix will keep track of which points have flashed in each step\n  flashing &lt;- day11_part1 == 10\n  \n  n_flashes &lt;- n_flashes + sum(flashing)\n  neighbors &lt;- apply(which(flashing, arr.ind = TRUE), 1,\n                     get_neighbors, simplify = FALSE)\n  \n  repeat {\n    # Keep track of newly flashing points for each iteration of neighbors\n    new_flashing &lt;- matrix(FALSE, nrow = 10, ncol = 10)\n     \n    for (neighbor in neighbors) {\n      day11_part1[neighbor] &lt;- day11_part1[neighbor] + 1\n      new_flashing &lt;- (!flashing & (day11_part1 == 10)) | new_flashing\n    }\n    \n    if (sum(new_flashing) == 0) break\n    else {\n      n_flashes &lt;- n_flashes + sum(new_flashing)\n      neighbors &lt;- apply(which(new_flashing, arr.ind = TRUE), 1,\n                         get_neighbors, simplify = FALSE)\n      flashing &lt;- flashing | new_flashing\n    }\n  }\n  day11_part1[day11_part1 &gt;= 10] &lt;- 0\n}\n\nn_flashes\n\n[1] 1713\n\n\nThis solution took me way way too long to get right because of the simplify argument in apply being TRUE by default. So when I was collecting a list of neighbors around a single point:\n\napply(which(matrix(c(T, F, F, F), nrow = 2, ncol = 2), arr.ind = TRUE), 1,\n      get_neighbors)\n\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    2\n[4,]    2\n[5,]    1\n[6,]    2\n[7,]    1\n[8,]    2\n\n\nI wouldn‚Äôt get a list as expected, unless setting simplify = FALSE:\n\napply(which(matrix(c(T, F, F, F), nrow = 2, ncol = 2), arr.ind = TRUE), 1,\n      get_neighbors, simplify = FALSE)\n\n[[1]]\n     row col\n[1,]   1   1\n[2,]   1   2\n[3,]   2   1\n[4,]   2   2\n\n\nAbout an hour lost, and a lesson learned.\nPython implementation:\n\nday11_df = np.array([list(row) for row in r.day11], dtype = int)\n\ndef get_neighbor_coords(row, col, max_row = 10, max_col = 10):\n  neighbor_coords = [[row + d for d in [0, -1, 1, 0, 0, -1, -1, 1, 1]],\n                     [col + d for d in [0, 0, 0, -1, 1, -1, 1, -1, 1]]]\n  neighbor_coords = np.array(neighbor_coords).T\n  \n  in_bounds = (neighbor_coords[:, 0] &gt;= 0) & \\\n    (neighbor_coords[:, 0] &lt; max_row) & \\\n    (neighbor_coords[:, 1] &gt;= 0) & \\\n    (neighbor_coords[:, 1] &lt; max_col)\n    \n  return(neighbor_coords[in_bounds, :])\n\nday11_part1 = day11_df.copy()\nn_flashes = 0\nfor step in range(100):\n  day11_part1 += 1\n  flashing = day11_part1 == 10\n  n_flashes += flashing.sum()\n  \n  flashing_loc = np.where(flashing) \n  neighbors = []\n  for row, col in zip(flashing_loc[0], flashing_loc[1]):\n    neighbors.append(get_neighbor_coords(row, col))\n    \n  while True:\n    new_flashing = np.zeros((10, 10), dtype = bool)\n    \n    for neighbor in neighbors:\n      for row,col in zip(neighbor[:,0], neighbor[:,1]):\n        day11_part1[row, col] += 1\n        new_flashing[row, col] = new_flashing[row, col] or \\\n          (not flashing[row, col] and day11_part1[row, col] == 10)\n          \n    if new_flashing.sum() == 0: break\n    else:\n      n_flashes += new_flashing.sum()\n      \n      flashing_loc = np.where(new_flashing) \n      neighbors = []\n      for row, col in zip(flashing_loc[0], flashing_loc[1]):\n        neighbors.append(get_neighbor_coords(row, col))\n      \n      flashing = flashing | new_flashing\n  \n  day11_part1[day11_part1 &gt;= 10] = 0\n  \nn_flashes\n\n1713\n\n\n\n\nPart 2\n\nIt seems like the individual flashes aren‚Äôt bright enough to navigate. However, you might have a better option: the flashes seem to be synchronizing!\nIf you can calculate the exact moments when the octopuses will all flash simultaneously, you should be able to navigate through the cavern. What is the first step during which all octopuses flash?\n\nI can get this answer by just modifying the loop slightly:\n\nday11_part2 &lt;- day11_mat\nflashing &lt;- day11_part2 == 10\n\nstep &lt;- 0\nwhile (!all(flashing)) {\n  step &lt;- step + 1\n  day11_part2 &lt;- day11_part2 + 1\n  \n  flashing &lt;- day11_part2 == 10\n  neighbors &lt;- apply(which(flashing, arr.ind = TRUE), 1,\n                     get_neighbors, simplify = FALSE)\n  \n  repeat {\n    new_flashing &lt;- matrix(FALSE, nrow = 10, ncol = 10)\n     \n    for (neighbor in neighbors) {\n      day11_part2[neighbor] &lt;- day11_part2[neighbor] + 1\n      new_flashing &lt;- (!flashing & (day11_part2 == 10)) | new_flashing\n    }\n    \n    if (sum(new_flashing) == 0) break\n    else {\n      neighbors &lt;- apply(which(new_flashing, arr.ind = TRUE), 1,\n                         get_neighbors, simplify = FALSE)\n      flashing &lt;- flashing | new_flashing\n    }\n  }\n  day11_part2[day11_part2 &gt;= 10] &lt;- 0\n}\nstep\n\n[1] 502\n\n\nPython:\n\nday11_part2 = day11_df.copy()\n\nstep = 0\nwhile not flashing.all():\n  step += 1\n  day11_part2 += 1\n  \n  flashing = day11_part2 == 10\n  flashing_loc = np.where(flashing) \n  neighbors = []\n  for row, col in zip(flashing_loc[0], flashing_loc[1]):\n    neighbors.append(get_neighbor_coords(row, col))\n    \n  while True:\n    new_flashing = np.zeros((10, 10), dtype = bool)\n    \n    for neighbor in neighbors:\n      for row,col in zip(neighbor[:,0], neighbor[:,1]):\n        day11_part2[row, col] += 1\n        new_flashing[row, col] = new_flashing[row, col] or \\\n          (not flashing[row, col] and day11_part2[row, col] == 10)\n          \n    if new_flashing.sum() == 0: break\n    else:\n      flashing_loc = np.where(new_flashing) \n      neighbors = []\n      for row, col in zip(flashing_loc[0], flashing_loc[1]):\n        neighbors.append(get_neighbor_coords(row, col))\n      \n      flashing = flashing | new_flashing\n  \n  day11_part2[day11_part2 &gt;= 10] = 0\n  \nstep\n\n502"
  },
  {
    "objectID": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#day-12-passage-pathing",
    "href": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#day-12-passage-pathing",
    "title": "Advent of Code 2021: Days 11-15",
    "section": "Day 12: Passage Pathing",
    "text": "Day 12: Passage Pathing\n\nday12 &lt;- read_lines(\"day12-input.txt\")\nday12\n\n [1] \"rf-RL\"    \"rf-wz\"    \"wz-RL\"    \"AV-mh\"    \"end-wz\"   \"end-dm\"  \n [7] \"wz-gy\"    \"wz-dm\"    \"cg-AV\"    \"rf-AV\"    \"rf-gy\"    \"end-mh\"  \n[13] \"cg-gy\"    \"cg-RL\"    \"gy-RL\"    \"VI-gy\"    \"AV-gy\"    \"dm-rf\"   \n[19] \"start-cg\" \"start-RL\" \"rf-mh\"    \"AV-start\" \"qk-mh\"    \"wz-mh\"   \n\n\n\nPart 1\n\nWith your submarine‚Äôs subterranean subsystems subsisting suboptimally, the only way you‚Äôre getting out of this cave anytime soon is by finding a path yourself. Not just a path - the only way to know if you‚Äôve found the best path is to find all of them.\nFortunately, the sensors are still mostly working, and so you build a rough map of the remaining caves (your puzzle input).\nYour goal is to find the number of distinct paths that start at start, end at end, and don‚Äôt visit small caves more than once. There are two types of caves: big caves (written in uppercase, like A) and small caves (written in lowercase, like b). It would be a waste of time to visit any small cave more than once, but big caves are large enough that it might be worth visiting them multiple times. So, all paths you find should visit small caves at most once, and can visit big caves any number of times.\nHow many paths through this cave system are there that visit small caves at most once?\n\nFirst, put together a list of all the cave connections:\n\ncave_connections &lt;- tibble(paths = day12) %&gt;%\n  separate(paths, into = c(\"cave1\", \"cave2\"), \"-\") \ncave_connections &lt;- cave_connections %&gt;%\n  # By adding the reverse connections, we have all bi-directional paths\n  bind_rows(cave_connections %&gt;% rename(cave1 = cave2, cave2 = cave1)) %&gt;%\n  # Paths can't end with start, or start with end\n  filter(cave1 != \"end\", cave2 != \"start\") %&gt;%\n  arrange(cave1)\ncave_connections\n\n# A tibble: 42 √ó 2\n   cave1 cave2\n   &lt;chr&gt; &lt;chr&gt;\n 1 AV    mh   \n 2 AV    gy   \n 3 AV    cg   \n 4 AV    rf   \n 5 cg    AV   \n 6 cg    gy   \n 7 cg    RL   \n 8 dm    rf   \n 9 dm    end  \n10 dm    wz   \n# ‚Ä¶ with 32 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nAll paths begin with the start cave, with these possible connections:\n\nstarting_paths &lt;- cave_connections %&gt;%\n  filter(cave1 == \"start\") %&gt;%\n  rowwise() %&gt;%\n  transmute(start_cave = cave1, path = list(cave2),\n            end_cave = cave2)\nstarting_paths\n\n# A tibble: 3 √ó 3\n# Rowwise: \n  start_cave path      end_cave\n  &lt;chr&gt;      &lt;list&gt;    &lt;chr&gt;   \n1 start      &lt;chr [1]&gt; cg      \n2 start      &lt;chr [1]&gt; RL      \n3 start      &lt;chr [1]&gt; AV      \n\n\nLastly, a helper function to identify small caves (I‚Äôm not aware of a base R function to check for case):\n\nis_lower &lt;- function(x) {\n  tolower(x) == x\n}\n\nMy strategy is to continuously loop, add new caves by left_joining cave_connections, add new caves to the path list, and remove paths which go through a small cave twice. The loop will break when there was no change from the previous iteration:\n\ntic()\ncave_paths &lt;- starting_paths\nrepeat {\n  prev_paths &lt;- cave_paths\n  \n  cave_paths &lt;- cave_paths %&gt;%\n    left_join(\n      cave_connections, by = c(\"end_cave\" = \"cave1\")\n    ) %&gt;%\n    # Remove paths which visit a small cave twice\n    filter(\n      !(is_lower(cave2) & (cave2 %in% path))\n    ) %&gt;%\n    # Add the new caves to the paths, unless NA (already at the end)\n    mutate(\n      path = ifelse(!is.na(cave2), list(c(path, cave2)), list(path)),\n      end_cave = ifelse(!is.na(cave2), cave2, end_cave)\n    ) %&gt;%\n    select(-cave2)\n  \n  if (all_equal(prev_paths, cave_paths) == TRUE) break \n}\ntoc()\n\n1.02 sec elapsed\n\n\n3421 unique paths were identified:\n\ncave_paths %&gt;%\n  mutate(path = str_c(c(start_cave, path), collapse = \",\")) %&gt;%\n  distinct(path)\n\n# A tibble: 3,421 √ó 1\n# Rowwise: \n   path                                   \n   &lt;chr&gt;                                  \n 1 start,cg,AV,mh,AV,gy,RL,rf,RL,wz,dm,end\n 2 start,cg,AV,mh,AV,gy,RL,rf,RL,wz,end   \n 3 start,cg,AV,mh,AV,gy,RL,rf,wz,dm,end   \n 4 start,cg,AV,mh,AV,gy,RL,rf,wz,end      \n 5 start,cg,AV,mh,AV,gy,RL,rf,dm,end      \n 6 start,cg,AV,mh,AV,gy,RL,rf,dm,wz,end   \n 7 start,cg,AV,mh,AV,gy,RL,wz,RL,rf,dm,end\n 8 start,cg,AV,mh,AV,gy,RL,wz,dm,end      \n 9 start,cg,AV,mh,AV,gy,RL,wz,rf,dm,end   \n10 start,cg,AV,mh,AV,gy,RL,wz,end         \n# ‚Ä¶ with 3,411 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nFor the Python solution, I‚Äôll use pandas:\n\ncave_connections = pd.DataFrame([path.split('-') for path in r.day12],\n                                columns = ['cave1', 'cave2'])\n                                \ncave_connections = pd.concat(\n  [cave_connections,\n   cave_connections.rename(columns = {'cave1': 'cave2', 'cave2': 'cave1'})],\n  ignore_index = True\n).query(\"cave2 != 'start'\").query(\"cave1 != 'end'\")\n\nstarting_paths = cave_connections.query(\"cave1 == 'start'\") \\\n  .rename(columns = {'cave1': 'start_cave', 'cave2': 'end_cave'})\nstarting_paths = starting_paths.assign(path = starting_paths['end_cave'])\n\n\ndef day12_part1():\n  n_paths = 0\n  cave_paths = starting_paths.copy()\n           \n  while True:\n    prev_paths = cave_paths.copy()\n    \n    cave_paths = pd.merge(cave_paths, cave_connections,\n                          left_on = 'end_cave', right_on = 'cave1', how = 'left')\n           \n    # Remove paths that go through a small cave twice\n    remove_paths = [str.islower(c) and (c in p.split(',')) \\\n                    for c, p in zip(cave_paths.cave2, cave_paths.path)]\n    cave_paths.drop(cave_paths[remove_paths].index, inplace = True)\n    \n    # Drop paths that have reached the end, and add to the count\n    remove_paths = cave_paths.cave2 == 'end'\n    n_paths += remove_paths.sum()\n    cave_paths.drop(cave_paths[remove_paths].index, inplace = True)\n    \n    # Re-structure the data frame for the next iteration\n    cave_paths = cave_paths \\\n      .assign(path = lambda df: df['path'] + ',' + df['cave2']) \\\n      .assign(end_cave = cave_paths['cave2']) \\\n      .filter(items = ['start_cave', 'path', 'end_cave'])\n    \n    if cave_paths.equals(prev_paths): break\n  \n  print(n_paths)\n  \ntimeit.repeat(\"day12_part1()\", \"from __main__ import day12_part1\",\n              repeat = 1, number = 1)\n\n3421\n[0.0810732999999999]\n\n\n\n\nPart 2\n\nAfter reviewing the available paths, you realize you might have time to visit a single small cave twice. Specifically, big caves can be visited any number of times, a single small cave can be visited at most twice, and the remaining small caves can be visited at most once. However, the caves named start and end can only be visited exactly once each: once you leave the start cave, you may not return to it, and once you reach the end cave, the path must end immediately.\nGiven these new rules, how many paths through this cave system are there?\n\nI will slightly modify the loop from part 1 to check if a small cave has been visited twice:\n\ntic()\ncave_paths &lt;- starting_paths %&gt;% mutate(small_cave_twice = FALSE)\nrepeat {\n  prev_paths &lt;- cave_paths\n  \n  cave_paths &lt;- cave_paths %&gt;%\n    left_join(\n      cave_connections, by = c(\"end_cave\" = \"cave1\")\n    ) %&gt;%\n    # Remove paths which visit a small cave twice more than once\n    filter(\n      !(small_cave_twice & is_lower(cave2) & (cave2 %in% path))\n    ) %&gt;%\n    # Add the new caves to the paths, unless NA (already at the end)\n    mutate(\n      path = ifelse(!is.na(cave2), list(c(path, cave2)), list(path)),\n      end_cave = ifelse(!is.na(cave2), cave2, end_cave)\n    ) %&gt;%\n    select(-cave2) %&gt;%\n    # Check to see if a small cave has been visited twice\n    mutate(\n      small_cave_twice = any(table(path[is_lower(path)]) &gt; 1)  \n    )\n  if (all_equal(prev_paths, cave_paths) == TRUE) break \n}\ntoc()\n\n128.87 sec elapsed\n\n\n84870 unique paths were identified:\n\ncave_paths %&gt;%\n  mutate(path = str_c(c(start_cave, path), collapse = \",\")) %&gt;%\n  distinct(path)\n\n# A tibble: 84,870 √ó 1\n# Rowwise: \n   path                                         \n   &lt;chr&gt;                                        \n 1 start,cg,AV,mh,AV,mh,AV,gy,RL,rf,RL,wz,dm,end\n 2 start,cg,AV,mh,AV,mh,AV,gy,RL,rf,RL,wz,end   \n 3 start,cg,AV,mh,AV,mh,AV,gy,RL,rf,wz,dm,end   \n 4 start,cg,AV,mh,AV,mh,AV,gy,RL,rf,wz,end      \n 5 start,cg,AV,mh,AV,mh,AV,gy,RL,rf,dm,end      \n 6 start,cg,AV,mh,AV,mh,AV,gy,RL,rf,dm,wz,end   \n 7 start,cg,AV,mh,AV,mh,AV,gy,RL,wz,RL,rf,dm,end\n 8 start,cg,AV,mh,AV,mh,AV,gy,RL,wz,dm,end      \n 9 start,cg,AV,mh,AV,mh,AV,gy,RL,wz,rf,dm,end   \n10 start,cg,AV,mh,AV,mh,AV,gy,RL,wz,end         \n# ‚Ä¶ with 84,860 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nThis solution was obviously inefficient, taking about 2 minutes to run.\n\n.. every problem has a solution that completes in at most 15 seconds on ten-year-old hardware.\n\nLet‚Äôs see if Python is any faster:\n\nfrom collections import Counter\n\ndef day12_part2():\n  n_paths = 0\n  cave_paths = starting_paths.copy()\n           \n  while True:\n    prev_paths = cave_paths.copy()\n    \n    cave_paths = pd.merge(cave_paths, cave_connections,\n                          left_on = 'end_cave', right_on = 'cave1', how = 'left')\n           \n    # Drop paths that have reached the end, and add to the count\n    remove_paths = cave_paths.cave2 == 'end'\n    n_paths += remove_paths.sum()\n    cave_paths.drop(cave_paths[remove_paths].index, inplace = True)\n    \n    # Re-structure the data frame for the next iteration\n    cave_paths = cave_paths \\\n      .assign(path = lambda df: df['path'] + ',' + df['cave2']) \\\n      .assign(end_cave = cave_paths['cave2']) \\\n      .filter(items = ['start_cave', 'path', 'end_cave'])\n    \n    # Get a list of the caves for each path\n    cave_list = [path.split(',') for path in cave_paths.path]\n    # Filter the list down to just small caves\n    cave_list = [[cave for cave in cl if str.islower(cave)] \\\n                 for cl in cave_list]\n    # Frequency counts of the small caves\n    cave_list = [list(Counter(cl).values()) for cl in cave_list]\n    # Determine the illegal paths\n    cave_list = [cc.count(2) &gt; 1 or cc.count(3) &gt; 0 for cc in cave_list]\n    # Finally, remove illegal paths\n    cave_paths.drop(cave_paths[cave_list].index, inplace = True)\n    \n    if cave_paths.equals(prev_paths): break\n  \n  print(n_paths)\n  \ntimeit.repeat(\"day12_part2()\", \"from __main__ import day12_part2\",\n              repeat = 1, number = 1)\n\n84870\n[6.137362400000001]\n\n\nMuch, much faster."
  },
  {
    "objectID": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#day-13-transparent-origami",
    "href": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#day-13-transparent-origami",
    "title": "Advent of Code 2021: Days 11-15",
    "section": "Day 13: Transparent Origami",
    "text": "Day 13: Transparent Origami\n\nday13 &lt;- read_lines(\"day13-input.txt\")\nhead(day13)\n\n[1] \"323,305\" \"512,845\" \"780,175\" \"308,50\"  \"266,604\" \"152,425\"\n\ntail(day13)\n\n[1] \"fold along x=81\" \"fold along y=55\" \"fold along x=40\" \"fold along y=27\"\n[5] \"fold along y=13\" \"fold along y=6\" \n\n\n\nPart 1\n\nYou reach another volcanically active part of the cave. It would be nice if you could do some kind of thermal imaging so you could tell ahead of time which caves are too hot to safely enter.\nFortunately, the submarine seems to be equipped with a thermal camera! When you activate it, you are greeted with:\nCongratulations on your purchase! To activate this infrared thermal imaging camera system, please enter the code found on page 1 of the manual.\nApparently, the Elves have never used this feature. To your surprise, you manage to find the manual; as you go to open it, page 1 falls out. It‚Äôs a large sheet of transparent paper! The transparent paper is marked with random dots and includes instructions on how to fold it up (your puzzle input).\nThe first section is a list of dots on the transparent paper. 0,0 represents the top-left coordinate. The first value, x, increases to the right. The second value, y, increases downward. So, the coordinate 3,0 is to the right of 0,0, and the coordinate 0,7 is below 0,0.\nThen, there is a list of fold instructions. Each instruction indicates a line on the transparent paper and wants you to fold the paper up (for horizontal y=... lines) or left (for vertical x=... lines).\nHow many dots are visible after completing just the first fold instruction on your transparent paper?\n\nSeparate the dots and the folding instructions:\n\nday13_dots &lt;- day13[1:(which(day13 == \"\") - 1)]\nday13_folds &lt;- day13[(which(day13 == \"\") + 1):length(day13)] %&gt;%\n  str_remove(\"fold along \") %&gt;%\n  strsplit(\"=\")\n\nhead(day13_dots); head(day13_folds, 3)\n\n[1] \"323,305\" \"512,845\" \"780,175\" \"308,50\"  \"266,604\" \"152,425\"\n\n\n[[1]]\n[1] \"x\"   \"655\"\n\n[[2]]\n[1] \"y\"   \"447\"\n\n[[3]]\n[1] \"x\"   \"327\"\n\n\nModel the paper as a matrix of booleans, and mark the dots:\n\ndots &lt;- strsplit(day13_dots, \",\") %&gt;%\n  map(as.integer) %&gt;%\n  reduce(rbind, deparse.level = 0)\n# Because R indices start at 1, not 0, adjust each point by 1\ndots &lt;- dots + 1\n# Also, because the coordinates are in (x,y) order, we need to reverse these\n#  points so that they may be used as (row,col) input\ndots &lt;- dots[,c(2, 1)]\n\n# Model the sheet of paper as a matrix big enough for the points\npaper &lt;- matrix(FALSE, nrow = max(dots[,1]), ncol = max(dots[,2]))\n# Mark the dots on the paper\npaper[dots] &lt;- TRUE\n\nFold once, using the first instruction:\n\npaper_part1 &lt;- paper\nfold &lt;- day13_folds[[1]]\n\nfold_dir &lt;- fold[1]\nfold_pos &lt;- as.integer(fold[2]) + 1\n\nif (fold_dir == \"x\") {\n  half1 &lt;- paper_part1[, 1:(fold_pos - 1)]\n  half2 &lt;- paper_part1[, (fold_pos + 1):ncol(paper_part1)]\n  # Reverse the half\n  half2 &lt;- half2[, ncol(half2):1]\n} else {\n  half1 &lt;- paper_part1[1:(fold_pos - 1), ]\n  half2 &lt;- paper_part1[(fold_pos + 1):nrow(paper_part1), ]\n  half2 &lt;- half2[nrow(half2):1, ]\n}\npaper_part1 &lt;- half1 | half2\n\n# Count the sum of visible dots\nsum(paper_part1)\n\n[1] 671\n\n\nIn Python with numpy arrays:\n\ndots = r.day13[0:r.day13.index('')]\ndots = np.array([d.split(',') for d in dots], dtype = int)\ndots = np.flip(dots, axis = 1)\n\nfolds = r.day13[(r.day13.index('') + 1):]\nfolds = [fold.replace('fold along ', '') for fold in folds]\nfolds = [fold.split('=') for fold in folds]\n\npaper = np.zeros((dots[:, 0].max() + 1, dots[:, 1].max() + 1), dtype = bool)\nfor dot in dots:\n  paper[tuple(dot)] = True\n  \npaper_part1 = paper.copy()\n\nfold = folds[0]\nfold_dir = fold[0]\nfold_loc = int(fold[1])\n\nhalf1 = paper_part1[:, 0:fold_loc]\nhalf2 = paper_part1[:, (fold_loc + 1):]\nhalf2 = np.flip(half2, axis = 1)\n\npaper_part1 = half1 | half2\npaper_part1.sum()\n\n671\n\n\n\n\nPart 2\n\nFinish folding the transparent paper according to the instructions. The manual says the code is always eight capital letters. What code do you use to activate the infrared thermal imaging camera system?\n\nNow loop over each fold:\n\npaper_part2 &lt;- paper\nfor (fold in day13_folds) {\n  fold_dir &lt;- fold[1]\n  fold_pos &lt;- as.integer(fold[2]) + 1\n  \n  if (fold_dir == \"x\") {\n    half1 &lt;- paper_part2[, 1:(fold_pos - 1)]\n    half2 &lt;- paper_part2[, (fold_pos + 1):ncol(paper_part2)]\n    # Reverse the half\n    half2 &lt;- half2[, ncol(half2):1]\n  } else {\n    half1 &lt;- paper_part2[1:(fold_pos - 1), ]\n    half2 &lt;- paper_part2[(fold_pos + 1):nrow(paper_part2), ]\n    # Reverse the half\n    half2 &lt;- half2[nrow(half2):1, ]\n  }\n  paper_part2 &lt;- half1 | half2\n}\n\nAnd use a plot to read the pattern of dots:\n\npaper_part2 %&gt;%\n  as_tibble() %&gt;%\n  mutate(row_num = 1:n()) %&gt;%\n  pivot_longer(cols = -row_num, names_to = \"col_num\") %&gt;%\n  mutate(col_num = as.integer(str_remove(col_num, \"V\"))) %&gt;%\n  filter(value) %&gt;%\n  ggplot(aes(x = col_num, y = row_num)) +\n  geom_point(size = 4) +\n  scale_y_reverse() +\n  theme_void()\n\n\n\n\nAnd in matplotlib:\n\npaper_part2 = paper.copy()\n\nfor fold in folds:\n  fold_dir = fold[0]\n  fold_loc = int(fold[1])\n  \n  if fold_dir == \"y\":\n    half1 = paper_part2[0:fold_loc, :]\n    half2 = paper_part2[(fold_loc + 1):, :]\n    half2 = np.flip(half2, axis = 0)\n  else:\n    half1 = paper_part2[:, 0:fold_loc]\n    half2 = paper_part2[:, (fold_loc + 1):]\n    half2 = np.flip(half2, axis = 1)\n  \n  paper_part2 = half1 | half2\n  \n\ndraw_points = np.where(paper_part2 == True)\n\nfig, ax = plt.subplots()\nax.scatter(draw_points[1], -draw_points[0])\nax.get_yaxis().set_visible(False)\nax.get_xaxis().set_visible(False)\nfig.set_size_inches(4, 0.8)\nplt.savefig('day13-fig.png')\n\n\nLooks like PCPHARKL."
  },
  {
    "objectID": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#day-14-extended-polymerization",
    "href": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#day-14-extended-polymerization",
    "title": "Advent of Code 2021: Days 11-15",
    "section": "Day 14: Extended Polymerization",
    "text": "Day 14: Extended Polymerization\n\nday14 &lt;- read_lines(\"day14-input.txt\")\nhead(day14)\n\n[1] \"PBFNVFFPCPCPFPHKBONB\" \"\"                     \"KK -&gt; S\"             \n[4] \"FO -&gt; B\"              \"PP -&gt; O\"              \"HN -&gt; S\"             \n\n\n\nPart 1\n\nThe incredible pressures at this depth are starting to put a strain on your submarine. The submarine has polymerization equipment that would produce suitable materials to reinforce the submarine, and the nearby volcanically-active caves should even have the necessary input elements in sufficient quantities.\nThe submarine manual contains instructions for finding the optimal polymer formula; specifically, it offers a polymer template and a list of pair insertion rules (your puzzle input). You just need to work out what polymer would result after repeating the pair insertion process a few times.\nThe first line is the polymer template - this is the starting point of the process.\nThe following section defines the pair insertion rules. A rule like AB -&gt; C means that when elements A and B are immediately adjacent, element C should be inserted between them. These insertions all happen simultaneously.\nApply 10 steps of pair insertion to the polymer template and find the most and least common elements in the result. What do you get if you take the quantity of the most common element and subtract the quantity of the least common element?\n\nSeparate the polymer template from the pair insertion instructions:\n\npolymer_template &lt;- day14[1]\npair_insertions &lt;- day14[-c(1, 2)]\npairs &lt;- map_chr(pair_insertions, ~strsplit(.x, \" -&gt; \")[[1]][1])\ninsertions &lt;- map_chr(pair_insertions, ~strsplit(.x, \" -&gt; \")[[1]][2])\n\n# Also get the three-letter replacement for the two-letter pairs\ninsertions3 &lt;- map2_chr(\n  pairs, insertions,\n  ~paste0(substr(.x, 1, 1), .y, substr(.x, 2, 2))\n)\n\nMy strategy for this part is to find the pairs at each step, then replace pairs with a numeric placeholder, e.g.¬†KK -&gt; K1K. Then replace the numeric placeholders with the three-letter combination, e.g.¬†K1K -&gt; KSK. By doing the replacements in this way, I avoid accidentally finding new pairs during a single step.\n\npolymer_template_part1 &lt;- polymer_template\nfor (step in 1:10) {\n  # Find the pair insertions relevant to this step\n  pair_idx &lt;- map_lgl(pairs, ~str_detect(polymer_template_part1, .x))\n  step_pairs &lt;- pairs[pair_idx]\n  step_insertions &lt;- insertions3[pair_idx]\n  \n  for (i in seq_along(step_pairs)) {\n    # While there are still pairs to replace, continuously replace them\n    while (str_detect(polymer_template_part1, step_pairs[i])) {\n      polymer_template_part1 &lt;-\n        str_replace_all(polymer_template_part1, step_pairs[i],\n                        paste0(substr(step_pairs[i], 1, 1), i,\n                               substr(step_pairs[i], 2, 2)))\n    }\n  }\n  \n  # Use a named list as the input to str_replace_all\n  names(step_insertions) &lt;-\n    paste0(map_chr(step_pairs, substr, 1, 1),\n           seq_along(step_insertions),\n           map_chr(step_pairs, substr, 2, 2))\n  # While there are still numeric placeholders, continually replace them\n  while (str_detect(polymer_template_part1, \"\\\\d+\")) {\n    polymer_template_part1 &lt;- str_replace_all(polymer_template_part1,\n                                              step_insertions)\n  }\n}\n\nstr_trunc(polymer_template_part1, width = 70)\n\n[1] \"POOVOPVHOKPNVPHPOSKBPONNVOPFHFPOOVSSKCBVPOOKNONNVHOKPOFOHSFNPOOVOPV...\"\n\n\nNote that I use while loops at the str_replace_all steps to account for triple letter sequences. For example, if there is a three-letter sequence PPP, then a single pass of str_replace_all will result in POPP. On the second pass, the correct POPOP is returned.\nNow count the letter/element frequency and subtract the most from the least frequent:\n\npart1_freq &lt;- table(strsplit(polymer_template_part1, \"\")[[1]])\npart1_freq\n\n\n   B    C    F    H    K    N    O    P    S    V \n2602 1916 1202 1240 2600 1744 1844  976 2979 2354 \n\nas.numeric(part1_freq[part1_freq == max(part1_freq)]) -\n  as.numeric(part1_freq[part1_freq == min(part1_freq)])\n\n[1] 2003\n\n\nPython:\n\npolymer_template = r.day14[0]\npair_insertions = [x.split(' -&gt; ') for x in r.day14[2:]]\npairs = [p[0] for p in pair_insertions]\ninsertions = [p[1] for p in pair_insertions]\ninsertions3 = [p[0] + i + p[1] for p, i in zip(pairs, insertions)]\n\n\npolymer_template_part1 = polymer_template\nfor step in range(10):\n  step_pairs = [p for p in pairs if p in polymer_template_part1]\n  step_insertions = [i for p, i in zip(pairs, insertions3) \\\n                     if p in polymer_template_part1]\n  placeholders = [p[0] + str(i) + p[1] for i, p in enumerate(step_pairs)]\n                     \n  for i, (pair, placeholder) in enumerate(zip(step_pairs, placeholders)):\n    while pair in polymer_template_part1:\n      polymer_template_part1 = polymer_template_part1.replace(\n        pair, placeholder\n      )\n      \n  for i, (insertion, placeholder) in enumerate(zip(step_insertions, placeholders)):\n    while placeholder in polymer_template_part1:\n      polymer_template_part1 = polymer_template_part1.replace(\n        placeholder, insertion\n      )\n      \nfrom collections import Counter\nelement_counts = Counter(polymer_template_part1)\nmax(element_counts.values()) - min(element_counts.values())\n\n2003\n\n\n\n\nPart 2\n\nThe resulting polymer isn‚Äôt nearly strong enough to reinforce the submarine. You‚Äôll need to run more steps of the pair insertion process; a total of 40 steps should do it.\nApply 40 steps of pair insertion to the polymer template and find the most and least common elements in the result. What do you get if you take the quantity of the most common element and subtract the quantity of the least common element?\n\nI probably should have seen this coming. The polymer from part 1 consists of 19457 elements, and will continue to grow exponentially and take too long to reach 40 steps. Similar to the lanternfish puzzle from day 6, I will need to instead find a way to count unique inputs (pairs of the polymer chain), then count their unique outputs at each step.\nConsider each pair KK, FO, PP, etc. Each pair results in two new pairs from the same list after inserting a letter in between. For example, FO becomes FBO which consists of the pairs FB and BO. For each unique pair, compile the resulting pairs after insertion:\n\npair_transitions &lt;-\n  tibble(pair = pairs, element = insertions, insertion3 = insertions3) %&gt;%\n  mutate(\n    new_pairs = map(insertion3, ~str_sub(.x, start = 1:2, end = 2:3))\n  )\npair_transitions %&gt;%\n  mutate(new_pairs = map_chr(new_pairs, str_c, collapse = \", \"))\n\n# A tibble: 100 √ó 4\n   pair  element insertion3 new_pairs\n   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;    \n 1 KK    S       KSK        KS, SK   \n 2 FO    B       FBO        FB, BO   \n 3 PP    O       POP        PO, OP   \n 4 HN    S       HSN        HS, SN   \n 5 CN    H       CHN        CH, HN   \n 6 VH    P       VPH        VP, PH   \n 7 BK    B       BBK        BB, BK   \n 8 VC    N       VNC        VN, NC   \n 9 CB    H       CHB        CH, HB   \n10 OC    K       OKC        OK, KC   \n# ‚Ä¶ with 90 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nThen it is a simple matter of counting the starting pairs, and counting the new pairs after each step. I will also keep a running count of the elements inserted at each step to calculate the puzzle solution at the end:\n\ntemplate_pairs &lt;- tibble(\n  pair = str_sub(polymer_template,\n                 start = 1:(nchar(polymer_template) - 1),\n                 end = 2:nchar(polymer_template))\n) %&gt;%\n  count(pair)\n\nelement_counts &lt;- tibble(element = strsplit(polymer_template, \"\")[[1]]) %&gt;%\n  count(element)\n\npart2_pairs &lt;- template_pairs\nfor (step in 1:40) {\n  part2_pairs &lt;- part2_pairs %&gt;% left_join(pair_transitions, by = \"pair\")\n  # Update the count with newly inserted elements\n  element_counts &lt;- element_counts %&gt;%\n    bind_rows(part2_pairs %&gt;% select(element, n)) %&gt;%\n    group_by(element) %&gt;%\n    summarise(n = sum(n), .groups = \"drop\")\n  \n  # Update the count of each pair\n  part2_pairs &lt;- part2_pairs %&gt;%\n    unnest(new_pairs) %&gt;%\n    select(pair = new_pairs, n) %&gt;%\n    group_by(pair) %&gt;%\n    summarise(n = sum(n), .groups = \"drop\")\n}\n\nelement_counts %&gt;% arrange(desc(n))\n\n# A tibble: 10 √ó 2\n   element             n\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 B       3225985458057\n 2 S       3148585011712\n 3 K       2929014164222\n 4 C       2455634949915\n 5 V       2288592953070\n 6 N       1804584793307\n 7 O       1728660985220\n 8 H       1278822335904\n 9 F       1081498818392\n10 P        949341457946\n\n\nSubstract the highest from the lowest frequency:\n\nelement_counts %&gt;%\n  summarise(freq_diff = max(n) - min(n)) %&gt;%\n  pull(freq_diff) %&gt;%\n  format(scientific = FALSE)\n\n[1] \"2276644000111\"\n\n\nFor the Python solution, I‚Äôll make use of Counter:\n\n# Use a dictionary to keep track of insertions\ninsertions_dict = {}\nfor line in r.day14[2:]:\n  pair, insertion = line.split(' -&gt; ')\n  insertions_dict[pair] = insertion\n\n# Get the pairs in the initial polymer template\npairs = [polymer_template[i:(i+2)] for i in range(len(polymer_template) - 1)]\n# And count them\npairs_counter = Counter(pairs)\n# Also count the elements\nelements_counter = Counter(polymer_template)\n\nfor _ in range(40):\n  # Note that you have to use a copy() of the counter dict or get this error:\n  #  RuntimeError: dictionary changed size during iteration\n  for (pair, n) in pairs_counter.copy().items():\n    # Count the element\n    element = insertions_dict[pair]\n    elements_counter[element] += n\n    \n    # Remove the pair\n    pairs_counter[pair] -= n\n    # Add the two new pairs\n    pairs_counter[pair[0] + element] += n\n    pairs_counter[element + pair[1]] += n\n    \nmax(elements_counter.values()) - min(elements_counter.values())\n\n2276644000111"
  },
  {
    "objectID": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#day-15-chiton",
    "href": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#day-15-chiton",
    "title": "Advent of Code 2021: Days 11-15",
    "section": "Day 15: Chiton",
    "text": "Day 15: Chiton\n\nday15 &lt;- read_lines(\"day15-input.txt\")\nhead(day15) %&gt;% str_trunc(70)\n\n[1] \"1277612293663378117618549828679918274822495311841997189739842189979...\"\n[2] \"9271393131522635231134923888495739243498692263922766681729855924329...\"\n[3] \"8386928948885115739239121267489851994495998157413791292128126786986...\"\n[4] \"8587465426836695543987965665319923697978898664688597797977756649524...\"\n[5] \"3613452297153349211825418198975341212117882643222179165399996886428...\"\n[6] \"2711886836459899996697348654589518929717652919364971266989157613172...\"\n\n\n\nPart 1\n\nYou‚Äôve almost reached the exit of the cave, but the walls are getting closer together. Your submarine can barely still fit, though; the main problem is that the walls of the cave are covered in chitons, and it would be best not to bump any of them.\nThe cavern is large, but has a very low ceiling, restricting your motion to two dimensions. The shape of the cavern resembles a square; a quick scan of chiton density produces a map of risk level throughout the cave (your puzzle input).\nYou start in the top left position, your destination is the bottom right position, and you cannot move diagonally. The number at each position is its risk level; to determine the total risk of an entire path, add up the risk levels of each position you enter (that is, don‚Äôt count the risk level of your starting position unless you enter it; leaving it adds no risk to your total). Your goal is to find a path with the lowest total risk.\nWhat is the lowest total risk of any path from the top left to the bottom right?\n\nPut the risk map into a matrix, and define a function to retrieve neighboring points:\n\nrisk_map &lt;- day15 %&gt;%\n  strsplit(\"\") %&gt;%\n  map(as.integer) %&gt;%\n  reduce(rbind, deparse.level = 0)\n\nd_coords &lt;- cbind(c(0, 0, -1, 1),\n                  c(-1, 1, 0, 0))\nget_neighbors &lt;- function(row, col,\n                          max_row = nrow(risk_map), max_col = ncol(risk_map)) {\n  nx &lt;- row + d_coords[,1]\n  ny &lt;- col + d_coords[,2]\n  \n  in_bounds &lt;- (nx &lt;= max_row) & (nx &gt; 0) & (ny &lt;= max_col) & (ny &gt; 0)\n  cbind(nx[in_bounds], ny[in_bounds])\n}\n\nI knew this problem wouldn‚Äôt be feasible by brute force (or at least part 2 wouldn‚Äôt be), but couldn‚Äôt come up with a non-naive method. Turning to the internet for a hint, I found Dijkstra‚Äôs algorithm for finding the shortest path between two nodes in a graph. Following along with the psuedocode, here is my implementation with R matrices:\n\ndist &lt;- matrix(Inf, nrow = nrow(risk_map), ncol = ncol(risk_map))\n# Set the starting point\ndist[1, 1] &lt;- 0\n\nvisited &lt;- matrix(FALSE, nrow = nrow(risk_map), ncol = ncol(risk_map))\n\nwhile (!all(visited)) {\n  # Get unvisited points\n  unvisited &lt;- which(!visited, arr.ind = TRUE)\n  # Find the unvisited point with the minimum distance\n  min_dist &lt;- unvisited[dist[unvisited] == min(dist[unvisited]), , drop = FALSE]\n  # Mark this point as visited\n  visited[min_dist[1,1], min_dist[1,2]] &lt;- TRUE\n  \n  # Get neighbors of the point\n  neighbors &lt;- get_neighbors(min_dist[1,1], min_dist[1,2],\n                             nrow(dist), ncol(dist))\n  # But exclude ones that have been visited\n  neighbors &lt;- neighbors[!visited[neighbors], , drop = FALSE]\n  \n  for (n in seq_len(nrow(neighbors))) {\n    nr &lt;- neighbors[n, 1]\n    nc &lt;- neighbors[n, 2]\n    dist_possible &lt;- dist[min_dist[1,1], min_dist[1,2]] + risk_map[nr, nc]\n    \n    if (dist_possible &lt; dist[nr, nc]) {\n      dist[nr, nc] &lt;- dist_possible\n    }\n  }\n  # If found the target point (bottom right corner) then stop\n  if (min_dist[1,1] == nrow(dist) & min_dist[1,2] == ncol(dist)) break\n}\n\nNow I just retrieve the distance (i.e.¬†accumulated risk) at the target point (the bottom right):\n\ndist[nrow(dist), ncol(dist)]\n\n[1] 589\n\n\n\n\nPart 2\n\nNow that you know how to find low-risk paths in the cave, you can try to find your way out.\nThe entire cave is actually five times larger in both dimensions than you thought; the area you originally scanned is just one tile in a 5x5 tile area that forms the full map. Your original map tile repeats to the right and downward; each time the tile repeats to the right or downward, all of its risk levels are 1 higher than the tile immediately up or left of it. However, risk levels above 9 wrap back around to 1.\nUsing the full map, what is the lowest total risk of any path from the top left to the bottom right?\n\nAssemble the full risk map:\n\nrisk_map_part2 &lt;- map(0:4, ~ risk_map + .x) %&gt;%\n  reduce(rbind)\nrisk_map_part2 &lt;- map(0:4, ~ risk_map_part2 + .x) %&gt;%\n  reduce(cbind)\nrisk_map_part2 &lt;- risk_map_part2 %% 9\n# The modulo operator returns 0 if a value = 9, but we want 9\nrisk_map_part2[risk_map_part2 == 0] &lt;- 9\nmax_row &lt;- nrow(risk_map_part2)\nmax_col &lt;- ncol(risk_map_part2)\n\nThis might push the limitations of my algorithm implementation, but I‚Äôll give it a try (and print out the runtime):\n\ndist &lt;- matrix(Inf, nrow = nrow(risk_map_part2), ncol = ncol(risk_map_part2))\n# Set the starting point\ndist[1, 1] &lt;- 0\n\nvisited &lt;-\n  matrix(FALSE, nrow = nrow(risk_map_part2), ncol = ncol(risk_map_part2))\n\n\ntic()\nwhile (!all(visited)) {\n  # Get unvisited points\n  unvisited &lt;- which(!visited, arr.ind = TRUE)\n  # Find the unvisited point with the minimum distance\n  min_dist &lt;- unvisited[dist[unvisited] == min(dist[unvisited]), , drop = FALSE]\n  # Mark this point as visited\n  visited[min_dist[1,1], min_dist[1,2]] &lt;- TRUE\n  \n  # Get neighbors of the point\n  neighbors &lt;- get_neighbors(min_dist[1,1], min_dist[1,2], max_row, max_col)\n  # But exclude ones that have been visited\n  neighbors &lt;- neighbors[!visited[neighbors], , drop = FALSE]\n  \n  for (n in seq_len(nrow(neighbors))) {\n    nr &lt;- neighbors[n, 1]\n    nc &lt;- neighbors[n, 2]\n    dist_possible &lt;- dist[min_dist[1,1], min_dist[1,2]] + risk_map_part2[nr, nc]\n    \n    if (dist_possible &lt; dist[nr, nc]) {\n      dist[nr, nc] &lt;- dist_possible\n    }\n  }\n  \n  # If found the target point (bottom right corner) then stop\n  if (min_dist[1,1] == nrow(dist) & min_dist[1,2] == ncol(dist)) break\n}\ntoc()\n\n1816.42 sec elapsed\n\n\n\ndist[nrow(dist), ncol(dist)]\n\n[1] 2885\n\n\nAbout 30 minutes, which is pretty bad."
  },
  {
    "objectID": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#stats",
    "href": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#stats",
    "title": "Advent of Code 2021: Days 11-15",
    "section": "Stats",
    "text": "Stats\nHere are my personal stats for days 10-15:\n\ntibble::tribble(\n  ~Part, ~Day, ~Time, ~Rank, ~Score,\n  1, 15, \"15:00:27\", 22738, 0,\n  2, 15, \"16:34:36\",  19838, 0,\n  1, 14, \"14:42:51\", 32640, 0,\n  2, 14, \"15:09:24\",  24583, 0,\n  1, 13, \"11:55:14\", 26780, 0,\n  2, 13, \"12:00:15\", 25708, 0,\n  1, 12, \"10:15:12\", 21781, 0,\n  2, 12, \"10:27:19\", 19557, 0,\n  1, 11, \"12:25:31\", 27526, 0,\n  2, 11,  \"12:31:12\", 27190, 0\n) %&gt;%\n  select(-Score) %&gt;%\n  pivot_wider(names_from = Part, values_from = c(Time, Rank),\n              names_glue = \"Part {Part}_{.value}\") %&gt;%\n  mutate(\n    `Time between parts` = as.numeric(hms(`Part 2_Time`) - hms(`Part 1_Time`),\n                                      \"minutes\") %&gt;% round(1)\n  ) %&gt;%\n  gt() %&gt;%\n  tab_spanner_delim(delim = \"_\", split = \"first\")\n\n\n\n\n\n\n\n\nDay\nTime\nRank\nTime between parts\n\n\nPart 1\nPart 2\nPart 1\nPart 2\n\n\n\n\n15\n15:00:27\n16:34:36\n22738\n19838\n94.2\n\n\n14\n14:42:51\n15:09:24\n32640\n24583\n26.6\n\n\n13\n11:55:14\n12:00:15\n26780\n25708\n5.0\n\n\n12\n10:15:12\n10:27:19\n21781\n19557\n12.1\n\n\n11\n12:25:31\n12:31:12\n27526\n27190\n5.7\n\n\n\n\n\n\n\nAnd here is my position on the private leaderboard:\n\nlibrary(httr)\n\nleaderboard &lt;- httr::GET(\n  url = \"https://adventofcode.com/2021/leaderboard/private/view/1032765.json\",\n  httr::set_cookies(session = Sys.getenv(\"AOC_COOKIE\"))\n) %&gt;%\n  content() %&gt;%\n  as_tibble() %&gt;%\n  unnest_wider(members) %&gt;%\n  arrange(desc(local_score)) %&gt;%\n  transmute(\n    Rank = 1:n(), Name = name, Score = local_score, Stars = stars\n  )\n\n\nleaderboard %&gt;%\n  gt() %&gt;%\n  text_transform(\n    locations = cells_body(columns = Stars),\n    fn = function(stars_col) {\n      map_chr(stars_col,\n              ~html(paste0(.x, fontawesome::fa('star', fill = 'gold'))))\n    }\n  ) %&gt;%\n  cols_align(\"left\") %&gt;%\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_body(\n      rows = (Name == \"taylordunn\")\n    )\n  ) %&gt;%\n  tab_options(container.height = 500)\n\n\n\n\n\n\n\n\nRank\nName\nScore\nStars\n\n\n\n\n1\nEmil Hvitfeldt\n3806\n30\n\n\n2\nDavid Robinson\n3768\n30\n\n\n3\nColin Rundel\n3768\n30\n\n\n4\ntrang1618\n3729\n30\n\n\n5\n@ClareHorscroft\n3728\n30\n\n\n6\n@_TanHo\n3683\n30\n\n\n7\nIldik√≥ Czeller\n3564\n30\n\n\n8\ndhimmel\n3504\n29\n\n\n9\nJaros≈Çaw Nirski\n3492\n30\n\n\n10\nJonathan Spring\n3392\n30\n\n\n11\npritikadasgupta\n3381\n30\n\n\n12\nJosh Gray\n3258\n30\n\n\n13\nJean-Rubin\n3172\n30\n\n\n14\ngpecci\n3158\n27\n\n\n15\nAnna Fergusson\n3003\n28\n\n\n16\nashbaldry\n2993\n30\n\n\n17\nTom Jemmett\n2982\n30\n\n\n18\nRiinu Pius\n2954\n30\n\n\n19\nmbjoseph\n2838\n30\n\n\n20\nMelinda Tang\n2824\n27\n\n\n21\njohn-b-edwards\n2751\n24\n\n\n22\n@Mid1995Sed\n2745\n30\n\n\n23\nCalum You\n2688\n30\n\n\n24\npatelis\n2682\n30\n\n\n25\nFarhan Reynaldo\n2631\n28\n\n\n26\nJaap Walhout\n2600\n29\n\n\n27\nSherry Zhang\n2592\n24\n\n\n28\n@_mnar99\n2577\n24\n\n\n29\nJim Leach\n2544\n30\n\n\n30\nmartigso\n2538\n30\n\n\n31\nhrushikeshrv\n2504\n24\n\n\n32\nlong39ng\n2494\n30\n\n\n33\nTokhir Dadaev\n2448\n26\n\n\n34\nfabio machado\n2399\n25\n\n\n35\njordi figueras puig\n2386\n27\n\n\n36\nKT421\n2371\n28\n\n\n37\ntaylordunn\n2360\n30\n\n\n38\nAlbertRapp\n2359\n29\n\n\n39\nDoortje Theunissen\n2320\n28\n\n\n40\nTJ Mahr\n2281\n30\n\n\n41\nAndrew Argeros\n2244\n22\n\n\n42\nDarrin Speegle\n2238\n29\n\n\n43\nAron Strandberg\n2231\n27\n\n\n44\n@E_E_Akcay\n2148\n24\n\n\n45\nJacqueline Nolis\n2105\n17\n\n\n46\nmkiang\n2002\n17\n\n\n47\nNathan Moore\n1934\n24\n\n\n48\nDerek Holliday\n1907\n18\n\n\n49\nduju211\n1896\n26\n\n\n50\nscalgary\n1830\n23\n\n\n51\nKelly N. Bodwin\n1827\n22\n\n\n52\ndelabj\n1810\n22\n\n\n53\nFlavien Petit\n1765\n21\n\n\n54\nHannesOberreiter\n1722\n18\n\n\n55\nMatt Onimus\n1650\n22\n\n\n56\nJeffrey Brabec\n1625\n22\n\n\n57\nAlex N\n1483\n17\n\n\n58\nCarlssonLeo\n1465\n20\n\n\n59\nrywhale\n1450\n20\n\n\n60\nDaniel Coulton\n1423\n18\n\n\n61\n@woodspock\n1419\n17\n\n\n62\nJenna Jordan\n1413\n24\n\n\n63\nZach Bogart üíô\n1388\n14\n\n\n64\nkarawoo\n1369\n15\n\n\n65\nArun Chavan\n1329\n17\n\n\n66\nTylerGrantSmith\n1258\n17\n\n\n67\nexunckly\n1243\n15\n\n\n68\nScott-Gee\n1182\n17\n\n\n69\nblongworth\n1182\n19\n\n\n70\npi55p00r\n1072\n15\n\n\n71\nNerwosolek\n1035\n15\n\n\n72\nGhislain Nono Gueye\n964\n14\n\n\n73\nErez Shomron\n951\n14\n\n\n74\nMiha Gazvoda\n916\n14\n\n\n75\ncramosu\n892\n10\n\n\n76\ncathblatter\n851\n13\n\n\n77\nSydney\n815\n11\n\n\n78\nA-Farina\n787\n15\n\n\n79\nMetaMoraleMundo\n768\n7\n\n\n80\n@mfarkhann\n757\n15\n\n\n81\njwinget\n745\n15\n\n\n82\nAndrew Tungate\n703\n15\n\n\n83\ncollinberke\n679\n8\n\n\n84\nldnam\n657\n6\n\n\n85\nEric Ekholm\n629\n11\n\n\n86\ncynthiahqy\n620\n14\n\n\n87\ndirkschumacher\n612\n10\n\n\n88\nAdam Mahood\n610\n6\n\n\n89\nGypeti Casino\n595\n12\n\n\n90\nmfiorina\n593\n10\n\n\n91\nMaya Gans\n585\n11\n\n\n92\nantdurrant\n565\n9\n\n\n93\nDavid Schoch\n545\n6\n\n\n94\nJulian Tagell\n469\n5\n\n\n95\nAmitLevinson\n469\n7\n\n\n96\nJosiah Parry\n450\n7\n\n\n97\nthedivtagguy\n430\n6\n\n\n98\nandrew-tungate-cms\n415\n6\n\n\n99\n@Maatspencer\n405\n8\n\n\n100\n@KentWeyrauch\n398\n8\n\n\n101\nWendy Christensen\n388\n6\n\n\n102\nEmryn Hofmann\n387\n7\n\n\n103\ncolumbaspexit\n377\n8\n\n\n104\nALBERT\n373\n4\n\n\n105\nAlan Feder\n341\n6\n\n\n106\nKevin Kent\n333\n7\n\n\n107\nolmgeorg\n323\n6\n\n\n108\nDaniel Gemara\n298\n4\n\n\n109\nquickcoffee\n279\n6\n\n\n110\nAndrew Fraser\n245\n3\n\n\n111\nsoto solo\n241\n3\n\n\n112\njennifer-furman\n240\n4\n\n\n113\nAdrian Perez\n213\n4\n\n\n114\nBilly Fryer\n206\n5\n\n\n115\nApril\n195\n2\n\n\n116\nLukas Gr√∂ninger\n171\n4\n\n\n117\nKyle Ligon\n166\n6\n\n\n118\nDuncan Gates\n127\n5\n\n\n119\nJose Pliego San Martin\n123\n2\n\n\n120\naleighbrown\n116\n2\n\n\n121\nBruno Mioto\n96\n3\n\n\n122\nchapmandu2\n64\n4\n\n\n123\n@jdknguyen\n36\n3\n\n\n124\nMatthew Wankiewicz\n20\n1\n\n\n125\nCaioBrighenti\n0\n0\n\n\n126\njacquietran\n0\n0\n\n\n127\nTony ElHabr\n0\n0\n\n\n128\nRizky Luthfianto\n0\n0\n\n\n129\nWiktor Jacaszek\n0\n0\n\n\n130\nNA\n0\n0\n\n\n\n\n\n\n\nPretty happy with 37th place so far."
  },
  {
    "objectID": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#reproducibility",
    "href": "posts/2021-12-11-advent-of-code-2021-days-11-15/index.html#reproducibility",
    "title": "Advent of Code 2021: Days 11-15",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\npython:         C:/Users/tdunn/Documents/.virtualenvs/r-reticulate/Scripts/python.exe\nlibpython:      C:/Users/tdunn/AppData/Local/r-reticulate/r-reticulate/pyenv/pyenv-win/versions/3.9.13/python39.dll\npythonhome:     C:/Users/tdunn/Documents/.virtualenvs/r-reticulate\nversion:        3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          C:/Users/tdunn/Documents/.virtualenvs/r-reticulate/Lib/site-packages/numpy\nnumpy_version:  1.23.3\n\nNOTE: Python version was forced by use_python function\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html",
    "href": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html",
    "title": "Advent of Code 2021: Days 16-20",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(tictoc)\nlibrary(lubridate)"
  },
  {
    "objectID": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#day-16-packet-decoder",
    "href": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#day-16-packet-decoder",
    "title": "Advent of Code 2021: Days 16-20",
    "section": "Day 16: Packet Decoder",
    "text": "Day 16: Packet Decoder\n\nday16 &lt;- read_lines(\"day16-input.txt\")\nday16 %&gt;% str_trunc(70)\n\n[1] \"C20D718021600ACDC372CD8DE7A057252A49C940239D68978F7970194EA7CCB3100...\"\n\n\n\nPart 1\n\n\nVery long puzzle statement\n\n\nAs you leave the cave and reach open waters, you receive a transmission from the Elves back on the ship.\nThe transmission was sent using the Buoyancy Interchange Transmission System (BITS), a method of packing numeric expressions into a binary sequence. Your submarine‚Äôs computer has saved the transmission in hexadecimal (your puzzle input).\nThe first step of decoding the message is to convert the hexadecimal representation into binary. Each character of hexadecimal corresponds to four bits of binary data:\nThe BITS transmission contains a single packet at its outermost layer which itself contains many other packets. The hexadecimal representation of this packet might encode a few extra 0 bits at the end; these are not part of the transmission and should be ignored.\nEvery packet begins with a standard header: the first three bits encode the packet version, and the next three bits encode the packet type ID. These two values are numbers; all numbers encoded in any packet are represented as binary with the most significant bit first. For example, a version encoded as the binary sequence 100 represents the number 4.\nPackets with type ID 4 represent a literal value. Literal value packets encode a single binary number. To do this, the binary number is padded with leading zeroes until its length is a multiple of four bits, and then it is broken into groups of four bits. Each group is prefixed by a 1 bit except the last group, which is prefixed by a 0 bit. These groups of five bits immediately follow the packet header.\nEvery other type of packet (any packet with a type ID other than 4) represent an operator that performs some calculation on one or more sub-packets contained within. Right now, the specific operations aren‚Äôt important; focus on parsing the hierarchy of sub-packets.\nAn operator packet contains one or more packets. To indicate which subsequent binary data represents its sub-packets, an operator packet can use one of two modes indicated by the bit immediately after the packet header; this is called the length type ID:\n\nIf the length type ID is 0, then the next 15 bits are a number that represents the total length in bits of the sub-packets contained by this packet.\nIf the length type ID is 1, then the next 11 bits are a number that represents the number of sub-packets immediately contained by this packet.\n\nFinally, after the length type ID bit and the 15-bit or 11-bit field, the sub-packets appear.\nFor now, parse the hierarchy of the packets throughout the transmission and add up all of the version numbers.\nDecode the structure of your hexadecimal-encoded BITS transmission; what do you get if you add up the version numbers in all packets?\n\n\nStart by defining the hexadecimal character mapping and converting the input:\n\nhex_map &lt;- c(\n  \"0\" = \"0000\", \"1\" = \"0001\", \"2\" = \"0010\", \"3\" = \"0011\", \"4\" = \"0100\",\n  \"5\" = \"0101\", \"6\" = \"0110\", \"7\" = \"0111\", \"8\" = \"1000\", \"9\" = \"1001\",\n  \"A\" = \"1010\", \"B\" = \"1011\", \"C\" = \"1100\", \"D\" = \"1101\", \"E\" = \"1110\",\n  \"F\" = \"1111\"\n)\ninput &lt;- str_replace_all(day16, hex_map)\nstr_trunc(input, 90)\n\n[1] \"110000100000110101110001100000000010000101100000000010101100110111000011011100101100110...\"\n\n\nSecond, define a function for parsing literal values (packets with type ID 4):\n\ndecode_literal_value &lt;- function(input, i) {\n  literal_value &lt;- \"\"\n  repeat {\n    lv &lt;- str_sub(input, i, i + 4)\n    literal_value &lt;- paste0(literal_value, str_sub(lv, 2, 5))\n    i &lt;- i + 5\n    # If the bit starts with 0, end of packet\n    if (str_sub(lv, 1, 1) == \"0\") break\n    # Else not the last group, keep reading\n  }\n  \n  list(i, literal_value)\n}\n\nNow define the recursive function to parse the full strings:\n\ndecode_packet_part1 &lt;- function(input, i = 1) {\n  packet_vers &lt;- c(str_sub(input, i, i + 2) %&gt;% strtoi(base = 2))\n  packet_type &lt;- str_sub(input, i + 3, i + 5) %&gt;% strtoi(base = 2)\n  i &lt;- i + 6\n  \n  # Literal values\n  if (packet_type == 4) {\n    literal_value &lt;- decode_literal_value(input, i)\n    i &lt;- literal_value[[1]]\n    value &lt;- strtoi(literal_value[[2]], base = 2)\n    return(list(value = value, index = i, packet_vers = packet_vers))\n  }\n  \n  # Otherwise an operator\n  values &lt;- c()\n  length_type &lt;- str_sub(input, i, i)\n  i &lt;- i + 1\n  \n  if (length_type == \"0\") {\n    # Next 15 bits are the number of bits in the sub-packets\n    n_bits &lt;- str_sub(input, i, i + 14) %&gt;% strtoi(base = 2)\n    i &lt;- i + 15\n    \n    end_bit &lt;- i + n_bits\n    while (i &lt; end_bit) {\n      subpacket &lt;- decode_packet_part1(input, i)\n      packet_vers &lt;- c(packet_vers, subpacket$packet_vers)\n      values &lt;- c(values, subpacket$value)\n      i &lt;- subpacket$index\n    }\n  } else if (length_type == \"1\") {\n    # Next 11 bits are the number of sub-packets\n    n_subpackets &lt;- str_sub(input, i, i + 10) %&gt;% strtoi(base = 2)\n    i &lt;- i + 11\n    \n    for (sp in 1:n_subpackets) {\n      subpacket &lt;- decode_packet_part1(input, i)\n      packet_vers &lt;- c(packet_vers, subpacket$packet_vers)\n      values &lt;- c(values, subpacket$value)\n      i &lt;- subpacket$index\n    }  \n  }\n  \n  return(list(value = values, index = i, packet_vers = packet_vers))\n}\n\nApply it to the input and sum up the versions:\n\npart1_results &lt;- decode_packet_part1(input)\nsum(part1_results$packet_vers)\n\n[1] 852\n\n\nPython:\n\nhex_map = {'0': '0000', '1': '0001', '2': '0010', '3': '0011', '4': '0100',\n           '5': '0101', '6': '0110', '7': '0111', '8': '1000', '9': '1001',\n           'A': '1010', 'B': '1011', 'C': '1100', 'D': '1101', 'E': '1110',\n           'F': '1111'}\n           \ninput = r.day16\nfor key, val in hex_map.items():\n  input = input.replace(key, val)\n  \ndef decode_literal_value(input, i = 0):\n  literal_value = \"\"\n  while True:\n    lv = input[i:i+5]\n    literal_value += lv[1:]\n    i += 5\n    \n    if lv[0] == \"0\": break\n    \n  return i, int(literal_value, 2)\n\nsum_packet_vers = 0\ndef decode_packet_part1(input, i = 0):\n  global sum_packet_vers\n  sum_packet_vers += int(input[i:i+3], 2)\n  packet_type = int(input[i+3:i+6], 2)\n  i += 6\n  \n  if packet_type == 4:\n    i, value = decode_literal_value(input, i)\n    return i, value\n  \n  values = []\n  length_type = input[i]\n  i += 1\n  \n  if length_type == \"0\":\n    n_bits = int(input[i:i+15], 2)\n    i += 15\n    end_bit = i + n_bits\n    while i &lt; end_bit:\n      i, value = decode_packet_part1(input, i)\n      values.append(value)\n  else:\n    n_subpackets = int(input[i:i+11], 2)\n    i += 11\n    for _ in range(n_subpackets):\n      i, value = decode_packet_part1(input, i)\n      values.append(value)\n  \n  return i, values\n\n_, _ = decode_packet_part1(input)\nsum_packet_vers\n\n852\n\n\n\n\nPart 2\n\nNow that you have the structure of your transmission decoded, you can calculate the value of the expression it represents.\nLiteral values (type ID 4) represent a single number as described above. The remaining type IDs are more interesting:\n\nPackets with type ID 0 are sum packets - their value is the sum of the values of their sub-packets. If they only have a single sub-packet, their value is the value of the sub-packet.\nPackets with type ID 1 are product packets - their value is the result of multiplying together the values of their sub-packets. If they only have a single sub-packet, their value is the value of the sub-packet.\nPackets with type ID 2 are minimum packets - their value is the minimum of the values of their sub-packets.\nPackets with type ID 3 are maximum packets - their value is the maximum of the values of their sub-packets.\nPackets with type ID 5 are greater than packets - their value is 1 if the value of the first sub-packet is greater than the value of the second sub-packet; otherwise, their value is 0. These packets always have exactly two sub-packets.\nPackets with type ID 6 are less than packets - their value is 1 if the value of the first sub-packet is less than the value of the second sub-packet; otherwise, their value is 0. These packets always have exactly two sub-packets.\nPackets with type ID 7 are equal to packets - their value is 1 if the value of the first sub-packet is equal to the value of the second sub-packet; otherwise, their value is 0. These packets always have exactly two sub-packets.\n\nUsing these rules, you can now work out the value of the outermost packet in your BITS transmission.\n\nDefine a function to compute packet values following the above rules:\n\ncompute_packet_val &lt;- function(packet_type, literal_values) {\n  switch(\n    packet_type + 1,\n    sum(literal_values),\n    prod(literal_values),\n    min(literal_values),\n    max(literal_values),\n    NA, # packet_type = 4\n    ifelse(literal_values[1] &gt; literal_values[2], 1, 0),\n    ifelse(literal_values[1] &lt; literal_values[2], 1, 0),\n    ifelse(literal_values[1] == literal_values[2], 1, 0),\n  )\n}\n\nThe solution to this part took about an hour of debugging before I realized that the base R function strtoi cannot handle numbers greater than 2^31. Define a custom strtoi that accounts for integer overflow:\n\nstrtoi_custom &lt;- function(x) {\n  y &lt;- as.numeric(strsplit(x, \"\")[[1]])\n  sum(y * 2^rev((seq_along(y) - 1)))\n}\n\nNow re-define the function from part 1 to return the value of the outermost packet in the transmission:\n\ndecode_packet_part2 &lt;- function(input, i = 1) {\n  packet_vers &lt;- c(str_sub(input, i, i + 2) %&gt;% strtoi(base = 2))\n  packet_type &lt;- str_sub(input, i + 3, i + 5) %&gt;% strtoi(base = 2)\n  i &lt;- i + 6\n  \n  if (packet_type == 4) {\n    literal_value &lt;- decode_literal_value(input, i)\n    i &lt;- literal_value[[1]]\n    value &lt;- strtoi_custom(literal_value[[2]])\n    return(list(value = value, index = i, packet_vers = packet_vers))\n  }\n  \n  # Otherwise, an operator\n  values &lt;- c()\n  length_type &lt;- str_sub(input, i, i)\n  i &lt;- i + 1\n  \n  if (length_type == \"0\") {\n    # Next 15 bits are the number of bits in the sub-packets\n    n_bits &lt;- str_sub(input, i, i + 14) %&gt;% strtoi(base = 2)\n    i &lt;- i + 15\n    \n    end_bit &lt;- i + n_bits\n    while (i &lt; end_bit) {\n      subpacket &lt;- decode_packet_part2(input, i)\n      packet_vers &lt;- c(packet_vers, subpacket$packet_vers)\n      values &lt;- c(values, subpacket$value)\n      i &lt;- subpacket$index\n    }\n  } else if (length_type == \"1\") {\n    # Next 11 bits are the number of sub-packets\n    n_subpackets &lt;- str_sub(input, i, i + 10) %&gt;% strtoi(base = 2)\n    i &lt;- i + 11\n    \n    for (sp in 1:n_subpackets) {\n      subpacket &lt;- decode_packet_part2(input, i)\n      packet_vers &lt;- c(packet_vers, subpacket$packet_vers)\n      values &lt;- c(values, subpacket$value)\n      i &lt;- subpacket$index\n    }  \n  }\n  \n  value &lt;- compute_packet_val(packet_type, values)\n  \n  return(list(value = value, index = i, packet_vers = packet_vers))\n}\n\npart2_results &lt;- decode_packet_part2(input)\nformat(part2_results$value, scientific = FALSE)\n\n[1] \"19348959966392\"\n\n\nPython:\n\ndef compute_packet_val(packet_type, values):\n  if packet_type == 0:\n    return sum(values)\n  elif packet_type == 1:\n    prod = 1\n    for v in values:\n      prod *= v\n    return prod\n  elif packet_type == 2:\n    return min(values)\n  elif packet_type == 3:\n    return max(values)\n  elif packet_type == 5:\n    return int(values[0] &gt; values[1]) \n  elif packet_type == 6:\n    return int(values[0] &lt; values[1]) \n  elif packet_type == 7:\n    return int(values[0] == values[1]) \n\ndef decode_packet_part2(input, i = 0):\n  packet_vers = int(input[i:i+3], 2)\n  packet_type = int(input[i+3:i+6], 2)\n  i += 6\n  \n  if packet_type == 4:\n    i, value = decode_literal_value(input, i)\n    return i, value\n  \n  values = []\n  length_type = input[i]\n  i += 1\n  \n  if length_type == \"0\":\n    n_bits = int(input[i:i+15], 2)\n    i += 15\n    end_bit = i + n_bits\n    while i &lt; end_bit:\n      i, value = decode_packet_part2(input, i)\n      values.append(value)\n  else:\n    n_subpackets = int(input[i:i+11], 2)\n    i += 11\n    for _ in range(n_subpackets):\n      i, value = decode_packet_part2(input, i)\n      values.append(value)\n  \n  value = compute_packet_val(packet_type, values) \n  \n  return i, value\n\n_, value = decode_packet_part2(input)\nvalue\n\n19348959966392"
  },
  {
    "objectID": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#day-17-trick-shot",
    "href": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#day-17-trick-shot",
    "title": "Advent of Code 2021: Days 16-20",
    "section": "Day 17: Trick Shot",
    "text": "Day 17: Trick Shot\n\nday17 &lt;- read_lines(\"day17-input.txt\")\nday17\n\n[1] \"target area: x=156..202, y=-110..-69\"\n\n\n\nPart 1\n\nYou finally decode the Elves‚Äô message. HI, the message says. You continue searching for the sleigh keys.\nAhead of you is what appears to be a large ocean trench. Could the keys have fallen into it? You‚Äôd better send a probe to investigate.\nThe probe launcher on your submarine can fire the probe with any integer velocity in the x (forward) and y (upward, or downward if negative) directions. For example, an initial x,y velocity like 0,10 would fire the probe straight up, while an initial velocity like 10,-1 would fire the probe forward at a slight downward angle.\nThe probe‚Äôs x,y position starts at 0,0. Then, it will follow some trajectory by moving in steps. On each step, these changes occur in the following order:\n\nThe probe‚Äôs x position increases by its x velocity.\nThe probe‚Äôs y position increases by its y velocity.\nDue to drag, the probe‚Äôs x velocity changes by 1 toward the value 0; that is, it decreases by 1 if it is greater than 0, increases by 1 if it is less than 0, or does not change if it is already 0.\nDue to gravity, the probe‚Äôs y velocity decreases by 1.\n\nFor the probe to successfully make it into the trench, the probe must be on some trajectory that causes it to be within a target area after any step. The submarine computer has already calculated this target area (your puzzle input).\nFind the initial velocity that causes the probe to reach the highest y position and still eventually be within the target area after any step. What is the highest y position it reaches on this trajectory?\n\nUse tidyr::extract and regex to get the bounds of the target area:\n\ntarget_area &lt;- tibble(x = day17) %&gt;%\n  extract(x,\n          into = c(\"xmin\", \"xmax\", \"ymin\", \"ymax\"),\n          regex = \"x=(.*)\\\\.\\\\.(.*), y=(.*)\\\\.\\\\.(.*)\",\n          convert = TRUE)\ntarget_area\n\n# A tibble: 1 √ó 4\n   xmin  xmax  ymin  ymax\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1   156   202  -110   -69\n\n\nSince the \\(x\\) and \\(y\\) velocity are independent, we can work out the correct \\(y\\) velocity. Due to the way gravity works in this problem, any velocity \\(v_y &gt; 0\\) starting at \\(y = 0\\) will be \\(-v_y\\) when it returns to \\(y = 0\\) on the way down. For example, for initial \\(v_y = [1, 5]\\) (vy_start), these are the steps and velocities when the probe reaches \\(y = 0\\) again:\n\ncrossing(vy_start = 1:5, step = 1:20) %&gt;%\n  # Each step decreases y velocity by 1\n  mutate(vy = vy_start - step + 1) %&gt;%\n  group_by(vy_start) %&gt;%\n  # The y position is the cumulative sum of y velocities up this point\n  mutate(y = cumsum(vy)) %&gt;%\n  ungroup() %&gt;%\n  filter(y == 0)\n\n# A tibble: 5 √ó 4\n  vy_start  step    vy     y\n     &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1        1     3    -1     0\n2        2     5    -2     0\n3        3     7    -3     0\n4        4     9    -4     0\n5        5    11    -5     0\n\n\nThen it is just a matter of maximizing vy_start such that it lands at the lowest possible point in the target area, i.e.¬†ymin. The highest possible vy_start is the one that will reach ymin in a single step from y = 0, i.e.¬†vy_max = abs(ymin) - 1.\n\n(vy_max &lt;- abs(target_area$ymin) - 1)\n\n[1] 109\n\n\nTo calculate the \\(y\\) trajectory for this initial velocity, use cumsum:\n\ntrajectory &lt;- crossing(vy_max, step = 1:1000) %&gt;%\n  mutate(vy = vy_max - step + 1, y = cumsum(vy))\ntrajectory\n\n# A tibble: 1,000 √ó 4\n   vy_max  step    vy     y\n    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    109     1   109   109\n 2    109     2   108   217\n 3    109     3   107   324\n 4    109     4   106   430\n 5    109     5   105   535\n 6    109     6   104   639\n 7    109     7   103   742\n 8    109     8   102   844\n 9    109     9   101   945\n10    109    10   100  1045\n# ‚Ä¶ with 990 more rows\n\n\nHere is the step where the probe reaches the edge of the target area:\n\n(target_step &lt;- trajectory %&gt;% filter(y == target_area$ymin))\n\n# A tibble: 1 √ó 4\n  vy_max  step    vy     y\n   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    109   220  -110  -110\n\n\nAnd here is the maximum height reached by the probe (the answer to part 1):\n\nmax(trajectory$y)\n\n[1] 5995\n\n\nAnd just to be sure, before answering, I need to verify that there is an initial \\(x\\) velocity, \\(v_x\\), that is slow enough (while still being an integer) that the probe will be between xmin and xmax (156 and 202), when it reaches the target area.\nDue to drag, the \\(x\\) position will be a cumulative sum of decreasing integers. As I learned on day 7, this is a triangular number with the value:\n\\[\n\\sum_{i=1}^n k = \\frac{n (n + 1)}{2}.\n\\]\nThis means that the maximum \\(x\\) position reached for an initial \\(x\\) velocity \\(v_x\\) is:\n\\[\nx_{\\text{max}} = \\frac{v_x (v_x + 1)}{2}\n\\]\nWe need to find a \\(v_x\\) that reaches the target \\(x\\) between [156, 202] on or before step 220 (which is when we reach the target \\(y\\)).\n\ntibble(vx_start = 1:30) %&gt;%\n  mutate(xmax = vx_start * (vx_start + 1) / 2) %&gt;%\n  filter(xmax &gt;= target_area$xmin, xmax &lt;= target_area$xmax)\n\n# A tibble: 2 √ó 2\n  vx_start  xmax\n     &lt;int&gt; &lt;dbl&gt;\n1       18   171\n2       19   190\n\n\nSo an initial \\(v_x\\) (vx_start) of 18 or 19 will reach the target area in fewer than 220 steps, which can be shown with the cumsum method as well:\n\ntibble(vx_start = c(18, 19)) %&gt;%\n  crossing(step = 1:target_step$step) %&gt;%\n  mutate(vx = vx_start - step + 1,\n         # x velocity will never go lower than 0\n         vx = ifelse(vx &lt; 0, 0, vx)) %&gt;%\n  group_by(vx_start) %&gt;%\n  mutate(x = cumsum(vx)) %&gt;%\n  ungroup() %&gt;%\n  filter(x &gt;= target_area$xmin, x &lt;= target_area$xmax) %&gt;%\n  group_by(vx_start, x, vx) %&gt;%\n  filter(step == min(step))\n\n# A tibble: 16 √ó 4\n# Groups:   vx_start, x, vx [16]\n   vx_start  step    vx     x\n      &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1       18    13     6   156\n 2       18    14     5   161\n 3       18    15     4   165\n 4       18    16     3   168\n 5       18    17     2   170\n 6       18    18     1   171\n 7       18    19     0   171\n 8       19    12     8   162\n 9       19    13     7   169\n10       19    14     6   175\n11       19    15     5   180\n12       19    16     4   184\n13       19    17     3   187\n14       19    18     2   189\n15       19    19     1   190\n16       19    20     0   190\n\n\n\n\nPart 2\n\nMaybe a fancy trick shot isn‚Äôt the best idea; after all, you only have one probe, so you had better not miss.\nTo get the best idea of what your options are for launching the probe, you need to find every initial velocity that causes the probe to eventually be within the target area after any step.\nHow many distinct initial velocity values cause the probe to be within the target area after any step?\n\nFind candidate \\(y\\) velocities, which reach the target area at some step in their trajectories:\n\nvy_candidates &lt;-\n  crossing(vy_start = seq(target_area$ymin, vy_max), step = 1:1000) %&gt;%\n  mutate(vy = vy_start - step + 1) %&gt;%\n  group_by(vy_start) %&gt;%\n  mutate(y = cumsum(vy)) %&gt;%\n  ungroup() %&gt;%\n  filter(between(y, target_area$ymin, target_area$ymax)) %&gt;%\n  select(-vy)\nvy_candidates\n\n# A tibble: 268 √ó 3\n   vy_start  step     y\n      &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     -110     1  -110\n 2     -109     1  -109\n 3     -108     1  -108\n 4     -107     1  -107\n 5     -106     1  -106\n 6     -105     1  -105\n 7     -104     1  -104\n 8     -103     1  -103\n 9     -102     1  -102\n10     -101     1  -101\n# ‚Ä¶ with 258 more rows\n\n\nThere are 192 unique starting \\(v_y\\) values which will reach the \\(y\\) target. Do the same for the \\(x\\) target:\n\nvx_candidates &lt;- \n  crossing(vx_start = 1:target_area$xmax, step = 1:1000) %&gt;%\n  mutate(vx = vx_start - step + 1, vx = ifelse(vx &lt; 0, 0, vx)) %&gt;%\n  group_by(vx_start) %&gt;%\n  mutate(x = cumsum(vx)) %&gt;%\n  ungroup() %&gt;%\n  filter(between(x, target_area$xmin, target_area$xmax)) %&gt;%\n  select(-vx)\nvx_candidates\n\n# A tibble: 2,125 √ó 3\n   vx_start  step     x\n      &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1       18    13   156\n 2       18    14   161\n 3       18    15   165\n 4       18    16   168\n 5       18    17   170\n 6       18    18   171\n 7       18    19   171\n 8       18    20   171\n 9       18    21   171\n10       18    22   171\n# ‚Ä¶ with 2,115 more rows\n\n\nThere are 121 unique starting \\(v_x\\) values which will reach the \\(x\\) target.\nTo determine which combination of \\(v_x\\) and \\(v_y\\) are valid, I need to find the combinations that line up in steps taken to reach the target:\n\nvxy_candidates &lt;- vx_candidates %&gt;%\n  inner_join(vy_candidates, by = \"step\") %&gt;%\n  filter(!is.na(vx_start), !is.na(vy_start))\nvxy_candidates %&gt;%\n  group_by(vx_start, vy_start) %&gt;%\n  summarise(steps = str_c(step, collapse = \", \"), .groups = \"drop\")\n\n# A tibble: 3,202 √ó 3\n   vx_start vy_start steps     \n      &lt;int&gt;    &lt;int&gt; &lt;chr&gt;     \n 1       18       -2 13        \n 2       18       -1 13, 14    \n 3       18        0 13, 14, 15\n 4       18        1 14, 15, 16\n 5       18        2 15, 16, 17\n 6       18        3 16, 17, 18\n 7       18        4 18, 19, 20\n 8       18        5 19, 20, 21\n 9       18        6 20, 21, 22\n10       18        7 22, 23, 24\n# ‚Ä¶ with 3,192 more rows\n\n\nThere are 3202 unique combinations of vx_start and vy_start which reach the target area during one or more steps."
  },
  {
    "objectID": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#day-18-snailfish",
    "href": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#day-18-snailfish",
    "title": "Advent of Code 2021: Days 16-20",
    "section": "Day 18: Snailfish",
    "text": "Day 18: Snailfish\n\nday18 &lt;- read_lines(\"day18-input.txt\")\nhead(day18)\n\n[1] \"[[7,[1,5]],[[5,7],[[0,8],2]]]\"        \n[2] \"[[[[7,3],[2,2]],3],[[[7,1],[9,1]],2]]\"\n[3] \"[[[3,[0,2]],[5,2]],8]\"                \n[4] \"[[[[1,5],8],[[0,5],[0,0]]],2]\"        \n[5] \"[[[[9,9],1],[8,[3,2]]],[0,8]]\"        \n[6] \"[[[[0,8],6],6],[[1,5],[[5,5],[4,6]]]]\"\n\n\n\nPart 1\n\n\nLong problem statement\n\n\nYou descend into the ocean trench and encounter some snailfish. They say they saw the sleigh keys! They‚Äôll even tell you which direction the keys went if you help one of the smaller snailfish with his math homework.\nSnailfish numbers aren‚Äôt like regular numbers. Instead, every snailfish number is a pair - an ordered list of two elements. Each element of the pair can be either a regular number or another pair.\nPairs are written as [x,y], where x and y are the elements within the pair. Here are some example snailfish numbers, one snailfish number per line:\n[1,2] [[1,2],3] [9,[8,7]] [[1,9],[8,5]]\nThis snailfish homework is about addition. To add two snailfish numbers, form a pair from the left and right parameters of the addition operator. For example, [1,2] + [[3,4],5] becomes [[1,2],[[3,4],5]].\nThere‚Äôs only one problem: snailfish numbers must always be reduced, and the process of adding two snailfish numbers can result in snailfish numbers that need to be reduced.\nTo reduce a snailfish number, you must repeatedly do the first action in this list that applies to the snailfish number:\n\nIf any pair is nested inside four pairs, the leftmost such pair explodes.\nIf any regular number is 10 or greater, the leftmost such regular number splits.\n\nOnce no action in the above list applies, the snailfish number is reduced.\nDuring reduction, at most one action applies, after which the process returns to the top of the list of actions. For example, if split produces a pair that meets the explode criteria, that pair explodes before other splits occur.\nTo explode a pair, the pair‚Äôs left value is added to the first regular number to the left of the exploding pair (if any), and the pair‚Äôs right value is added to the first regular number to the right of the exploding pair (if any). Exploding pairs will always consist of two regular numbers. Then, the entire exploding pair is replaced with the regular number 0.\nTo split a regular number, replace it with a pair; the left element of the pair should be the regular number divided by two and rounded down, while the right element of the pair should be the regular number divided by two and rounded up. For example, 10 becomes [5,5], 11 becomes [5,6], 12 becomes [6,6], and so on.\nThe homework assignment involves adding up a list of snailfish numbers (your puzzle input). The snailfish numbers are each listed on a separate line. Add the first snailfish number and the second, then add that result and the third, then add that result and the fourth, and so on until all numbers in the list have been used once.\nTo check whether it‚Äôs the right answer, the snailfish teacher only checks the magnitude of the final sum. The magnitude of a pair is 3 times the magnitude of its left element plus 2 times the magnitude of its right element. The magnitude of a regular number is just that number.\nAdd up all of the snailfish numbers from the homework assignment in the order they appear. What is the magnitude of the final sum?\n\n\nFor reading the data in, I‚Äôll parse it like JSON with the jsonlite package:\n\nlibrary(jsonlite)\ninput &lt;- map(day18, fromJSON, simplifyVector = FALSE)\n\nThrough lots of trial and error working through the examples, I wrote the following helper functions:\n\n\nHelper functions\n\nThis gets the nested location and value of the first-occurring exploding pair:\n\nget_explode_pair &lt;- function(sn) {\n  ei &lt;- purrr::detect_index(sn, ~vec_depth(.x) &gt;= 5)\n   \n  if (ei &gt; 0) {\n    ep &lt;- pluck(sn, ei)\n    # Get the indicies that specify the exploding pair\n    vd &lt;- vec_depth(sn[[ei]])\n    for (d in seq(vd - 1, 2)) {\n      ei &lt;- c(ei, detect_index(ep, ~vec_depth(.x) == d))\n      ep &lt;- pluck(ep, tail(ei, 1))\n    }\n    return(list(ei, unlist(ep)))\n  } else {\n    return(list())\n  }\n}\nex1 &lt;- fromJSON(\"[[[[[9,8],1],2],3],4]\", simplifyVector = FALSE)\n(res &lt;- get_explode_pair(ex1))\n\n[[1]]\n[1] 1 1 1 1\n\n[[2]]\n[1] 9 8\n\n\nThis finds the left- and right-most values adjacent to an exploding pair:\n\nfind_adjacent_idx &lt;- function(sn, ei, side = c(\"L\", \"R\")) {\n  if (side == \"L\") {\n    # Remove levels until we find a node to the left\n    while (tail(ei, 1) == 1) {\n      ei &lt;- head(ei, -1)\n      if (length(ei) == 0) return(ei) # no nodes to the left\n    }\n    ei[length(ei)] &lt;- ei[length(ei)] - 1\n    \n    # Add levels until we find the right-most regular number\n    while (is.list(pluck(sn, !!!ei))) {\n      ei &lt;- c(ei, length(pluck(sn, !!!ei)))\n    }\n    \n  } else if (side == \"R\") {\n    # Remove levels until we find a node to the right\n    ei[length(ei)] &lt;- ei[length(ei)] + 1\n    while (is.null(pluck(sn, !!!ei))) {\n      ei &lt;- head(ei, -1)\n      if (length(ei) == 0) return(ei) # no nodes to the right\n      ei[length(ei)] &lt;- ei[length(ei)] + 1\n    }\n    \n    # Add levels until we find the left-most regular number\n    while (is.list(pluck(sn, !!!ei))) {\n      ei &lt;- c(ei, 1)\n    }\n  }\n  return(ei)\n}\n\nright_idx &lt;- find_adjacent_idx(ex1, res[[1]], side = \"R\")\nright_idx\n\n[1] 1 1 1 2\n\n\nThis explodes the pair at the given location:\n\nexplode_pair &lt;- function(sn, res) {\n  ei &lt;- res[[1]]\n  ep &lt;- res[[2]]\n  \n  left_idx &lt;- find_adjacent_idx(sn, ei, side = \"L\")\n  if (length(left_idx) &gt; 0) {\n    pluck(sn, !!!left_idx) &lt;- pluck(sn, !!!left_idx) + ep[1]\n  }\n  right_idx &lt;- find_adjacent_idx(sn, ei, side = \"R\")\n  if (length(right_idx) &gt; 0) {\n    pluck(sn, !!!right_idx) &lt;- pluck(sn, !!!right_idx) + ep[2]\n  }\n  \n  pluck(sn, !!!ei) &lt;- 0\n  return(sn)\n}\nexplode_pair(ex1, res)\n\n[[1]]\n[[1]][[1]]\n[[1]][[1]][[1]]\n[[1]][[1]][[1]][[1]]\n[1] 0\n\n[[1]][[1]][[1]][[2]]\n[1] 9\n\n\n[[1]][[1]][[2]]\n[1] 2\n\n\n[[1]][[2]]\n[1] 3\n\n\n[[2]]\n[1] 4\n\n\nThis finds the index and value of a split:\n\nget_split &lt;- function(sn) {\n  # Find the first value &gt;= 10 (if it exists)\n  sv &lt;- unlist(sn)\n  sv &lt;- sv[sv &gt;= 10][1]\n  if (is.na(sv)) {\n    return(list())\n  } else {\n    si &lt;- purrr::detect_index(sn, ~any(unlist(.x) == sv))\n    sn &lt;- pluck(sn, si)\n    #while (length(sn) &gt; 1) {\n    while (is.list(sn)) {\n      si &lt;- c(si, detect_index(sn, ~any(unlist(.x) == sv)))\n      sn &lt;- pluck(sn, tail(si, 1))\n    }\n  }\n  return(list(si, sv))\n}\n\nex2 &lt;- fromJSON(\"[[[[0,7],4],[15,[0,13]]],[1,1]]\", simplifyVector = FALSE)\n(res &lt;- get_split(ex2))\n\n[[1]]\n[1] 1 2 1\n\n[[2]]\n[1] 15\n\n\nThis splits the value at the given location:\n\nsplit_val &lt;- function(sn, res) {\n  sv &lt;- list(floor(res[[2]] / 2), ceiling(res[[2]] / 2))\n  si &lt;- res[[1]]\n  pluck(sn, !!!si) &lt;- sv\n  return(sn)\n}\nsplit_val(ex2, res)\n\n[[1]]\n[[1]][[1]]\n[[1]][[1]][[1]]\n[[1]][[1]][[1]][[1]]\n[1] 0\n\n[[1]][[1]][[1]][[2]]\n[1] 7\n\n\n[[1]][[1]][[2]]\n[1] 4\n\n\n[[1]][[2]]\n[[1]][[2]][[1]]\n[[1]][[2]][[1]][[1]]\n[1] 7\n\n[[1]][[2]][[1]][[2]]\n[1] 8\n\n\n[[1]][[2]][[2]]\n[[1]][[2]][[2]][[1]]\n[1] 0\n\n[[1]][[2]][[2]][[2]]\n[1] 13\n\n\n\n\n[[2]]\n[[2]][[1]]\n[1] 1\n\n[[2]][[2]]\n[1] 1\n\n\n\nThen the main loop to reduce a snailfish number (or two numbers added together) is:\n\nreduce_snailfish_num &lt;- function(sn1, sn2 = NULL) {\n  if (is.null(sn2)) {\n    sn &lt;- sn1\n  } else {\n    sn &lt;- list(sn1, sn2)\n  }\n  \n  repeat {\n    res_explode &lt;- get_explode_pair(sn)\n    if (!is_empty(res_explode)) {\n      sn &lt;- explode_pair(sn, res_explode)\n    } else {\n      res_split &lt;- get_split(sn)\n      if (!is_empty(res_split)) {\n        sn &lt;- split_val(sn, res_split)\n      } else {\n        break\n      }\n    }\n  }\n  sn\n}\n\nApplying this loop recursively to the input of 100 snailfish numbers can be done with purrr::reduce:\n\nreduced_input &lt;- reduce(input, reduce_snailfish_num)\n\nThen the function to calculate the magnitudes:\n\ncalc_magnitude &lt;- function(reduced_input) {\n  repeat {\n    if (!is.list(reduced_input)) return(reduced_input)\n    \n    vd &lt;- vec_depth(reduced_input)\n    reduced_input &lt;- reduced_input %&gt;%\n      map_depth(\n        .depth = vd - 2,\n        function(x) {\n          if (length(x) == 2) 3 * x[[1]] + 2 * x[[2]]\n          else x\n        }\n      ) %&gt;%\n      map_depth(.depth = vd - 2, unlist)\n  }\n}\n\nCompute the magnitude of the input:\n\ncalc_magnitude(reduced_input)\n\n[1] 3987\n\n\n\n\nPart 2\n\nYou notice a second question on the back of the homework assignment:\nWhat is the largest magnitude you can get from adding only two of the snailfish numbers?\n\nThe input contains 100 snailfish numbers, so we have to consider 99 * 99 = 9801 unique additions (because addition is commutative in this scenario). I predict long computation time, so break out the big (i.e.¬†parallel) guns:\n\nlibrary(furrr)\n\nLoading required package: future\n\nfuture::plan(multisession, workers = 6)\n\n\ntic()\npart2_results &lt;- crossing(i = 1:length(input), j = 1:length(input)) %&gt;%\n  filter(i != j) %&gt;%\n  mutate(\n    reduced_input = future_map2(\n      i, j,\n      ~reduce_snailfish_num(c(input[.x], input[.y]))\n    ),\n    magnitude = future_map_dbl(reduced_input, calc_magnitude)\n  )\ntoc()\n\n73.75 sec elapsed\n\n\nAnd the maximum magnitude:\n\nmax(part2_results$magnitude)\n\n[1] 4500"
  },
  {
    "objectID": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#day-19-beacon-scanner",
    "href": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#day-19-beacon-scanner",
    "title": "Advent of Code 2021: Days 16-20",
    "section": "Day 19: Beacon Scanner",
    "text": "Day 19: Beacon Scanner\n\nday19 &lt;- read_lines(\"day19-input.txt\")\nhead(day19, 30)\n\n [1] \"--- scanner 0 ---\" \"712,493,-580\"      \"-705,-746,944\"    \n [4] \"-468,-742,927\"     \"-906,-760,-387\"    \"523,612,505\"      \n [7] \"-973,425,-333\"     \"361,-777,-655\"     \"-172,-143,-8\"     \n[10] \"403,-606,-574\"     \"-655,761,677\"      \"-670,-738,898\"    \n[13] \"459,511,-563\"      \"713,-623,478\"      \"-605,714,848\"     \n[16] \"-18,-8,104\"        \"510,641,739\"       \"-991,515,-523\"    \n[19] \"556,559,577\"       \"709,-678,647\"      \"789,-632,517\"     \n[22] \"-976,-761,-391\"    \"-966,491,-435\"     \"-611,679,810\"     \n[25] \"383,-654,-565\"     \"641,491,-564\"      \"-790,-798,-400\"   \n[28] \"\"                  \"--- scanner 1 ---\" \"-286,612,-671\"    \n\n\n\nday19_ex &lt;- read_lines(\"day19-input-example.txt\")\n\n\nPart 1\n\n\nLong problem statement\n\n\nAs your probe drifted down through this area, it released an assortment of beacons and scanners into the water. It‚Äôs difficult to navigate in the pitch black open waters of the ocean trench, but if you can build a map of the trench using data from the scanners, you should be able to safely reach the bottom.\nThe beacons and scanners float motionless in the water; they‚Äôre designed to maintain the same position for long periods of time. Each scanner is capable of detecting all beacons in a large cube centered on the scanner; beacons that are at most 1000 units away from the scanner in each of the three axes (x, y, and z) have their precise position determined relative to the scanner. However, scanners cannot detect other scanners. The submarine has automatically summarized the relative positions of beacons detected by each scanner (your puzzle input).\nFor example, if a scanner is at x,y,z coordinates 500,0,-500 and there are beacons at -500,1000,-1500 and 1501,0,-500, the scanner could report that the first beacon is at -1000,1000,-1000 (relative to the scanner) but would not detect the second beacon at all.\nUnfortunately, while each scanner can report the positions of all detected beacons relative to itself, the scanners do not know their own position. You‚Äôll need to determine the positions of the beacons and scanners yourself.\nThe scanners and beacons map a single contiguous 3d region. This region can be reconstructed by finding pairs of scanners that have overlapping detection regions such that there are at least 12 beacons that both scanners detect within the overlap. By establishing 12 common beacons, you can precisely determine where the scanners are relative to each other, allowing you to reconstruct the beacon map one scanner at a time.\nUnfortunately, there‚Äôs a second problem: the scanners also don‚Äôt know their rotation or facing direction. Due to magnetic alignment, each scanner is rotated some integer number of 90-degree turns around all of the x, y, and z axes. That is, one scanner might call a direction positive x, while another scanner might call that direction negative y. Or, two scanners might agree on which direction is positive x, but one scanner might be upside-down from the perspective of the other scanner. In total, each scanner could be in any of 24 different orientations: facing positive or negative x, y, or z, and considering any of four directions ‚Äúup‚Äù from that facing.\nBecause all coordinates are relative, in this example, all ‚Äúabsolute‚Äù positions will be expressed relative to scanner 0 (using the orientation of scanner 0 and as if scanner 0 is at coordinates 0,0,0).\nAssemble the full map of beacons. How many beacons are there?\n\n\n\ninput &lt;- day19\nscanners &lt;-\n  split(input[input != \"\"], cumsum(input == \"\")[input != \"\"]) %&gt;%\n  map_dfr(\n    function(x) {\n      enframe(x[-1], name = \"i\") %&gt;%\n        mutate(scanner = str_extract(x[1], \"\\\\d+\") %&gt;% as.integer())\n    }\n  ) %&gt;%\n  separate(value, into = c(\"x\", \"y\", \"z\"), sep = \",\", convert = TRUE)\nscanners\n\n# A tibble: 723 √ó 5\n       i     x     y     z scanner\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n 1     1   712   493  -580       0\n 2     2  -705  -746   944       0\n 3     3  -468  -742   927       0\n 4     4  -906  -760  -387       0\n 5     5   523   612   505       0\n 6     6  -973   425  -333       0\n 7     7   361  -777  -655       0\n 8     8  -172  -143    -8       0\n 9     9   403  -606  -574       0\n10    10  -655   761   677       0\n# ‚Ä¶ with 713 more rows\n\n\nI want to generate a ‚Äúblueprint‚Äù for each scanner that uniquely identifies it with relative positions of beacons. To do this, I will compute the pairwise squared distances (\\(x^2 + y^2 +z^2\\)) between each beacon for a single scanner.\n\nscanners_pair_dist &lt;-\n  inner_join(scanners, scanners, by = \"scanner\", suffix = c(\"1\", \"2\")) %&gt;%\n  filter(i1 != i2) %&gt;%\n  mutate(dist2 = (x1 - x2)^2 + (y1 - y2)^2 + (z1 - z2)^2) %&gt;%\n  select(scanner, i1, i2, dist2)  \nscanners_pair_dist\n\n# A tibble: 17,952 √ó 4\n   scanner    i1    i2   dist2\n     &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n 1       0     1     2 5865586\n 2       0     1     3 5188674\n 3       0     1     4 4225182\n 4       0     1     5 1227107\n 5       0     1     6 2904858\n 6       0     1     7 1741726\n 7       0     1     8 1513136\n 8       0     1     9 1303318\n 9       0     1    10 3520562\n10       0     1    11 5609769\n# ‚Ä¶ with 17,942 more rows\n\n\nFind the overlap between scanners by joining with pairwise distances:\n\nscanners_overlap &lt;-\n  inner_join(scanners_pair_dist, scanners_pair_dist, by = \"dist2\",\n             suffix = c(\"_s1\", \"_s2\")) %&gt;%\n  filter(scanner_s1 &lt; scanner_s2) %&gt;%\n  pivot_longer(cols = c(i1_s2, i2_s2),\n               names_to = \"name\", values_to = \"i_s2\") %&gt;%\n  count(scanner_s1, scanner_s2, i_s1 = i1_s1, i_s2) %&gt;%\n  group_by(scanner_s1, scanner_s2, i_s1) %&gt;%\n  slice_max(n) %&gt;%\n  ungroup() %&gt;%\n  group_by(scanner_s1, scanner_s2) %&gt;%\n  filter(n() &gt;= 12) %&gt;%\n  ungroup() %&gt;%\n  select(-n)\nscanners_overlap\n\n# A tibble: 386 √ó 4\n   scanner_s1 scanner_s2  i_s1  i_s2\n        &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1          0         11     2    19\n 2          0         11     3     7\n 3          0         11     4    22\n 4          0         11     6     2\n 5          0         11    10     3\n 6          0         11    11    25\n 7          0         11    14    21\n 8          0         11    17    26\n 9          0         11    21    17\n10          0         11    22    16\n# ‚Ä¶ with 376 more rows\n\n\nThis is where I got stuck. Couldn‚Äôt figure out how to reorient the scanners, until I read David Robinson‚Äôs solution. I‚Äôll slightly adapt my solution following his code.\nFirst, put each scanner‚Äôs beacon positions into a three-column matrix:\n\nscanner_matrices &lt;- scanners %&gt;%\n  group_by(scanner) %&gt;%\n  summarize(matrix = list(cbind(x, y, z)))\nscanner_matrices\n\n# A tibble: 28 √ó 2\n   scanner matrix        \n     &lt;int&gt; &lt;list&gt;        \n 1       0 &lt;int [26 √ó 3]&gt;\n 2       1 &lt;int [25 √ó 3]&gt;\n 3       2 &lt;int [26 √ó 3]&gt;\n 4       3 &lt;int [26 √ó 3]&gt;\n 5       4 &lt;int [25 √ó 3]&gt;\n 6       5 &lt;int [26 √ó 3]&gt;\n 7       6 &lt;int [26 √ó 3]&gt;\n 8       7 &lt;int [26 √ó 3]&gt;\n 9       8 &lt;int [26 √ó 3]&gt;\n10       9 &lt;int [26 √ó 3]&gt;\n# ‚Ä¶ with 18 more rows\n\n\nThen get the matrix representations of just the overlapping beacons of each scanner pair:\n\noverlap_matrices &lt;- scanners_overlap %&gt;%\n  group_by(scanner_s1, scanner_s2) %&gt;%\n  summarize(i_s1 = list(i_s1), i_s2 = list(i_s2), .groups = \"drop\") %&gt;%\n  ungroup() %&gt;%\n  # Attach the matrix of beacon positions for each scanner\n  inner_join(scanner_matrices %&gt;% select(scanner, matrix1 = matrix),\n             by = c(\"scanner_s1\" = \"scanner\")) %&gt;%\n  inner_join(scanner_matrices %&gt;% select(scanner, matrix2 = matrix),\n             by = c(\"scanner_s2\" = \"scanner\")) %&gt;%\n  # Index the matrices by the overlapping beacons\n  mutate(matrix1 = map2(matrix1, i_s1, ~ .x[.y, ]),\n         matrix2 = map2(matrix2, i_s2, ~ .x[.y, ]))\noverlap_matrices\n\n# A tibble: 32 √ó 6\n   scanner_s1 scanner_s2 i_s1       i_s2       matrix1        matrix2       \n        &lt;int&gt;      &lt;int&gt; &lt;list&gt;     &lt;list&gt;     &lt;list&gt;         &lt;list&gt;        \n 1          0         11 &lt;int [12]&gt; &lt;int [12]&gt; &lt;int [12 √ó 3]&gt; &lt;int [12 √ó 3]&gt;\n 2          0         12 &lt;int [12]&gt; &lt;int [12]&gt; &lt;int [12 √ó 3]&gt; &lt;int [12 √ó 3]&gt;\n 3          0         24 &lt;int [12]&gt; &lt;int [12]&gt; &lt;int [12 √ó 3]&gt; &lt;int [12 √ó 3]&gt;\n 4          1         10 &lt;int [12]&gt; &lt;int [12]&gt; &lt;int [12 √ó 3]&gt; &lt;int [12 √ó 3]&gt;\n 5          1         11 &lt;int [12]&gt; &lt;int [12]&gt; &lt;int [12 √ó 3]&gt; &lt;int [12 √ó 3]&gt;\n 6          1         19 &lt;int [12]&gt; &lt;int [12]&gt; &lt;int [12 √ó 3]&gt; &lt;int [12 √ó 3]&gt;\n 7          2          7 &lt;int [12]&gt; &lt;int [12]&gt; &lt;int [12 √ó 3]&gt; &lt;int [12 √ó 3]&gt;\n 8          3         12 &lt;int [12]&gt; &lt;int [12]&gt; &lt;int [12 √ó 3]&gt; &lt;int [12 √ó 3]&gt;\n 9          3         20 &lt;int [12]&gt; &lt;int [12]&gt; &lt;int [12 √ó 3]&gt; &lt;int [12 √ó 3]&gt;\n10          3         24 &lt;int [12]&gt; &lt;int [12]&gt; &lt;int [12 √ó 3]&gt; &lt;int [12 √ó 3]&gt;\n# ‚Ä¶ with 22 more rows\n\n\nThought this part was clever from David ‚Äì matching the standard deviations of the of the columns to determine the correct ordering of (x,y,z), then using relative positions to correct direction:\n\nfind_matrix_trans &lt;- function(m1, m2) {\n  m1_sds &lt;- round(apply(m1, 2, sd))\n  m2_sds &lt;- round(apply(m2, 2, sd))\n\n  # Reorient m2's xyz to match m1\n  reorderer &lt;- match(m2_sds, m1_sds)\n  m1 &lt;- m1[, reorderer]\n\n  # Reorient the direction\n  facing &lt;- (m2[2, ] - m2[1, ]) / (m1[2, ] - m1[1, ])\n  m1 &lt;- t(facing * t(m1))\n\n  # Find the difference\n  delta &lt;- (m2 - m1)[1, ]\n  list(reorderer = reorderer, facing = facing, delta = delta)\n}\noverlap_matrices_trans &lt;- overlap_matrices %&gt;%\n  transmute(scanner_s1, scanner_s2,\n            trans = map2(matrix1, matrix2, find_matrix_trans))\noverlap_matrices_trans$trans[[1]]\n\n$reorderer\n[1] 1 3 2\n\n$facing\n x  y  z \n-1  1  1 \n\n$delta\n    x     y     z \n-1384    28    51 \n\n\nThen this function applies the transformation:\n\napply_matrix_trans &lt;- function(m, trans) {\n  ret &lt;- t(trans$facing * t(m[, trans$reorderer]) + trans$delta)\n  colnames(ret) &lt;- c(\"x\", \"y\", \"z\")\n  ret\n}\noverlap_matrices$matrix2[[1]] %&gt;%\n  apply_matrix_trans(overlap_matrices_trans$trans[[1]])\n\n         x    y    z\n [1,] -705 -667 1023\n [2,] -468 -663 1006\n [3,] -906 -681 -308\n [4,] -973  504 -254\n [5,] -655  840  756\n [6,] -670 -659  977\n [7,] -605  793  927\n [8,] -991  594 -444\n [9,] -976 -682 -312\n[10,] -966  570 -356\n[11,] -611  758  889\n[12,] -790 -719 -321\n\n\nCompile the transformations in both directions:\n\noverlap_matrices_trans &lt;- overlap_matrices_trans %&gt;%\n  bind_rows(\n    overlap_matrices %&gt;%\n      mutate(trans = map2(matrix2, matrix1, find_matrix_trans)) %&gt;%\n      select(scanner_s1 = scanner_s2, scanner_s2 = scanner_s1, trans)\n  )\n\nPut the matrix transformations into a tbl_graph:\n\nlibrary(tidygraph)\nlibrary(igraph)\n# Need to re-assign tidyr::crossing from igraph::crossing\ncrossing &lt;- tidyr::crossing\n\ng &lt;- graph_from_data_frame(overlap_matrices_trans) %&gt;%\n  as_tbl_graph() %&gt;%\n  arrange(as.integer(name))\ng\n\n# A tbl_graph: 28 nodes and 64 edges\n#\n# A directed simple graph with 1 component\n#\n# Node Data: 28 √ó 1 (active)\n  name \n  &lt;chr&gt;\n1 0    \n2 1    \n3 2    \n4 3    \n5 4    \n6 5    \n# ‚Ä¶ with 22 more rows\n#\n# Edge Data: 64 √ó 3\n   from    to trans           \n  &lt;int&gt; &lt;int&gt; &lt;list&gt;          \n1     1    12 &lt;named list [3]&gt;\n2     1    13 &lt;named list [3]&gt;\n3     1    25 &lt;named list [3]&gt;\n# ‚Ä¶ with 61 more rows\n\n\nFinally, apply the series of transformations. This is done in a shortest path graph type fashion, with edges connecting the scanners with 12 or more beacons in common:\n\nres &lt;- map(\n  seq_len(nrow(scanner_matrices)),\n  function(i) {\n    edges &lt;-\n      as.numeric(igraph::shortest_paths(g, i, 1, output = \"epath\")$epath[[1]])\n    \n    trans &lt;- overlap_matrices_trans$trans[edges]\n    \n    reduce(trans, apply_matrix_trans, .init = scanner_matrices$matrix[[i]])\n  }\n)\n\nThen the unique beacon locations:\n\nmap_dfr(res, as_tibble) %&gt;% distinct(x, y, z)\n\n# A tibble: 342 √ó 3\n       x     y     z\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   712   493  -580\n 2  -705  -746   944\n 3  -468  -742   927\n 4  -906  -760  -387\n 5   523   612   505\n 6  -973   425  -333\n 7   361  -777  -655\n 8  -172  -143    -8\n 9   403  -606  -574\n10  -655   761   677\n# ‚Ä¶ with 332 more rows\n\n\n\n\nPart 2\n\nSometimes, it‚Äôs a good idea to appreciate just how big the ocean is. Using the Manhattan distance, how far apart do the scanners get?\nWhat is the largest Manhattan distance between any two scanners?\n\n\nmap_dfr(res, as_tibble) %&gt;%\n  distinct(x1 = x, y1 = y, z1 = z) %&gt;%\n  mutate(b1 = row_number()) %&gt;%\n  crossing(., rename(., b2 = b1, x2 = x1, y2 = y1, z2 = z1)) %&gt;%\n  filter(b1 &lt; b2) %&gt;%\n  mutate(manhattan = abs(x1 - x2) + abs(y1 - y2) + abs(z1 - z2)) %&gt;%\n  pull(manhattan) %&gt;%\n  max(na.rm = TRUE)\n\n[1] 15762\n\n\nDefinitely the hardest puzzle yet. And reading online, that seems to be the consensus. Big shout outs to David Robinson and the R for Data Science Slack group for posting their solutions."
  },
  {
    "objectID": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#day-20-trench-map",
    "href": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#day-20-trench-map",
    "title": "Advent of Code 2021: Days 16-20",
    "section": "Day 20: Trench Map",
    "text": "Day 20: Trench Map\n\nday20 &lt;- read_lines(\"day20-input.txt\")\nhead(day20) %&gt;% str_trunc(70)\n\n[1] \"########.###.##.##..##..#...##..##..####.##...##..#.#.#####....#......\"\n[2] \"\"                                                                      \n[3] \"...#....##.###...##.#..#....#.###.##.....#..#...###....#.###..###.#...\"\n[4] \".#...#.#.#.####..#...#..#.#..###.##..#.####...#....###..###...#.##....\"\n[5] \"#####.#.#..##..##..#.#.###.#..##.##.##.##.....#..#....#.#.##.#.##.#...\"\n[6] \"...##.#...######.#..##..###...##..#.##.##.#..#####.##.#..##..#.####...\"\n\n\n\nPart 1\n\nWith the scanners fully deployed, you turn their attention to mapping the floor of the ocean trench.\nWhen you get back the image from the scanners, it seems to just be random noise. Perhaps you can combine an image enhancement algorithm and the input image (your puzzle input) to clean it up a little.\nThe first section is the image enhancement algorithm. It is normally given on a single line, but it has been wrapped to multiple lines in this example for legibility. The second section is the input image, a two-dimensional grid of light pixels (#) and dark pixels (.).\nThe image enhancement algorithm describes how to enhance an image by simultaneously converting all pixels in the input image into an output image. Each pixel of the output image is determined by looking at a 3x3 square of pixels centered on the corresponding input image pixel.\nStarting from the top-left and reading across each row, these pixels are ..., then #.., then .#.; combining these forms ...#...#.. By turning dark pixels (.) into 0 and light pixels (#) into 1, the binary number 000100010 can be formed, which is 34 in decimal. The image enhancement algorithm string is exactly 512 characters long, enough to match every possible 9-bit binary number.\nThrough advances in imaging technology, the images being operated on here are infinite in size. Every pixel of the infinite output image needs to be calculated exactly based on the relevant pixels of the input image. The small input image you have is only a small region of the actual infinite input image; the rest of the input image consists of dark pixels (.).\nStart with the original input image and apply the image enhancement algorithm twice, being careful to account for the infinite size of the images. How many pixels are lit in the resulting image?\n\nSplit the algorithm from the input image:\n\nget_input &lt;- function(raw_input) {\n  algo &lt;- str_split(raw_input[1], \"\")[[1]]\n  raw_input &lt;- tibble(x = raw_input[-c(1, 2)]) %&gt;%\n    transmute(row = row_number(),\n              value = str_split(x, pattern = \"\")) %&gt;%\n    unnest(value) %&gt;%\n    group_by(row) %&gt;%\n    mutate(col = 1:n()) %&gt;%\n    ungroup()\n  list(algo, raw_input)\n}\nday20 &lt;- get_input(day20)\n\nAlso get the example input to show how the functions work:\n\nday20_ex &lt;- read_lines(\"day20-input-example.txt\")\nday20_ex &lt;- get_input(day20_ex)\nalgo &lt;- day20_ex[[1]]; input &lt;- day20_ex[[2]]\n\nDefine a helper function to visualize the grid:\n\nplot_grid &lt;- function(input) {\n  input %&gt;%\n    mutate(value = str_replace(value, \"\\\\.\", \"¬∑\")) %&gt;%\n    ggplot(aes(x = col, y = -row)) +\n    geom_text(aes(label = value)) +\n    theme_void()\n}\nplot_grid(input)\n\n\n\n\nA function to add border rows and columns:\n\nadd_borders &lt;- function(input) {\n  crossing(\n    row = seq(min(input$row) - 1, max(input$row) + 1),\n    col = seq(min(input$col) - 1, max(input$col) + 1)\n  ) %&gt;% \n    left_join(input, by = c(\"row\", \"col\"))\n}\nadd_borders(input) %&gt;%\n  # Visualize borders with a \"B\"\n  mutate(value = replace_na(value, \"B\")) %&gt;%\n  plot_grid()\n\n\n\n\nThis function returns the numeric representation of the binary ‚Äú#‚Äù and ‚Äú.‚Äù strings:\n\nget_output_bin_num &lt;- function(value) {\n  value %&gt;%\n    str_replace_all(\"\\\\.\", \"0\") %&gt;%\n    str_replace_all(\"\\\\#\", \"1\") %&gt;%\n    strtoi(base = 2)\n}\nget_output_bin_num(\"...#...#.\")\n\n[1] 34\n\n\nNow the function to enhance the image.\n\nneighbors &lt;- tibble(drow = c(-1, -1, -1, 0, 0, 0, 1, 1, 1),\n                    dcol = c(-1, 0, 1, -1, 0, 1, -1, 0, 1))\nenhance_image &lt;- function(input, border_value) {\n  input &lt;- add_borders(input)\n  \n  input %&gt;%\n    crossing(neighbors) %&gt;%\n    mutate(row2 = row + drow, col2 = col + dcol) %&gt;%\n    left_join(input %&gt;% rename(value2 = value),\n              by = c(\"row2\" = \"row\", \"col2\" = \"col\")) %&gt;%\n    mutate(value2 = replace_na(value2, border_value)) %&gt;%\n    group_by(row, col) %&gt;%\n    summarise(bin_str = paste0(value2, collapse = \"\"), .groups = \"drop\") %&gt;%\n    mutate(bin_num = get_output_bin_num(bin_str),\n           value = algo[bin_num + 1]) %&gt;%\n    select(row, col, value)\n}\ninput %&gt;%\n  enhance_image(border_value = \".\") %&gt;%\n  enhance_image(border_value = \".\") %&gt;%\n  plot_grid()\n\n\n\n\nGet the real input:\n\nalgo &lt;- day20[[1]]\ninput &lt;- day20[[2]]\n\nThere is a trick to the real input: the infinitely expanding border does not always expand with ‚Äú.‚Äù values like in the example. A string of 9 ‚Äú.‚Äù or ‚Äú#‚Äù gives the following values from the algorithm:\n\nalgo[get_output_bin_num(\".........\") + 1]\n\n[1] \"#\"\n\nalgo[get_output_bin_num(\"#########\") + 1]\n\n[1] \".\"\n\n\nSo the expanding border alternates between ‚Äú.‚Äù and ‚Äú#‚Äù values.\nEnhance the image twice, alternating the border symbol, and count the ‚Äú#‚Äù:\n\npart1 &lt;- reduce(\n  1:2, function(prev, current) {\n    list(\n      image = enhance_image(prev$image, prev$border_value),\n      border_value = if_else(prev$border_value == \".\", \"#\", \".\")\n    )\n  }, .init = list(image = input, border_value = \".\")\n)\nsum(part1$image$value == \"#\")\n\n[1] 5249\n\n\n\n\nPart 2\n\nYou still can‚Äôt quite make out the details in the image. Maybe you just didn‚Äôt enhance it enough.\nIf you enhance the starting input image in the above example a total of 50 times, 3351 pixels are lit in the final output image.\nStart again with the original input image and apply the image enhancement algorithm 50 times. How many pixels are lit in the resulting image?\n\nSame implementation as part 1, just takes a little longer:\n\ntic()\npart2 &lt;- reduce(\n  1:50, function(prev, current) {\n    list(\n      image = enhance_image(prev$image, prev$border_value),\n      border_value = if_else(prev$border_value == \".\", \"#\", \".\")\n    )\n  }, .init = list(image = input, border_value = \".\")\n)\ntoc()\n\n16.12 sec elapsed\n\nsum(part2$image$value == \"#\")\n\n[1] 15714"
  },
  {
    "objectID": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#stats",
    "href": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#stats",
    "title": "Advent of Code 2021: Days 16-20",
    "section": "Stats",
    "text": "Stats\nHere are my personal stats for these last 5 days:\n\ntibble::tribble(\n  ~Part, ~Day, ~Time, ~Rank, ~Score,\n  1, 20, \"19:01:44\", 14482, 0,\n  2, 20, \"19:02:45\", 14121, 0,\n  1, 19, \"&gt;24h\", 10784, 0,\n  2, 19, \"&gt;24h\", 10507, 0,\n  1, 18, \"16:49:00\", 10889, 0,\n  2, 18, \"23:57:22\", 13109, 0,\n  1, 17, \"11:28:12\", 16798, 0,\n  2, 17, \"11:58:11\", 15435, 0,\n  1, 16, \"12:48:03\", 14816, 0,\n  2, 16, \"14:50:34\", 14616, 0\n) %&gt;%\n  select(-Score) %&gt;%\n  pivot_wider(names_from = Part, values_from = c(Time, Rank),\n              names_glue = \"Part {Part}_{.value}\") %&gt;%\n  mutate(\n    `Time between parts` = as.numeric(hms(`Part 2_Time`) - hms(`Part 1_Time`),\n                                      \"minutes\") %&gt;% round(1)\n  ) %&gt;%\n  gt() %&gt;%\n  tab_spanner_delim(delim = \"_\", split = \"first\") %&gt;%\n  sub_missing(columns = \"Time between parts\", missing_text = \"\")\n\n\n\n\n\n\n\n\nDay\nTime\nRank\nTime between parts\n\n\nPart 1\nPart 2\nPart 1\nPart 2\n\n\n\n\n20\n19:01:44\n19:02:45\n14482\n14121\n1.0\n\n\n19\n&gt;24h\n&gt;24h\n10784\n10507\n\n\n\n18\n16:49:00\n23:57:22\n10889\n13109\n428.4\n\n\n17\n11:28:12\n11:58:11\n16798\n15435\n30.0\n\n\n16\n12:48:03\n14:50:34\n14816\n14616\n122.5\n\n\n\n\n\n\n\nAnd here is my position on the private leaderboard:\n\nlibrary(httr)\n\nleaderboard &lt;- httr::GET(\n  url = \"https://adventofcode.com/2021/leaderboard/private/view/1032765.json\",\n  httr::set_cookies(session = Sys.getenv(\"AOC_COOKIE\"))\n) %&gt;%\n  content() %&gt;%\n  as_tibble() %&gt;%\n  unnest_wider(members) %&gt;%\n  arrange(desc(local_score)) %&gt;%\n  transmute(\n    Rank = 1:n(), Name = name, Score = local_score, Stars = stars\n  )\n\n\nleaderboard %&gt;%\n  gt() %&gt;%\n  text_transform(\n    locations = cells_body(columns = Stars),\n    fn = function(stars_col) {\n      map_chr(stars_col,\n              ~html(paste0(.x, fontawesome::fa('star', fill = 'gold'))))\n    }\n  ) %&gt;%\n  cols_align(\"left\") %&gt;%\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_body(\n      rows = (Name == \"taylordunn\")\n    )\n  ) %&gt;%\n  tab_options(container.height = 500)\n\n\n\n\n\n\n\n\nRank\nName\nScore\nStars\n\n\n\n\n1\nColin Rundel\n5094\n40\n\n\n2\nDavid Robinson\n5082\n40\n\n\n3\n@ClareHorscroft\n5027\n40\n\n\n4\ntrang1618\n5019\n40\n\n\n5\nIldik√≥ Czeller\n4879\n40\n\n\n6\npritikadasgupta\n4630\n40\n\n\n7\nAnna Fergusson\n4480\n40\n\n\n8\n@_TanHo\n4433\n36\n\n\n9\nJosh Gray\n4432\n40\n\n\n10\nashbaldry\n4261\n40\n\n\n11\nTom Jemmett\n4224\n40\n\n\n12\ndhimmel\n4046\n33\n\n\n13\nJaros≈Çaw Nirski\n3990\n34\n\n\n14\nmbjoseph\n3987\n40\n\n\n15\nJonathan Spring\n3918\n34\n\n\n16\nJean-Rubin\n3899\n36\n\n\n17\nEmil Hvitfeldt\n3836\n30\n\n\n18\nJim Leach\n3753\n40\n\n\n19\nRiinu Pius\n3684\n36\n\n\n20\nCalum You\n3641\n38\n\n\n21\ntaylordunn\n3533\n40\n\n\n22\nDarrin Speegle\n3531\n40\n\n\n23\n@Mid1995Sed\n3513\n36\n\n\n24\nMelinda Tang\n3265\n31\n\n\n25\nTJ Mahr\n3221\n38\n\n\n26\ngpecci\n3184\n27\n\n\n27\nJaap Walhout\n2959\n32\n\n\n28\npatelis\n2936\n32\n\n\n29\nFarhan Reynaldo\n2849\n30\n\n\n30\nKT421\n2809\n32\n\n\n31\nmartigso\n2784\n32\n\n\n32\njohn-b-edwards\n2775\n24\n\n\n33\nfabio machado\n2773\n28\n\n\n34\nhrushikeshrv\n2762\n27\n\n\n35\nlong39ng\n2747\n32\n\n\n36\nAlbertRapp\n2619\n31\n\n\n37\njordi figueras puig\n2615\n29\n\n\n38\nSherry Zhang\n2615\n24\n\n\n39\n@_mnar99\n2597\n24\n\n\n40\nTokhir Dadaev\n2573\n27\n\n\n41\nAron Strandberg\n2454\n29\n\n\n42\nDusty Turner\n2445\n33\n\n\n43\n@E_E_Akcay\n2355\n26\n\n\n44\nDoortje Theunissen\n2344\n28\n\n\n45\nAndrew Argeros\n2265\n22\n\n\n46\nJacqueline Nolis\n2122\n17\n\n\n47\nNathan Moore\n2074\n25\n\n\n48\nmkiang\n2019\n17\n\n\n49\nduju211\n2005\n27\n\n\n50\nscalgary\n1962\n24\n\n\n51\nDerek Holliday\n1925\n18\n\n\n52\nMatt Onimus\n1885\n24\n\n\n53\nKelly N. Bodwin\n1842\n22\n\n\n54\ndelabj\n1830\n22\n\n\n55\nJenna Jordan\n1797\n28\n\n\n56\nFlavien Petit\n1784\n21\n\n\n57\nHannesOberreiter\n1740\n18\n\n\n58\nCarlssonLeo\n1653\n22\n\n\n59\nJeffrey Brabec\n1641\n22\n\n\n60\nAlex N\n1498\n17\n\n\n61\nrywhale\n1467\n20\n\n\n62\nDaniel Coulton\n1440\n18\n\n\n63\n@woodspock\n1433\n17\n\n\n64\nZach Bogart üíô\n1402\n14\n\n\n65\nkarawoo\n1382\n15\n\n\n66\nblongworth\n1353\n21\n\n\n67\nArun Chavan\n1343\n17\n\n\n68\nTylerGrantSmith\n1269\n17\n\n\n69\nexunckly\n1254\n15\n\n\n70\nScott-Gee\n1193\n17\n\n\n71\nErez Shomron\n1148\n17\n\n\n72\nNerwosolek\n1140\n16\n\n\n73\npi55p00r\n1082\n15\n\n\n74\nGhislain Nono Gueye\n972\n14\n\n\n75\nMiha Gazvoda\n924\n14\n\n\n76\ncramosu\n902\n10\n\n\n77\ncathblatter\n860\n13\n\n\n78\nmfiorina\n839\n15\n\n\n79\nSydney\n821\n11\n\n\n80\nA-Farina\n793\n15\n\n\n81\nMetaMoraleMundo\n775\n7\n\n\n82\n@mfarkhann\n760\n15\n\n\n83\njwinget\n753\n15\n\n\n84\nAndrew Tungate\n710\n15\n\n\n85\ncollinberke\n687\n8\n\n\n86\nldnam\n663\n6\n\n\n87\nEric Ekholm\n635\n11\n\n\n88\ncynthiahqy\n623\n14\n\n\n89\ndirkschumacher\n619\n10\n\n\n90\nAdam Mahood\n616\n6\n\n\n91\nGypeti Casino\n599\n12\n\n\n92\nMaya Gans\n589\n11\n\n\n93\nantdurrant\n568\n9\n\n\n94\nDavid Schoch\n551\n6\n\n\n95\nAmitLevinson\n473\n7\n\n\n96\nJulian Tagell\n473\n5\n\n\n97\nJosiah Parry\n454\n7\n\n\n98\nthedivtagguy\n436\n6\n\n\n99\nandrew-tungate-cms\n421\n6\n\n\n100\n@Maatspencer\n409\n8\n\n\n101\n@KentWeyrauch\n404\n8\n\n\n102\nWendy Christensen\n391\n6\n\n\n103\nEmryn Hofmann\n390\n7\n\n\n104\ncolumbaspexit\n382\n8\n\n\n105\nALBERT\n377\n4\n\n\n106\nAlan Feder\n345\n6\n\n\n107\nKevin Kent\n335\n7\n\n\n108\nolmgeorg\n327\n6\n\n\n109\nDaniel Gemara\n302\n4\n\n\n110\nquickcoffee\n282\n6\n\n\n111\nAndrew Fraser\n248\n3\n\n\n112\nsoto solo\n244\n3\n\n\n113\njennifer-furman\n242\n4\n\n\n114\nAdrian Perez\n216\n4\n\n\n115\nBilly Fryer\n209\n5\n\n\n116\nApril\n197\n2\n\n\n117\nLukas Gr√∂ninger\n173\n4\n\n\n118\nKyle Ligon\n166\n6\n\n\n119\nDuncan Gates\n129\n5\n\n\n120\nJose Pliego San Martin\n125\n2\n\n\n121\naleighbrown\n118\n2\n\n\n122\nBruno Mioto\n98\n3\n\n\n123\nchapmandu2\n64\n4\n\n\n124\n@jdknguyen\n36\n3\n\n\n125\nMatthew Wankiewicz\n20\n1\n\n\n126\nCaioBrighenti\n0\n0\n\n\n127\nRizky Luthfianto\n0\n0\n\n\n128\nNA\n0\n0\n\n\n129\nTony ElHabr\n0\n0\n\n\n130\njacquietran\n0\n0\n\n\n131\nWiktor Jacaszek\n0\n0"
  },
  {
    "objectID": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#reproducibility",
    "href": "posts/2021-12-16-advent-of-code-2021-days-16-20/index.html#reproducibility",
    "title": "Advent of Code 2021: Days 16-20",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\npython:         C:/Users/tdunn/Documents/.virtualenvs/r-reticulate/Scripts/python.exe\nlibpython:      C:/Users/tdunn/AppData/Local/r-reticulate/r-reticulate/pyenv/pyenv-win/versions/3.9.13/python39.dll\npythonhome:     C:/Users/tdunn/Documents/.virtualenvs/r-reticulate\nversion:        3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          C:/Users/tdunn/Documents/.virtualenvs/r-reticulate/Lib/site-packages/numpy\nnumpy_version:  1.23.3\n\nNOTE: Python version was forced by use_python function\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html",
    "href": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html",
    "title": "Advent of Code 2021: Days 21-25",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(tictoc)\nlibrary(lubridate)\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#day-21-dirac-dice",
    "href": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#day-21-dirac-dice",
    "title": "Advent of Code 2021: Days 21-25",
    "section": "Day 21: Dirac Dice",
    "text": "Day 21: Dirac Dice\n\nday21 &lt;- read_lines(\"day21-input.txt\")\nday21 %&gt;% str_trunc(70)\n\n[1] \"Player 1 starting position: 8\" \"Player 2 starting position: 1\"\n\n\n\nPart 1\n\nThere‚Äôs not much to do as you slowly descend to the bottom of the ocean. The submarine computer challenges you to a nice game of Dirac Dice.\nThis game consists of a single die, two pawns, and a game board with a circular track containing ten spaces marked 1 through 10 clockwise. Each player‚Äôs starting space is chosen randomly (your puzzle input). Player 1 goes first.\nPlayers take turns moving. On each player‚Äôs turn, the player rolls the die three times and adds up the results. Then, the player moves their pawn that many times forward around the track (that is, moving clockwise on spaces in order of increasing value, wrapping back around to 1 after 10). So, if a player is on space 7 and they roll 2, 2, and 1, they would move forward 5 times, to spaces 8, 9, 10, 1, and finally stopping on 2.\nAfter each player moves, they increase their score by the value of the space their pawn stopped on. Players‚Äô scores start at 0. So, if the first player starts on space 7 and rolls a total of 5, they would stop on space 2 and add 2 to their score (for a total score of 2). The game immediately ends as a win for any player whose score reaches at least 1000.\nSince the first game is a practice game, the submarine opens a compartment labeled deterministic dice and a 100-sided die falls out. This die always rolls 1 first, then 2, then 3, and so on up to 100, after which it starts over at 1 again. Play using this die.\nThe moment either player wins, what do you get if you multiply the score of the losing player by the number of times the die was rolled during the game?\n\n\nplayer1 &lt;- str_extract(day21[1], \"position: \\\\d\") %&gt;% readr::parse_number()\nplayer2 &lt;- str_extract(day21[2], \"position: \\\\d\") %&gt;% readr::parse_number()\nplayer1; player2\n\n[1] 8\n\n\n[1] 1\n\n\nDefine a custom modulo function that returns 1-10 instead of 0-9:\n\ncustom_mod &lt;- function(val) {\n  ((val - 1) %% 10) + 1\n}\n\nLoop until a winner is found:\n\nplayer1_score &lt;- player2_score &lt;- dice &lt;- turn &lt;- 0\n\nwhile (player1_score &lt; 1000 & player2_score &lt; 1000) {\n  turn &lt;- turn + 1\n  dice &lt;- dice + 3\n  \n  roll &lt;- sum(seq(dice, dice - 2))\n  if (turn %% 2 == 1) {\n    player1 &lt;- custom_mod(player1 + roll)\n    player1_score &lt;- player1_score + player1\n  } else {\n    player2 &lt;- custom_mod(player2 + roll)\n    player2_score &lt;- player2_score + player2\n  }\n}\nplayer1_score; player2_score\n\n[1] 1000\n\n\n[1] 694\n\n\nMultiply the turn number by 3 to get the number of dice rolls, then multiply that by the losing players score:\n\n(turn * 3) * player2_score\n\n[1] 518418\n\n\nPython:\n\nplayer1 = int(r.day21[0][-1])\nplayer2 = int(r.day21[1][-1])\n\nplayer1_score, player2_score, dice, turn = 0, 0, 0, 0\n\nwhile player1_score &lt; 1000 and player2_score &lt; 1000:\n  turn += 1\n  dice += 3\n  \n  roll = sum(range(dice - 2, dice + 1))\n  if turn % 2 == 1:\n    player1 = ((player1 + roll - 1) % 10) + 1\n    player1_score += player1\n  else:\n    player2 = ((player2 + roll - 1) % 10) + 1\n    player2_score += player2\n\nturn * 3 * player2_score\n\n518418\n\n\n\n\nPart 2\n\nNow that you‚Äôre warmed up, it‚Äôs time to play the real game.\nA second compartment opens, this time labeled Dirac dice. Out of it falls a single three-sided die.\nAs you experiment with the die, you feel a little strange. An informational brochure in the compartment explains that this is a quantum die: when you roll it, the universe splits into multiple copies, one copy for each possible outcome of the die. In this case, rolling the die always splits the universe into three copies: one where the outcome of the roll was 1, one where it was 2, and one where it was 3.\nThe game is played the same as before, although to prevent things from getting too far out of hand, the game now ends when either player‚Äôs score reaches at least 21.\n\nOn each turn, the Dirac dice is rolled three times, leading to \\(3 \\times 3 \\times 3 = 27\\) permutations, i.e.¬†27 universes. However, the sum of these three rolls can only take on the following values:\n\nroll_perm &lt;- crossing(r1 = 1:3, r2 = 1:3, r3 = 1:3) %&gt;%\n  mutate(roll = r1 + r2 + r3) %&gt;%\n  count(roll)\nroll_perm\n\n# A tibble: 7 √ó 2\n   roll     n\n  &lt;int&gt; &lt;int&gt;\n1     3     1\n2     4     3\n3     5     6\n4     6     7\n5     7     6\n6     8     3\n7     9     1\n\n\nDefine a function to roll the dice for a single player, but keep track of the states of the game (instead of each game individually), where a state is uniquely identified by the positions and scores of players 1 and 2.\n\nroll_dirac &lt;- function(players, player = 1) {\n  players &lt;- players %&gt;% crossing(roll_perm)\n  if (player == 1) {\n    players &lt;- players %&gt;%\n      mutate(pos1 = custom_mod(pos1 + roll), score1 = score1 + pos1,\n             n_univ = n_univ * n)\n  } else {\n    players &lt;- players %&gt;%\n      mutate(pos2 = custom_mod(pos2 + roll), score2 = score2 + pos2,\n             n_univ = n_univ * n)\n  }\n  players %&gt;%\n      group_by(pos1, score1, pos2, score2) %&gt;%\n      summarise(n_univ = sum(n_univ), .groups = \"drop\")\n}\n\n# Sample input\nplayers &lt;- tibble(pos1 = 4, score1 = 0, pos2 = 8, score2 = 0, n_univ = 1)\n# After two turns\nplayers %&gt;% roll_dirac(player = 1) %&gt;% roll_dirac(player = 2)\n\n# A tibble: 49 √ó 5\n    pos1 score1  pos2 score2 n_univ\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1      1     1      1      6\n 2     1      1     2      2     18\n 3     1      1     3      3     36\n 4     1      1     4      4     42\n 5     1      1     5      5     36\n 6     1      1     6      6     18\n 7     1      1     7      7      6\n 8     2      2     1      1      3\n 9     2      2     2      2      9\n10     2      2     3      3     18\n# ‚Ä¶ with 39 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nAfter each roll, the number of universes (n_univ) is updated for each state.\nThen a function to find the winners in each universe. The loop works by removing winning states (winner_universes) from the active states (players) until none remain:\n\nfind_winners &lt;- function(players) {\n  turn &lt;- 0\n  # Empty tibble to compile winners\n  winner_universes &lt;- players %&gt;% filter(FALSE)\n  \n  while (nrow(players) &gt; 0) {\n    turn &lt;- turn + 1\n    \n    players &lt;- roll_dirac(players,\n                          player = ifelse(turn %% 2 == 1, 1, 2))\n    \n    winners &lt;- (players$score1 &gt; 20) | (players$score2 &gt; 20)\n    if (any(winners)) {\n      winner_universes &lt;- winner_universes %&gt;%\n        bind_rows(players %&gt;% filter(winners))\n      \n      players &lt;- players %&gt;% filter(!winners)\n    }\n  }\n  winner_universes\n}\n\nwinner_universes &lt;- find_winners(players)\n\nwinner_universes %&gt;%\n  mutate(winner = ifelse(score1 &gt; 20, \"Player 1\", \"Player 2\")) %&gt;%\n  group_by(winner) %&gt;%\n  summarise(n_univ = sum(n_univ), .groups = \"drop\") %&gt;%\n  mutate(n_univ = format(n_univ, scientific = FALSE))\n\n# A tibble: 2 √ó 2\n  winner   n_univ         \n  &lt;chr&gt;    &lt;chr&gt;          \n1 Player 1 444356092776315\n2 Player 2 341960390180808\n\n\nNow apply it to my input to find the puzzle answer:\n\nplayers &lt;- tibble(pos1 = player1, score1 = 0, pos2 = player2, score2 = 0,\n                  n_univ = 1)\nwinner_universes &lt;- find_winners(players)\n\nwinner_universes %&gt;%\n  mutate(winner = ifelse(score1 &gt; 20, \"Player 1\", \"Player 2\")) %&gt;%\n  group_by(winner) %&gt;%\n  summarise(n_univ = sum(n_univ), .groups = \"drop\") %&gt;%\n  mutate(n_univ = format(n_univ, scientific = FALSE))\n\n# A tibble: 2 √ó 2\n  winner   n_univ         \n  &lt;chr&gt;    &lt;chr&gt;          \n1 Player 1 346642902541848\n2 Player 2 262939886779945\n\n\nPython with numpy and pandas:\n\nfrom itertools import product\nroll_perm = list(product([1, 2, 3], [1, 2, 3], [1, 2, 3]))\nroll_perm = [sum(rolls) for rolls in roll_perm]\nroll_perm = pd.DataFrame(roll_perm, columns = ['roll']) \\\n  .value_counts().to_frame('n').reset_index()\n  \ndef roll_dirac(players, player = 1):\n  if player == 1:\n    players = players.merge(roll_perm, how = 'outer', on = 'join_key')\n    players = players \\\n      .assign(pos1 = (players['pos1'] + players['roll'] - 1) % 10 + 1)\n    players = players \\\n      .assign(score1 = players['score1'] + players['pos1'],\n              n_univ = players['n_univ'] * players['n'])\n  else:\n    players = players.merge(roll_perm, how = 'outer', on = 'join_key')\n    players = players \\\n      .assign(pos2 = (players['pos2'] + players['roll'] - 1) % 10 + 1)\n    players = players \\\n      .assign(score2 = players['score2'] + players['pos2'],\n              n_univ = players['n_univ'] * players['n'])\n    \n  players = players \\\n    .groupby(['pos1', 'score1', 'pos2', 'score2', 'join_key']) \\\n    .agg({'n_univ': 'sum'}).reset_index()\n  return players\n\ndef find_winners(players):\n  turn = 0\n  winner_universes = players.drop(players.index)\n  \n  while len(players) &gt; 0:\n    turn += 1\n    \n    player = 1 if (turn % 2 == 1) else 2\n    players = roll_dirac(players, player)\n    \n    winners = (players['score1'].values &gt; 20) | (players['score2'].values &gt; 20)\n    if any(winners):\n      winner_universes = pd.concat([winner_universes, players.loc[winners]])\n      players = players.loc[~winners]\n  \n  return winner_universes\n\nplayers = pd.DataFrame({'pos1': player1, 'pos2': player2,\n                        'score1': 0, 'score2': 0, 'n_univ': 1}, index = [0])\n  \n# I'm not aware of a pandas function similar to tidyr::crossing, so use a hacky\n#  join_key column\nroll_perm['join_key'] = 0\nplayers['join_key'] = 0\n\nwinner_universes = find_winners(players)\n\nwinner_universes \\\n  .assign(winner = np.where(winner_universes['score1'] &gt; 20,\n                            'Player 1', 'Player 2')) \\\n  .groupby(['winner']) \\\n  .agg({'n_univ': 'sum'})\n\n                   n_univ\nwinner                   \nPlayer 1  346642902541848\nPlayer 2  262939886779945"
  },
  {
    "objectID": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#day-22-reactor-reboot",
    "href": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#day-22-reactor-reboot",
    "title": "Advent of Code 2021: Days 21-25",
    "section": "Day 22: Reactor Reboot",
    "text": "Day 22: Reactor Reboot\n\nday22 &lt;- read_lines(\"day22-input.txt\")\nday22 %&gt;% head()\n\n[1] \"on x=-45..0,y=-44..9,z=-39..10\"   \"on x=-22..26,y=-21..25,z=-2..43\" \n[3] \"on x=-17..35,y=-35..15,z=-27..25\" \"on x=-10..38,y=-46..6,z=-19..31\" \n[5] \"on x=-19..28,y=-48..4,z=-47..4\"   \"on x=-12..41,y=-45..7,z=-47..1\"  \n\n\n\nPart 1\n\nOperating at these extreme ocean depths has overloaded the submarine‚Äôs reactor; it needs to be rebooted.\nThe reactor core is made up of a large 3-dimensional grid made up entirely of cubes, one cube per integer 3-dimensional coordinate (x,y,z). Each cube can be either on or off; at the start of the reboot process, they are all off. (Could it be an old model of a reactor you‚Äôve seen before?)\nTo reboot the reactor, you just need to set all of the cubes to either on or off by following a list of reboot steps (your puzzle input). Each step specifies a cuboid (the set of all cubes that have coordinates which fall within ranges for x, y, and z) and whether to turn all of the cubes in that cuboid on or off.\nThe initialization procedure only uses cubes that have x, y, and z positions of at least -50 and at most 50. For now, ignore cubes outside this region.\nExecute the reboot steps. Afterward, considering only cubes in the region x=-50..50,y=-50..50,z=-50..50, how many cubes are on?\n\nParse the reboot steps:\n\nreboot_steps &lt;- tibble(x = day22) %&gt;%\n  extract(\n    col = x, into = c(\"on\", \"xmin\", \"xmax\", \"ymin\", \"ymax\", \"zmin\", \"zmax\"),\n    regex = \"(.*) x=(.*)\\\\.\\\\.(.*),y=(.*)\\\\.\\\\.(.*),z=(.*)\\\\.\\\\.(.*)\",\n    convert = TRUE\n  ) %&gt;%\n  # Use a logical to indicate on/off\n  mutate(on = on == \"on\", step_num = row_number())\nreboot_steps\n\n# A tibble: 420 √ó 8\n   on     xmin  xmax  ymin  ymax  zmin  zmax step_num\n   &lt;lgl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n 1 TRUE    -45     0   -44     9   -39    10        1\n 2 TRUE    -22    26   -21    25    -2    43        2\n 3 TRUE    -17    35   -35    15   -27    25        3\n 4 TRUE    -10    38   -46     6   -19    31        4\n 5 TRUE    -19    28   -48     4   -47     4        5\n 6 TRUE    -12    41   -45     7   -47     1        6\n 7 TRUE    -31    15   -49    -1   -18    30        7\n 8 TRUE    -15    32   -36    12   -12    33        8\n 9 TRUE    -47     2   -35    12    -9    37        9\n10 TRUE     -8    39   -41    10    -1    45       10\n# ‚Ä¶ with 410 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nFor part 1, I only need the steps within -50 and +50 in each dimension:\n\nreboot_steps_part1 &lt;- reboot_steps %&gt;%\n  filter(xmax &gt;= -50, ymax &gt;= -50, zmax &gt;= -50,\n         xmin &lt;= 50, ymin &lt;= 50, zmin &lt;= 50)\n\nRepresent the 100x100x100 grid of cubes with an array and a function to update it from a single step:\n\ncubes &lt;- array(FALSE, dim = c(100, 100, 100))\n\n# For my own convenience, add 50 to each range so that I can index the array\n#  with 1-100\nreboot_steps_part1 &lt;- reboot_steps_part1 %&gt;%\n  mutate(across(xmin:zmax, ~ . + 50))\n\nswitch_cubes &lt;- function(cubes, reboot_step) {\n  cubes[reboot_step$xmin:reboot_step$xmax,\n        reboot_step$ymin:reboot_step$ymax,\n        reboot_step$zmin:reboot_step$zmax] &lt;- reboot_step$on\n  return(cubes)\n}\n\nNow apply it and take the sum to get the number of on cubes:\n\nfor (reboot_step in reboot_steps_part1 %&gt;% split(.$step_num)) {\n  cubes &lt;- switch_cubes(cubes, reboot_step)\n}\nsum(cubes)\n\n[1] 543306\n\n\n\n\nPart 2\n\nNow that the initialization procedure is complete, you can reboot the reactor.\nStarting again with all cubes off, execute all reboot steps. Afterward, considering all cubes, how many cubes are on?\n\nMy solution to part 1 won‚Äôt do here.\n\nreboot_steps %&gt;%\n  summarise(across(c(xmin, ymin, zmin), min),\n            across(c(xmax, ymax, zmax), max))\n\n# A tibble: 1 √ó 6\n    xmin   ymin   zmin  xmax  ymax  zmax\n   &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 -97543 -97547 -94763 95984 97644 96256\n\n\nCan‚Äôt really work with a 20k x 20k x 20k grid in R.\nInstead, for the cuboid defined in each step, I will consider every cuboid that came before it. If I find any intersecting cuboids, I will separate them into smaller cuboids that don‚Äôt intersect. For the on/off instruction of each cuboid, I will keep the last instruction.\n\nget_cuboids &lt;- function(reboot_steps) {\n  cuboids &lt;- tibble(xmin = numeric(), xmax = numeric(),\n                    ymin = numeric(), ymax = numeric(),\n                    zmin = numeric(), zmax = numeric())\n  \n  n_steps &lt;- nrow(reboot_steps)\n  for (cuboid1 in reboot_steps %&gt;% split(.$step_num)) {\n    message(paste0(\"Step \", cuboid1$step_num, \" of \", n_steps))\n    \n    # After a lot of trial and error with the example input, I found out that I\n    #  had an off-by-one error that was fixed by nudging the upper bounds\n    cuboid1 &lt;- cuboid1 %&gt;%\n      mutate(xmax = xmax + 1, ymax = ymax + 1, zmax = zmax + 1)\n    \n    # Declare an empty tibble to compile new cuboids\n    new_cuboids &lt;- cuboids %&gt;% filter(FALSE)\n    # Loop over the cuboids gathered so far\n    for (i in seq_len(nrow(cuboids))) {\n      cuboid2 &lt;- cuboids %&gt;% slice(i)\n      \n      # Check if the new cuboid overlaps with the old cuboid\n      x_overlap &lt;- (cuboid1$xmax &gt; cuboid2$xmin) & (cuboid1$xmin &lt; cuboid2$xmax)\n      y_overlap &lt;- (cuboid1$ymax &gt; cuboid2$ymin) & (cuboid1$ymin &lt; cuboid2$ymax)\n      z_overlap &lt;- (cuboid1$zmax &gt; cuboid2$zmin) & (cuboid1$zmin &lt; cuboid2$zmax)\n      \n      if (x_overlap & y_overlap & z_overlap) {\n        # If the left edge of cuboid2 is to the left of cuboid1\n        if (cuboid2$xmin &lt; cuboid1$xmin) {\n          # Slice off that portion into a new cuboid\n          new_cuboids &lt;- new_cuboids %&gt;%\n            bind_rows(cuboid2 %&gt;% mutate(xmax = cuboid1$xmin))\n          # And shrink cuboid2, as the new cuboid covers that area already\n          cuboid2 &lt;- cuboid2 %&gt;% mutate(xmin = cuboid1$xmin)\n        }\n        # Do the same for the other 5 directions\n        if (cuboid2$xmax &gt; cuboid1$xmax) {\n          new_cuboids &lt;- new_cuboids %&gt;%\n            bind_rows(cuboid2 %&gt;% mutate(xmin = cuboid1$xmax))\n          cuboid2 &lt;- cuboid2 %&gt;% mutate(xmax = cuboid1$xmax)\n        }\n        if (cuboid2$ymin &lt; cuboid1$ymin) {\n          new_cuboids &lt;- new_cuboids %&gt;%\n            bind_rows(cuboid2 %&gt;% mutate(ymax = cuboid1$ymin))\n          cuboid2 &lt;- cuboid2 %&gt;% mutate(ymin = cuboid1$ymin)\n        }\n        if (cuboid2$ymax &gt; cuboid1$ymax) {\n          new_cuboids &lt;- new_cuboids %&gt;%\n            bind_rows(cuboid2 %&gt;% mutate(ymin = cuboid1$ymax))\n          cuboid2 &lt;- cuboid2 %&gt;% mutate(ymax = cuboid1$ymax)\n        }\n        if (cuboid2$zmin &lt; cuboid1$zmin) {\n          new_cuboids &lt;- new_cuboids %&gt;%\n            bind_rows(cuboid2 %&gt;% mutate(zmax = cuboid1$zmin))\n          cuboid2 &lt;- cuboid2 %&gt;% mutate(zmin = cuboid1$zmin)\n        }\n        if (cuboid2$zmax &gt; cuboid1$zmax) {\n          new_cuboids &lt;- new_cuboids %&gt;%\n            bind_rows(cuboid2 %&gt;% mutate(zmin = cuboid1$zmax))\n          cuboid2 &lt;- cuboid2 %&gt;% mutate(zmax = cuboid1$zmax)\n        }\n      }\n      else {\n        new_cuboids &lt;- new_cuboids %&gt;% bind_rows(cuboid2)\n      }\n    }\n    new_cuboids &lt;- new_cuboids %&gt;% bind_rows(cuboid1)\n    cuboids &lt;- new_cuboids\n  }\n  return(cuboids)\n}\n\nAs a demonstration, consider the first 3 steps (which happen to have overlapping cuboids) in just 2 dimensions:\n\nreboot_steps3 &lt;- reboot_steps %&gt;% slice(1:3)\n\np &lt;- reboot_steps3 %&gt;%\n  mutate(step_num = factor(step_num)) %&gt;%\n  ggplot(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax)) +\n  geom_rect(aes(fill = step_num),alpha = 0.5)\np\n\n\n\n\nThese are the resulting cuboids after splitting them at the intersections:\n\np +\n  geom_rect(\n    data = get_cuboids(reboot_steps3) %&gt;%\n      # Shrink the areas slightly to visualize the borders\n      mutate(across(c(xmin, ymin), ~ .x + 0.5),\n             across(c(xmax, ymax), ~ .x - 0.5)),\n    fill = NA, color = \"black\")\n\n\n\n\nNow run it on the full input (and time it because I expect it to take a long time):\n\ntic()\ncubes &lt;- get_cuboids(reboot_steps)\ntoc()\n\n1062.81 sec elapsed\n\n\n~17 minute run time.\nNow I have a list of cuboids which are non-overlapping. I can filter down to just the ones that are on and count their volumes to get the total number of on cubes:\n\ncubes %&gt;%\n  filter(on) %&gt;%\n  mutate(volume = (xmax - xmin) * (ymax - ymin) * (zmax - zmin)) %&gt;%\n  summarise(n_on = sum(volume) %&gt;% format(scientific = FALSE))\n\n# A tibble: 1 √ó 1\n  n_on            \n  &lt;chr&gt;           \n1 1285501151402480"
  },
  {
    "objectID": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#day-23-amphipod",
    "href": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#day-23-amphipod",
    "title": "Advent of Code 2021: Days 21-25",
    "section": "Day 23: Amphipod",
    "text": "Day 23: Amphipod\n\nday23 &lt;- read_lines(\"day23-input.txt\")\nday23 %&gt;% str_c(collapse = \"\\n\") %&gt;% message()\n\n#############\n#...........#\n###A#D#A#B###\n  #C#C#D#B#\n  #########\n\n\n\nPart 1\n\nA group of amphipods notice your fancy submarine and flag you down. ‚ÄúWith such an impressive shell,‚Äù one amphipod says, ‚Äúsurely you can help us with a question that has stumped our best scientists.‚Äù\nThey go on to explain that a group of timid, stubborn amphipods live in a nearby burrow. Four types of amphipods live there: Amber (A), Bronze (B), Copper (C), and Desert (D). They live in a burrow that consists of a hallway and four side rooms. The side rooms are initially full of amphipods, and the hallway is initially empty.\nThey give you a diagram of the situation (your puzzle input), including locations of each amphipod (A, B, C, or D, each of which is occupying an otherwise open space), walls (#), and open space (.).\nFor example:\n\n#############\n#...........#\n###B#C#B#D###\n  #A#D#C#A#\n  #########`\n\nThe amphipods would like a method to organize every amphipod into side rooms so that each side room contains one type of amphipod and the types are sorted A-D going left to right, like this:\n\n#############\n#...........#\n###A#B#C#D###\n  #A#B#C#D#\n  #########\n\nAmphipods can move up, down, left, or right so long as they are moving into an unoccupied open space. Each type of amphipod requires a different amount of energy to move one step: Amber amphipods require 1 energy per step, Bronze amphipods require 10 energy, Copper amphipods require 100, and Desert ones require 1000. The amphipods would like you to find a way to organize the amphipods that requires the least total energy.\nHowever, because they are timid and stubborn, the amphipods have some extra rules:\n\nAmphipods will never stop on the space immediately outside any room. They can move into that space so long as they immediately continue moving. (Specifically, this refers to the four open spaces in the hallway that are directly above an amphipod starting position.)\nAmphipods will never move from the hallway into a room unless that room is their destination room and that room contains no amphipods which do not also have that room as their own destination. If an amphipod‚Äôs starting room is not its destination room, it can stay in that room until it leaves the room. (For example, an Amber amphipod will not move from the hallway into the right three rooms, and will only move into the leftmost room if that room is empty or if it only contains other Amber amphipods.)\nOnce an amphipod stops moving in the hallway, it will stay in that spot until it can move into a room. (That is, once any amphipod starts moving, any other amphipods currently in the hallway are locked in place and will not move again until they can move fully into a room.)\n\nWhat is the least energy required to organize the amphipods?\n\nI‚Äôve been slacking a bit on my Python, so I‚Äôll attempt this one in Python first (and if I have time, reproduce the solution in R). Parse the data into a list of lists:\n\nletters = [[c for c in line if c.isalpha()] for line in r.day23[2:4]]\nletters\n\n[['A', 'D', 'A', 'B'], ['C', 'C', 'D', 'B']]\n\n\nThe first element is the top row of each room, and the second is the bottom. Now represent the initial state of the amphipod locations with a tuple of the hallway (starting empty) and the rooms:\n\nrooms = [(letter1, letter2) for letter1, letter2 in zip(*letters)]\n# Represent the empty hallway with 11 None values\nhallway = (None, ) * 11\ninitial_state = (hallway, *rooms)\ninitial_state\n\n((None, None, None, None, None, None, None, None, None, None, None), ('A', 'C'), ('D', 'C'), ('A', 'D'), ('B', 'B'))\n\n\nDefine the target state ‚Äì A, B, C and D in the rooms in order left to right:\n\ntarget_state = ((None, ) * 11, ('A', 'A'), ('B', 'B'), ('C', 'C'), ('D', 'D'))\n\nAlso define some helpful dict mappings that define connections between rooms, halls, amphipods and their energy costs to move:\n\n# The rooms targeted by each amphipod\ntarget_rooms = {'A': 1, 'B': 2, 'C': 3, 'D': 4}\n# This dict maps rooms (numeric 1 = A, 2 = B, etc) to numeric locations in the\n#  hallway, e.g. room B leads to hallway location 4\nroom_to_hall = {1: 2, 2: 4, 3: 6, 4: 8}\nenergy_costs = {'A': 1, 'B': 10, 'C': 100, 'D': 1000}\n\nDefine the workhorse function of the problem, which loops over all amphipods and returns possible states and their energy costs:\n\ndef get_possible_moves(state):\n  # For each room, consider moving the top-most amphipods\n  for i in range(1, 5):\n    # Look for the first non-empty space\n    if state[i][0] is not None:\n      # The top spot is occupied\n      top_loc = 0\n    elif state[i][1] is not None:\n      # The bottom spot is occupied \n      top_loc = 1\n    else:\n      # Otherwise, nothing in this room so continue\n      continue\n    \n    # In order to mutate the tuple state, need to convert to list of lists\n    state_list = list(map(list, state))\n    # The amphipod letter at the top of the room\n    letter = state_list[i][top_loc]\n    \n    # If this letter is in the right room, and everything below it is as well\n    if target_rooms[letter] == i and \\\n        all(letter == letter_below for letter_below in state[i][top_loc:]):\n      continue # Don't move it\n          \n    # Move it\n    steps = top_loc \n    state_list[i][top_loc] = None\n    \n    # Find spaces in the hallway that it could possibly move\n    possible_locs = []\n    # Look to the left of the room first\n    for j in range(room_to_hall[i]):\n      # If not in front of the door\n      if j not in [2, 4, 6, 8]:\n        possible_locs.append(j)\n      # If that space in the hallway is occupied, it is not possible to move\n      if state_list[0][j] is not None:\n        possible_locs.clear()\n    # Look to the right of the room second\n    for j in range(room_to_hall[i], 11):\n      if state_list[0][j] is not None:\n        break\n      if j not in [2, 4, 6, 8]:\n        possible_locs.append(j)\n    \n    # The new states will have unique hallways, as a letter moves from a room\n    new_state = list(map(tuple, state_list))\n    hallway = state[0]\n    \n    for loc in possible_locs:\n      hallway_list = list(hallway)\n      hallway_list[loc] = letter\n      new_state[0] = tuple(hallway_list)\n      # Count the number of steps to get to this space, and multiply by energy\n      energy = (steps + 1 + abs(loc - room_to_hall[i])) * energy_costs[letter]\n      yield tuple(new_state), energy\n      \n  # For each amphipod in the hallway, consider moving into rooms\n  for i,letter in enumerate(state[0]):\n    if letter is None: continue\n    \n    # Find the target room for this letter\n    target_room = target_rooms[letter]\n    # And its current occupants\n    room_letters = set(state[target_room]).discard(None)\n    # And the hallway location right outside\n    target_hallway = room_to_hall[target_room]\n    # If the room has other letters in it, don't both moving into it\n    if room_letters and {letter} != room_letters:\n      continue\n    \n    # If to the left of the target location\n    if i &lt; target_hallway:\n      # Consider all locations to the left\n      hall_locs = slice(i + 1, target_hallway + 1)\n    else:\n      # Otherwise, all locations to the right\n      hall_locs = slice(target_hallway, i)\n    \n    # If there is an amphipod in the way, break\n    for loc in state[0][hall_locs]:\n      if loc is not None:\n        break\n    else:\n      steps = abs(i - target_hallway)\n      state_list = list(map(list, state))\n      # Remove it from the hall\n      state_list[0][i] = None\n      # Get the list of current room occupants\n      room_list = state_list[target_room]\n      \n      # Consider all locations in the room\n      for room_loc, other_letter in reversed(list(enumerate(room_list))):\n        if other_letter is None: break\n      \n      # If the top location is empty (as expected) move the amphipod there\n      assert room_list[room_loc] is None\n      room_list[room_loc] = letter\n      steps += room_loc + 1\n      \n      energy = steps * energy_costs[letter]\n      yield tuple(map(tuple, state_list)), steps * energy\n\nNow to define a function that repeatedly looks for new states until it finds the target state. To drastically reduce computation time, I will take advantage of memoization from the functools package. This works by caching the results of a function for specific inputs (here the state), and returning the cached result instead of re-computing it.\n\nfrom functools import cache\n\n@cache\ndef steps_to_target(state):\n  if state == target_state: return 0\n  \n  possible_costs = []\n  \n  for new_state, energy in get_possible_moves(state):\n    possible_costs.append(energy + steps_to_target(new_state))\n    \n  return min(possible_costs)\n\n\nsteps_to_target(initial_state)\n\nError in py_call_impl(callable, dots$args, dots$keywords): RecursionError: maximum recursion depth exceeded in comparison\n\n\nEven with memoization, this requires me to adjust the recursion depth:\n\nimport sys\nprint(sys.getrecursionlimit())\n\n1000\n\nsys.setrecursionlimit(30000)\n\n\nsteps_to_target(initial_state)\n\nUnfortunately, I can‚Äôt get this working in the RStudio IDE with reticulate ‚Äì the R session immediately crashes. I ran it outside of RStudio and found that the least energy required to reach the target state was 13495.\n\n\nPart 2\n\nAs you prepare to give the amphipods your solution, you notice that the diagram they handed you was actually folded up. As you unfold it, you discover an extra part of the diagram.\nBetween the first and second lines of text that contain amphipod starting positions, insert the following lines:\n\n  #D#C#B#A#\n  #D#B#A#C#\n\nSo, the above example now becomes:\n\n#############\n#...........#\n###B#C#B#D###\n  #D#C#B#A#\n  #D#B#A#C#\n  #A#D#C#A#\n  #########\n\nThe amphipods still want to be organized into rooms similar to before:\n\n#############\n#...........#\n###A#B#C#D###\n  #A#B#C#D#\n  #A#B#C#D#\n  #A#B#C#D#\n  #########\n\nUsing the initial configuration from the full diagram, what is the least energy required to organize the amphipods?\n\nThankfully I predicted the rooms getting bigger in part 2, so I only have to add a couple lines of code to the get_possible_moves function:\n\ndef get_possible_moves(state):\n  for i in range(1, 5):\n    if state[i][0] is not None:\n      top_loc = 0\n    elif state[i][1] is not None:\n      top_loc = 1\n    elif state[i][2] is not None:\n      top_loc = 2\n    elif state[i][3] is not None:\n      top_loc = 3\n    else:\n      # Otherwise, nothing in this room so continue\n      continue\n  # Rest of the function remains the same\n  # ...\n\nThe answer to this part is 53767 energy."
  },
  {
    "objectID": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#day-24-arithmetic-logic-unit",
    "href": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#day-24-arithmetic-logic-unit",
    "title": "Advent of Code 2021: Days 21-25",
    "section": "Day 24: Arithmetic Logic Unit",
    "text": "Day 24: Arithmetic Logic Unit\n\nday24 &lt;- read_lines(\"day24-input.txt\")\nhead(day24)\n\n[1] \"inp w\"    \"mul x 0\"  \"add x z\"  \"mod x 26\" \"div z 1\"  \"add x 10\"\n\n\n\nPart 1\n\n\nVery long puzzle statement\n\n\nMagic smoke starts leaking from the submarine‚Äôs arithmetic logic unit (ALU). Without the ability to perform basic arithmetic and logic functions, the submarine can‚Äôt produce cool patterns with its Christmas lights!\nIt also can‚Äôt navigate. Or run the oxygen system.\nDon‚Äôt worry, though - you probably have enough oxygen left to give you enough time to build a new ALU.\nThe ALU is a four-dimensional processing unit: it has integer variables w, x, y, and z. These variables all start with the value 0. The ALU also supports six instructions:\n\ninp a - Read an input value and write it to variable a.\nadd a b - Add the value of a to the value of b, then store the result in variable a.\nmul a b - Multiply the value of a by the value of b, then store the result in variable a.\ndiv a b - Divide the value of a by the value of b, truncate the result to an integer, then store the result in variable a. (Here, ‚Äútruncate‚Äù means to round the value toward zero.)\nmod a b - Divide the value of a by the value of b, then store the remainder in variable a. (This is also called the modulo operation.)\neql a b - If the value of a and b are equal, then store the value 1 in variable a. Otherwise, store the value 0 in variable a.\n\nIn all of these instructions, a and b are placeholders; a will always be the variable where the result of the operation is stored (one of w, x, y, or z), while b can be either a variable or a number. Numbers can be positive or negative, but will always be integers.\nThe ALU has no jump instructions; in an ALU program, every instruction is run exactly once in order from top to bottom. The program halts after the last instruction has finished executing.\n(Program authors should be especially cautious; attempting to execute div with b=0 or attempting to execute mod with a&lt;0 or b&lt;=0 will cause the program to crash and might even damage the ALU. These operations are never intended in any serious ALU program.)\nOnce you have built a replacement ALU, you can install it in the submarine, which will immediately resume what it was doing when the ALU failed: validating the submarine‚Äôs model number. To do this, the ALU will run the MOdel Number Automatic Detector program (MONAD, your puzzle input).\nSubmarine model numbers are always fourteen-digit numbers consisting only of digits 1 through 9. The digit 0 cannot appear in a model number.\nWhen MONAD checks a hypothetical fourteen-digit model number, it uses fourteen separate inp instructions, each expecting a single digit of the model number in order of most to least significant. (So, to check the model number 13579246899999, you would give 1 to the first inp instruction, 3 to the second inp instruction, 5 to the third inp instruction, and so on.) This means that when operating MONAD, each input instruction should only ever be given an integer value of at least 1 and at most 9.\nThen, after MONAD has finished running all of its instructions, it will indicate that the model number was valid by leaving a 0 in variable z. However, if the model number was invalid, it will leave some other non-zero value in z.\nMONAD imposes additional, mysterious restrictions on model numbers, and legend says the last copy of the MONAD documentation was eaten by a tanuki. You‚Äôll need to figure out what MONAD does some other way.\nTo enable as many submarine features as possible, find the largest valid fourteen-digit model number that contains no 0 digits. What is the largest model number accepted by MONAD?\n\n\nThese are some very confusing instructions. My TLDR is that the puzzle input is the code, through which I run many different 14 digit model numbers, and find the largest valid number (that contains no 0 digits). Parse the MONAD instruction list:\n\nmonad &lt;- tibble(x = day24) %&gt;%\n  mutate(step_num = row_number()) %&gt;%\n  separate(x, into = c(\"operation\", \"var1\", \"var2\"), sep = \" \", fill = \"right\")\nmonad\n\n# A tibble: 252 √ó 4\n   operation var1  var2  step_num\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;\n 1 inp       w     &lt;NA&gt;         1\n 2 mul       x     0            2\n 3 add       x     z            3\n 4 mod       x     26           4\n 5 div       z     1            5\n 6 add       x     10           6\n 7 eql       x     w            7\n 8 eql       x     0            8\n 9 mul       y     0            9\n10 add       y     25          10\n# ‚Ä¶ with 242 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\n252 steps in the program. How many input operations are there?\n\nmonad %&gt;% count(operation)\n\n# A tibble: 6 √ó 2\n  operation     n\n  &lt;chr&gt;     &lt;int&gt;\n1 add          98\n2 div          14\n3 eql          28\n4 inp          14\n5 mod          14\n6 mul          84\n\n\n14 inputs (inp) for the 14 digit model number. So I need make a function which takes any 14 digit model number (that doesn‚Äôt contain 0), and will be considered valid if the z variable is non-zero.\n\ncheck_model_number &lt;- function(model_number) {\n  model_number &lt;- strsplit(model_number, \"\")[[1]]\n  \n  # Place the model number into the input instructions\n  monad$var2[is.na(monad$var2)] &lt;- model_number\n  \n  reduce(monad %&gt;% split(.$step_num), run_monad_instruction,\n         .init = list(w = 0, x = 0, y = 0, z = 0))\n}\n\nrun_monad_instruction &lt;- function(vars, instr) {\n  # Variable 2 is either a number\n  if (!is.na(as.numeric(instr$var2))) {\n    var2 &lt;- as.numeric(instr$var2)\n  # Or variable which has a number assigned\n  } else {\n    var2 &lt;- as.numeric(vars[[instr$var2]])\n  }\n  \n  switch(instr$operation,\n    \"inp\" = {\n      vars[[instr$var1]] &lt;- var2\n    },\n    \"add\" = {\n      vars[[instr$var1]] &lt;- vars[[instr$var1]] + var2\n    },\n    \"mul\" = {\n      vars[[instr$var1]] &lt;- vars[[instr$var1]] * var2\n    },\n    \"div\" = {\n      stopifnot(var2 != 0)\n      vars[[instr$var1]] &lt;- trunc(vars[[instr$var1]] / var2)\n    },\n    \"mod\" = {\n      stopifnot(vars[[instr$var1]] &gt;= 0)\n      stopifnot(var2 &gt; 0)\n      vars[[instr$var1]] &lt;- vars[[instr$var1]] %% var2\n    },\n    \"eql\" = {\n      vars[[instr$var1]] &lt;- ifelse(vars[[instr$var1]] == var2, 1, 0)\n    }\n  )\n  return(vars)\n}\n\nNow I can check any 14 digit number, like the example input 13579246899999:\n\ncheck_model_number(\"13579246899999\")\n\n$w\n[1] 9\n\n$x\n[1] 1\n\n$y\n[1] 20\n\n$z\n[1] 134689198\n\n\nThis number returns a non-zero z, and so is invalid.\nThe problem now is: how to check so many model numbers? I could start from 99999999999999 and decrease by 1 until I find a valid number.\n\ncheck_model_number(\"99999999999999\")\n\n$w\n[1] 9\n\n$x\n[1] 1\n\n$y\n[1] 20\n\n$z\n[1] 8937728\n\n\nBut with 9^14 = 22,876,792,454,961 possible numbers to check, the brute force approach would not finish in any reasonable time frame. There has to be a simpler way to approach the problem. Looking at the raw input, there is definitely a pattern to the instructions. The input instructions come at regular intervals (1st, 19th, 37th, 55th, ‚Ä¶). Split the monad by the input instructions, and compare instructions side-by-side:\n\nday24 %&gt;%\n  split(cumsum(str_detect(., \"inp\"))) %&gt;%\n  map_dfc(as_tibble) %&gt;%\n  rename_with(~str_replace(., \"value...\", \"input \")) %&gt;%\n  mutate(step_num = row_number()) %&gt;%\n  gt(rowname_col = \"step_num\")\n\n\n\n\n\n\n\n\n\ninput 1\ninput 2\ninput 3\ninput 4\ninput 5\ninput 6\ninput 7\ninput 8\ninput 9\ninput 10\ninput 11\ninput 12\ninput 13\ninput 14\n\n\n\n\n1\ninp w\ninp w\ninp w\ninp w\ninp w\ninp w\ninp w\ninp w\ninp w\ninp w\ninp w\ninp w\ninp w\ninp w\n\n\n2\nmul x 0\nmul x 0\nmul x 0\nmul x 0\nmul x 0\nmul x 0\nmul x 0\nmul x 0\nmul x 0\nmul x 0\nmul x 0\nmul x 0\nmul x 0\nmul x 0\n\n\n3\nadd x z\nadd x z\nadd x z\nadd x z\nadd x z\nadd x z\nadd x z\nadd x z\nadd x z\nadd x z\nadd x z\nadd x z\nadd x z\nadd x z\n\n\n4\nmod x 26\nmod x 26\nmod x 26\nmod x 26\nmod x 26\nmod x 26\nmod x 26\nmod x 26\nmod x 26\nmod x 26\nmod x 26\nmod x 26\nmod x 26\nmod x 26\n\n\n5\ndiv z 1\ndiv z 1\ndiv z 1\ndiv z 26\ndiv z 1\ndiv z 26\ndiv z 1\ndiv z 26\ndiv z 1\ndiv z 1\ndiv z 26\ndiv z 26\ndiv z 26\ndiv z 26\n\n\n6\nadd x 10\nadd x 13\nadd x 15\nadd x -12\nadd x 14\nadd x -2\nadd x 13\nadd x -12\nadd x 15\nadd x 11\nadd x -3\nadd x -13\nadd x -12\nadd x -13\n\n\n7\neql x w\neql x w\neql x w\neql x w\neql x w\neql x w\neql x w\neql x w\neql x w\neql x w\neql x w\neql x w\neql x w\neql x w\n\n\n8\neql x 0\neql x 0\neql x 0\neql x 0\neql x 0\neql x 0\neql x 0\neql x 0\neql x 0\neql x 0\neql x 0\neql x 0\neql x 0\neql x 0\n\n\n9\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\n\n\n10\nadd y 25\nadd y 25\nadd y 25\nadd y 25\nadd y 25\nadd y 25\nadd y 25\nadd y 25\nadd y 25\nadd y 25\nadd y 25\nadd y 25\nadd y 25\nadd y 25\n\n\n11\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\n\n\n12\nadd y 1\nadd y 1\nadd y 1\nadd y 1\nadd y 1\nadd y 1\nadd y 1\nadd y 1\nadd y 1\nadd y 1\nadd y 1\nadd y 1\nadd y 1\nadd y 1\n\n\n13\nmul z y\nmul z y\nmul z y\nmul z y\nmul z y\nmul z y\nmul z y\nmul z y\nmul z y\nmul z y\nmul z y\nmul z y\nmul z y\nmul z y\n\n\n14\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\nmul y 0\n\n\n15\nadd y w\nadd y w\nadd y w\nadd y w\nadd y w\nadd y w\nadd y w\nadd y w\nadd y w\nadd y w\nadd y w\nadd y w\nadd y w\nadd y w\n\n\n16\nadd y 10\nadd y 5\nadd y 12\nadd y 12\nadd y 6\nadd y 4\nadd y 15\nadd y 3\nadd y 7\nadd y 11\nadd y 2\nadd y 12\nadd y 4\nadd y 11\n\n\n17\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\nmul y x\n\n\n18\nadd z y\nadd z y\nadd z y\nadd z y\nadd z y\nadd z y\nadd z y\nadd z y\nadd z y\nadd z y\nadd z y\nadd z y\nadd z y\nadd z y\n\n\n\n\n\n\n\nThe only steps which differ from input to input are:\n\nStep 1: inp w which will take a different digit from the model number every time\nStep 5: div z 1 and div z 26\nStep 6: add x 10, 13, 15, -12, 14, -2, 11, -3, -13\nStep 16: add y 10, 5, 12, 6, 4, 15, 3, 7, 11, 2\n\nSo I can write a simple function which takes a few variables and runs through all the steps:\n\nmonad_simple_step &lt;- function(z = 0, w = 1, step5 = 1, step6 = 10, step16 = 3) {\n  # Step 2\n  x &lt;- 0\n  # Step 3\n  x &lt;- z\n  # Step 4\n  x &lt;- x %% 26\n  # Step 5\n  z &lt;- trunc(z / step5)\n  # Step 6\n  x &lt;- x + step6\n  # Step 7\n  x &lt;- as.numeric(x == w)\n  # Step 8\n  x &lt;- as.numeric(x == 0)\n  # Step 9\n  y &lt;- 0\n  # Step 10\n  y &lt;- y + 25\n  # Step 11\n  y &lt;- y * x\n  # Step 12\n  y &lt;- y + 1\n  # Step 13\n  z &lt;- z * y\n  # Step 14\n  y &lt;- 0\n  # Step 15\n  y &lt;- y + w\n  # Step 16\n  y &lt;- y + step16\n  # Step 17\n  y &lt;- y * x\n  # Step 18\n  z &lt;- z + y\n  # Output\n  z\n}\nmonad_simple_step(w = 5)\n\n[1] 8\n\nmonad_simple_step(z = 8, w = 3, step5 = 26, step6 = -12, step16 = 4)\n\n[1] 7\n\n\nObviously a lot of these steps can be simplified further. A simpler function could have two lines like this:\n\nmonad_simple_step &lt;- function(z = 0, w = 1, step5 = 1, step6 = 10, step16 = 3) {\n  x &lt;- as.numeric(((z %% 26) + step6) != w)\n  trunc(z / step5) * ((25 * x) + 1) + (w + step16) * x\n}\nmonad_simple_step(w = 5)\n\n[1] 8\n\nmonad_simple_step(z = 8, w = 3, step5 = 26, step6 = -12, step16 = 4)\n\n[1] 7\n\n\nI can also simplify my monad input down to just the important steps:\n\nmonad_simple &lt;- monad %&gt;%\n  mutate(input_step = cumsum(operation == \"inp\")) %&gt;%\n  group_by(input_step) %&gt;%\n  mutate(sub_step = row_number()) %&gt;%\n  ungroup() %&gt;%\n  filter(sub_step %in% c(5, 6, 16)) %&gt;%\n  select(input_step, sub_step, var2) %&gt;%\n  pivot_wider(names_from = sub_step, values_from = var2) %&gt;%\n  rename_with(~paste0(\"step\", .), .cols = c(`5`, `6`, `16`)) %&gt;%\n  mutate(across(everything(), as.integer))\nmonad_simple\n\n# A tibble: 14 √ó 4\n   input_step step5 step6 step16\n        &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1          1     1    10     10\n 2          2     1    13      5\n 3          3     1    15     12\n 4          4    26   -12     12\n 5          5     1    14      6\n 6          6    26    -2      4\n 7          7     1    13     15\n 8          8    26   -12      3\n 9          9     1    15      7\n10         10     1    11     11\n11         11    26    -3      2\n12         12    26   -13     12\n13         13    26   -12      4\n14         14    26   -13     11\n\n\nThis function will run a model number through the above steps and return the value of z:\n\ncheck_model_number_simple &lt;- function(model_number) {\n  m &lt;- monad_simple %&gt;%\n    mutate(w = as.integer(strsplit(model_number, \"\")[[1]]))\n  \n  reduce(\n    transpose(m),\n    function(z, args) {\n      monad_simple_step(z, args$w, args$step5, args$step6, args$step16)\n    },\n    .init = 0\n  )\n}\ncheck_model_number_simple(\"99999999999999\")\n\n[1] 8937728\n\n\nThis will run much faster than my previous check_model_number function, but doesn‚Äôt solve the main problem: it still requires checking every model number until we find z = 0. So it needs to be broken down even more.\nConsider the line:\n\nx &lt;- as.numeric(((z %% 26) + step6) != w)\n\nThis will only ever be 0 or 1, depending on equality with the input value w (which ranges 1 to 9). Also notice that step6 and the sign of step5 are related:\n\nmonad_simple %&gt;% count(step5, step6)\n\n# A tibble: 9 √ó 3\n  step5 step6     n\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1    10     1\n2     1    11     1\n3     1    13     2\n4     1    14     1\n5     1    15     2\n6    26   -13     2\n7    26   -12     3\n8    26    -3     1\n9    26    -2     1\n\n\nSince w can only take values 1 to 9, any values of step6 &gt; 9 will also lead to inequality with w and therefore x = 1. The values of step6 &gt; 9 correspond to step5 = 1, and so x is always 1 in these cases.\nThe x value works sort of as an on/off binary switch that feeds into the next line:\n\nz &lt;- trunc(z / step5) * ((25 * x) + 1) + (w + step16) * x\n\nBreak this down into three terms:\n\nterm1 &lt;- trunc(z / step5)\nterm2 &lt;- (25 * x) + 1\nterm3 &lt;- (w + step16) * x\nz &lt;- term1 * term2 + term3\n\n\nIf x = 0, then term2 = 1, and this term does nothing to z\nIf x = 0, then term3 = 0, and this term does nothing to z\nIf x = 1, then term2 = 26\nIf x = 1, then term3 = w + step16\n\nThe number 26 occurring so frequently means we should be thinking in base 26 numbers. Here is what happens to z under different conditions, and how we can think of z in base 26:\n\nterm1\n\nstep5 = 1\n\nterm1 = z (no change)\n\nstep5 = 26\n\nterm1 = z / 26 (remove an order)\n\n\nterm2\n\n= 1 (no change)\n= 26 (add an order)\n\nterm3\n\n= 0 no change)\n= w + step16 (add to the number)\n\n\nI‚Äôm not very familiar with these terms as a non-CS grad, but apparently this mimics a stack data structure which can push a value (adds it to the stack) or pop the most recent (remove from the stack).\nIf step5 = 1, then term3 is pushing the value of w + step16 onto the stack (which will be less than 26 for the possible values of w and step16). Otherwise step5 = 26, which pops the last value from the stack. To get a valid model number, we need z = 0, so all digits need to be popped off the stack. There are 7 each of step5 = 1 and 26 so we can equally push and pop the digits until the stack is empty.\nThis means that the lines with step5 = 26, x = as.numeric(((z %% 26) + step6) != w) must be 0, which requires that the pushed digit w + step16 from the previous step be equal to the next step‚Äôs w - step6.\nMy specific input corresponds to the following push and pop operations on the 14 digits of a model number:\n\nmonad_simple %&gt;%\n  transmute(\n    input_step,\n    push_pop = ifelse(step5 == 1, \"push\", \"pop\"),\n    step6, step16,\n    operation = ifelse(\n      push_pop == \"push\",\n      glue::glue(\"push model_num[{input_step}] + {step16}\"),\n      glue::glue(\"pop requires model_num[{input_step}] = popped_val {step6}\")\n    )\n  ) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\ninput_step\npush_pop\nstep6\nstep16\noperation\n\n\n\n\n1\npush\n10\n10\npush model_num[1] + 10\n\n\n2\npush\n13\n5\npush model_num[2] + 5\n\n\n3\npush\n15\n12\npush model_num[3] + 12\n\n\n4\npop\n-12\n12\npop requires model_num[4] = popped_val -12\n\n\n5\npush\n14\n6\npush model_num[5] + 6\n\n\n6\npop\n-2\n4\npop requires model_num[6] = popped_val -2\n\n\n7\npush\n13\n15\npush model_num[7] + 15\n\n\n8\npop\n-12\n3\npop requires model_num[8] = popped_val -12\n\n\n9\npush\n15\n7\npush model_num[9] + 7\n\n\n10\npush\n11\n11\npush model_num[10] + 11\n\n\n11\npop\n-3\n2\npop requires model_num[11] = popped_val -3\n\n\n12\npop\n-13\n12\npop requires model_num[12] = popped_val -13\n\n\n13\npop\n-12\n4\npop requires model_num[13] = popped_val -12\n\n\n14\npop\n-13\n11\npop requires model_num[14] = popped_val -13\n\n\n\n\n\n\n\nSo the model number requirements unique to my MONAD are:\n\nmodel_num[4] = model_num[3] - 12 + 12\nmodel_num[6] = model_num[5] - 2 + 6\nmodel_num[8] = model_num[7] - 12 + 15\nmodel_num[11] = model_num[10] - 3 + 11\nmodel_num[12] = model_num[9] - 13 + 7\nmodel_num[13] = model_num[2] - 12 + 5\nmodel_num[14] = model_num[1] - 13 + 10\n\nThen finding the maximum valid model number doesn‚Äôt require any programming, just some basic logic and picking the max possible value for each digit in order:\n\nmodel_num[1] = 9\n\nmodel_num[14] = 9 - 3 = 6\n\nmodel_num[2] = 9\n\nmodel_num[13] = 9 - 7 = 2\n\nmodel_num[3] = 9\n\nmodel_num[4] = 9\n\nmodel_num[5] = 5\n\nmodel_num[6] = 5 + 4 = 9\n\nmodel_num[7] = 6\n\nmodel_num[8] = 6 + 3 = 9\n\nmodel_num[10] = 1\n\nmodel_num[11] = 1 + 8 = 9\n\nmodel_num[9] = 9\n\nmodel_num[12] = 9 - 6 = 3\n\n\nThis corresponds to 99995969919326. Before submitting it, I can check that z = 0 with my functions:\n\ncheck_model_number(\"99995969919326\")\n\n$w\n[1] 6\n\n$x\n[1] 0\n\n$y\n[1] 0\n\n$z\n[1] 0\n\ncheck_model_number_simple(\"99995969919326\")\n\n[1] 0\n\n\n\n\nPart 2\n\nAs the submarine starts booting up things like the Retro Encabulator, you realize that maybe you don‚Äôt need all these submarine features after all.\nWhat is the smallest model number accepted by MONAD?\n\nThe same rules as part 1 apply, I just need to pick the lowest digits possible:\n\nmodel_num[1] = 4\n\nmodel_num[14] = 4 - 3 = 1\n\nmodel_num[2] = 8\n\nmodel_num[13] = 8 - 7 = 1\n\nmodel_num[3] = 1\n\nmodel_num[4] = 1\n\nmodel_num[5] = 1\n\nmodel_num[6] = 1 + 4 = 5\n\nmodel_num[7] = 1\n\nmodel_num[8] = 1 + 3 = 4\n\nmodel_num[10] = 1\n\nmodel_num[11] = 1 + 8 = 9\n\nmodel_num[9] = 7\n\nmodel_num[12] = 7 - 6 = 1\n\n\nThis corresponds to 48111514719111.\n\ncheck_model_number(\"48111514719111\")\n\n$w\n[1] 1\n\n$x\n[1] 0\n\n$y\n[1] 0\n\n$z\n[1] 0\n\ncheck_model_number_simple(\"48111514719111\")\n\n[1] 0"
  },
  {
    "objectID": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#day-25-sea-cucumber",
    "href": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#day-25-sea-cucumber",
    "title": "Advent of Code 2021: Days 21-25",
    "section": "Day 25: Sea Cucumber",
    "text": "Day 25: Sea Cucumber\n\nday25 &lt;- read_lines(\"day25-input.txt\")\nhead(day25) %&gt;% str_trunc(70)\n\n[1] \".v.v.v&gt;...&gt;vv&gt;v&gt;&gt;&gt;.v&gt;..v&gt;.v.&gt;.&gt;v&gt;.v.v.&gt;&gt;v...&gt;.&gt;....&gt;.&gt;vv&gt;&gt;&gt;.....&gt;&gt;....\"\n[2] \"&gt;.v&gt;..v.&gt;&gt;vv.&gt;&gt;v...v.&gt;.&gt;..v.&gt;.&gt;.&gt;vvv..&gt;&gt;&gt;.&gt;...&gt;v&gt;.&gt;&gt;v....&gt;&gt;&gt;&gt;...&gt;v....\"\n[3] \".vv&gt;&gt;...v&gt;..v..v&gt;v.&gt;vvv.v.v&gt;&gt;.....v&gt;&gt;.......&gt;...v&gt;.v&gt;.v.vv.v.&gt;v&gt;v.v...\"\n[4] \".&gt;.&gt;v.&gt;vv.v...v.&gt;&gt;v&gt;.&gt;v..vv&gt;..v.v.&gt;...v....&gt;vvvvvv&gt;&gt;&gt;v..vv.v.v&gt;&gt;.&gt;&gt;...\"\n[5] \"..&gt;.....v.&gt;&gt;..&gt;&gt;.v&gt;v.&gt;.&gt;&gt;.&gt;..vv.v&gt;&gt;.&gt;..&gt;.&gt;..v&gt;&gt;...&gt;.&gt;&gt;.vv&gt;&gt;&gt;..&gt;&gt;&gt;&gt;....\"\n[6] \"v...&gt;..&gt;..v&gt;.vv.&gt;&gt;.&gt;...&gt;&gt;....v.......v&gt;...v.vvvvv&gt;..v...&gt;.v.&gt;.&gt;&gt;v&gt;v...\"\n\n\n\nPart 1\n\nThis is it: the bottom of the ocean trench, the last place the sleigh keys could be. Your submarine‚Äôs experimental antenna still isn‚Äôt boosted enough to detect the keys, but they must be here. All you need to do is reach the seafloor and find them.\nAt least, you‚Äôd touch down on the seafloor if you could; unfortunately, it‚Äôs completely covered by two large herds of sea cucumbers, and there isn‚Äôt an open space large enough for your submarine.\nYou suspect that the Elves must have done this before, because just then you discover the phone number of a deep-sea marine biologist on a handwritten note taped to the wall of the submarine‚Äôs cockpit.\n‚ÄúSea cucumbers? Yeah, they‚Äôre probably hunting for food. But don‚Äôt worry, they‚Äôre predictable critters: they move in perfectly straight lines, only moving forward when there‚Äôs space to do so. They‚Äôre actually quite polite!‚Äù\nYou explain that you‚Äôd like to predict when you could land your submarine.\n‚ÄúOh that‚Äôs easy, they‚Äôll eventually pile up and leave enough space for‚Äì wait, did you say submarine? And the only place with that many sea cucumbers would be at the very bottom of the Mariana‚Äì‚Äù You hang up the phone.\nThere are two herds of sea cucumbers sharing the same region; one always moves east (&gt;), while the other always moves south (v). Each location can contain at most one sea cucumber; the remaining locations are empty (.). The submarine helpfully generates a map of the situation (your puzzle input).\nEvery step, the sea cucumbers in the east-facing herd attempt to move forward one location, then the sea cucumbers in the south-facing herd attempt to move forward one location. When a herd moves forward, every sea cucumber in the herd first simultaneously considers whether there is a sea cucumber in the adjacent location it‚Äôs facing (even another sea cucumber facing the same direction), and then every sea cucumber facing an empty location simultaneously moves into that location.\nDue to strong water currents in the area, sea cucumbers that move off the right edge of the map appear on the left edge, and sea cucumbers that move off the bottom edge of the map appear on the top edge. Sea cucumbers always check whether their destination location is empty before moving, even if that destination is on the opposite side of the map. To find a safe place to land your submarine, the sea cucumbers need to stop moving.\nFind somewhere safe to land your submarine. What is the first step on which no sea cucumbers move?\n\nSeems like a nice easy one to finish. Put the input into a matrix:\n\nseafloor &lt;- day25 %&gt;%\n  map(~strsplit(., \"\")[[1]]) %&gt;%\n  unlist() %&gt;%\n  matrix(nrow = length(day25), byrow = TRUE)\nseafloor[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] \".\"  \"v\"  \".\"  \"v\"  \".\"  \"v\"  \"&gt;\"  \".\"  \".\"  \".\"  \n [2,] \"&gt;\"  \".\"  \"v\"  \"&gt;\"  \".\"  \".\"  \"v\"  \".\"  \"&gt;\"  \"&gt;\"  \n [3,] \".\"  \"v\"  \"v\"  \"&gt;\"  \"&gt;\"  \".\"  \".\"  \".\"  \"v\"  \"&gt;\"  \n [4,] \".\"  \"&gt;\"  \".\"  \"&gt;\"  \"v\"  \".\"  \"&gt;\"  \"v\"  \"v\"  \".\"  \n [5,] \".\"  \".\"  \"&gt;\"  \".\"  \".\"  \".\"  \".\"  \".\"  \"v\"  \".\"  \n [6,] \"v\"  \".\"  \".\"  \".\"  \"&gt;\"  \".\"  \".\"  \"&gt;\"  \".\"  \".\"  \n [7,] \"v\"  \"&gt;\"  \"v\"  \"&gt;\"  \".\"  \"v\"  \"&gt;\"  \".\"  \".\"  \"v\"  \n [8,] \".\"  \".\"  \".\"  \".\"  \".\"  \".\"  \".\"  \"v\"  \".\"  \".\"  \n [9,] \"v\"  \"&gt;\"  \".\"  \"v\"  \"&gt;\"  \".\"  \".\"  \"v\"  \".\"  \".\"  \n[10,] \".\"  \"v\"  \"&gt;\"  \"&gt;\"  \"&gt;\"  \"&gt;\"  \"&gt;\"  \"&gt;\"  \"v\"  \".\"  \n\n\nDefine functions to move sea cucumbers east or south:\n\nmax_col &lt;- ncol(seafloor)\nmax_row &lt;- nrow(seafloor)\n\nmove_east &lt;- function(seafloor) {\n  east_idx &lt;- which(seafloor == \"&gt;\", arr.ind = TRUE)\n  \n  seafloor_new &lt;- seafloor\n  for (i in 1:nrow(east_idx)) {\n    # Find the adjacent space (one column over)\n    adjacent_col &lt;- east_idx[i, 2] + 1\n    # If off the right edge of the map, back to the left\n    if (adjacent_col &gt; max_col) {\n      adjacent_col &lt;- adjacent_col - max_col\n    }\n    # If unoccupied, move it\n    if (seafloor[east_idx[i, 1], adjacent_col] == \".\") {\n      seafloor_new[east_idx[i, 1], adjacent_col] &lt;- \"&gt;\"\n      seafloor_new[east_idx[i, 1], east_idx[i, 2]] &lt;- \".\"\n    }\n  }\n  return(seafloor_new)\n}\n\nmove_south &lt;- function(seafloor) {\n  south_idx &lt;- which(seafloor == \"v\", arr.ind = TRUE)\n  \n  seafloor_new &lt;- seafloor\n  for (i in 1:nrow(south_idx)) {\n    # Find the adjacent space (one row down)\n    adjacent_row &lt;- south_idx[i, 1] + 1\n    # If off the bottom edge of the map, back to the top\n    if (adjacent_row &gt; max_row) {\n      adjacent_row &lt;- adjacent_row - max_row\n    }\n    # If unoccupied, move it\n    if (seafloor[adjacent_row, south_idx[i, 2]] == \".\") {\n      seafloor_new[adjacent_row, south_idx[i, 2]] &lt;- \"v\"\n      seafloor_new[south_idx[i, 1], south_idx[i, 2]] &lt;- \".\"\n    }\n  }\n  return(seafloor_new)\n}\n\nRepeat steps in a loop until the matrix isn‚Äôt changing:\n\nstep &lt;- 0\nrepeat {\n  step &lt;- step + 1\n  seafloor_new &lt;- seafloor %&gt;% move_east() %&gt;% move_south()\n  \n  # If the map hasn't changed, break\n  if (all(seafloor_new == seafloor)) break\n  seafloor &lt;- seafloor_new\n}\nstep\n\n[1] 507\n\n\n\n\nPart 2\n\nSuddenly, the experimental antenna control console lights up:\nSleigh keys detected!\nAccording to the console, the keys are directly under the submarine. You landed right on them! Using a robotic arm on the submarine, you move the sleigh keys into the airlock.\nNow, you just need to get them to Santa in time to save Christmas! You check your clock - it is Christmas. There‚Äôs no way you can get them back to the surface in time.\nJust as you start to lose hope, you notice a button on the sleigh keys: remote start. You can start the sleigh from the bottom of the ocean! You just need some way to boost the signal from the keys so it actually reaches the sleigh. Good thing the submarine has that experimental antenna! You‚Äôll definitely need 50 stars to boost it that far, though.\nThe experimental antenna control console lights up again:\n\nEnergy source detected.\nIntegrating energy source from device \"sleigh keys\"...done.\nInstalling device drivers...done.\nRecalibrating experimental antenna...done.\nBoost strength due to matching signal phase: 1 star\n\nOnly 49 stars to go.\nIf you like, you can [Remotely Start the Sleigh Again].\n\nThankfully I solved all the puzzles in order, and so immediately had all 49 stars required to get the 50th. Clicking on the remote start link reveals the end page:\n\nYou use all fifty stars to boost the signal and remotely start the sleigh! Now, you just have to find your way back to the surface‚Ä¶\n‚Ä¶did you know crab submarines come with colored lights?\nCongratulations! You‚Äôve finished every puzzle in Advent of Code 2021! I hope you had as much fun solving them as I had making them for you."
  },
  {
    "objectID": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#stats-and-conclusions",
    "href": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#stats-and-conclusions",
    "title": "Advent of Code 2021: Days 21-25",
    "section": "Stats and conclusions",
    "text": "Stats and conclusions\nHere are my personal stats for these last 5 days:\n\ntibble::tribble(\n  ~Part, ~Day, ~Time, ~Rank,\n  1, 25, \"&gt;24h\", 11179,\n  2, 25, \"&gt;24h\", 6765,\n  1, 24, \"&gt;24h\", 8152, \n  2, 24, \"&gt;24h\", 8018,\n  1, 23, \"&gt;24h\", 9585, \n  2, 23, \"&gt;24h\", 6847,\n  1, 22, \"12:44:05\", 12908,\n  2, 22, \"23:02:28\", 8867,\n  1, 21, \"09:40:13\", 14341,\n  2, 21, \"11:35:27\", 9163\n) %&gt;%\n  pivot_wider(names_from = Part, values_from = c(Time, Rank),\n              names_glue = \"Part {Part}_{.value}\") %&gt;%\n  mutate(\n    `Time between parts` = as.numeric(hms(`Part 2_Time`) - hms(`Part 1_Time`),\n                                      \"minutes\") %&gt;% round(1)\n  ) %&gt;%\n  gt() %&gt;%\n  tab_spanner_delim(delim = \"_\", split = \"first\") %&gt;%\n  sub_missing(columns = \"Time between parts\", missing_text = \"\")\n\n\n\n\n\n\n\n\nDay\nTime\nRank\nTime between parts\n\n\nPart 1\nPart 2\nPart 1\nPart 2\n\n\n\n\n25\n&gt;24h\n&gt;24h\n11179\n6765\n\n\n\n24\n&gt;24h\n&gt;24h\n8152\n8018\n\n\n\n23\n&gt;24h\n&gt;24h\n9585\n6847\n\n\n\n22\n12:44:05\n23:02:28\n12908\n8867\n618.4\n\n\n21\n09:40:13\n11:35:27\n14341\n9163\n115.2\n\n\n\n\n\n\n\nI really lagged behind in the last few days, mostly due to work and family time around Christmas, but I also found the puzzle difficulty ramped up a lot. This is probably a similar story to others on the private leaderboard:\n\nlibrary(httr)\n\nleaderboard &lt;- httr::GET(\n  url = \"https://adventofcode.com/2021/leaderboard/private/view/1032765.json\",\n  httr::set_cookies(session = Sys.getenv(\"AOC_COOKIE\"))\n) %&gt;%\n  content() %&gt;%\n  as_tibble() %&gt;%\n  unnest_wider(members) %&gt;%\n  arrange(desc(local_score)) %&gt;%\n  transmute(\n    Rank = 1:n(), Name = name, Score = local_score, Stars = stars\n  )\n\n\nleaderboard %&gt;%\n  gt() %&gt;%\n  text_transform(\n    locations = cells_body(columns = Stars),\n    fn = function(stars_col) {\n      map_chr(stars_col,\n              ~html(paste0(.x, fontawesome::fa('star', fill = 'gold'))))\n    }\n  ) %&gt;%\n  cols_align(\"left\") %&gt;%\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_body(\n      rows = (Name == \"taylordunn\")\n    )\n  ) %&gt;%\n  tab_options(container.height = 500)\n\n\n\n\n\n\n\n\nRank\nName\nScore\nStars\n\n\n\n\n1\nColin Rundel\n6386\n50\n\n\n2\n@ClareHorscroft\n6280\n50\n\n\n3\ntrang1618\n6269\n50\n\n\n4\nIldik√≥ Czeller\n6156\n50\n\n\n5\npritikadasgupta\n5858\n50\n\n\n6\nDavid Robinson\n5846\n46\n\n\n7\nAnna Fergusson\n5726\n50\n\n\n8\nashbaldry\n5516\n50\n\n\n9\nTom Jemmett\n5485\n50\n\n\n10\nJean-Rubin\n4955\n45\n\n\n11\nJosh Gray\n4892\n44\n\n\n12\nDarrin Speegle\n4724\n50\n\n\n13\ntaylordunn\n4717\n50\n\n\n14\nmbjoseph\n4436\n44\n\n\n15\n@_TanHo\n4433\n36\n\n\n16\ndhimmel\n4427\n36\n\n\n17\n@Mid1995Sed\n4377\n43\n\n\n18\nJim Leach\n4233\n44\n\n\n19\nRiinu Pius\n4172\n40\n\n\n20\nJonathan Spring\n4159\n36\n\n\n21\nCalum You\n4138\n42\n\n\n22\nJaros≈Çaw Nirski\n3990\n34\n\n\n23\nTJ Mahr\n3899\n44\n\n\n24\nEmil Hvitfeldt\n3836\n30\n\n\n25\nMelinda Tang\n3363\n32\n\n\n26\ngpecci\n3184\n27\n\n\n27\njordi figueras puig\n3099\n33\n\n\n28\nfabio machado\n3019\n30\n\n\n29\nJaap Walhout\n2959\n32\n\n\n30\npatelis\n2936\n32\n\n\n31\nFarhan Reynaldo\n2849\n30\n\n\n32\nKT421\n2809\n32\n\n\n33\nmartigso\n2784\n32\n\n\n34\njohn-b-edwards\n2775\n24\n\n\n35\nhrushikeshrv\n2762\n27\n\n\n36\nlong39ng\n2747\n32\n\n\n37\nAlbertRapp\n2619\n31\n\n\n38\nSherry Zhang\n2615\n24\n\n\n39\n@_mnar99\n2597\n24\n\n\n40\nTokhir Dadaev\n2573\n27\n\n\n41\nAron Strandberg\n2569\n30\n\n\n42\nDusty Turner\n2445\n33\n\n\n43\n@E_E_Akcay\n2355\n26\n\n\n44\nDoortje Theunissen\n2344\n28\n\n\n45\nAndrew Argeros\n2265\n22\n\n\n46\nNathan Moore\n2199\n26\n\n\n47\nJacqueline Nolis\n2122\n17\n\n\n48\nMatt Onimus\n2106\n26\n\n\n49\nmkiang\n2019\n17\n\n\n50\nduju211\n2005\n27\n\n\n51\nscalgary\n1962\n24\n\n\n52\nDerek Holliday\n1925\n18\n\n\n53\nKelly N. Bodwin\n1842\n22\n\n\n54\ndelabj\n1830\n22\n\n\n55\nJenna Jordan\n1797\n28\n\n\n56\nFlavien Petit\n1784\n21\n\n\n57\nCarlssonLeo\n1763\n23\n\n\n58\nHannesOberreiter\n1740\n18\n\n\n59\nJeffrey Brabec\n1641\n22\n\n\n60\nAlex N\n1498\n17\n\n\n61\nrywhale\n1467\n20\n\n\n62\nDaniel Coulton\n1440\n18\n\n\n63\n@woodspock\n1433\n17\n\n\n64\nZach Bogart üíô\n1402\n14\n\n\n65\nkarawoo\n1382\n15\n\n\n66\nErez Shomron\n1364\n20\n\n\n67\nblongworth\n1353\n21\n\n\n68\nArun Chavan\n1343\n17\n\n\n69\nTylerGrantSmith\n1269\n17\n\n\n70\nexunckly\n1254\n15\n\n\n71\nScott-Gee\n1193\n17\n\n\n72\nNerwosolek\n1140\n16\n\n\n73\npi55p00r\n1082\n15\n\n\n74\nGhislain Nono Gueye\n972\n14\n\n\n75\nMiha Gazvoda\n924\n14\n\n\n76\ncramosu\n902\n10\n\n\n77\ncathblatter\n860\n13\n\n\n78\nmfiorina\n839\n15\n\n\n79\nSydney\n821\n11\n\n\n80\nA-Farina\n793\n15\n\n\n81\nMetaMoraleMundo\n775\n7\n\n\n82\n@mfarkhann\n760\n15\n\n\n83\njwinget\n753\n15\n\n\n84\nAndrew Tungate\n710\n15\n\n\n85\ncollinberke\n687\n8\n\n\n86\nldnam\n663\n6\n\n\n87\nEric Ekholm\n635\n11\n\n\n88\ncynthiahqy\n623\n14\n\n\n89\ndirkschumacher\n619\n10\n\n\n90\nAdam Mahood\n616\n6\n\n\n91\nGypeti Casino\n599\n12\n\n\n92\nMaya Gans\n589\n11\n\n\n93\nantdurrant\n568\n9\n\n\n94\nDavid Schoch\n551\n6\n\n\n95\nJulian Tagell\n473\n5\n\n\n96\nAmitLevinson\n473\n7\n\n\n97\nJosiah Parry\n454\n7\n\n\n98\nthedivtagguy\n436\n6\n\n\n99\nandrew-tungate-cms\n421\n6\n\n\n100\n@Maatspencer\n409\n8\n\n\n101\n@KentWeyrauch\n404\n8\n\n\n102\nWendy Christensen\n391\n6\n\n\n103\nEmryn Hofmann\n390\n7\n\n\n104\ncolumbaspexit\n382\n8\n\n\n105\nALBERT\n377\n4\n\n\n106\nAlan Feder\n345\n6\n\n\n107\nKevin Kent\n335\n7\n\n\n108\nolmgeorg\n327\n6\n\n\n109\nDaniel Gemara\n302\n4\n\n\n110\nquickcoffee\n282\n6\n\n\n111\nAndrew Fraser\n248\n3\n\n\n112\nsoto solo\n244\n3\n\n\n113\njennifer-furman\n242\n4\n\n\n114\nAdrian Perez\n216\n4\n\n\n115\nBilly Fryer\n209\n5\n\n\n116\nApril\n197\n2\n\n\n117\nLukas Gr√∂ninger\n173\n4\n\n\n118\nKyle Ligon\n166\n6\n\n\n119\nDuncan Gates\n129\n5\n\n\n120\nJose Pliego San Martin\n125\n2\n\n\n121\naleighbrown\n118\n2\n\n\n122\nBruno Mioto\n98\n3\n\n\n123\nchapmandu2\n64\n4\n\n\n124\n@jdknguyen\n36\n3\n\n\n125\nMatthew Wankiewicz\n20\n1\n\n\n126\nCaioBrighenti\n0\n0\n\n\n127\nWiktor Jacaszek\n0\n0\n\n\n128\njacquietran\n0\n0\n\n\n129\nRizky Luthfianto\n0\n0\n\n\n130\nTony ElHabr\n0\n0\n\n\n131\nNA\n0\n0\n\n\n\n\n\n\n\n13th place out of 130, which I‚Äôm very happy with, especially since the puzzles released at 1AM my time so I generally didn‚Äôt get to start until after work the next day.\nMy main takeaways, which may help me in future years of Advent of Code:\n\nRead the full puzzle instruction.\nDon‚Äôt be afraid of base R. As a huge tidyvese advocate, I found some problems much easier to approach with base R, especially the matrix class.\nThat being said, wow does my pandas code feel more cumbersome/ugly compared to the tidyverse equivalent (though this may just be because I am much more experienced with the latter).\nWhen a problem seems like it will take too long to run, consider these in order:\n\nkeeping track of unique states, rather than each state individually,\nparallelization with future and furrr, and\nmemoization (caching the result of computationally expensive functions).\n\nRead the full puzzle instruction.\nJoining a private leaderboard makes for great motivation. I really enjoyed ‚Äúcompeting‚Äù in Tan Ho‚Äôs leaderboard ‚Äì ‚Äúcompeting‚Äù in quotes because it served more as a place to complain and help each other via the R for Data Science Slack group."
  },
  {
    "objectID": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#reproducibility",
    "href": "posts/2021-12-21-advent-of-code-2021-days-21-25/index.html#reproducibility",
    "title": "Advent of Code 2021: Days 21-25",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\npython:         C:/Users/tdunn/Documents/.virtualenvs/r-reticulate/Scripts/python.exe\nlibpython:      C:/Users/tdunn/AppData/Local/r-reticulate/r-reticulate/pyenv/pyenv-win/versions/3.9.13/python39.dll\npythonhome:     C:/Users/tdunn/Documents/.virtualenvs/r-reticulate\nversion:        3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          C:/Users/tdunn/Documents/.virtualenvs/r-reticulate/Lib/site-packages/numpy\nnumpy_version:  1.23.3\n\nNOTE: Python version was forced by use_python function\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(httr)\nlibrary(gt)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#introduction",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#introduction",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Introduction",
    "text": "Introduction\nWith this post, I will explore the Canadian COVID-19 tracker API and, depending on how it goes, turn some of the code into an R package. For an introduction to working with APIs, see this vignette from the httr package.\n\n\n\n\n\n\nNote\n\n\n\nIn 2022, I ported my website from Distill to Quarto. This required me to re-run all the code in this post, so the data will go up to 2022-08-21, not 2021-12-28 (the original date of the post)."
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#summary",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#summary",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Summary",
    "text": "Summary\nThe first data I will retrieve is the data summaries overall, by province, and by health region. To save typing it every time, the following base_url is required for all GET requests:\n\nbase_url &lt;- \"https://api.covid19tracker.ca/\"\n\n\nOverall\nModify the URL with summary to get the latest data across all provinces:\n\napi_url &lt;- paste0(base_url, \"summary\")\n\nSend the GET request with httr:\n\nresp &lt;- httr::GET(api_url)\nresp\n\nResponse [https://api.covid19tracker.ca/summary]\n  Date: 2022-10-27 16:55\n  Status: 200\n  Content-Type: application/json\n  Size: 690 B\n\n\nThis returned a response object with the following structure:\n\nstr(resp, max.level = 1)\n\nList of 10\n $ url        : chr \"https://api.covid19tracker.ca/summary\"\n $ status_code: int 200\n $ headers    :List of 12\n  ..- attr(*, \"class\")= chr [1:2] \"insensitive\" \"list\"\n $ all_headers:List of 1\n $ cookies    :'data.frame':    0 obs. of  7 variables:\n $ content    : raw [1:690] 7b 22 64 61 ...\n $ date       : POSIXct[1:1], format: \"2022-10-27 16:55:54\"\n $ times      : Named num [1:6] 0 0.0453 0.0862 0.179 0.2352 ...\n  ..- attr(*, \"names\")= chr [1:6] \"redirect\" \"namelookup\" \"connect\" \"pretransfer\" ...\n $ request    :List of 7\n  ..- attr(*, \"class\")= chr \"request\"\n $ handle     :Class 'curl_handle' &lt;externalptr&gt; \n - attr(*, \"class\")= chr \"response\"\n\n\nThe status_code is the first thing to check:\n\nresp$status_code\n\n[1] 200\n\n\nAn HTTP status code of 200 is the standard indicator of a successful request.\nOnce confirmed successful, the content returned from the request is:\n\nhead(resp$content, 25)\n\n [1] 7b 22 64 61 74 61 22 3a 5b 7b 22 6c 61 74 65 73 74 5f 64 61 74 65 22 3a 22\n\n\nLooks like the raw data is represented in hexadecimal. The httr::content() function can parse this data:\n\ncontent_parsed &lt;- httr::content(resp, as = \"parsed\")\nstr(content_parsed)\n\nList of 2\n $ data        :List of 1\n  ..$ :List of 23\n  .. ..$ latest_date                : chr \"2022-10-26\"\n  .. ..$ change_cases               : chr \"2317\"\n  .. ..$ change_fatalities          : chr \"52\"\n  .. ..$ change_tests               : chr \"0\"\n  .. ..$ change_hospitalizations    : chr \"24\"\n  .. ..$ change_criticals           : chr \"-3\"\n  .. ..$ change_recoveries          : chr \"0\"\n  .. ..$ change_vaccinations        : chr \"84199\"\n  .. ..$ change_vaccinated          : chr \"2099\"\n  .. ..$ change_boosters_1          : chr \"6693\"\n  .. ..$ change_boosters_2          : chr \"59646\"\n  .. ..$ change_vaccines_distributed: chr \"0\"\n  .. ..$ total_cases                : chr \"4278841\"\n  .. ..$ total_fatalities           : chr \"45808\"\n  .. ..$ total_tests                : chr \"61472226\"\n  .. ..$ total_hospitalizations     : chr \"5629\"\n  .. ..$ total_criticals            : chr \"279\"\n  .. ..$ total_recoveries           : chr \"3815086\"\n  .. ..$ total_vaccinations         : chr \"91945442\"\n  .. ..$ total_vaccinated           : chr \"31624598\"\n  .. ..$ total_boosters_1           : chr \"19362861\"\n  .. ..$ total_boosters_2           : chr \"7266271\"\n  .. ..$ total_vaccines_distributed : chr \"101586018\"\n $ last_updated: chr \"2022-10-26 23:19:02\"\n\n\n\nPer the documentation of httr, it is good practice to check the mime type of the response before parsing it.\n\nhttr::http_type(resp)\n\n[1] \"application/json\"\n\n\nHere, the data is JSON, which is a type that can be parsed with httr::content (via the jsonlite package). We can also parse this data manually as follows (not run):\n\njsonlite::fromJSON(\n  content(resp, \"text\"),\n  simplifyVector = FALSE\n)\n\n\nThe returned data structure is a list of lists. data is a list with all of the summary statistics, while last_updated gives a timestamp of when the data was last updated. Put the data into a data frame:\n\nsummary_overall &lt;- content_parsed$data %&gt;% data.frame()\nglimpse(summary_overall)\n\nRows: 1\nColumns: 23\n$ latest_date                 &lt;chr&gt; \"2022-10-26\"\n$ change_cases                &lt;chr&gt; \"2317\"\n$ change_fatalities           &lt;chr&gt; \"52\"\n$ change_tests                &lt;chr&gt; \"0\"\n$ change_hospitalizations     &lt;chr&gt; \"24\"\n$ change_criticals            &lt;chr&gt; \"-3\"\n$ change_recoveries           &lt;chr&gt; \"0\"\n$ change_vaccinations         &lt;chr&gt; \"84199\"\n$ change_vaccinated           &lt;chr&gt; \"2099\"\n$ change_boosters_1           &lt;chr&gt; \"6693\"\n$ change_boosters_2           &lt;chr&gt; \"59646\"\n$ change_vaccines_distributed &lt;chr&gt; \"0\"\n$ total_cases                 &lt;chr&gt; \"4278841\"\n$ total_fatalities            &lt;chr&gt; \"45808\"\n$ total_tests                 &lt;chr&gt; \"61472226\"\n$ total_hospitalizations      &lt;chr&gt; \"5629\"\n$ total_criticals             &lt;chr&gt; \"279\"\n$ total_recoveries            &lt;chr&gt; \"3815086\"\n$ total_vaccinations          &lt;chr&gt; \"91945442\"\n$ total_vaccinated            &lt;chr&gt; \"31624598\"\n$ total_boosters_1            &lt;chr&gt; \"19362861\"\n$ total_boosters_2            &lt;chr&gt; \"7266271\"\n$ total_vaccines_distributed  &lt;chr&gt; \"101586018\"\n\n\nAll of these variables are character type, and should be converted into integer and Date types:\n\nsummary_overall &lt;- summary_overall %&gt;%\n  mutate(\n    across(matches(\"^change|^total\"), as.integer),\n    across(matches(\"date\"), as.Date)\n  )\nglimpse(summary_overall)\n\nRows: 1\nColumns: 23\n$ latest_date                 &lt;date&gt; 2022-10-26\n$ change_cases                &lt;int&gt; 2317\n$ change_fatalities           &lt;int&gt; 52\n$ change_tests                &lt;int&gt; 0\n$ change_hospitalizations     &lt;int&gt; 24\n$ change_criticals            &lt;int&gt; -3\n$ change_recoveries           &lt;int&gt; 0\n$ change_vaccinations         &lt;int&gt; 84199\n$ change_vaccinated           &lt;int&gt; 2099\n$ change_boosters_1           &lt;int&gt; 6693\n$ change_boosters_2           &lt;int&gt; 59646\n$ change_vaccines_distributed &lt;int&gt; 0\n$ total_cases                 &lt;int&gt; 4278841\n$ total_fatalities            &lt;int&gt; 45808\n$ total_tests                 &lt;int&gt; 61472226\n$ total_hospitalizations      &lt;int&gt; 5629\n$ total_criticals             &lt;int&gt; 279\n$ total_recoveries            &lt;int&gt; 3815086\n$ total_vaccinations          &lt;int&gt; 91945442\n$ total_vaccinated            &lt;int&gt; 31624598\n$ total_boosters_1            &lt;int&gt; 19362861\n$ total_boosters_2            &lt;int&gt; 7266271\n$ total_vaccines_distributed  &lt;int&gt; 101586018\n\n\n\n\nProvince\nInstead of aggregating over all provinces, I can use /summary/split to get province-level summaries:\n\nresp &lt;- httr::GET(paste0(base_url, \"summary/split\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 2)\n\nList of 2\n $ data        :List of 13\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n $ last_updated: chr \"2022-10-26 23:19:02\"\n\n\nThe data list now has 13 lists corresponding to the 13 provinces and territories. Look at the structure of one of them:\n\nstr(content_parsed$data[[1]])\n\nList of 24\n $ province                   : chr \"ON\"\n $ date                       : chr \"2022-10-21\"\n $ change_cases               : int 0\n $ change_fatalities          : int 0\n $ change_tests               : int 0\n $ change_hospitalizations    : int 0\n $ change_criticals           : int 0\n $ change_recoveries          : int 0\n $ change_vaccinations        : int 0\n $ change_vaccinated          : int 0\n $ change_boosters_1          : int 0\n $ change_boosters_2          : int 0\n $ change_vaccines_distributed: int 0\n $ total_cases                : int 1477669\n $ total_fatalities           : int 14603\n $ total_tests                : int 25776503\n $ total_hospitalizations     : int 1663\n $ total_criticals            : int 158\n $ total_recoveries           : int 1448143\n $ total_vaccinations         : int 35825406\n $ total_vaccinated           : int 12319953\n $ total_boosters_1           : int 7624409\n $ total_boosters_2           : int 3076429\n $ total_vaccines_distributed : int 38554461\n\n\nThis is the same structure as the overall summary, but with the extra variable province indicating that these numbers are specific to Ontario.\nA shortcut to compiling all of these lists into a single data frame with a row per province/territory is to use dplyr::bind_rows():\n\nsummary_province &lt;- bind_rows(content_parsed$data)\nglimpse(summary_province)\n\nRows: 13\nColumns: 24\n$ province                    &lt;chr&gt; \"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", ‚Ä¶\n$ date                        &lt;chr&gt; \"2022-10-21\", \"2022-10-26\", \"2022-10-21\", ‚Ä¶\n$ change_cases                &lt;int&gt; 0, 1013, 0, 0, 0, 0, 0, 0, 1304, 0, 0, 0, 0\n$ change_fatalities           &lt;int&gt; 0, 24, 0, 0, 0, 0, 0, 0, 28, 0, 0, 0, 0\n$ change_tests                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ change_hospitalizations     &lt;int&gt; 0, -24, 0, 0, 0, 0, 0, 0, 48, 0, 0, 0, 0\n$ change_criticals            &lt;int&gt; 0, 2, 0, 0, 0, 0, 0, 0, -5, 0, 0, 0, 0\n$ change_recoveries           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ change_vaccinations         &lt;int&gt; 0, 26264, 0, 0, 0, 0, 0, 0, 57935, 0, 0, 0‚Ä¶\n$ change_vaccinated           &lt;int&gt; 0, 253, 0, 0, 0, 0, 0, 0, 1846, 0, 0, 0, 0\n$ change_boosters_1           &lt;int&gt; 0, 1121, 0, 0, 0, 0, 0, 0, 5572, 0, 0, 0, 0\n$ change_boosters_2           &lt;int&gt; 0, 10290, 0, 0, 0, 0, 0, 0, 49356, 0, 0, 0‚Ä¶\n$ change_vaccines_distributed &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ total_cases                 &lt;int&gt; 1477669, 1223930, 106158, 70545, 150957, 3‚Ä¶\n$ total_fatalities            &lt;int&gt; 14603, 16904, 451, 438, 2199, 4423, 46, 14‚Ä¶\n$ total_tests                 &lt;int&gt; 25776503, 16733572, 1777959, 782596, 15498‚Ä¶\n$ total_hospitalizations      &lt;int&gt; 1663, 1942, 44, 22, 291, 389, 10, 147, 111‚Ä¶\n$ total_criticals             &lt;int&gt; 158, 52, 8, 1, 5, 21, 1, 5, 28, 0, 0, 0, 0\n$ total_recoveries            &lt;int&gt; 1448143, 1099129, 43018, 65138, 129338, 30‚Ä¶\n$ total_vaccinations          &lt;int&gt; 35825406, 22125898, 2235685, 1757172, 3001‚Ä¶\n$ total_vaccinated            &lt;int&gt; 12319953, 7214095, 842600, 661271, 1101542‚Ä¶\n$ total_boosters_1            &lt;int&gt; 7624409, 4638755, 503257, 395689, 607635, ‚Ä¶\n$ total_boosters_2            &lt;int&gt; 3076429, 2108496, 0, 0, 146806, 1091493, 7‚Ä¶\n$ total_vaccines_distributed  &lt;int&gt; 38554461, 23092789, 2518832, 2212445, 3842‚Ä¶\n\n\nbind_rows() also automatically converts the numeric columns to integer, but the date column is still character:\n\nsummary_province &lt;- summary_province %&gt;% mutate(date = as.Date(date))\n\n\n\nHealth region\nData my be split even further by health region with summary/split/hr:\n\nresp &lt;- httr::GET(paste0(base_url, \"summary/split/hr\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 2\n $ data        :List of 92\n $ last_updated: chr \"2022-10-26 23:19:02\"\n\n\nThis data consists of 92 entries with mostly the same variables as previous summaries:\n\nstr(content_parsed$data[[1]])\n\nList of 22\n $ hr_uid                 : int 1201\n $ date                   : chr \"2022-08-10\"\n $ change_cases           : NULL\n $ change_fatalities      : NULL\n $ change_tests           : NULL\n $ change_hospitalizations: NULL\n $ change_criticals       : NULL\n $ change_recoveries      : NULL\n $ change_vaccinations    : NULL\n $ change_vaccinated      : NULL\n $ change_boosters_1      : NULL\n $ change_boosters_2      : NULL\n $ total_cases            : int 0\n $ total_fatalities       : int 1\n $ total_tests            : NULL\n $ total_hospitalizations : NULL\n $ total_criticals        : NULL\n $ total_recoveries       : int 640\n $ total_vaccinations     : NULL\n $ total_vaccinated       : NULL\n $ total_boosters_1       : NULL\n $ total_boosters_2       : NULL\n\n\nThe differences are the hr_uid column in place of province, and the lack of change_vaccines_distributed and total_vaccines_distributed, presumably because these numbers aren‚Äôt available at this granularity.\n\nsummary_region &lt;- bind_rows(content_parsed$data) %&gt;%\n  mutate(date = as.Date(date))\nglimpse(summary_region)\n\nRows: 92\nColumns: 21\n$ hr_uid                  &lt;int&gt; 1201, 1202, 1203, 1204, 1301, 1302, 1303, 1304‚Ä¶\n$ date                    &lt;date&gt; 2022-08-10, 2022-08-10, 2022-08-10, 2022-08-1‚Ä¶\n$ total_cases             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4‚Ä¶\n$ total_fatalities        &lt;int&gt; 1, 1, 5, 58, 2, 1, 0, 0, 4, 0, 0, 3, 0, 1, 0, ‚Ä¶\n$ total_recoveries        &lt;int&gt; 640, 943, 947, 6344, 15766, 11569, 8848, 4374,‚Ä¶\n$ total_tests             &lt;int&gt; NA, NA, NA, NA, 293294, 135345, 146674, 78585,‚Ä¶\n$ total_hospitalizations  &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1,‚Ä¶\n$ total_criticals         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1,‚Ä¶\n$ total_vaccinations      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ total_vaccinated        &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ total_boosters_1        &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ total_boosters_2        &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ change_vaccinations     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ change_vaccinated       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ change_boosters_1       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ change_cases            &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ change_fatalities       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ change_hospitalizations &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ change_criticals        &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ change_boosters_2       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ change_recoveries       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n\n\nhr_uid is a unique identifier for each health region. A lookup table is available through the API with regions:\n\nresp &lt;- httr::GET(paste0(base_url, \"regions\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 1\n $ data:List of 92\n\n\nThere are 92 elements, matching the 92 health regions in the summary data, with the following structure:\n\nregions &lt;- bind_rows(content_parsed$data)\nglimpse(regions)\n\nRows: 92\nColumns: 4\n$ hr_uid   &lt;int&gt; 471, 472, 473, 474, 475, 476, 591, 592, 593, 594, 595, 1011, ‚Ä¶\n$ province &lt;chr&gt; \"SK\", \"SK\", \"SK\", \"SK\", \"SK\", \"SK\", \"BC\", \"BC\", \"BC\", \"BC\", \"‚Ä¶\n$ engname  &lt;chr&gt; \"Far North\", \"North\", \"Central\", \"Saskatoon\", \"Regina\", \"Sout‚Ä¶\n$ frename  &lt;chr&gt; \"Far North\", \"North\", \"Central\", \"Saskatoon\", \"Regina\", \"Sout‚Ä¶\n\n\nAdd the health region to the summary_region data:\n\nsummary_region &lt;- regions %&gt;%\n  left_join(summary_region, by = \"hr_uid\")\nglimpse(summary_region)\n\nRows: 92\nColumns: 24\n$ hr_uid                  &lt;int&gt; 471, 472, 473, 474, 475, 476, 591, 592, 593, 5‚Ä¶\n$ province                &lt;chr&gt; \"SK\", \"SK\", \"SK\", \"SK\", \"SK\", \"SK\", \"BC\", \"BC\"‚Ä¶\n$ engname                 &lt;chr&gt; \"Far North\", \"North\", \"Central\", \"Saskatoon\", ‚Ä¶\n$ frename                 &lt;chr&gt; \"Far North\", \"North\", \"Central\", \"Saskatoon\", ‚Ä¶\n$ date                    &lt;date&gt; 2022-10-08, 2022-10-08, 2022-10-08, 2022-10-0‚Ä¶\n$ total_cases             &lt;int&gt; 0, 0, 0, 0, 0, 0, 68979, 169214, 78383, 38762,‚Ä¶\n$ total_fatalities        &lt;int&gt; 8, 16, 5, 6, 3, 7, 670, 1807, 1049, 511, 386, ‚Ä¶\n$ total_recoveries        &lt;int&gt; 10248, 24271, 9069, 29440, 23925, 14978, 39899‚Ä¶\n$ total_tests             &lt;int&gt; 66969, 206300, 115789, 380252, 290508, 176859,‚Ä¶\n$ total_hospitalizations  &lt;int&gt; 2, 61, 22, 171, 54, 22, 48, 135, 110, 77, 15, ‚Ä¶\n$ total_criticals         &lt;int&gt; 0, 7, 1, 17, 5, 1, 3, 10, 4, 3, 1, 1, 1, 3, 0,‚Ä¶\n$ total_vaccinations      &lt;int&gt; 70534, 319807, 201758, 513380, 430413, 273939,‚Ä¶\n$ total_vaccinated        &lt;int&gt; 33236, 152611, 98324, 249513, 208970, 133557, ‚Ä¶\n$ total_boosters_1        &lt;int&gt; NA, NA, NA, NA, NA, NA, 413203, 967396, 829512‚Ä¶\n$ total_boosters_2        &lt;int&gt; NA, NA, NA, NA, NA, NA, 181063, 348400, 268145‚Ä¶\n$ change_vaccinations     &lt;int&gt; NA, NA, NA, NA, NA, NA, 23869, 56412, 39975, 3‚Ä¶\n$ change_vaccinated       &lt;int&gt; NA, NA, NA, NA, NA, NA, 272, 1033, 1053, 595, ‚Ä¶\n$ change_boosters_1       &lt;int&gt; NA, NA, NA, NA, NA, NA, 1261, 2998, 2300, 1876‚Ä¶\n$ change_cases            &lt;int&gt; NA, NA, NA, NA, NA, NA, 122, 171, 153, 114, 67‚Ä¶\n$ change_fatalities       &lt;int&gt; NA, NA, NA, NA, NA, NA, 11, 13, 17, 8, 4, NA, ‚Ä¶\n$ change_hospitalizations &lt;int&gt; NA, NA, NA, NA, NA, NA, -24, 21, 15, 15, -5, N‚Ä¶\n$ change_criticals        &lt;int&gt; NA, NA, NA, NA, NA, NA, -1, 2, 0, 0, 1, NA, NA‚Ä¶\n$ change_boosters_2       &lt;int&gt; NA, NA, NA, NA, NA, NA, 15716, 40202, 30843, 2‚Ä¶\n$ change_recoveries       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#reports",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#reports",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Reports",
    "text": "Reports\n\nOverall\nReports are much like summaries, but for every day instead of just the most recent.\n\nresp &lt;- httr::GET(paste0(base_url, \"reports\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 3\n $ province    : chr \"All\"\n $ last_updated: chr \"2022-10-26 23:19:02\"\n $ data        :List of 1004\n\n\nAn additional top-level variable province defines the scope of the report. The data list here consists of 705 elements with the following structure:\n\nstr(content_parsed$data[[1]])\n\nList of 23\n $ date                       : chr \"2020-01-25\"\n $ change_cases               : int 1\n $ change_fatalities          : int 0\n $ change_tests               : int 2\n $ change_hospitalizations    : int 0\n $ change_criticals           : int 0\n $ change_recoveries          : int 0\n $ change_vaccinations        : int 0\n $ change_vaccinated          : int 0\n $ change_boosters_1          : int 0\n $ change_boosters_2          : int 0\n $ change_vaccines_distributed: int 0\n $ total_cases                : int 1\n $ total_fatalities           : int 0\n $ total_tests                : int 2\n $ total_hospitalizations     : int 0\n $ total_criticals            : int 0\n $ total_recoveries           : int 0\n $ total_vaccinations         : int 0\n $ total_vaccinated           : int 0\n $ total_boosters_1           : int 0\n $ total_boosters_2           : int 0\n $ total_vaccines_distributed : int 0\n\n\nThis first element has many zeroes, which makes sense as the date (January 25th, 2020) corresponds to the first confirmed case of COVID in Canada. The last element of this list should have today‚Äôs data:\n\nstr(content_parsed$data[[length(content_parsed$data)]])\n\nList of 23\n $ date                       : chr \"2022-10-26\"\n $ change_cases               : int 2317\n $ change_fatalities          : int 52\n $ change_tests               : int 0\n $ change_hospitalizations    : int 24\n $ change_criticals           : int -3\n $ change_recoveries          : int 0\n $ change_vaccinations        : int 84199\n $ change_vaccinated          : int 2099\n $ change_boosters_1          : int 6693\n $ change_boosters_2          : int 59646\n $ change_vaccines_distributed: int 0\n $ total_cases                : int 1836623\n $ total_fatalities           : int 21915\n $ total_tests                : int 23647154\n $ total_hospitalizations     : int 3060\n $ total_criticals            : int 80\n $ total_recoveries           : int 1622808\n $ total_vaccinations         : int 31580797\n $ total_vaccinated           : int 10646491\n $ total_boosters_1           : int 6401148\n $ total_boosters_2           : int 2733841\n $ total_vaccines_distributed : int 35371888\n\n\nThe data may be bound together in the same way:\n\nreport_overall &lt;- bind_rows(content_parsed$data) %&gt;%\n  mutate(date = as.Date(date))\n\n\n\nProvince\nTo split data by province, the two-letter code is provided as reports/province/{code}:\n\nresp &lt;- httr::GET(paste0(base_url, \"reports/province/ns\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nreport_ns &lt;- bind_rows(content_parsed$data) %&gt;%\n  mutate(date = as.Date(date))\nglimpse(report_ns)\n\nRows: 1,001\nColumns: 23\n$ date                        &lt;date&gt; 2020-01-25, 2020-01-26, 2020-01-27, 2020-‚Ä¶\n$ change_cases                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ change_fatalities           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ change_tests                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ change_hospitalizations     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ change_criticals            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ change_recoveries           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ change_vaccinations         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ change_vaccinated           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ change_boosters_1           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ change_boosters_2           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ change_vaccines_distributed &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_cases                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_fatalities            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_tests                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_hospitalizations      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_criticals             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_recoveries            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_vaccinations          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_vaccinated            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_boosters_1            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_boosters_2            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_vaccines_distributed  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n\n\n\n\nHealth region\nSimilarly, provide the hr_uid in reports/regions/{hr_uid} to get health region reports:\n\nresp &lt;- httr::GET(paste0(base_url, \"reports/regions/1204\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nreport_ns_central &lt;- bind_rows(content_parsed$data) %&gt;%\n  mutate(date = as.Date(date))\nglimpse(report_ns_central)\n\nRows: 856\nColumns: 7\n$ date              &lt;date&gt; 2020-01-15, 2020-01-16, 2020-01-17, 2020-01-18, 202‚Ä¶\n$ change_cases      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ change_fatalities &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ total_cases       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ total_fatalities  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ change_recoveries &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ total_recoveries  &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n\n\nI chose Nova Scotia central because it is where I live and, looking at this data, it clearly isn‚Äôt being updated day-to-day:\n\nreport_ns_central %&gt;%\n  filter(date &gt;= \"2021-12-20\", date &lt; \"2021-12-28\") %&gt;%\n  glimpse()\n\nRows: 8\nColumns: 7\n$ date              &lt;date&gt; 2021-12-20, 2021-12-21, 2021-12-22, 2021-12-23, 202‚Ä¶\n$ change_cases      &lt;int&gt; 136, 63, 0, NA, NA, NA, NA, NA\n$ change_fatalities &lt;int&gt; 0, 0, 0, NA, NA, NA, NA, NA\n$ total_cases       &lt;int&gt; 6718, 6781, 6781, 6781, 6781, 6781, 6781, 6781\n$ total_fatalities  &lt;int&gt; 87, 87, 87, 87, 87, 87, 87, 87\n$ change_recoveries &lt;int&gt; 100, 113, 0, NA, NA, NA, NA, NA\n$ total_recoveries  &lt;int&gt; 6231, 6344, 6344, 6344, 6344, 6344, 6344, 6344\n\n\nThere has, unfortunately, been hundreds of cases per day here recently. These numbers are reflected in the province report however:\n\nreport_ns %&gt;%\n  filter(date &gt;= \"2021-12-20\", date &lt; \"2021-12-28\") %&gt;%\n  glimpse()\n\nRows: 8\nColumns: 23\n$ date                        &lt;date&gt; 2021-12-20, 2021-12-21, 2021-12-22, 2021-‚Ä¶\n$ change_cases                &lt;int&gt; 485, 522, 537, 689, 611, 569, 578, 581\n$ change_fatalities           &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0\n$ change_tests                &lt;int&gt; 34815, 0, 10254, 0, 0, 0, 0, 0\n$ change_hospitalizations     &lt;int&gt; 2, 1, 0, 4, 0, 0, 0, 0\n$ change_criticals            &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0\n$ change_recoveries           &lt;int&gt; 263, 0, 0, 0, 0, 0, 0, 0\n$ change_vaccinations         &lt;int&gt; 13018, 8953, 10225, 7903, 0, 0, 0, 0\n$ change_vaccinated           &lt;int&gt; 423, 296, 366, 275, 0, 0, 0, 0\n$ change_boosters_1           &lt;int&gt; 9403, 7298, 8283, 6367, 0, 0, 0, 0\n$ change_boosters_2           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0\n$ change_vaccines_distributed &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0\n$ total_cases                 &lt;int&gt; 11318, 11840, 12377, 13066, 13677, 14246, ‚Ä¶\n$ total_fatalities            &lt;int&gt; 110, 110, 111, 111, 111, 111, 111, 111\n$ total_tests                 &lt;int&gt; 1506218, 1506218, 1516472, 1516472, 151647‚Ä¶\n$ total_hospitalizations      &lt;int&gt; 9, 10, 10, 14, 14, 14, 14, 14\n$ total_criticals             &lt;int&gt; 2, 3, 3, 4, 4, 4, 4, 4\n$ total_recoveries            &lt;int&gt; 8643, 8643, 8643, 8643, 8643, 8643, 8643, ‚Ä¶\n$ total_vaccinations          &lt;int&gt; 1731205, 1740158, 1750383, 1758286, 175828‚Ä¶\n$ total_vaccinated            &lt;int&gt; 792552, 792848, 793214, 793489, 793489, 79‚Ä¶\n$ total_boosters_1            &lt;int&gt; 83071, 90369, 98652, 105019, 105019, 10501‚Ä¶\n$ total_boosters_2            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0\n$ total_vaccines_distributed  &lt;int&gt; 1950040, 1950040, 1950040, 1950040, 195004‚Ä¶\n\n\n\n\nParameters\nThe reports have a number of optional parameters available to alter the API request.\nThe fill_dates option fills dates with missing entries:\n\ncontent_parsed &lt;- paste0(base_url, \"reports/regions/1204?fill_dates=false\") %&gt;%\n  httr::GET() %&gt;%\n  content(as = \"parsed\")\n\n\nbind_rows(content_parsed$data) %&gt;% glimpse()\n\nRows: 939\nColumns: 8\n$ date              &lt;chr&gt; \"2020-01-15\", \"2020-01-16\", \"2020-01-17\", \"2020-01-1‚Ä¶\n$ change_cases      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ change_fatalities &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ total_cases       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ total_fatalities  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ change_recoveries &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ total_recoveries  &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ fill              &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n\n\nThe stat argument allows one to specify a single statistic to return:\n\ncontent_parsed &lt;- paste0(base_url, \"reports/province/ns?stat=cases\") %&gt;%\n  httr::GET() %&gt;%\n  content(as = \"parsed\")\n\n\nbind_rows(content_parsed$data) %&gt;% glimpse()\n\nRows: 1,001\nColumns: 3\n$ date         &lt;chr&gt; \"2020-01-25\", \"2020-01-26\", \"2020-01-27\", \"2020-01-28\", \"‚Ä¶\n$ change_cases &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ total_cases  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n\n\nThe date parameter returns a report from a single date:\n\ncontent_parsed &lt;- paste0(base_url, \"reports/province/ab?date=2021-12-25\") %&gt;%\n  httr::GET() %&gt;%\n  content(as = \"parsed\")\n\n\nbind_rows(content_parsed$data) %&gt;% glimpse()\n\nRows: 1\nColumns: 23\n$ date                        &lt;chr&gt; \"2021-12-25\"\n$ change_cases                &lt;int&gt; 2484\n$ change_fatalities           &lt;int&gt; 0\n$ change_tests                &lt;int&gt; 11479\n$ change_hospitalizations     &lt;int&gt; 0\n$ change_criticals            &lt;int&gt; 0\n$ change_recoveries           &lt;int&gt; 0\n$ change_vaccinations         &lt;int&gt; 0\n$ change_vaccinated           &lt;int&gt; 0\n$ change_boosters_1           &lt;int&gt; 0\n$ change_boosters_2           &lt;int&gt; 0\n$ change_vaccines_distributed &lt;int&gt; 0\n$ total_cases                 &lt;int&gt; 351199\n$ total_fatalities            &lt;int&gt; 3299\n$ total_tests                 &lt;int&gt; 6347374\n$ total_hospitalizations      &lt;int&gt; 318\n$ total_criticals             &lt;int&gt; 64\n$ total_recoveries            &lt;int&gt; 335047\n$ total_vaccinations          &lt;int&gt; 7452649\n$ total_vaccinated            &lt;int&gt; 3211241\n$ total_boosters_1            &lt;int&gt; 761153\n$ total_boosters_2            &lt;int&gt; 0\n$ total_vaccines_distributed  &lt;int&gt; 8799859\n\n\nLastly, the after and before parameters return on/after and on/before specific dates:\n\ncontent_parsed &lt;-\n  paste0(base_url, \"reports/province/qc?after=2021-12-24&before=2021-12-26\") %&gt;%\n  httr::GET() %&gt;%\n  content(as = \"parsed\")\n\n\nbind_rows(content_parsed$data) %&gt;% glimpse()\n\nRows: 3\nColumns: 23\n$ date                        &lt;chr&gt; \"2021-12-24\", \"2021-12-25\", \"2021-12-26\"\n$ change_cases                &lt;int&gt; 10031, 9206, 8231\n$ change_fatalities           &lt;int&gt; 2, 4, 10\n$ change_tests                &lt;int&gt; 55863, 53334, 44022\n$ change_hospitalizations     &lt;int&gt; 0, 0, 0\n$ change_criticals            &lt;int&gt; 0, 0, 0\n$ change_recoveries           &lt;int&gt; 3017, 3559, 0\n$ change_vaccinations         &lt;int&gt; 85039, 24435, 117\n$ change_vaccinated           &lt;int&gt; 2120, 638, 9\n$ change_boosters_1           &lt;int&gt; 78745, 22612, 98\n$ change_boosters_2           &lt;int&gt; 186, 124, 2\n$ change_vaccines_distributed &lt;int&gt; 0, 0, 0\n$ total_cases                 &lt;int&gt; 521126, 530332, 538563\n$ total_fatalities            &lt;int&gt; 11660, 11664, 11674\n$ total_tests                 &lt;int&gt; 14573238, 14626572, 14670594\n$ total_hospitalizations      &lt;int&gt; 473, 473, 473\n$ total_criticals             &lt;int&gt; 91, 91, 91\n$ total_recoveries            &lt;int&gt; 460647, 464206, 464206\n$ total_vaccinations          &lt;int&gt; 15034784, 15059219, 15059336\n$ total_vaccinated            &lt;int&gt; 6701798, 6702436, 6702445\n$ total_boosters_1            &lt;int&gt; 1021886, 1044498, 1044596\n$ total_boosters_2            &lt;int&gt; 3469, 3593, 3595\n$ total_vaccines_distributed  &lt;int&gt; 16179459, 16179459, 16179459\n\n\nNote how parameters can be combined as above, by separating the arguments with &."
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#vaccination-data",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#vaccination-data",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Vaccination data",
    "text": "Vaccination data\nWe have already seen the vaccination data returned by summary and report requests. The variables include:\n\nvaccinations: total doses administered\nvaccinated: total number of people with two doses\nboosters_1: total number of boosters (3rd dose) administered\nvaccines_administered: total number of doses delivered to province\n\nAt the summary level:\n\nsummary_province %&gt;%\n  filter(province == \"NS\") %&gt;%\n  select(date, matches(\"vacc|boost\")) %&gt;%\n  glimpse()\n\nRows: 1\nColumns: 11\n$ date                        &lt;date&gt; 2022-10-21\n$ change_vaccinations         &lt;int&gt; 0\n$ change_vaccinated           &lt;int&gt; 0\n$ change_boosters_1           &lt;int&gt; 0\n$ change_boosters_2           &lt;int&gt; 0\n$ change_vaccines_distributed &lt;int&gt; 0\n$ total_vaccinations          &lt;int&gt; 2235685\n$ total_vaccinated            &lt;int&gt; 842600\n$ total_boosters_1            &lt;int&gt; 503257\n$ total_boosters_2            &lt;int&gt; 0\n$ total_vaccines_distributed  &lt;int&gt; 2518832\n\n\nAt the report level:\n\nreport_ns %&gt;%\n  select(date, matches(\"vacc|boost\")) %&gt;%\n  tail() %&gt;%\n  glimpse()\n\nRows: 6\nColumns: 11\n$ date                        &lt;date&gt; 2022-10-16, 2022-10-17, 2022-10-18, 2022-‚Ä¶\n$ change_vaccinations         &lt;int&gt; 0, 0, 0, 0, 0, 0\n$ change_vaccinated           &lt;int&gt; 0, 0, 0, 0, 0, 0\n$ change_boosters_1           &lt;int&gt; 0, 0, 0, 0, 0, 0\n$ change_boosters_2           &lt;int&gt; 0, 0, 0, 0, 0, 0\n$ change_vaccines_distributed &lt;int&gt; 0, 0, 0, 0, 0, 0\n$ total_vaccinations          &lt;int&gt; 2235685, 2235685, 2235685, 2235685, 223568‚Ä¶\n$ total_vaccinated            &lt;int&gt; 842600, 842600, 842600, 842600, 842600, 84‚Ä¶\n$ total_boosters_1            &lt;int&gt; 503257, 503257, 503257, 503257, 503257, 50‚Ä¶\n$ total_boosters_2            &lt;int&gt; 0, 0, 0, 0, 0, 0\n$ total_vaccines_distributed  &lt;int&gt; 2518832, 2518832, 2518832, 2518832, 251883‚Ä¶\n\n\n\nSubregions\nVaccination date is also available at the subregion level for certain provinces and territories. The API documentation doesn‚Äôt actually specify which provinces and territories, but I can find out by requesting the data as follows:\n\nresp &lt;- httr::GET(paste0(base_url, \"reports/sub-regions/summary\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nsubregion_vacc_summary &lt;- bind_rows(content_parsed$data) %&gt;%\n  mutate(date = as.Date(date))\n\nglimpse(subregion_vacc_summary)\n\nRows: 806\nColumns: 11\n$ code           &lt;chr&gt; \"SK001\", \"SK002\", \"SK003\", \"SK004\", \"SK005\", \"SK006\", \"‚Ä¶\n$ date           &lt;date&gt; 2022-02-06, 2022-02-06, 2022-02-06, 2022-02-06, 2022-0‚Ä¶\n$ total_dose_1   &lt;int&gt; 18733, 1959, 16606, 66614, 67900, 32682, 263867, 27576,‚Ä¶\n$ percent_dose_1 &lt;chr&gt; \"0.70048\", \"0.68282\", \"0.75147\", \"0.86337\", \"0.79184\", ‚Ä¶\n$ source_dose_1  &lt;chr&gt; \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"‚Ä¶\n$ total_dose_2   &lt;int&gt; 16875, 1706, 14655, 58628, 63117, 30866, 249513, 26220,‚Ä¶\n$ percent_dose_2 &lt;chr&gt; \"0.63101\", \"0.59463\", \"0.66318\", \"0.75986\", \"0.73606\", ‚Ä¶\n$ source_dose_2  &lt;chr&gt; \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"‚Ä¶\n$ total_dose_3   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 367‚Ä¶\n$ percent_dose_3 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"0.‚Ä¶\n$ source_dose_3  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"pe‚Ä¶\n\n\nThe code labels can be retrieved via sub-regions:\n\nresp &lt;- httr::GET(paste0(base_url, \"sub-regions\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nsubregions &lt;- bind_rows(content_parsed$data)\n\nglimpse(subregions)\n\nRows: 806\nColumns: 5\n$ code       &lt;chr&gt; \"AB001\", \"AB002\", \"AB003\", \"AB004\", \"AB005\", \"AB006\", \"AB00‚Ä¶\n$ province   &lt;chr&gt; \"AB\", \"AB\", \"AB\", \"AB\", \"AB\", \"AB\", \"AB\", \"AB\", \"AB\", \"AB\",‚Ä¶\n$ zone       &lt;chr&gt; \"SOUTH\", \"SOUTH\", \"SOUTH\", \"SOUTH\", \"SOUTH\", \"SOUTH\", \"SOUT‚Ä¶\n$ region     &lt;chr&gt; \"CROWSNEST PASS\", \"PINCHER CREEK\", \"FORT MACLEOD\", \"CARDSTO‚Ä¶\n$ population &lt;int&gt; 6280, 8344, 6753, 16595, 25820, 19028, 11104, 6409, 27753, ‚Ä¶\n\n\n806 subregions, which matches the count from the summary, with the following distribution by province:\n\nsubregions %&gt;% count(province) %&gt;% gt()\n\n\n\n\n\n\n\n\nprovince\nn\n\n\n\n\nAB\n132\n\n\nMB\n79\n\n\nNL\n38\n\n\nNT\n30\n\n\nON\n514\n\n\nSK\n13"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#vaccine-age-groups",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#vaccine-age-groups",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Vaccine age groups",
    "text": "Vaccine age groups\n\nOverall\nVaccine data by age groups is reported week-by-week, and accessed with vaccines/age-groups:\n\nresp &lt;- httr::GET(paste0(base_url, \"vaccines/age-groups\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nvaccine_age_groups &lt;- bind_rows(content_parsed$data) %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  filter(date &lt;= \"2021-12-28\")\n\nglimpse(vaccine_age_groups)\n\nRows: 54\nColumns: 2\n$ date &lt;date&gt; 2020-12-19, 2020-12-26, 2021-01-02, 2021-01-09, 2021-01-16, 2021‚Ä¶\n$ data &lt;chr&gt; \"{\\\"80+\\\": {\\\"full\\\": 0, \\\"group\\\": \\\"80+\\\", \\\"partial\\\": 335, \\\"‚Ä¶\n\n\nThe data here is returned as an un-parsed JSON string. Per the API documentation, it has to do with shifting reporting standards across weeks:\n\ndue to reporting standard shifts overtime, the JSON string data may not be consistent across weeks. Minimal effort is taken to normalize some of this data.\n\nLook at the first element of data:\n\nvaccine_age_groups$data[[1]] %&gt;% str_trunc(80)\n\n[1] \"{\\\"80+\\\": {\\\"full\\\": 0, \\\"group\\\": \\\"80+\\\", \\\"partial\\\": 335, \\\"atleast1\\\": 335}, \\\"0-15\\\":...\"\n\n\nParse the JSON:\n\njsonlite::fromJSON(vaccine_age_groups$data[[1]]) %&gt;%\n  str()\n\nList of 8\n $ 80+         :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"80+\"\n  ..$ partial : int 335\n  ..$ atleast1: int 335\n $ 0-15        :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"0-15\"\n  ..$ partial : int 0\n  ..$ atleast1: int 0\n $ 16-69       :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"16-69\"\n  ..$ partial : int 11768\n  ..$ atleast1: int 11768\n $ 70-74       :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"70-74\"\n  ..$ partial : int 174\n  ..$ atleast1: int 174\n $ 75-79       :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"75-79\"\n  ..$ partial : int 85\n  ..$ atleast1: int 85\n $ unknown     :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"Unknown\"\n  ..$ partial : int 0\n  ..$ atleast1: int 0\n $ all_ages    :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"All ages\"\n  ..$ partial : int 12362\n  ..$ atleast1: int 12362\n $ not_reported:List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"Not reported\"\n  ..$ partial : int 0\n  ..$ atleast1: int 0\n\n\nTo see how the reporting has changed over time, here is the most recent age group vaccination data:\n\njsonlite::fromJSON(\n  vaccine_age_groups$data[[length(vaccine_age_groups$data)]]\n) %&gt;%\n  str()\n\nList of 13\n $ 0-4         :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"0-4\"\n  ..$ partial : int 276\n  ..$ atleast1: int 276\n $ 80+         :List of 4\n  ..$ full    : int 1640558\n  ..$ group   : chr \"80+\"\n  ..$ partial : int 30717\n  ..$ atleast1: int 1671275\n $ 05-11       :List of 4\n  ..$ full    : int 44794\n  ..$ group   : chr \"05-11\"\n  ..$ partial : int 1206532\n  ..$ atleast1: int 1251326\n $ 12-17       :List of 4\n  ..$ full    : int 2039683\n  ..$ group   : chr \"12-17\"\n  ..$ partial : int 111096\n  ..$ atleast1: int 2150779\n $ 18-29       :List of 4\n  ..$ full    : int 4916940\n  ..$ group   : chr \"18-29\"\n  ..$ partial : int 259662\n  ..$ atleast1: int 5176602\n $ 30-39       :List of 4\n  ..$ full    : int 4475399\n  ..$ group   : chr \"30-39\"\n  ..$ partial : int 185294\n  ..$ atleast1: int 4660693\n $ 40-49       :List of 4\n  ..$ full    : int 4250041\n  ..$ group   : chr \"40-49\"\n  ..$ partial : int 125031\n  ..$ atleast1: int 4375072\n $ 50-59       :List of 4\n  ..$ full    : int 4526830\n  ..$ group   : chr \"50-59\"\n  ..$ partial : int 104559\n  ..$ atleast1: int 4631389\n $ 60-69       :List of 4\n  ..$ full    : int 4459359\n  ..$ group   : chr \"60-69\"\n  ..$ partial : int 78044\n  ..$ atleast1: int 4537403\n $ 70-79       :List of 4\n  ..$ full    : int 2970726\n  ..$ group   : chr \"70-79\"\n  ..$ partial : int 41346\n  ..$ atleast1: int 3012072\n $ unknown     :List of 4\n  ..$ full    : int 2021\n  ..$ group   : chr \"Unknown\"\n  ..$ partial : int 804\n  ..$ atleast1: int 2825\n $ all_ages    :List of 4\n  ..$ full    : int 29326351\n  ..$ group   : chr \"All ages\"\n  ..$ partial : int 2143361\n  ..$ atleast1: int 31469712\n $ not_reported:List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"Not reported\"\n  ..$ partial : int 0\n  ..$ atleast1: int 0\n\n\nEach JSON data point can be converted to a data frame as follows:\n\njsonlite::fromJSON(vaccine_age_groups$data[[1]]) %&gt;%\n  bind_rows(.id = \"group_code\") %&gt;%\n  gt()\n\n\n\n\n\n\n\n\ngroup_code\nfull\ngroup\npartial\natleast1\n\n\n\n\n80+\n0\n80+\n335\n335\n\n\n0-15\n0\n0-15\n0\n0\n\n\n16-69\n0\n16-69\n11768\n11768\n\n\n70-74\n0\n70-74\n174\n174\n\n\n75-79\n0\n75-79\n85\n85\n\n\nunknown\n0\nUnknown\n0\n0\n\n\nall_ages\n0\nAll ages\n12362\n12362\n\n\nnot_reported\n0\nNot reported\n0\n0\n\n\n\n\n\n\n\nUse map and unnest to apply this to each row of the data:\n\nvaccine_age_groups &lt;- vaccine_age_groups %&gt;%\n  mutate(\n    data = map(\n      data,\n      ~jsonlite::fromJSON(.x) %&gt;% bind_rows(.id = \"group_code\")\n    )\n  ) %&gt;%\n  unnest(data)\nglimpse(vaccine_age_groups)\n\nRows: 603\nColumns: 6\n$ date       &lt;date&gt; 2020-12-19, 2020-12-19, 2020-12-19, 2020-12-19, 2020-12-19‚Ä¶\n$ group_code &lt;chr&gt; \"80+\", \"0-15\", \"16-69\", \"70-74\", \"75-79\", \"unknown\", \"all_a‚Ä¶\n$ full       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ group      &lt;chr&gt; \"80+\", \"0-15\", \"16-69\", \"70-74\", \"75-79\", \"Unknown\", \"All a‚Ä¶\n$ partial    &lt;int&gt; 335, 0, 11768, 174, 85, 0, 12362, 0, 2229, 5, 40170, 649, 4‚Ä¶\n$ atleast1   &lt;int&gt; 335, 0, 11768, 174, 85, 0, 12362, 0, 2229, 5, 40170, 649, 4‚Ä¶\n\n\nThe unique groups:\n\nvaccine_age_groups %&gt;% count(group_code, group) %&gt;% rmarkdown::paged_table()\n\n\n\n  \n\n\n\nVisualize how the age ranges evolve over time:\n\n# Make it a function that will allow splits later\nplot_age_ranges &lt;- function(vaccine_age_groups, split = \"overall\", ncol = 3) {\n  p &lt;- vaccine_age_groups %&gt;%\n    filter(str_detect(group, \"\\\\d\")) %&gt;%\n    separate(group, into = c(\"age_min\", \"age_max\"),\n             sep = \"-\", fill = \"right\", remove = FALSE) %&gt;%\n    mutate(\n      age_min = readr::parse_number(age_min),\n      # Set the upper range of the age to 100 (arbitrarility)\n      age_max = replace_na(age_max, replace = \"100\") %&gt;% as.numeric(),\n      age_mid = (age_max + age_min) / 2,\n      group = fct_reorder(group, age_mid)\n    ) %&gt;%\n    ggplot(aes(x = date, color = group)) +\n    geom_errorbar(aes(ymin = age_min, ymax = age_max)) +\n    geom_text(\n      data = . %&gt;%\n        slice_min(date) %&gt;%\n        mutate(age_mid = (age_max + age_min) / 2),\n      aes(label = group, y = age_mid),\n      hjust = 1, nudge_x = -3, show.legend = FALSE\n    ) +\n    geom_text(\n      data = . %&gt;%\n        slice_max(date) %&gt;%\n        mutate(age_mid = (age_max + age_min) / 2),\n      aes(label = group, y = age_mid),\n      hjust = 0, nudge_x = 3, show.legend = FALSE\n    ) +\n    expand_limits(x = c(min(vaccine_age_groups$date) - 10,\n                        max(vaccine_age_groups$date) + 10)) +\n    scale_color_viridis_d(end = 0.8) +\n    theme(legend.position = \"none\") +\n    labs(x = \"Date\", y = \"Age\",\n         title = \"Age ranges for weekly vaccination reports, by date\")\n  \n  if (split == \"province\") p + facet_wrap(~province, ncol = ncol)\n  else if (split == \"region\") p + facet_wrap(~hr_uid, ncol = ncol)\n  else {p}\n}\n\nplot_age_ranges(vaccine_age_groups)\n\n\n\n\nUnsurprisingly, the age ranges become more granular overtime, with the exception of 70-79 which was originally split into 70-74 and 75-79.\n\n\nProvince\nAs with the other data, adding /split to the query returns vaccination data by province:\n\ncontent_parsed &lt;- paste0(base_url, \"vaccines/age-groups/split\") %&gt;%\n  httr::GET() %&gt;%\n  content(as = \"parsed\")\n\n\nvaccine_age_groups_province &lt;- bind_rows(content_parsed$data) %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  filter(date &lt;= \"2021-12-28\")\nglimpse(vaccine_age_groups_province)\n\nRows: 1,564\nColumns: 3\n$ date     &lt;date&gt; 2020-12-14, 2020-12-15, 2020-12-16, 2020-12-16, 2020-12-17, ‚Ä¶\n$ data     &lt;chr&gt; \"{\\\"0-4\\\": {\\\"full\\\": 0, \\\"group\\\": \\\"0-4\\\", \\\"partial\\\": 1, ‚Ä¶\n$ province &lt;chr&gt; \"QC\", \"QC\", \"QC\", \"ON\", \"QC\", \"ON\", \"QC\", \"ON\", \"BC\", \"NL\", \"‚Ä¶\n\n\n\nvaccine_age_groups_province &lt;- vaccine_age_groups_province %&gt;%\n  mutate(\n    data = map(\n      data,\n      ~jsonlite::fromJSON(.x) %&gt;% bind_rows(.id = \"group_code\")\n    )\n  ) %&gt;%\n  unnest(data)\nglimpse(vaccine_age_groups_province)\n\nRows: 15,715\nColumns: 7\n$ date       &lt;date&gt; 2020-12-14, 2020-12-14, 2020-12-14, 2020-12-14, 2020-12-14‚Ä¶\n$ group_code &lt;chr&gt; \"0-4\", \"80+\", \"05-11\", \"12-17\", \"18-29\", \"30-39\", \"40-49\", ‚Ä¶\n$ full       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ group      &lt;chr&gt; \"0-4\", \"80+\", \"05-11\", \"12-17\", \"18-29\", \"30-39\", \"40-49\", ‚Ä¶\n$ partial    &lt;int&gt; 1, 169, 0, 0, 9, 11, 23, 22, 26, 35, 1, 328, 0, 2, 127, 216‚Ä¶\n$ atleast1   &lt;int&gt; 1, 169, 0, 0, 9, 11, 23, 22, 26, 35, 1, 328, 0, 2, 127, 216‚Ä¶\n$ province   &lt;chr&gt; \"QC\", \"QC\", \"QC\", \"QC\", \"QC\", \"QC\", \"QC\", \"QC\", \"QC\", \"QC\",‚Ä¶\n\n\n\nvaccine_age_groups_province %&gt;%\n  filter(province == \"QC\") %&gt;%\n  plot_age_ranges(split = \"province\", ncol = 1)\n\n\n\n\nA single province can also be obtained by altering the query with vaccines/age-groups/province/{code}:\n\ncontent_parsed &lt;- paste0(base_url, \"vaccines/age-groups/province/ns\") %&gt;%\n  httr::GET() %&gt;%\n  content(as = \"parsed\")\n\n\nvaccine_age_groups_ns &lt;- bind_rows(content_parsed$data) %&gt;%\n  mutate(\n    date = as.Date(date),\n    data = map(data, ~jsonlite::fromJSON(.x) %&gt;% bind_rows(.id = \"group_code\"))\n  ) %&gt;%\n  unnest(data) %&gt;%\n  filter(date &lt;= \"2021-12-28\")\nplot_age_ranges(vaccine_age_groups_ns)\n\n\n\n\n\n\nParameters\nThis query also has the after and before parameters available:\n\ncontent_parsed &lt;- paste0(base_url,\n                         \"vaccines/age-groups/province/ns?after=2021-11-01\") %&gt;%\n  httr::GET() %&gt;%\n  content(as = \"parsed\")\n\n\nglimpse(bind_rows(content_parsed$data))\n\nRows: 13\nColumns: 2\n$ date &lt;chr&gt; \"2021-11-06\", \"2021-11-13\", \"2021-11-20\", \"2021-11-27\", \"2021-12-‚Ä¶\n$ data &lt;chr&gt; \"{\\\"0-4\\\": {\\\"full\\\": 0, \\\"group\\\": \\\"0-4\\\", \\\"partial\\\": 0, \\\"at‚Ä¶\n\n\nA specific age group can also be queried with the group parameter. The value must be passed in URL encoding. For example, the 80+ range:\n\ncontent_parsed &lt;- paste0(base_url,\n                         \"vaccines/age-groups?after=2021-11-01&group=80%2B\") %&gt;%\n  httr::GET() %&gt;%\n  content(as = \"parsed\")\n\n\nbind_rows(content_parsed$data) %&gt;%\n  mutate(\n    date = as.Date(date),\n    data = map(data, ~jsonlite::fromJSON(.x) %&gt;% bind_rows(.id = \"group_code\"))\n  ) %&gt;%\n  unnest(data) %&gt;%\n  filter(date &lt;= \"2021-12-28\") %&gt;%\n  glimpse()\n\nRows: 8\nColumns: 5\n$ date     &lt;date&gt; 2021-11-06, 2021-11-13, 2021-11-20, 2021-11-27, 2021-12-04, ‚Ä¶\n$ full     &lt;int&gt; 1581895, 1585409, 1588815, 1592112, 1630884, 1633436, 1637238‚Ä¶\n$ group    &lt;chr&gt; \"80+\", \"80+\", \"80+\", \"80+\", \"80+\", \"80+\", \"80+\", \"80+\"\n$ partial  &lt;int&gt; 39515, 38628, 37810, 37087, 28833, 28621, 28542, 30717\n$ atleast1 &lt;int&gt; 1621410, 1624037, 1626625, 1629199, 1659717, 1662057, 1665780‚Ä¶\n\n\nThe utils package has a URLencode function for translating the age groups:\n\nvaccine_age_groups %&gt;%\n  distinct(group_code) %&gt;%\n  mutate(group_encoded = utils::URLencode(group_code, reserved = TRUE)) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\ngroup_code\ngroup_encoded\n\n\n\n\n80+\n80%2B\n\n\n0-15\n0-15\n\n\n16-69\n16-69\n\n\n70-74\n70-74\n\n\n75-79\n75-79\n\n\nunknown\nunknown\n\n\nall_ages\nall_ages\n\n\nnot_reported\nnot_reported\n\n\n0-17\n0-17\n\n\n18-69\n18-69\n\n\n18-29\n18-29\n\n\n30-39\n30-39\n\n\n40-49\n40-49\n\n\n50-59\n50-59\n\n\n60-69\n60-69\n\n\n70-79\n70-79\n\n\n0-4\n0-4\n\n\n05-11\n05-11\n\n\n12-17\n12-17"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#provinces",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#provinces",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Provinces",
    "text": "Provinces\nThe API also provides a list of provinces and some population/geographical data:\n\ncontent_parsed &lt;- paste0(base_url, \"provinces\") %&gt;%\n  httr::GET() %&gt;%\n  content(as = \"parsed\")\n\n\nprovinces &lt;- bind_rows(content_parsed)\nglimpse(provinces)\n\nRows: 16\nColumns: 10\n$ id          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\n$ code        &lt;chr&gt; \"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\", \"NL\"‚Ä¶\n$ name        &lt;chr&gt; \"Ontario\", \"Quebec\", \"Nova Scotia\", \"New Brunswick\", \"Mani‚Ä¶\n$ population  &lt;int&gt; 14826276, 8604495, 992055, 789225, 1383765, 5214805, 16431‚Ä¶\n$ area        &lt;int&gt; 917741, 1356128, 53338, 71450, 553556, 925186, 5660, 59167‚Ä¶\n$ gdp         &lt;int&gt; 857384, 439375, 44354, 36966, 72688, 295401, 6994, 80679, ‚Ä¶\n$ geographic  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0\n$ data_status &lt;chr&gt; \"Reported\", \"Reported\", \"No report expected today\", \"Repor‚Ä¶\n$ updated_at  &lt;chr&gt; \"2022-10-22T00:09:07.000000Z\", \"2022-10-27T05:09:04.000000‚Ä¶\n$ density     &lt;dbl&gt; 16.15518540, 6.34489886, 18.59940380, 11.04583625, 2.49977‚Ä¶\n\n\nThe extra elements reported here are not related to any particular province/territory:\n\nprovinces %&gt;% filter(is.na(population)) %&gt;% glimpse()\n\nRows: 3\nColumns: 10\n$ id          &lt;int&gt; 14, 15, 16\n$ code        &lt;chr&gt; \"_RC\", \"FA\", \"NFR\"\n$ name        &lt;chr&gt; \"Repatriated Canadians\", \"Federal Allocation\", \"National F‚Ä¶\n$ population  &lt;int&gt; NA, NA, NA\n$ area        &lt;int&gt; NA, NA, NA\n$ gdp         &lt;int&gt; NA, NA, NA\n$ geographic  &lt;int&gt; 0, 0, 0\n$ data_status &lt;chr&gt; \"\", \"\", \"\"\n$ updated_at  &lt;chr&gt; NA, \"2022-03-10T20:41:40.000000Z\", \"2022-08-15T22:51:36.00‚Ä¶\n$ density     &lt;dbl&gt; NA, NA, NA\n\n\nThe geo_only parameter can be set to true to exclude these:\n\npaste0(base_url, \"provinces?geo_only=true\") %&gt;%\n  httr::GET() %&gt;%\n  content(as = \"parsed\") %&gt;%\n  bind_rows() %&gt;%\n  glimpse()\n\nRows: 13\nColumns: 10\n$ id          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n$ code        &lt;chr&gt; \"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\", \"NL\"‚Ä¶\n$ name        &lt;chr&gt; \"Ontario\", \"Quebec\", \"Nova Scotia\", \"New Brunswick\", \"Mani‚Ä¶\n$ population  &lt;int&gt; 14826276, 8604495, 992055, 789225, 1383765, 5214805, 16431‚Ä¶\n$ area        &lt;int&gt; 917741, 1356128, 53338, 71450, 553556, 925186, 5660, 59167‚Ä¶\n$ gdp         &lt;int&gt; 857384, 439375, 44354, 36966, 72688, 295401, 6994, 80679, ‚Ä¶\n$ geographic  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ data_status &lt;chr&gt; \"Reported\", \"Reported\", \"No report expected today\", \"Repor‚Ä¶\n$ updated_at  &lt;chr&gt; \"2022-10-22T00:09:07.000000Z\", \"2022-10-27T05:09:04.000000‚Ä¶\n$ density     &lt;dbl&gt; 16.15518540, 6.34489886, 18.59940380, 11.04583625, 2.49977‚Ä¶\n\n\nA helpful variable is data_status, which indicates if the daily numbers have been reported:\n\nprovinces %&gt;%\n  select(name, data_status, updated_at) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nname\ndata_status\nupdated_at\n\n\n\n\nOntario\nReported\n2022-10-22T00:09:07.000000Z\n\n\nQuebec\nReported\n2022-10-27T05:09:04.000000Z\n\n\nNova Scotia\nNo report expected today\n2022-08-15T22:52:42.000000Z\n\n\nNew Brunswick\nReported\n2022-08-15T22:50:10.000000Z\n\n\nManitoba\nNo report expected today\n2022-10-21T04:17:34.000000Z\n\n\nBritish Columbia\nReported\n2022-10-21T04:50:19.000000Z\n\n\nPrince Edward Island\nReported\n2022-08-15T22:50:27.000000Z\n\n\nSaskatchewan\nNo report expected today\n2022-10-21T16:51:04.000000Z\n\n\nAlberta\nReported\n2022-10-27T05:18:23.000000Z\n\n\nNewfoundland and Labrador\nNo report expected today\n2022-08-15T22:50:18.000000Z\n\n\nNorthwest Territories\nNo report expected today\n2022-08-15T22:50:34.000000Z\n\n\nYukon\nReported\n2022-08-15T22:51:21.000000Z\n\n\nNunavut\nNo report expected today\n2022-08-15T22:51:02.000000Z\n\n\nRepatriated Canadians\n\nNA\n\n\nFederal Allocation\n\n2022-03-10T20:41:40.000000Z\n\n\nNational Federal Reserve\n\n2022-08-15T22:51:36.000000Z\n\n\n\n\n\n\n\ndata_status may take on the following values:\n\n\n\n\n\n\n\n\n\n\ndata_status\nMeaning\n\n\n\n\n\n\n\nWaiting for report\nThis status indicated that an update is expected to happen in the current day, but has not yet occurred.\n\n\n\n\n\nIn progress\nThis status indicates that an update is in-progress and will be completed soon. Note that when this status is indicated, some or all data may not be updated yet.\n\n\n\n\n\nReported\nWhen this status is indicated, the province has been updated with final data for the day, and the update is complete.\n\n\n\n\n\nNo report expected today\nWhen this status is indicated, the province is not expected to provide an update on the current day, and one should not be expected.\n\n\n\n\n\nCustom\nCustom statuses are used to communicate certain issues with a province‚Äôs update including delays or partial updates.\n\n\n\n\n\n\nThe density variable is population density, which is computed by dividing population by area:\n\nprovinces %&gt;%\n  transmute(name, population, area, density,\n            density_manual = population / area) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nname\npopulation\narea\ndensity\ndensity_manual\n\n\n\n\nOntario\n14826276\n917741\n16.15518540\n16.15518540\n\n\nQuebec\n8604495\n1356128\n6.34489886\n6.34489886\n\n\nNova Scotia\n992055\n53338\n18.59940380\n18.59940380\n\n\nNew Brunswick\n789225\n71450\n11.04583625\n11.04583625\n\n\nManitoba\n1383765\n553556\n2.49977419\n2.49977419\n\n\nBritish Columbia\n5214805\n925186\n5.63649363\n5.63649363\n\n\nPrince Edward Island\n164318\n5660\n29.03144876\n29.03144876\n\n\nSaskatchewan\n1179844\n591670\n1.99409130\n1.99409130\n\n\nAlberta\n4442879\n642317\n6.91695689\n6.91695689\n\n\nNewfoundland and Labrador\n520553\n373872\n1.39232946\n1.39232946\n\n\nNorthwest Territories\n45504\n1183085\n0.03846216\n0.03846216\n\n\nYukon\n42986\n474391\n0.09061302\n0.09061302\n\n\nNunavut\n39403\n1936113\n0.02035160\n0.02035160\n\n\nRepatriated Canadians\nNA\nNA\nNA\nNA\n\n\nFederal Allocation\nNA\nNA\nNA\nNA\n\n\nNational Federal Reserve\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#next-steps",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#next-steps",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Next steps",
    "text": "Next steps\nI‚Äôm impressed by the organization and accessibility of this API, and decided to write a simple R package to wrap it. In my next post, I‚Äôll detail my steps and thought process."
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#reproducibility",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/index.html#reproducibility",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(ggtext)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#tldr",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#tldr",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "TL;DR",
    "text": "TL;DR\nTo see the finished product, check out the package website and the source code. The package was published on CRAN recently, and can be downloaded with:\n\ninstall.packages(\"canadacovid\")"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#introduction",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#introduction",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Introduction",
    "text": "Introduction\nIn my previous post, I explored the Canadian COVID-19 tracker API and decided to make an API wrapper package to facilitate using it in R."
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#naming",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#naming",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Naming",
    "text": "Naming\nThe first, and some would say the hardest step for writing a package (or any piece of code for that matter) is naming it. I considered a lot of options, and in the end decided on the admittedly boring canadacovid, which I verified was available:\n\navailable::available(\"canadacovid\", browse = FALSE)"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#general-steps",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#general-steps",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "General steps",
    "text": "General steps\nWith the usethis and devtools packages, getting an R package started with best practices is very easy.\nFirst, I create the package, set it up on GitHub, and write some minor documentation:\n\nusethis::create_tidy_package(\"canadacovid\"): creates the RStudio project and opens it.\nConnect GitHub.\n\nusethis::use_git() creates the local repo.\nusethis::use_github() creates the repo on GitHub and makes an initial commit and push.\nusethis::use_tidy_github() adds some files to follow tidyverse package conventions, e.g.¬†a template contributing.md.\nusethis::use_github_actions() configures a basic R CMD check workflow on GitHub Actions.1\nusethis::use_tidy_github_actions() sets up additional workflows for GitHub Actions.\n\nAs a tidyverse enthusiast, I always setup my packages to use magittr‚Äôs pipe operator %&gt;% with usethis::use_pipe.\nusethis::use_testthat() sets up the testthat package and directory structure.\n\nThis step actually isn‚Äôt necessary, as it was already done by create_tidy_package.\n\nUpdate the DESCRIPTION file, particularly the Title, Description and Authors fields.\nRun devtools::document() which will update the NAMESPACE file with the magittr pipe.\n\nImmediately after pushing these setup steps to GitHub, GitHub Actions got to work but failed on two of three workflows:\n\nBoth the R CMD and the test coverage workflows failed because I haven‚Äôt written any tests yet (or functions to be tested for that matter). Which brings me to the main package development process:\n\nusethis::use_r(\"file-name\") creates the R/file-name.R file.\nAdd one or more R functions to file-name.R.\nusethis::use_package(\"dependency\") to declare any package dependencies, updating the DESCRIPTION file.\nDocument the function(s):\n\nIn RStudio, select Code -&gt; Insert Roxygen Skeleton (or Ctrl+Alt+Shift+R on Windows).\nRun devtools::document() to generate the .Rd file and update NAMESPACE.\nOptional: check the documentation with ?func-name.\nOptional: if using examples, check them with devtools::check_examples().\n\nTry it out.\n\ndevtools::load_all() to load all package functions (Ctrl+Shift+L on Windows).\ndevtools::check() runs R CMD check on the package.\n\nWrite tests with testthat.\n\nuse_test(\"file-name\") creates the tests/testthat/test-file-name.R file, paired to R/file-name.R file.\ndevtools::test() or usethis::test_package() runs all tests.\ndevtools::test_active_file() tests just the active file.\n\nOptional: update the README.Rmd with new functionality.\n\ndevtools::build_readme() to knit.\n\nOptional: write or update a vignette to incorporate new functionality.\n\nusethis::use_vignette(\"vignette-name\") to initialize the vignette file.\ndevtools::build_vignettes() to knit the vignettes."
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#summary",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#summary",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Summary",
    "text": "Summary\nThe first function I will add retrieves the latest summary data from the API. I‚Äôll call this file summary.R, and create it (and it‚Äôs associated test file) with:\n\nusethis::use_r(\"summary\")\nusethis::use_test(\"summary\")\n\nAnd here is the get_summary function added to R/summary.R:\n\nget_summary &lt;- function(split = c(\"overall\", \"province\", \"region\")) {\n  split &lt;- match.arg(split)\n  base_url &lt;- \"https://api.covid19tracker.ca/summary\"\n  split_path &lt;- switch(split,\n                       overall = \"\", province = \"/split\", region = \"/split/hr\")\n  url &lt;- paste0(base_url, split_path)\n\n  resp &lt;- httr::GET(url)\n\n  if (httr::http_error(resp)) {\n    stop(paste(\"API requested failed with code\", httr::status_code(resp)),\n         call. = FALSE)\n  }\n\n  if (httr::http_type(resp) != \"application/json\") {\n    stop(\"API did not return JSON\", call. = FALSE)\n  }\n\n  content_parsed &lt;-\n    jsonlite::fromJSON(httr::content(resp, \"text\", encoding = \"UTF-8\"),\n                       simplifyVector = FALSE)\n\n  dplyr::bind_rows(content_parsed$data) %&gt;%\n    dplyr::bind_cols(content_parsed[\"last_updated\"]) %&gt;%\n    dplyr::mutate(\n      dplyr::across(tidyselect::matches(\"^change|total\"), as.integer),\n      dplyr::across(tidyselect::matches(\"date\"), as.Date),\n      last_updated = as.POSIXct(.data$last_updated)\n    )\n}\n\nSome notes:\n\nSee my previous post for a more thorough explanation of how this code is interacting with the API and processing the data.\nFor all of the imported functions, it is good practice to explicitly state the package, e.g.¬†httr::GET.\n\nThe exception is the very last line where I use rlang to refer to the variable .data$last_updated. I find it clunky to include package names within pipe operations like that.\n\nmatch.arg is a helpful base R function which matches the split parameter to just one of the given values. It returns an error if an unexpected value is provided.\nI followed the advice from this httr vignette and turned API errors into R errors.\n\nThe httr::http_error conditional returns an error message if the GET request failed, with the resulting HTTP status code.\nThe httr::http_type conditional returns an error message if the content is not in JSON format as expected.\n\nInstead of using the as = \"parsed\" argument to httr::content, I parse the raw text directly using jsonlite::fromJSON.\n\nSee this warning from the httr documentation for the reason.\n\n\nAdd the dependencies:\n\nusethis::use_package(\"httr\", type = \"Imports\")\nusethis::use_package(\"jsonlite\", type = \"Imports\")\nusethis::use_package(\"dplyr\", type = \"Imports\")\nusethis::use_package(\"tidyselect\", type = \"Imports\")\nusethis::use_package(\"rlang\", type = \"Imports\")\n\nI‚Äôve explicitly set the type to ‚ÄúImports‚Äù (which wasn‚Äôt necessary as this is the default) to make the point that it is recommended over ‚ÄúDepends‚Äù:\n\nUnless there is a good reason otherwise, you should always list packages in Imports not Depends. That‚Äôs because a good package is self-contained, and minimises changes to the global environment (including the search path).\n\nNext, some Roxygen documentation:\n\n#' Get the most recent summary data\n#'\n#' Runs a GET request of summary data from the COVID-19 tracker API, and\n#' returns parsed data.\n#' Via the `split` argument, data my be \"overall\" (all provinces/territories\n#' combined), by \"province\" (one row per province/territory) or by \"region\"\n#' (one row per health region).\n#'\n#' @param split One of \"overall\", \"province\", or \"region\" to specify how the\n#'   data is split.\n#'\n#' @return A data frame containing the summary data.\n#' @export\n#'\n#' @examples\n#'\n#' get_summary()\n#' get_summary(\"province\")\n#' get_summary(\"region\")\n#'\n#' @importFrom httr GET http_error http_type content\n#' @importFrom jsonlite fromJSON\n#' @importFrom dplyr bind_rows bind_cols mutate across\n#' @importFrom tidyselect matches\n#' @importFrom rlang .data\nget_summary &lt;- function(split = c(\"overall\", \"province\", \"region\")) {\n  ...\n\nNow I‚Äôll edit the test-summary.R file with some simple tests (and run it here as an example):\n\nlibrary(testthat)\n\n\ntest_that(\"get_summary works\", {\n  expect_error(get_summary(split = \"provice\"), \"arg\")\n\n  summary_overall &lt;- get_summary()\n  expect_equal(nrow(summary_overall), 1)\n  expect_equal(ncol(summary_overall), 24)\n  expect_false(any(is.na(summary_overall)))\n\n  summary_province &lt;- get_summary(split = \"province\")\n  expect_equal(nrow(summary_province), 13)\n  expect_equal(ncol(summary_province), 25)\n  expect_false(any(is.na(summary_province)))\n  expect_setequal(summary_province$province,\n                  c(\"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\",\n                    \"NL\", \"NT\", \"YT\", \"NU\"))\n\n  summary_region &lt;- get_summary(split = \"region\")\n  expect_equal(nrow(summary_region), 92)\n})\n\nTest passed ü•≥\n\n\nEverything passed successfully. Here is what was tested, from top to bottom:\n\nAn error was returned for a misspelled split argument.\nsummary_overall has the expected number of rows, columns and no values are NA.\nsummary_province has the expected number of rows, columns and no values are NA. Also the expected 13 provinces/territories are returned.\nsummary_region has the expected number of rows. The number of columns will vary, because columns with all missing (NULL) values will be dropped.\n\nNow with a documented and tested function, I do the following:\n\ndevtools::check() to run a R CMD check (also runs the testthat tests), which passes.\nUpdate the README.Rmd with installation instructions, and an example of using get_summary(). Then build with devtools::build_readme().\n\nI pushed this first iteration of the package to GitHub and, after about 10 minutes, all of the GitHub Actions workflows were successful:\n\n\nDetour: refactoring code (already)\nBefore continuing with other functions, it occurred to me that I would end up re-writing this particular block of code multiple times:\n\nresp &lt;- httr::GET(url)\n\nif (httr::http_error(resp)) {\n  stop(paste(\"API requested failed with code\", httr::status_code(resp)),\n       call. = FALSE)\n}\n\nif (httr::http_type(resp) != \"application/json\") {\n  stop(\"API did not return JSON\", call. = FALSE)\n}\n\ncontent_parsed &lt;-\n  jsonlite::fromJSON(httr::content(resp, \"text\", encoding = \"UTF-8\"),\n                     simplifyVector = FALSE)\n\nA quote from R for Data Science:\n\nYou should consider writing a function whenever you‚Äôve copied and pasted a block of code more than twice (i.e.¬†you now have three copies of the same code).\n\nSo instead of copying and pasting this code block, I made a new function in a new file, api.R:\n\nusethis::use_r(\"api\")\nusethis::use_test(\"api\")\n\n\n#' Get content and parse it\n#'\n#' Sends a GET request to https://api.covid19tracker.ca/.\n#' If the request is successful and the returned content is JSON, formats it and\n#' returns it parsed (via `jsonlite::fromJSON`).\n#'\n#' @param url A string URL linking to the API. If it does not contain the base\n#'   \"https://api.covid19tracker.ca\", then `url` will be combined with the base\n#'   to attempt to make a valid URL (and return a warning).\n#'\n#' @return A list.\n#' @export\n#'\n#' @examples\n#'\n#' get_content_parsed(\"https://api.covid19tracker.ca/provinces\")\n#'\n#' @importFrom httr GET http_error http_type content\n#' @importFrom jsonlite fromJSON\n#' @importFrom stringr str_detect\nget_content_parsed &lt;- function(url) {\n  base_url &lt;- \"https://api.covid19tracker.ca\"\n  if (!stringr::str_detect(url, base_url)) {\n    url &lt;- paste0(base_url, \"/\", url)\n    warning(\n      paste0(\"Provided URL did not include base (\", base_url, \").\\n\",\n             \"Combined URL with base for GET request: \", url)\n    )\n  }\n\n  resp &lt;- httr::GET(url)\n\n  if (httr::http_error(resp)) {\n    stop(paste(\"API requested failed with code\", httr::status_code(resp)),\n         call. = FALSE)\n  }\n\n  if (httr::http_type(resp) != \"application/json\") {\n    stop(\"API did not return JSON\", call. = FALSE)\n  }\n\n  jsonlite::fromJSON(httr::content(resp, \"text\", encoding = \"UTF-8\"),\n                     simplifyVector = FALSE)\n}\n\nThis function expects a full url, but will add ‚Äúhttps://api.covid19tracker.ca‚Äù if it is missing (and return a warning to say so). Some simple tests for get_content_parsed:\n\ntest_that(\"get_content_parsed works\", {\n  expect_warning(get_content_parsed(\"provinces\"), \"base\")\n  expect_error(\n    get_content_parsed(\"https://api.covid19tracker.ca/provices\"), \"API\"\n  )\n\n  provinces &lt;- get_content_parsed(\"https://api.covid19tracker.ca/provinces\")\n  expect_true(is.list(provinces))\n  expect_equal(lengths(provinces), rep(12, 16))\n})\n\nTest passed ü•≥\n\n\nEverything checks out, so I replace some of the code in get_summary with a call to get_content_parsed.\nI don‚Äôt need to re-write tests for get_summary because it is functionally the same, but I do re-run those tests to make sure I didn‚Äôt break it with these changes (this is the whole point of unit testing). I then run devtools::check() and push changes to GitHub."
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#reports",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#reports",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Reports",
    "text": "Reports\nWriting the function to get reports follows the same procedure:\n\nusethis::use_r(\"reports\")\nusethis::use_test(\"reports\")\n\nThe reports are a bit more complicated because the queries can accept a few different parameters. Here is my first draft of get_reports, along with Roxygen documentation:\n\n#' Get the day-to-day reports\n#'\n#' Runs a GET request of reports data from the COVID-19 tracker API, and\n#' returns parsed data.\n#' Via the `split` argument, data may be \"overall\" (all provinces/territories\n#' combined), or by \"province\".\n#' Alternatively, provide one or more two-letter codes (e.g. \"AB\") to `province`\n#' to return reports for specific provinces, or one or more numeric `region`\n#' codes (e.g. \"1204\") to return specific health regions.\n#'\n#' @param split One of \"overall\", or \"province\" to specify how the\n#'   data is split. An \"overall\" report gives cumulative numbers across Canada.\n#'   Splitting by \"province\" returns all reports for all provinces/territories.\n#' @param province One or more province/territory codes (\"AB\", \"BC\", \"MB\", \"NB\",\n#'   \"NL\", \"NS\", \"NT\", \"NU\", \"ON\", \"PE\", \"QC\", \"SK\", \"YT\") to get reports.\n#'   Upper, lower and mixed case strings are accepted.\n#' @param region One or more health region IDs to get reports. Numeric and\n#'   character values are accepted.\n#' @param fill_dates When TRUE, the response fills in any missing dates with\n#'   blank entries.\n#' @param stat Returns only the specified statistics, e.g. \"cases\".\n#' @param date Returns reports from only the specified date.\n#' @param after Returns reports from only on or after the specified date.\n#' @param before Returns reports from only on or before the specified date.\n#'\n#' @return A data frame containing the reports data, one row per day. Includes\n#'   a `province` variable if data is split by province, and a `hr_uid` variable\n#'   if data is split by health region.\n#' @export\n#'\n#' @examples\n#'\n#' get_reports()\n#' get_reports(\"province\")\n#' get_reports(province = c(\"AB\", \"SK\"))\n#' get_reports(region = 1204)\n#' get_reports(region = c(\"472\", 1204), stat = \"cases\")\n#' @importFrom dplyr bind_rows bind_cols mutate across\n#' @importFrom tidyselect matches\n#' @importFrom rlang .data\n#' @importFrom purrr imap_chr map_dfr\n#' @importFrom tibble lst\nget_reports &lt;- function(split = c(\"overall\", \"province\"),\n                        province = NULL, region = NULL,\n                        fill_dates = NULL, stat = NULL, date = NULL,\n                        after = NULL, before = NULL) {\n  base_url &lt;- \"https://api.covid19tracker.ca/reports/\"\n  province_codes &lt;- c(\n    \"AB\", \"BC\", \"MB\", \"NB\", \"NL\", \"NS\", \"NT\", \"NU\", \"ON\",\n    \"PE\", \"QC\", \"SK\", \"YT\"\n  )\n\n  split &lt;- match.arg(split)\n  if (split == \"province\") province &lt;- province_codes\n\n  parameters &lt;- tibble::lst(fill_dates, stat, date, after, before)\n  # Remove NULL parameters\n  parameters &lt;- parameters[lengths(parameters) == 1]\n  if (length(parameters) &gt; 0) {\n    params_url &lt;- purrr::imap_chr(parameters, ~ paste0(.y, \"=\", tolower(.x))) %&gt;%\n      paste(collapse = \"&\")\n    params_url &lt;- paste0(\"?\", params_url)\n  } else {\n    params_url &lt;- \"\"\n  }\n\n  if (!is.null(province)) {\n    province &lt;- match.arg(toupper(province), province_codes, several.ok = TRUE)\n\n    reports &lt;- purrr::map_dfr(\n      province,\n      function(province) {\n        url &lt;- paste0(base_url, \"province/\", province, params_url)\n        content_parsed &lt;- get_content_parsed(url)\n\n        dplyr::bind_cols(\n          content_parsed[c(\"province\", \"last_updated\")],\n          dplyr::bind_rows(content_parsed$data)\n        )\n      }\n    )\n  } else if (!is.null(region)) {\n    reports &lt;- purrr::map_dfr(\n      region,\n      function(region) {\n        url &lt;- paste0(base_url, \"regions/\", region, params_url)\n        content_parsed &lt;- get_content_parsed(url)\n\n        dplyr::bind_cols(\n          content_parsed[c(\"hr_uid\", \"last_updated\")],\n          dplyr::bind_rows(content_parsed$data)\n        )\n      }\n    )\n  } else {\n    content_parsed &lt;- get_content_parsed(paste0(base_url, params_url))\n    reports &lt;- dplyr::bind_cols(\n      content_parsed[\"last_updated\"],\n      dplyr::bind_rows(content_parsed$data)\n    )\n  }\n\n  reports %&gt;%\n    dplyr::mutate(\n      dplyr::across(tidyselect::matches(\"^change|total\"), as.integer),\n      dplyr::across(tidyselect::matches(\"date\"), as.Date),\n      last_updated = as.POSIXct(.data$last_updated)\n    )\n}\n\n\nAnd the new dependencies:\n\nusethis::use_package(\"tibble\")\nusethis::use_package(\"purrr\")\n\n\nThe get_content_parsed function paid off already ‚Äì I used it three times above, saving ~20 lines of code each time.\nNote that I didn‚Äôt allow split = \"region\" as an option because, if the function queries all regions, 92 separate GET requests are sent to the API in a short period. This eventually fails with HTTP code 429, indicating too many requests. This is called ‚Äúrate limiting‚Äù, and the error code is a polite way of asking someone to stop spamming requests. I might alter this function in the future to send these requests slower, either with a call to Sys.sleep() or with the polite package.\nget_reports is a more complex function than get_summary, and so I wrote more tests to try to capture that complexity:\n\ntest_that(\"get_reports works\", {\n  reports_overall &lt;- get_reports()\n  expect_equal(ncol(reports_overall), 24)\n  expect_false(any(is.na(reports_overall)))\n\n  reports_province &lt;- get_reports(split = \"province\")\n  expect_equal(dplyr::n_distinct(reports_province$province), 13)\n  expect_equal(min(reports_province$date), min(reports_overall$date))\n  expect_equal(max(reports_province$date), max(reports_overall$date))\n\n  reports_ns_nb_nv &lt;- get_reports(province = c(\"NS\", \"nb\", \"nU\", \"test\"))\n  expect_equal(\n    unique(reports_ns_nb_nv$province), c(\"NS\", \"NB\", \"NU\")\n  )\n  expect_equal(\n    reports_province %&gt;% dplyr::filter(province == \"NS\"),\n    reports_ns_nb_nv %&gt;% dplyr::filter(province == \"NS\")\n  )\n\n  expect_error(get_reports(split = \"region\"), \"arg\")\n\n  reports_592_2407_3561 &lt;- get_reports(region = c(592, \"2407\", 3561))\n  expect_equal(\n    unique(reports_592_2407_3561$hr_uid),\n    c(592, 2407, 3561)\n  )\n\n  reports_criticals &lt;- get_reports(split = \"province\", stat = \"criticals\")\n  expect_equal(ncol(reports_criticals), 5)\n  expect_setequal(names(reports_criticals),\n                  c(\"province\", \"last_updated\", \"date\",\n                    \"change_criticals\", \"total_criticals\"))\n\n  report_2021_07_20 &lt;- get_reports(province = \"MB\", date = \"2021-07-20\")\n  expect_equal(report_2021_07_20$date, as.Date(\"2021-07-20\"))\n  expect_equal(report_2021_07_20$province, \"MB\")\n\n  report_date_range &lt;- get_reports(region = 3570,\n                                   after = \"2021-10-28\", before = \"2021-11-02\")\n  expect_equal(min(report_date_range$date), as.Date(\"2021-10-28\"))\n  expect_equal(max(report_date_range$date), as.Date(\"2021-11-02\"))\n})\n\nTest passed üòÄ\n\n\n\nDetour: rate limiting\nPushing the reports file and tests to GitHub resulted in failed R CMD checks due to API errors with the HTTP code 429. This is because all my testthat tests were running on 9 different platforms and flooding the API with too many requests.\nThere are more sophisticated ways to limit the request rate (e.g.¬†using the re-written httr2 package, the vcr package, the httptest package) but I decided to use the humble Sys.sleep(). To control the delay time across all tests, I added a setup file at tests/testthat/setup.R with a very simple function:\n\nrequest_sleep &lt;- function(seconds = 10) {\n  Sys.sleep(seconds)\n}\n\nI then called this function before every GET request in my tests so that there was at least a 10 second delay between each (I initially tried 5 seconds, but it was still too fast)."
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#vaccination-data",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#vaccination-data",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Vaccination data",
    "text": "Vaccination data\nThe vaccination data is organized a bit differently ‚Äì see the API documentation or my exploration of it.\n\nSummary and reports\nThe summary and reports vaccination data is simply the same summary and reports from before, but with only the statistics related to vaccination: vaccinations, vaccinated, boosters_1, and vaccines_distributed. This function simply wraps my previous get_summary() and get_reports() functions, and returns the relevant variables:\n\n\nget_vaccination_data() documentation\n#' Get vaccination data\n#'\n#' Runs a GET request of vaccination data from the COVID-19 tracker API, and\n#' returns parsed data.\n#' Data may be returned as `type` = \"summary\" (the most recent data) or\n#' `type` = \"reports\" (day-to-day reports).\n#' Via the `split` argument, data may be \"overall\" (all provinces/territories\n#' combined), by \"province\", or by \"region\".\n#' Alternatively, provide one or more two-letter codes (e.g. \"AB\") to `province`\n#' to return reports for specific provinces, or one or more numeric `region`\n#' codes (e.g. \"1204\") to return specific health regions.\n#'\n#' @param type One of \"summary\" (most recent data) or \"reports\" (day-to-day\n#'   data).\n#' @param split One of \"overall\", \"province\", or \"region\" to specify how the\n#'   data is split. An \"overall\" summary or report gives cumulative numbers\n#'   across Canada. Splitting by \"province\" returns data for all\n#'   provinces/territories. Splitting by \"region\" is only available for\n#'   \"summary\" data, and returns data for all health regions.\n#' @param province One or more province/territory codes (\"AB\", \"BC\", \"MB\", \"NB\",\n#'   \"NL\", \"NS\", \"NT\", \"NU\", \"ON\", \"PE\", \"QC\", \"SK\", \"YT\") to get reports.\n#'   Upper, lower and mixed case strings are accepted.\n#' @param region One or more health region IDs to get reports. Numeric and\n#'   character values are accepted.\n#' @param fill_dates When TRUE, the response fills in any missing dates with\n#'   blank entries.\n#' @param stat Returns only the specified statistics, e.g. \"cases\".\n#' @param date Returns reports from only the specified date.\n#' @param after Returns reports from only on or after the specified date.\n#' @param before Returns reports from only on or before the specified date.\n#'\n#' @return A data frame containing the vaccinations data. Includes\n#'   a `province` variable if data is split by province, and a `hr_uid` variable\n#'   if data is split by health region.\n#' @export\n#'\n#' @examples\n#'\n#' get_vaccination_data()\n#' get_vaccination_data(split = \"province\")\n#' get_vaccination_data(type = \"reports\", split = \"overall\")\n#' get_vaccination_data(type = \"reports\", split = \"overall\",\n#'                      date = \"2021-12-25\")\n#' @importFrom dplyr select\n#' @importFrom tidyselect matches\n\n\n\nget_vaccination_data &lt;- function(type = c(\"summary\", \"reports\"),\n                                 split = c(\"overall\", \"province\", \"region\"),\n                                 province = NULL, region = NULL,\n                                 fill_dates = NULL, stat = NULL, date = NULL,\n                                 after = NULL, before = NULL) {\n  type &lt;- match.arg(type)\n  split &lt;- match.arg(split)\n\n  if (type == \"summary\") {\n    vaccination_data &lt;- get_summary(split)\n  } else {\n    # Getting reports for each region sends too many requests to the API\n    if (split == \"region\") {\n      stop(paste(\n        \"For `type` = 'reports', only \",\n        \"`split` = 'overall' and 'province' are available.\"\n      ))\n    }\n\n    vaccination_data &lt;- get_reports(\n      split, province, region, fill_dates, stat,\n      date, after, before\n    )\n  }\n\n  vaccination_data %&gt;%\n    dplyr::select(\n      tidyselect::matches(\"province|hr_uid\"),\n      tidyselect::matches(\"date|last_updated\"),\n      tidyselect::matches(\"vacc|boost\")\n    )\n}\n\nBecause this function can return both summaries and reports, I have to put in a stop() condition if reports by health region is requested, as this is not allowed by the API.\nThe tests for get_vaccination_data make use of the new request_sleep function:\n\ntest_that(\"get_vaccination_data works\", {\n  request_sleep()\n  vaccination_data_summary &lt;- get_vaccination_data()\n  expect_equal(nrow(vaccination_data_summary), 1)\n  expect_equal(ncol(vaccination_data_summary), 12)\n\n  request_sleep()\n  vaccination_data_report &lt;- get_vaccination_data(type = \"reports\")\n  expect_equal(ncol(vaccination_data_report), 12)\n  expect_false(any(is.na(vaccination_data_report)))\n\n  request_sleep()\n  vaccination_data_summary_region &lt;- get_vaccination_data(split = \"region\")\n  expect_equal(nrow(vaccination_data_summary_region), 92)\n  expect_error(get_vaccination_data(type = \"reports\", split = \"region\"))\n\n  request_sleep()\n  vaccination_data_ns_pe &lt;- get_vaccination_data(type = \"reports\",\n                                                 province = c(\"NS\", \"pe\"))\n  expect_equal(unique(vaccination_data_ns_pe$province), c(\"NS\", \"PE\"))\n\n  request_sleep()\n  expect_equal(nrow(vaccination_data_ns_pe),\n               nrow(get_reports(province = c(\"NS\", \"pe\"))))\n})\n\nTest passed üåà\n\n\n\n\nSub-regions\nIn some provinces/territories, there is additional vaccination data at the sub-region level, which requires unique requests from the API:\n\n\nget_subregion_vaccination_data() documentation\n#' Get sub-region vaccination data\n#'\n#' Runs a GET request of sub-region vaccination data from the COVID-19 tracker\n#' API, and returns parsed data.\n#' The `dates` argument specifies the time frame of the data: \"current\"\n#' (the default; latest report for each sub-region), \"recent\"\n#' (15 most recent reports for each sub-region), and \"all\" (returns all reports\n#' for one or more sub-regions specified by the `subregion_code` argument).\n#' To get a list of available sub-regions, use the function `get_subregions()`.\n#'\n#' Note that sub-region vaccination data is only for select provinces and\n#' territories. Also the percentages reported differ between percent of total\n#' population, and percent of eligible population.\n#' See the API documentation for more details:\n#' https://api.covid19tracker.ca/docs/1.0/vaccinations.\n#'\n#' @param dates One of \"current\", \"recent\", or \"all\" to specify the time frame\n#'   of the reports returned. If choosing \"all\" reports, must also provide one\n#'   or more sub-region codes.\n#' @param subregion_code One or more sub-region codes. Returns all reports for\n#'   those sub-regions (even if `dates` is not \"all\")\n#'\n#' @return A data frame with one row per sub-region report.\n#' @export\n#'\n#' @examples\n#'\n#' get_subregion_vaccination_data()\n#' get_subregion_vaccination_data(\"recent\")\n#' get_subregion_vaccination_data(\"all\", subregion_code = c(\"ON382\", \"SK007\"))\n#' @importFrom dplyr bind_cols bind_rows mutate\n#' @importFrom purrr map_dfr\n#' @importFrom tidyselect matches\n\n\n\nget_subregion_vaccination_data &lt;- function(dates = c(\"current\", \"recent\", \"all\"),\n                                           subregion_code = NULL) {\n  dates &lt;- match.arg(dates)\n  base_url &lt;- \"https://api.covid19tracker.ca/reports/sub-regions/\"\n  dates_path &lt;- switch(dates,\n    current = \"summary\",\n    recent = \"recent\",\n    all = \"\"\n  )\n  url &lt;- paste0(base_url, dates_path)\n\n  if (is.null(subregion_code)) {\n    if (dates == \"all\") {\n      stop(\"Must specify sub-region(s) to return all vaccination reports.\")\n    }\n    content_parsed &lt;- get_content_parsed(url)\n\n    vaccination_data &lt;- dplyr::bind_cols(\n      content_parsed[\"last_updated\"],\n      dplyr::bind_rows(content_parsed$data)\n    )\n  } else {\n    vaccination_data &lt;- purrr::map_dfr(\n      subregion_code,\n      function(subregion_code) {\n        url &lt;- paste0(url, subregion_code)\n        content_parsed &lt;- get_content_parsed(url)\n\n        dplyr::bind_cols(\n          content_parsed[\"sub_region\"],\n          dplyr::bind_rows(content_parsed$data)\n        )\n      }\n    )\n  }\n\n  vaccination_data %&gt;%\n    dplyr::mutate(\n      dplyr::across(tidyselect::matches(\"^total\"), as.integer),\n      dplyr::across(tidyselect::matches(\"^percent\"), as.numeric),\n      dplyr::across(tidyselect::matches(\"date\"), as.Date),\n      dplyr::across(tidyselect::matches(\"last_updated\"), as.POSIXct)\n    )\n}\n\nAnother stop condition prevents sending too many requests: this time when all subregion data is requested.\n\ntest_that(\"get_subregion_vaccination_data works\", {\n  request_sleep()\n  subregion_vaccination_data_current &lt;- get_subregion_vaccination_data()\n  expect_equal(nrow(subregion_vaccination_data_current), 806)\n  expect_equal(ncol(subregion_vaccination_data_current), 12)\n\n  request_sleep()\n  subregion_vaccination_data_recent &lt;-\n    get_subregion_vaccination_data(dates = \"recent\")\n  expect_true(nrow(subregion_vaccination_data_recent) &gt; 0)\n})\n\nTest passed üòÄ"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#vaccine-age-groups",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#vaccine-age-groups",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Vaccine age groups",
    "text": "Vaccine age groups\nThese data are week-by-week vaccination statistics for various age groups. The request URL is /vaccines/age-groups and takes a few parameters:\n\n\nget_vaccine_age_groups() documentation\n#' Get vaccination reports by age group\n#'\n#' Runs a GET request of vaccination data by age groups from the COVID-19\n#' tracker API, and returns parsed data.\n#' Via the `split` argument, data may be \"overall\" (all provinces/territories\n#' combined), or by \"province\".\n#' Alternatively, provide one or more two-letter codes (e.g. \"AB\") to `province`\n#' to return reports for specific provinces.\n#'\n#' @param split One of \"overall\", or \"province\" to specify how the\n#'   data is split. An \"overall\" report gives cumulative numbers across Canada.\n#'   Splitting by \"province\" returns all reports for all provinces/territories.\n#' @param province One or more province/territory codes (\"AB\", \"BC\", \"MB\", \"NB\",\n#'   \"NL\", \"NS\", \"NT\", \"NU\", \"ON\", \"PE\", \"QC\", \"SK\", \"YT\") to get reports.\n#'   Upper, lower and mixed case strings are accepted.\n#' @param group A specific age group to return, for example: \"0-4\", \"05-11\",\n#'   \"30-39\", \"80+\", \"not_reported\"\n#' @param after Returns reports from only on or after the specified date,\n#'   in YYYY-MM-DD format.\n#' @param before Returns reports from only on or before the specified date,\n#'   in YYYY-MM-DD format.\n#'\n#' @return A data frame with, one row per age group per date. Includes\n#'   a `province` variable if data is split by province.\n#' @export\n#'\n#' @examples\n#'\n#' get_vaccine_age_groups()\n#' get_vaccine_age_groups(split = \"province\")\n#' get_vaccine_age_groups(province = c(\"AB\", \"SK\"))\n#' get_vaccine_age_groups(province = \"NS\", group = \"18-29\")\n#' get_vaccine_age_groups(group = \"80+\", after = \"2021-12-01\")\n#' @importFrom dplyr bind_rows bind_cols mutate across\n#' @importFrom tidyselect matches\n#' @importFrom rlang .data\n#' @importFrom purrr imap_chr map_dfr discard\n#' @importFrom tibble lst\n#' @importFrom jsonlite fromJSON\n#' @importFrom tidyr unnest\n#' @importFrom utils URLencode\n\n\n\nget_vaccine_age_groups &lt;- function(split = c(\"overall\", \"province\"),\n                                   province = NULL,\n                                   group = NULL, before = NULL, after = NULL) {\n  base_url &lt;- \"https://api.covid19tracker.ca/vaccines/age-groups\"\n  province_codes &lt;- c(\n    \"AB\", \"BC\", \"MB\", \"NB\", \"NL\", \"NS\", \"NT\", \"NU\", \"ON\",\n    \"PE\", \"QC\", \"SK\", \"YT\"\n  )\n\n  split &lt;- match.arg(split)\n  if (split == \"province\") {\n    base_url &lt;- paste0(base_url, \"/split\")\n  } else if (!is.null(province)) {\n    province &lt;- match.arg(toupper(province), province_codes, several.ok = TRUE)\n    base_url &lt;- paste0(base_url, \"/province/\", province)\n  }\n\n  parameters &lt;- tibble::lst(group, before, after)\n  # Remove NULL parameters\n  parameters &lt;- parameters[lengths(parameters) == 1]\n  if (length(parameters) &gt; 0) {\n    params_url &lt;- purrr::imap_chr(\n      parameters,\n      ~ paste0(.y, \"=\", utils::URLencode(.x, reserved = TRUE))\n    ) %&gt;%\n      paste(collapse = \"&\")\n    params_url &lt;- paste0(\"?\", params_url)\n  } else {\n    params_url &lt;- \"\"\n  }\n\n  purrr::map_dfr(\n    base_url,\n    function(base_url) {\n      url &lt;- paste0(base_url, params_url)\n      content_parsed &lt;- get_content_parsed(url)\n\n      # Because age ranges can change over time, some data returned is NULL\n      #  if the `group` param is used\n      if (!is.null(group)) {\n        # So discard NULL elements\n        content_parsed$data &lt;- purrr::discard(content_parsed$data,\n                                              ~ is.null(.x$data))\n      }\n\n      if (!is.null(province)) {\n        dplyr::bind_cols(\n          content_parsed[\"province\"],\n          dplyr::bind_rows(content_parsed$data)\n        )\n      } else {\n        dplyr::bind_rows(content_parsed$data)\n      }\n    }\n  ) %&gt;%\n    dplyr::mutate(\n      data = purrr::map(\n        .data$data,\n        ~jsonlite::fromJSON(.x) %&gt;% dplyr::bind_rows(.id = \"group_code\")\n      )\n    ) %&gt;%\n    tidyr::unnest(.data$data) %&gt;%\n    dplyr::mutate(dplyr::across(tidyselect::matches(\"date\"), as.Date))\n}\n\n\nAnd the new dependencies:\n\nusethis::use_package(\"tidyr\")\nusethis::use_package(\"utils\")\n\n\nThere were some tricky aspects to this function, like how the content is JSON within JSON and the URL encoding for the age group parameter (see my previous post). Run some tests:\n\ntest_that(\"get_vaccine_age_groups works\", {\n  request_sleep()\n  vacc_age_overall &lt;- get_vaccine_age_groups()\n  expect_equal(dplyr::n_distinct(vacc_age_overall$group), 19)\n\n  request_sleep()\n  vacc_age_province &lt;- get_vaccine_age_groups(split = \"province\")\n  expect_equal(dplyr::n_distinct(vacc_age_province$province), 13)\n\n  request_sleep()\n  vacc_age_mb_nt &lt;- get_vaccine_age_groups(province = c(\"test\", \"mB\", \"NT\"))\n  expect_setequal(unique(vacc_age_mb_nt$province), c(\"MB\", \"NT\"))\n\n  request_sleep()\n  vacc_age_ns_18_29 &lt;- get_vaccine_age_groups(province = \"NS\", group = \"18-29\")\n  expect_setequal(unique(vacc_age_ns_18_29$province), c(\"NS\"))\n  expect_setequal(unique(vacc_age_ns_18_29$group), c(\"18-29\"))\n\n  request_sleep()\n  vacc_age_80p_date_range &lt;-\n    get_vaccine_age_groups(group = \"80+\",\n                           after = \"2021-03-20\", before = \"2021-05-10\")\n  expect_equal(unique(vacc_age_80p_date_range$group), \"80+\")\n  expect_true(min(vacc_age_80p_date_range$date) &gt;= \"2021-03-20\")\n  expect_true(max(vacc_age_80p_date_range$date) &lt;= \"2021-05-10\")\n\n  request_sleep()\n  vacc_age_not_reported &lt;- get_vaccine_age_groups(group = \"not_reported\")\n  expect_equal(unique(vacc_age_not_reported$group), \"Not reported\")\n\n  request_sleep()\n  expect_error(get_vaccine_age_groups(group = \"90+\"))\n})\n\nTest passed üéâ"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#regions-and-sub-regions",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#regions-and-sub-regions",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Regions and sub-regions",
    "text": "Regions and sub-regions\nHealth regions from get_summary and get_reports are specified by the non-descript hr_uid variable only. Also, the sub-regions from get_subregion_vaccination_data are specified by code only. Naturally, the API provides lists of regions and sub-regions, which I wrap with two new functions in the regions.R file:\n\nusethis::use_r(\"regions\")\nusethis::use_test(\"regions\")\n\n\n#' Get health regions\n#'\n#' Returns a list of health regions in the COVID-19 tracker.\n#' By default (`hr_uid` and `province` `NULL`), returns all 92 regions.\n#'\n#' @param hr_uid One or more health region UIDs (e.g. 3553) to return.\n#' @param province One or more provinces to return.\n#'\n#' @return A data frame with a row per health region.\n#' @export\n#'\n#' @examples\n#'\n#' get_regions()\n#' get_regions(hr_uid = c(\"2414\", 591))\n#' get_regions(province = c(\"ns\", \"SK\"))\n#' @importFrom dplyr bind_rows\n#' @importFrom purrr map_dfr\nget_regions &lt;- function(hr_uid = NULL, province = NULL) {\n  base_url &lt;- \"https://api.covid19tracker.ca/\"\n  if (!is.null(hr_uid)) {\n    url &lt;- paste0(base_url, \"regions/\", hr_uid)\n  } else if (!is.null(province)) {\n    url &lt;- paste0(base_url, \"province/\", province, \"/regions\")\n  } else {\n    url &lt;- paste0(base_url, \"regions\")\n  }\n\n  purrr::map_dfr(\n    url,\n    function(url) {\n      content_parsed &lt;- get_content_parsed(url)\n      if (!is.null(province)) {\n        dplyr::bind_rows(content_parsed)\n      } else {\n        dplyr::bind_rows(content_parsed$data)\n      }\n    }\n  )\n}\n\n#' Get sub-regions\n#'\n#' Returns a list of sub-regions in the COVID-19 tracker.\n#' By default, returns all 805 sub-regions.\n#'\n#' @param subregion_code One or more sub-regions to be returned.\n#'\n#' @return A data frame with a row per sub-region.\n#' @export\n#'\n#' @examples\n#'\n#' get_subregions()\n#' get_subregions(\"AB001\")\n#' get_subregions(c(\"SK003\", \"SK005\"))\n#' @importFrom dplyr bind_rows\n#' @importFrom purrr map_dfr\nget_subregions &lt;- function(subregion_code = NULL) {\n  base_url &lt;- \"https://api.covid19tracker.ca/sub-regions\"\n\n  if (is.null(subregion_code)) {\n    url &lt;- base_url\n  } else {\n    url &lt;- paste0(base_url, \"/\", subregion_code)\n  }\n\n  purrr::map_dfr(\n    url,\n    function(url) {\n      content_parsed &lt;- get_content_parsed(url)\n      dplyr::bind_rows(content_parsed$data)\n    }\n  )\n}\n\nNot very complicated, and neither are the corresponding tests:\n\ntest_that(\"get_regions works\", {\n  request_sleep()\n  regions &lt;- get_regions()\n  expect_true(\"hr_uid\" %in% names(regions))\n  expect_equal(nrow(regions), 92)\n  expect_equal(dplyr::n_distinct(regions$province), 13)\n\n  request_sleep()\n  regions_2418_3534 &lt;- get_regions(hr_uid = c(2418, \"3534\"))\n  expect_equal(regions_2418_3534$hr_uid, c(2418, 3534))\n\n  request_sleep()\n  regions_mb_bc &lt;- get_regions(province = c(\"mb\", \"bC\"))\n  expect_equal(unique(regions_mb_bc$province), c(\"MB\", \"BC\"))\n})\n\nTest passed ü•á\n\ntest_that(\"get_subregions works\", {\n  request_sleep()\n  subregions &lt;- get_subregions()\n  expect_equal(nrow(subregions), 806)\n  expect_equal(dplyr::n_distinct(subregions$province), 6)\n\n  request_sleep()\n  subregions_3 &lt;- get_subregions(c(\"ON322\", \"SK010\", \"MB029\", \"test\"))\n  expect_setequal(unique(subregions_3$province), c(\"ON\", \"SK\", \"MB\"))\n})\n\nTest passed ü•á"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#provinces",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#provinces",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Provinces",
    "text": "Provinces\nThe last function I added to the package is get_provinces, which has some population/geographical data, as well as a data_status variable indicating if a province‚Äôs daily numbers have been updated:\n\n#' Get provinces and territories\n#'\n#' @param geo_only Logical, indicating if only provinces/territories should be\n#'   returned. If FALSE, also returned non-geographic entities like\n#'   Repatriated Canadians and the Federal Allocation for vaccinations.\n#'\n#' @return A data frame with a row per province/territory.\n#' @export\n#'\n#' @examples\n#'\n#' get_provinces()\n#' get_provinces(geo_only = FALSE)\n#' @importFrom dplyr bind_rows mutate\n#' @importFrom rlang .data\nget_provinces &lt;- function(geo_only = TRUE) {\n  base_url &lt;- \"https://api.covid19tracker.ca/provinces\"\n  if (geo_only) {\n    api_params &lt;- \"?geo_only=true\"\n  } else {\n    api_params &lt;- \"\"\n  }\n  url &lt;- paste0(base_url, api_params)\n\n  content_parsed &lt;- get_content_parsed(url)\n\n  dplyr::bind_rows(content_parsed) %&gt;%\n    dplyr::mutate(\n      # Use logical type instead of 0/1\n      geographic = .data$geographic == 1,\n      updated_at = as.POSIXct(.data$updated_at)\n    )\n}\n\n\ntest_that(\"get_provinces\", {\n  request_sleep()\n  provinces &lt;- get_provinces()\n  expect_equal(nrow(provinces), 13)\n  expect_equal(ncol(provinces), 10)\n  expect_setequal(provinces$code,\n                  c(\"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\",\n                    \"NL\", \"NT\", \"YT\", \"NU\"))\n\n  request_sleep()\n  provinces_geo_false &lt;- get_provinces(geo_only = FALSE)\n\n  expect_equal(nrow(provinces_geo_false), 16)\n  expect_equal(ncol(provinces_geo_false), 10)\n  expect_setequal(provinces_geo_false$code,\n                  c(\"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\",\n                    \"NL\", \"NT\", \"YT\", \"NU\",\n                    \"_RC\", \"FA\", \"NFR\"))\n\n})\n\nTest passed üòÄ"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#an-example-use-case",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#an-example-use-case",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "An example use case",
    "text": "An example use case\nNow with the package up-and-running on GitHub, I‚Äôll install it with:\n\nremotes::install_github(\"taylordunn/canadacovid\")\n\nGet cases per day across Canada, and compute a 7-day rolling average:\n\nreports_overall &lt;- canadacovid::get_reports() %&gt;%\n  mutate(\n    date, change_cases,\n    change_cases_rolling_avg = zoo::rollapply(change_cases, 7, mean,\n                                              align = \"right\", fill = NA)\n  )\n\nMake a plot of the cases:\n\ncanada_red &lt;- \"#FF0000\"\ncanada_red_desat &lt;- \"#bf3f3f\"\np_title &lt;- glue::glue(\n  \"COVID-19 cases reported per day in Canada (with \",\n  \"&lt;span style='color:{canada_red}'&gt;7-day rolling average&lt;/span&gt;)\"\n)\n\nreports_overall %&gt;%\n  ggplot(aes(x = date, y = change_cases)) +\n  geom_col(aes(y = change_cases), fill = canada_red, alpha = 0.5, width = 1) +\n  geom_line(aes(y = change_cases_rolling_avg),\n            color = canada_red, size = 2) +\n  scale_y_continuous(expand = c(0, 0), labels = scales::comma) +\n  labs(y = \"Cases\", x = \"Date\", title = p_title) +\n  theme(plot.title = ggtext::element_markdown())"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#hex-sticker",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#hex-sticker",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Hex sticker",
    "text": "Hex sticker\nTo represent the package, I‚Äôll quickly put together a hex sticker based on the above plot. I‚Äôm no artist, so I‚Äôll use the hexSticker package to make it.\nFirst iteration of the plot:\n\n# Use data from before the omicron peak, which (sadly) extends the y axis a lot\nd &lt;- reports_overall %&gt;%\n  filter(date &lt; \"2021-12-01\", !is.na(change_cases_rolling_avg))\np &lt;- d %&gt;%\n  ggplot(aes(x = date, y = change_cases_rolling_avg)) +\n  geom_ribbon(aes(ymin = 0, ymax = change_cases_rolling_avg),\n              fill = canada_red, alpha = 0.5) +\n  geom_line(color = canada_red, size = 1.5) +\n  dunnr::remove_axis(\"y\") +\n  labs(x = NULL)\np\n\n\n\n\nFor some extra visual interest, I‚Äôll mark the peaks in the plot. My lazy approach for finding those days is to first roughly group the data into waves:\n\nwaves &lt;- as.Date(c(\"2020-03-01\", \"2020-07-01\", \"2021-03-01\",\n                   \"2021-07-15\", \"2021-11-01\"))\np + geom_vline(xintercept = waves)\n\n\n\n\nThen find the maximum case count in each wave:\n\npeaks &lt;- d %&gt;%\n  mutate(wave = cut(date, breaks = waves, labels = paste0(\"wave \", 1:4))) %&gt;%\n  filter(!is.na(wave)) %&gt;%\n  group_by(wave) %&gt;%\n  filter(change_cases_rolling_avg == max(change_cases_rolling_avg)) %&gt;%\n  ungroup() %&gt;%\n  select(wave, date, change_cases, change_cases_rolling_avg)\npeaks\n\n# A tibble: 4 √ó 4\n  wave   date       change_cases change_cases_rolling_avg\n  &lt;fct&gt;  &lt;date&gt;            &lt;int&gt;                    &lt;dbl&gt;\n1 wave 1 2020-05-03         2794                    1795.\n2 wave 2 2021-01-10         8324                    8260.\n3 wave 3 2021-04-17         8866                    8730.\n4 wave 4 2021-09-17         5061                    4445.\n\n\nNow add these as white horizontal lines on the plot:\n\np &lt;- p +\n  geom_vline(xintercept = peaks$date, color = \"white\", size = 1) +\n  # Re-draw the line so that it appears over the white lines\n  geom_line(color = canada_red, size = 1.5)\np\n\n\n\n\nRemove un-needed elements from the plot:\n\np &lt;- p +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme(\n    plot.margin = margin(t = 0, r = 0, b = 0, l = 0),\n    axis.text.x = element_blank(),\n    axis.line.x = element_line(size = 1),\n    axis.ticks.x = element_blank()\n  )\np\n\n\n\n\nFinally, put it into a hexSticker and save the image:\n\n# Save the figure separately so I can control the size\nggsave(\"hex-plot.png\", plot = p, width = 10, height = 5, dpi = 500)\n\nlibrary(hexSticker)\nsysfonts::font_add_google(\"Inter\")\nshowtext::showtext_auto()\n\ncanada_red_a0.5 &lt;- grDevices::adjustcolor(canada_red, alpha.f = 0.5)\n\nhexSticker::sticker(\n  subplot = \"hex-plot.png\", s_width = 0.7, s_x = 1.0, s_y = 0.8,\n  package = \"canadacovid\",\n  p_family = \"Inter\", p_size = 30, p_color = canada_red,\n  h_fill = \"white\", h_color = canada_red_a0.5,\n  filename = \"canadacovid-sticker.png\", dpi = 500\n)\n\n\n\n\n\n\nNot bad. Not great."
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#reproducibility",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#reproducibility",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#footnotes",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/index.html#footnotes",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsing GitHub Actions is new to me, and probably overkill for a small hobby project like this, but I was inspired by Jim Hester‚Äôs RStudio::Conf 2020 talk to give it a try.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#introduction",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#introduction",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Introduction",
    "text": "Introduction\nThis is part 3 of working with Canadian COVID-19 data via the tracker API. In the previous post, I detailed the development of the canadacovid package which was recently published on CRAN. Here, I will set up GitHub Actions to periodically download data from the API. Much of what I do here was learned from Simon Couch‚Äôs great tutorial on the subject and this bookdown project ‚ÄúGitHub Actions with R‚Äù.\n\n\n\n\n\n\nNote\n\n\n\nSince writing this post, I‚Äôve put this data pipeline to use with a Shiny dashboard reporting and visualizing the latest Canadian COVID-19 numbers. Check out the dashboard here, and the source code here."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#the-goal",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#the-goal",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "The goal",
    "text": "The goal\nI want a scheduled task that periodically (every hour?) runs a script to check the API for updated COVID-19 data (overall numbers and by province). If there is updated data, then store it on GitHub. I also want to keep the API requests to a minimum if possible."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#making-it-an-r-package",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#making-it-an-r-package",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Making it an R package",
    "text": "Making it an R package\nThe R script to accomplish this will fairly simple, but it is essential to be very explicit about assumptions when running code remotely. I could use something like renv or a Docker container, but the best way to declare minimal dependencies for a piece of R code is to use a package. I‚Äôll call it canadacoviddata and make it quickly with usethis:\n\nusethis::create_package(\"canadacoviddata\")\nusethis::use_git()\nusethis::use_github()\n\nThis sets up the necessary files and folder structure, and initializes the repository on GitHub for me. A couple more commands I usually run for R packages:\n\nusethis::use_mit_license(\"Taylor Dunn\")\nusethis::use_pipe() # Use the `%&gt;%` pipe from `magittr`\n\nI know ahead of time two packages I will definitely want for downloading the data (my own canadacovid) and wrangling it (dplyr), so I add them as dependencies:\n\nusethis::use_dev_package(\"canadacovid\") # use_dev_package() uses GitHub version\nusethis::use_package(\"dplyr\")\n\nI then run devtools::document() and push the changes to GitHub."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#getting-the-data",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#getting-the-data",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Getting the data",
    "text": "Getting the data\nThe first data I want is the provinces table:\n\nprovinces &lt;- canadacovid::get_provinces()\nglimpse(provinces)\n\nRows: 13\nColumns: 10\n$ id          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n$ code        &lt;chr&gt; \"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\", \"NL\"‚Ä¶\n$ name        &lt;chr&gt; \"Ontario\", \"Quebec\", \"Nova Scotia\", \"New Brunswick\", \"Mani‚Ä¶\n$ population  &lt;int&gt; 14826276, 8604495, 992055, 789225, 1383765, 5214805, 16431‚Ä¶\n$ area        &lt;int&gt; 917741, 1356128, 53338, 71450, 553556, 925186, 5660, 59167‚Ä¶\n$ gdp         &lt;int&gt; 857384, 439375, 44354, 36966, 72688, 295401, 6994, 80679, ‚Ä¶\n$ geographic  &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE‚Ä¶\n$ data_status &lt;chr&gt; \"Reported\", \"Reported\", \"No report expected today\", \"Repor‚Ä¶\n$ updated_at  &lt;dttm&gt; 2022-10-21 18:09:07, 2022-10-26 23:09:04, 2022-08-15 16:52‚Ä¶\n$ density     &lt;dbl&gt; 16.15518540, 6.34489886, 18.59940380, 11.04583625, 2.4997‚Ä¶\n\n\nAdd the script R/download-data.R which will hold all the functions:\n\nusethis::use_r(\"download-data\")\n\nI also need a place to store the data. In an R package, the main options are the data and data-raw folders. Files in data are ‚Äúinternal‚Äù will be automatically loaded upon loading the package (library(canadacoviddata)), while those in data-raw are external but are available to users via system.file(\"extdata\", \"provinces\", package = \"canadacoviddata\"). See the data chapter of the R Packages book for more information. I‚Äôll go with data-raw:\n\ndir.create(\"data-raw\")\n\nA very simple function to download and save the data to the data-raw/ folder could look like this:\n\ndownload_provinces &lt;- function() {\n  canadacovid::get_provinces() %&gt;%\n    saveRDS(file = paste0(\"data-raw/provinces.rds\"))\n}\n\nAnd there is nothing wrong with this function, but I‚Äôm going to use a package I‚Äôve been meaning to try: pins."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#storing-data-with-pins",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#storing-data-with-pins",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Storing data with pins",
    "text": "Storing data with pins\npins allows me to store R objects remotely (on boards), and retrieve and update that data when necessary. For example, create a temporary board (that will be deleted once the R session ends):\n\nlibrary(pins)\n\nboard &lt;- board_temp()\nboard\n\nPin board &lt;pins_board_folder&gt;\nPath: 'C:/Users/tdunn/AppData/Local/Temp/RtmpwrfMl5/pins-2c2862845a0c'\nCache size: 0\n\n\nThen save provinces to the board:\n\nboard %&gt;% pin_write(provinces, \"provinces\", type = \"rds\")\n\nCreating new version '20221027T182338Z-0f5f4'\nWriting to pin 'provinces'\n\n\nThen retrieve it:\n\nboard %&gt;% pin_read(\"provinces\")\n\n# A tibble: 13 √ó 10\n      id code  name    popul‚Ä¶¬π   area    gdp geogr‚Ä¶¬≤ data_‚Ä¶¬≥ updated_at         \n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;lgl&gt;   &lt;chr&gt;   &lt;dttm&gt;             \n 1     1 ON    Ontario  1.48e7 9.18e5 857384 TRUE    Report‚Ä¶ 2022-10-21 18:09:07\n 2     2 QC    Quebec   8.60e6 1.36e6 439375 TRUE    Report‚Ä¶ 2022-10-26 23:09:04\n 3     3 NS    Nova S‚Ä¶  9.92e5 5.33e4  44354 TRUE    No rep‚Ä¶ 2022-08-15 16:52:42\n 4     4 NB    New Br‚Ä¶  7.89e5 7.14e4  36966 TRUE    Report‚Ä¶ 2022-08-15 16:50:10\n 5     5 MB    Manito‚Ä¶  1.38e6 5.54e5  72688 TRUE    No rep‚Ä¶ 2022-10-20 22:17:34\n 6     6 BC    Britis‚Ä¶  5.21e6 9.25e5 295401 TRUE    Report‚Ä¶ 2022-10-20 22:50:19\n 7     7 PE    Prince‚Ä¶  1.64e5 5.66e3   6994 TRUE    Report‚Ä¶ 2022-08-15 16:50:27\n 8     8 SK    Saskat‚Ä¶  1.18e6 5.92e5  80679 TRUE    No rep‚Ä¶ 2022-10-21 10:51:04\n 9     9 AB    Alberta  4.44e6 6.42e5 344812 TRUE    Report‚Ä¶ 2022-10-26 23:18:23\n10    10 NL    Newfou‚Ä¶  5.21e5 3.74e5  33241 TRUE    No rep‚Ä¶ 2022-08-15 16:50:18\n11    11 NT    Northw‚Ä¶  4.55e4 1.18e6   4730 TRUE    No rep‚Ä¶ 2022-08-15 16:50:34\n12    12 YT    Yukon    4.30e4 4.74e5   3046 TRUE    Report‚Ä¶ 2022-08-15 16:51:21\n13    13 NU    Nunavut  3.94e4 1.94e6   3421 TRUE    No rep‚Ä¶ 2022-08-15 16:51:02\n# ‚Ä¶ with 1 more variable: density &lt;dbl&gt;, and abbreviated variable names\n#   ¬π‚Äãpopulation, ¬≤‚Äãgeographic, ¬≥‚Äãdata_status\n# ‚Ñπ Use `colnames()` to see all variable names\n\n\nUsing a pins board to store data has a few advantages, like versioning and caching to avoid excessive computations and downloads. Another nice feature is that I can easily get metadata, like when the data was created:\n\nboard %&gt;% pin_meta(\"provinces\")\n\nList of 11\n $ file       : chr \"provinces.rds\"\n $ file_size  : 'fs_bytes' int 874\n $ pin_hash   : chr \"0f5f40fd893b97a5\"\n $ type       : chr \"rds\"\n $ title      : chr \"provinces: a pinned 13 x 10 data frame\"\n $ description: NULL\n $ created    : POSIXct[1:1], format: \"2022-10-27 14:23:00\"\n $ api_version: num 1\n $ user       : list()\n $ name       : chr \"provinces\"\n $ local      :List of 3\n  ..$ dir    : 'fs_path' chr \"C:/Users/tdunn/AppData/Local/Temp/RtmpwrfMl5/pins-2c2862845a0c/provinces/20221027T182338Z-0f5f4\"\n  ..$ url    : NULL\n  ..$ version: chr \"20221027T182338Z-0f5f4\"\n\n\npins has numerous options for storing boards, including RStudio Connect, Amazon S3, and Google Cloud Platform. I want to keep this package and the data in the same repository, so I‚Äôll register a board on this GitHub repository. Unfortunately, I have to use the legacy pins API for this task, because GitHub boards haven‚Äôt been implemented in the modern API as of me writing this:1\n\nboard &lt;- board_register_github(\n  name = \"github\", repo = \"taylordunn/canadacoviddata\", path = \"data-raw\"\n)\n\nNow write the provinces data:\n\npins::pin(provinces, name = \"provinces\", board = \"github\")\n\nThe data get immediately pushed to the GitHub repository (under the data-raw/provinces/ directory) in both CSV and RDS formats:\n\nTo incorporate this into the package, I‚Äôll first add pins as a dependency:\n\nusethis::use_package(\"pins\")\n\nThen add a function to register_github_board()2 and re-write download_provinces(). The R/download-data.R script now looks like this (with some added roxygen documentation):\n\n#' Register the pins board\n#'\n#' The `pins::board_register_github()` function requires a GitHub personal\n#' access token be available through the environment variable `GITHUB_PAT`.\n#'\n#' @export\n#' @importFrom pins board_register_github\nregister_github_board &lt;- function() {\n  pins::board_register_github(\n    name = \"github\", repo = \"taylordunn/canadacoviddata\", path = \"data-raw\",\n    token = Sys.getenv(\"GITHUB_PAT\")\n  )\n}\n\n#' Retrieve and pin the provinces data\n#'\n#' Retrieves the `provinces` data from the Canadian COVID-19 tracker API\n#' and uploads it to the given `pins` board.\n#'\n#' @param board The name of the `pins` board to write the data.\n#'\n#' @export\n#' @importFrom canadacovid get_provinces\n#' @importFrom pins pin\ndownload_provinces &lt;- function(board = \"github\") {\n  canadacovid::get_provinces() %&gt;%\n    pins::pin(name = \"provinces\", board = board)\n}"
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#github-actions-workflow",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#github-actions-workflow",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "GitHub Actions workflow",
    "text": "GitHub Actions workflow\nNow that the functions are in place, I need to tell GitHub when and how to use them. For setting up GitHub actions, I first add the folders and files:\n\nCreated the .github/workflows/ directory.\nAdded ^\\\\.github$ to .Rbuildignore (because it does not need to be part of the installed package).\nAdded the empty .github/workflows/update-data.yaml file.\n\nAt the top of the update-data.yaml file, I need to define the frequency at which the workflow is run. I think I want data to be updated every hour at minute 0. The cron expression to specify this schedule looks like this:\n\non:\n  schedule:\n    - cron: \"0 * * * *\"\n\nFrom left to right, the \"0 * * * *\" string corresponds to:\n\n0: at minute 0 of the hour.\n*: every hour.\n*: every day.\n*: every month.\n*: every day of the week.\n\nDefining the jobs was mostly copy and paste:\n\njobs:\n  update-data:\n    runs-on: ${{ matrix.config.os }}\n\n    name: ${{ matrix.config.os }} (${{ matrix.config.r }})\n\n    strategy:\n      fail-fast: false\n      matrix:\n        config:\n          - {os: ubuntu-latest, r: 'release'}\n\n    env:\n      R_REMOTES_NO_ERRORS_FROM_WARNINGS: true\n      RSPM: ${{ matrix.config.rspm }}\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - uses: actions/checkout@v2\n\n      - uses: r-lib/actions/setup-r@master\n        with:\n          r-version: ${{ matrix.config.r }}\n          http-user-agent: ${{ matrix.config.http-user-agent }}\n\n      - uses: r-lib/actions/setup-pandoc@master\n\n      - name: Query dependencies\n        run: |\n          install.packages(\"remotes\")\n          install.packages(\"sessioninfo\")\n          install.packages(\"devtools\")\n          saveRDS(remotes::dev_package_deps(dependencies = TRUE), \".github/depends.rds\", version = 2)\n          writeLines(sprintf(\"R-%i.%i\", getRversion()$major, getRversion()$minor), \".github/r-version\")\n        shell: Rscript {0}\n\n      - name: Cache R packages\n        uses: actions/cache@v1\n        with:\n          path: ${{ env.R_LIBS_USER }}\n          key: ${{ runner.os }}-${{ hashFiles('.github/r-version') }}-1-${{ hashFiles('.github/depends.rds') }}\n          restore-keys: ${{ runner.os }}-${{ hashFiles('.github/r-version') }}-1-\n\n      - name: Install dependencies\n        run: |\n          remotes::install_deps(dependencies = TRUE)\n        shell: Rscript {0}\n\n      - name: Update data\n        run: |\n          devtools::load_all(\".\")\n          register_github_board()\n          download_provinces()\n        shell: Rscript {0}\n\nThe interesting bits, from top to bottom:\n\nSpecify that the job will be run on the latest release version of Ubuntu.\nAdd some environment variables like my GitHub PAT\n\nNote that I don‚Äôt need to add a PAT manually. At the start of each workflow run, GitHub automatically creates a unique PAT secret for authentication.\n\nInstall R.\nInstall the remotes and sessioninfo packages for downloading and managing dependencies, and the devtools package for load_all().\nInstall the dependencies for the canadacoviddata package (as defined in the DESCRIPTION file).\nCache R packages for future workflow runs.\nRun the R code that updates the data.\n\nThe R code to download the provinces data is simply three lines:\n\ndevtools::load_all(\".\") # Loads the package functions, kind of like `source()`\nregister_github_board()\ndownload_provinces()\n\nI pushed the workflow to GitHub and patiently waited about 20 minutes for the hour mark (probably should have made the workflow more frequent for quicker development/iteration) et voila:\n\nFailure. The error at the bottom tells me that the pins package was not found. It definitely should have been installed because it is explicitly listed under Imports of the DESCRIPTION file, so something must have gone wrong upstream. Digging into the logs, I found that the errors began with installing the curl package:\n\nAfter some Googling, I found that I could install the missing liburl library on the Ubuntu runner by adding the following step in the workflow YAML (before ‚ÄúQuery dependencies‚Äù):\n\n      - name: Install curl headers\n        run: sudo apt-get install libcurl4-openssl-dev\n\nAnother problem with the workflow was that the R packages were not being cached as expected. It didn‚Äôt cause the workflow to fail, but it was taking ~13 minutes per run. This was the warning returned in the cache step:\n\nI found this GitHub issue and response from the authors, and the solution to update the version of the cache action:\n\n      - name: Cache R packages\n        uses: actions/cache@v2\n\nThis cut down the workflow run time to ~8 minutes."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#adding-functionality",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#adding-functionality",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Adding functionality",
    "text": "Adding functionality\nA list of provinces isn‚Äôt exactly the point of this post, which is to continuously retrieve COVID-19 data. The reason I started with provinces is for the updated_at variable:\n\nprovinces %&gt;% pull(updated_at, name = code)\n\n                       ON                        QC                        NS \n\"2022-10-21 18:09:07 CST\" \"2022-10-26 23:09:04 CST\" \"2022-08-15 16:52:42 CST\" \n                       NB                        MB                        BC \n\"2022-08-15 16:50:10 CST\" \"2022-10-20 22:17:34 CST\" \"2022-10-20 22:50:19 CST\" \n                       PE                        SK                        AB \n\"2022-08-15 16:50:27 CST\" \"2022-10-21 10:51:04 CST\" \"2022-10-26 23:18:23 CST\" \n                       NL                        NT                        YT \n\"2022-08-15 16:50:18 CST\" \"2022-08-15 16:50:34 CST\" \"2022-08-15 16:51:21 CST\" \n                       NU \n\"2022-08-15 16:51:02 CST\" \n\n\nThis timestamp tells me when the province/territory last reported their COVID-19 data. By comparing new and old timestamps, I can query the API only when there is updated data, and avoid excessive requests. Here is the re-written download_provinces():\n\ndownload_provinces &lt;- function(board = \"github\") {\n  old_provinces &lt;- pins::pin_get(\"provinces\", board = board)\n  new_provinces &lt;- canadacovid::get_provinces()\n\n  updated_provinces &lt;- new_provinces %&gt;%\n    dplyr::anti_join(old_provinces, by = c(\"name\", \"updated_at\"))\n\n  if (nrow(updated_provinces) &gt; 0) {\n    pins::pin(new_provinces, name = \"provinces\", board = board)\n  }\n  return(updated_provinces$code)\n}\n\nIn addition to saving provinces to the pins board, this function now returns a list of provinces which have been updated since the last workflow run. Then a new function takes the list of provinces, retrieves the latest reports from the API, and writes it to the pins board:\n\ndownload_reports &lt;- function(provinces_codes, board = \"github\") {\n  for (prov in provinces_codes) {\n    if (prov == \"overall\") {\n      new_report &lt;- canadacovid::get_reports(\"overall\")\n    } else {\n      new_report &lt;- canadacovid::get_reports(province = prov)\n    }\n    \n    new_report &lt;- new_report %&gt;%\n      dplyr::mutate(\n        change_active = .data$change_cases - .data$change_recoveries -\n          .data$change_fatalities,\n        total_active = .data$total_cases - .data$total_recoveries -\n          .data$total_fatalities,\n        positivity_rate = .data$change_cases / .data$change_tests\n      )\n    \n    pins::pin(new_report,\n              name = paste0(\"reports_\", tolower(prov)), board = board)\n  }\n}\n\nI also compute some extra variables here that I am interested in: change_active (estimated change in active cases), total_active (estimated total cases), and positivity_rate (percentage of tests which were postivie for COVID).\nThen to incorporate the new functionality, I update the workflow script:\n\n      - name: Update data\n        run: |\n          devtools::load_all(\".\")\n          register_github_board()\n          updated_provinces &lt;- download_provinces()\n          if (length(updated_provinces) &gt; 0) {\n            download_reports(updated_provinces)\n            download_reports(\"overall\")\n          }\n        shell: Rscript {0}\n\nAfter letting this run for a while, here is how the data-raw folder on the GitHub repo looks:\n\nNote how the age of the files is different between provinces/territories (‚Äú3 hours ago‚Äù, ‚Äú9 hours ago‚Äù, etc), which shows that the selective data retrieval is working."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#conclusion",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#conclusion",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Conclusion",
    "text": "Conclusion\nThanks to some great R packages and online resources, it wasn‚Äôt too hard to set up a simple ETL (extract, transform, load) pipeline that periodically runs with GitHub actions.\nTo see the full version of the workflow, check it out on GitHub here."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#reproducibility",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#reproducibility",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#footnotes",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/index.html#footnotes",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that I don‚Äôt need to provide my personal access token argument to register the board, because it is automatically retrieved from gitcreds.‚Ü©Ô∏é\nThe package function looks for the PAT in the environment variables so that I don‚Äôt need to install gitcreds when running remotely.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/index.html",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/index.html",
    "title": "Analyzing US baby names over the years",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(gt)\nlibrary(rmarkdown)\nlibrary(patchwork)\nlibrary(ggtext)\nlibrary(glue)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td_minimal())\nset_geom_fonts()\nset_palette()\n\nsex_pal &lt;- c(\"M\" = \"#8ecefd\", \"F\" = \"#f88b9d\")"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#load-the-data",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#load-the-data",
    "title": "Analyzing US baby names over the years",
    "section": "Load the data",
    "text": "Load the data\n\ntt &lt;- tidytuesdayR::tt_load(\"2022-03-22\")\n\n--- Compiling #TidyTuesday Information for 2022-03-22 ----\n\n\n--- There are 8 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 8: `applicants.csv`\n    Downloading file 2 of 8: `babynames.csv`\n    Downloading file 3 of 8: `births.csv`\n    Downloading file 4 of 8: `lifetables.csv`\n    Downloading file 5 of 8: `maorinames.csv`\n    Downloading file 6 of 8: `nz_births.csv`\n    Downloading file 7 of 8: `nz_lifetables.csv`\n    Downloading file 8 of 8: `nz_names.csv`\n\n\n--- Download complete ---\n\n\nThere are 8 data frames this week. For this post, I‚Äôll be exploring babynames, which are baby name counts by year and sex in the US:\n\nbabynames &lt;- tt$babynames\nglimpse(babynames)\n\nRows: 1,924,665\nColumns: 5\n$ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,‚Ä¶\n$ sex  &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", ‚Ä¶\n$ name &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",‚Ä¶\n$ n    &lt;dbl&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,‚Ä¶\n$ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016‚Ä¶"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#data-exploration",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#data-exploration",
    "title": "Analyzing US baby names over the years",
    "section": "Data exploration",
    "text": "Data exploration\nThe data span the following years:\n\nc(min(babynames$year), max(babynames$year))\n\n[1] 1880 2017\n\n\nThe prop variable is presumably the proportion of babies with that name and sex in a given year:\n\nbabynames %&gt;%\n  group_by(sex, year) %&gt;%\n  mutate(n_total = sum(n)) %&gt;%\n  ungroup() %&gt;%\n  mutate(prop_manual = n / n_total) %&gt;%\n  select(prop, prop_manual)\n\n# A tibble: 1,924,665 √ó 2\n     prop prop_manual\n    &lt;dbl&gt;       &lt;dbl&gt;\n 1 0.0724      0.0776\n 2 0.0267      0.0286\n 3 0.0205      0.0220\n 4 0.0199      0.0213\n 5 0.0179      0.0192\n 6 0.0162      0.0173\n 7 0.0151      0.0162\n 8 0.0145      0.0155\n 9 0.0135      0.0145\n10 0.0132      0.0142\n# ‚Ä¶ with 1,924,655 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nNot quite ‚Äì my manually calculated prop_manual is slightly higher. It may be due to un-documented names not being counted in the babynames data frame. The applicants data has the number of social security number applications. Is this the denominator?\n\nbabynames %&gt;%\n  left_join(tt$applicants, by = c(\"year\", \"sex\")) %&gt;%\n  mutate(prop_manual = n / n_all) %&gt;%\n  select(prop, prop_manual)\n\n# A tibble: 1,924,665 √ó 2\n     prop prop_manual\n    &lt;dbl&gt;       &lt;dbl&gt;\n 1 0.0724      0.0724\n 2 0.0267      0.0267\n 3 0.0205      0.0205\n 4 0.0199      0.0199\n 5 0.0179      0.0179\n 6 0.0162      0.0162\n 7 0.0151      0.0151\n 8 0.0145      0.0145\n 9 0.0135      0.0135\n10 0.0132      0.0132\n# ‚Ä¶ with 1,924,655 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nThat looks right.\nNot much more to do in terms of data exploration or cleaning here. I‚Äôll jump right into the analysis."
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#name-creativity-over-the-years",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#name-creativity-over-the-years",
    "title": "Analyzing US baby names over the years",
    "section": "Name creativity over the years",
    "text": "Name creativity over the years\nThe number of unique names by year and sex:\n\nd &lt;- babynames %&gt;% count(year, sex, name = \"n_names\")\nd_max &lt;- d %&gt;% group_by(sex) %&gt;% filter(n_names == max(n_names))\n\nd %&gt;%\n  ggplot(aes(x = year, y = n_names, color = sex)) +\n  geom_line(size = 1) +\n  geom_linerange(\n    data = d_max,\n    aes(ymin = 0, ymax = n_names, xmin = year, xmax = year, color = sex),\n    lty = 2, size = 1, show.legend = FALSE\n  ) +\n  geom_text(\n    data = d_max,\n    aes(x = year, y = c(16000, 5000), label = year),\n    hjust = 0, nudge_x = 1, show.legend = FALSE\n  ) +\n  labs(title = \"Number of unique names\", y = NULL, color = NULL) +\n  scale_color_manual(values = sex_pal) +\n  theme(legend.position = c(0.2, 0.7))\n\n\n\n\nThere is an dip in the number of unique names after peaking in 2007 and 2008 ‚Äì I‚Äôm assuming this is related to a decrease in overall births following the financial crisis. If we normalize by the number of births, does this trend still appear?\n\nd &lt;- babynames %&gt;%\n  group_by(year, sex) %&gt;%\n  summarise(n_names = n(), n_births = sum(n),\n            names_per_1000_births = 1000 * n_names / n_births,\n            .groups = \"drop\")\nd_max &lt;- d %&gt;%\n  group_by(sex) %&gt;%\n  filter(names_per_1000_births == max(names_per_1000_births))\n\nd %&gt;%\n  ggplot(aes(x = year, y = names_per_1000_births, color = sex)) +\n  geom_line(size = 1) +\n  geom_linerange(\n    data = d_max,\n    aes(ymin = 0, ymax = names_per_1000_births,\n        xmin = year, xmax = year, color = sex),\n    lty = 2, size = 1, show.legend = FALSE\n  ) +\n  geom_text(\n    data = d_max,\n    aes(x = year, y = c(2.5, 3.5), label = year),\n    hjust = 0, nudge_x = 1, show.legend = FALSE\n  ) +\n  labs(title = \"Number of unique names / 1000 births\",\n       y = NULL, color = NULL) +\n  scale_color_manual(values = sex_pal) +\n  theme(legend.position = c(0.5, 0.7))\n\n\n\n\nCreativity for male names peaked around 1900, and is on the rise again. Creativity for female names has been higher than male names since ~1920, and peaked in 2010.\nAnother way to think about creativity is the proportion of babies being given the most popular names in a given year:\n\nd &lt;- babynames %&gt;%\n  group_by(year, sex) %&gt;%\n  mutate(name_rank = rank(-n)) %&gt;%\n  ungroup() %&gt;%\n  # Try a few different cut-offs\n  crossing(top_x = c(10, 25, 50, 100, 200)) %&gt;%\n  mutate(top_name = name_rank &lt;= top_x)  %&gt;%\n  group_by(year, sex, top_x) %&gt;%\n  summarise(p_top = sum(n[top_name]) / sum(n), .groups = \"drop\") \nd %&gt;%\n  mutate(top_x = fct_reorder(paste0(\"Top \", top_x, \" names\"), top_x)) %&gt;%\n  ggplot(aes(x = year, y = p_top, color = sex)) +\n  geom_line(size = 1) +\n  facet_wrap(~ top_x) +\n  scale_color_manual(values = sex_pal) +\n  theme(legend.position = c(0.8, 0.2))\n\n\n\n\nOut of these, I prefer the top 50 visualization:\n\nd %&gt;%\n  filter(top_x == 50) %&gt;%\n  ggplot(aes(x = year, y = p_top, color = sex)) +\n  geom_line(size = 1) +\n  scale_y_continuous(NULL, limits = c(0, 1), labels = scales::percent) +\n  scale_color_manual(values = sex_pal) +\n  labs(\n    x = NULL,\n    title = paste0(\"Baby &lt;span style='color:#8ecefd'&gt;boy&lt;/span&gt; and \",\n                   \"&lt;span style='color:#f88b9d'&gt;girl&lt;/span&gt; names \",\n                   \"are getting more creative over time\"),\n    subtitle = \"Proportion of names in the top 50 most common versus year\"\n  ) +\n  theme(legend.position = \"none\", plot.title = element_markdown())"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#gender-neutral-names",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#gender-neutral-names",
    "title": "Analyzing US baby names over the years",
    "section": "Gender-neutral names",
    "text": "Gender-neutral names\nBefore getting into gender-neutral names, I‚Äôm curious which are the least gender-neutral in this data?\n\nd &lt;- babynames %&gt;%\n  group_by(name, sex) %&gt;%\n  summarise(n = sum(n), .groups = \"drop\") %&gt;%\n  complete(name, sex, fill = list(n = 0)) %&gt;%\n  pivot_wider(names_from = sex, values_from = n) %&gt;%\n  rename(female = `F`, male = M) %&gt;%\n  mutate(total = female + male) %&gt;%\n  arrange(desc(total)) %&gt;%\n  filter(female == 0 | male == 0) %&gt;%\n  select(-total) %&gt;%\n  pivot_longer(cols = -name, names_to = \"sex\", values_to = \"count\") %&gt;%\n  filter(count &gt; 0) %&gt;%\n  mutate(name = fct_reorder(name, count))\n\nd %&gt;%\n  group_by(sex) %&gt;%\n  slice_max(count, n = 10) %&gt;%\n  ggplot(aes(y = name, x = count, fill = sex)) +\n  geom_col() +\n  facet_wrap(~ sex, nrow = 1, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"#f88b9d\", \"#8ecefd\")) +\n  labs(\n    y = NULL, x = NULL,\n    title = paste0(\"Top 10 most common \",\n                   \"&lt;span style='color:#f88b9d'&gt;100% female&lt;/span&gt; and \",\n                   \"&lt;span style='color:#8ecefd'&gt;100% male&lt;/span&gt; names\"),\n    subtitle = \"US baby names from 1880 to 2017\"\n  ) +\n  theme(legend.position = \"none\", plot.title = element_markdown(),\n        panel.grid.major.y = element_blank(),\n        strip.background = element_blank(), strip.text = element_blank())\n\n\n\n\nDetermining the most popular gender-neutral names is not as straightforward. I‚Äôll include names with 40-60% of males/females (over all years), and order by top 10 frequency:\n\nd &lt;- babynames %&gt;%\n  group_by(name) %&gt;%\n  summarise(n_total = sum(n), p_female = sum(n[sex == \"F\"]) / n_total)\nd %&gt;%\n  filter(p_female &gt; 0.4, p_female &lt; 0.6) %&gt;%\n  slice_max(n_total, n = 9) %&gt;%\n  left_join(babynames, by = \"name\") %&gt;%\n  mutate(\n    name = fct_reorder(name, n_total),\n    name_label = ifelse(\n      n_total == max(n_total),\n      glue(\"{name} ({scales::percent(p_female, 0.1)} female)\"),\n      glue(\"{name} ({scales::percent(p_female, 0.1)})\")\n    ) %&gt;%\n      fct_reorder(desc(n_total))\n  ) %&gt;%\n  ggplot(aes(x = year, y = n)) +\n  geom_area(aes(fill = sex)) +\n  facet_wrap(~ name_label, scales = \"free_y\", nrow = 3) +\n  scale_fill_manual(values = sex_pal) +\n  labs(\n    y = \"Births\", x = NULL, fill = NULL,\n    title = \"The most frequent gender-neutral names (40-60% male/female)\"\n  ) +\n  theme(legend.position = c(0.1, 0.9))\n\n\n\n\nNext, I‚Äôd like to visualize how the number of gender-neutral names changed over time. To do this, I‚Äôll compute the ratio of females to males for each name in a given year (excluding names which have 0 females or 0 males):\n\nd &lt;- babynames %&gt;%\n  group_by(year, name) %&gt;%\n  # Keep only names with both sexes\n  filter(\"F\" %in% sex, \"M\" %in% sex) %&gt;%\n  summarise(ratio_female = n[sex == \"F\"] / n[sex == \"M\"], .groups = \"drop\")\nglimpse(d)\n\nRows: 168,381\nColumns: 3\n$ year         &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 188‚Ä¶\n$ name         &lt;chr&gt; \"Addie\", \"Allie\", \"Alma\", \"Alpha\", \"Alva\", \"Anna\", \"Annie‚Ä¶\n$ ratio_female &lt;dbl&gt; 3.425000e+01, 3.387097e+00, 1.978571e+01, 4.333333e+00, 2‚Ä¶\n\n\nThen I‚Äôll plot each ratio as a point and use the ggpointdensity package to color by the density of points:\n\nd %&gt;%\n  ggplot(aes(x = year, y = ratio_female)) +\n  ggpointdensity::geom_pointdensity(size = 3, adjust = 1, alpha = 0.2,\n                                    show.legend = FALSE) +\n  dunnr::scale_color_td(palette = \"div5\", type = \"continuous\") +\n  scale_y_log10(breaks = 10^seq(-2, 2),\n                labels = c(\"1:100\", \"1:10\", \"1:1\", \"10:1\", \"100:1\")) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"The amount of gender-neutral names is increasing\",\n    subtitle = \"Each point represents the ratio of females:males for a single name in a single year.\\nColor indicates the density of points.\",\n    caption = paste(\"data: {babynames} R package\",\n                    \"plot: Taylor Dunn\", sep =  \" | \")\n  ) +\n  theme(plot.subtitle = element_text(size = 10))"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#my-name",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#my-name",
    "title": "Analyzing US baby names over the years",
    "section": "My name",
    "text": "My name\nSelfishly, I of course want to explore my name and its different variations.\nFirst, here are the names which start with ‚ÄúTayl‚Äù or ‚ÄúTail‚Äù and have an ‚Äúr‚Äù in them:\n\nbabynames_taylor &lt;- babynames %&gt;%\n  filter(str_detect(name, \"Tayl|Tail\") & str_detect(name, \"r\")) %&gt;%\n  group_by(name) %&gt;%\n  mutate(n_total = sum(n)) %&gt;%\n  ungroup()\nbabynames_taylor %&gt;%\n  distinct(name, n_total) %&gt;%\n  arrange(desc(n_total)) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nUnsurprised to see ‚ÄúTaylor‚Äù as the most common spelling. I can‚Äôt say I‚Äôve met anyone with a double-name like ‚ÄúTaylorann‚Äù or ‚ÄúTaylormarie‚Äù before.\nI‚Äôll take the top 6 from these and plot births over time:\n\nbabynames_taylor %&gt;%\n  distinct(name, n_total) %&gt;%\n  slice_max(n_total, n = 6) %&gt;%\n  left_join(babynames_taylor %&gt;% select(-n_total), by = \"name\") %&gt;%\n  group_by(name, sex) %&gt;%\n  mutate(n_cumsum = cumsum(n)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    name_label = glue(\"{name} (n = {n_total})\") %&gt;% fct_reorder(desc(n_total))\n  ) %&gt;%\n  ggplot(aes(x = year, y = n, fill = sex)) +\n  geom_area() +\n  scale_fill_manual(values = sex_pal) +\n  facet_wrap(~ name_label, scales = \"free_y\") +\n  coord_cartesian(xlim = c(1960, 2020)) +\n  labs(y = \"Births\", x = \"Year\", fill = NULL,\n       title = \"Top 6 variations of 'Taylor'\") +\n  theme(legend.position = c(0.1, 0.9))\n\n\n\n\nNow just focusing on ‚ÄúTaylor‚Äù, which year was the name most popular for each sex?\n\nbabynames_taylor %&gt;%\n  group_by(sex) %&gt;%\n  filter(n == max(n)) %&gt;%\n  ungroup() %&gt;%\n  select(sex, year, n) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nsex\nyear\nn\n\n\n\n\nM\n1992\n8240\n\n\nF\n1993\n21266\n\n\n\n\n\n\n\nThe most popular year for male Taylors happens to be the year I was born: 1992. Female Taylors peaked in frequency the next year.\nLastly, a plot of the proportion and number of Taylors by sex over time:\n\nd &lt;- babynames_taylor %&gt;%\n  filter(name == \"Taylor\", year &gt;= 1960) %&gt;%\n  select(year, sex, n) %&gt;%\n  complete(year, sex, fill = list(n = 0)) %&gt;%\n  group_by(year) %&gt;%\n  mutate(p_female = n[sex == \"F\"] / sum(n)) %&gt;%\n  ungroup()\np_title &lt;- glue(\n  \"&lt;span style='color:{sex_pal[1]}'&gt;Male&lt;/span&gt; and \",\n  \"&lt;span style='color:{sex_pal[2]}'&gt;Female&lt;/span&gt; Taylors over time\",\n)\np1 &lt;- d %&gt;%\n  distinct(year, p_female) %&gt;%\n  ggplot(aes(x = year)) +\n  geom_ribbon(aes(ymin = 0, ymax = p_female), fill = sex_pal[\"F\"]) +\n  geom_ribbon(aes(ymin = p_female, ymax = 1.0), fill = sex_pal[\"M\"]) +\n  geom_line(aes(y = p_female), color = \"white\", size = 1) +\n  geom_hline(yintercept = 0.5, lty = 2, color = \"white\", size = 1) +\n  scale_y_continuous(labels = scales::percent,\n                     expand = c(0, 0)) +\n  scale_x_continuous(breaks = NULL, expand = c(0, 0)) +\n  labs(x = NULL, y = \"Proportion by sex\", title = p_title) +\n  theme(plot.title = element_markdown())\np2 &lt;- d %&gt;%\n  ggplot(aes(x = year, y = n, fill = sex)) +\n  geom_area() +\n  scale_fill_manual(values = sex_pal) +\n  scale_x_continuous(\"Year\", breaks = seq(1960, 2020, 10), expand = c(0, 0)) +\n  theme(legend.position = \"none\", panel.grid.major.x = element_blank()) +\n  labs(y = \"Number of births\")\n\np1 / p2"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#reproducibility",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/index.html#reproducibility",
    "title": "Analyzing US baby names over the years",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html",
    "title": "Predicting bike ridership: getting the data",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(httr)\nlibrary(lubridate)\nlibrary(gt)\nlibrary(glue)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#introduction",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#introduction",
    "title": "Predicting bike ridership: getting the data",
    "section": "Introduction",
    "text": "Introduction\nIn 2016, the city of Halifax installed its first cyclist tracker on Agricola Street. Last year, the city made bike counter data available on their open data platform. As a cyclist and Haligonian, this is of course interesting to me personally. As a data scientist, this seems like a nice opportunity to work through a machine learning project end-to-end: from retrieving, exploring, and processing the data, to building and evaluating models, to producing an end product. (A REST API? A Shiny app? TBD.)\nIn this post, I get and explore data from two sources: (1) the aforementioned bike counter data from city of Halifax, and (2) historical weather data from the government of Canada."
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#getting-bicycle-count-data",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#getting-bicycle-count-data",
    "title": "Predicting bike ridership: getting the data",
    "section": "Getting bicycle count data",
    "text": "Getting bicycle count data\nThe bicycle counts were easy enough to find on Halifax‚Äôs platform: (https://catalogue-hrm.opendata.arcgis.com/datasets/45d4ecb0cb48469186e683ebc54eb188_0/explore?showTable=true). Each data set comes with a nice API explorer for constructing queries. I‚Äôll use httr to GET the data with the basic query provided there:\n\nquery_url &lt;- \"https://services2.arcgis.com/11XBiaBYA9Ep0yNJ/arcgis/rest/services/Bicycle_Counts/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json\"\nresp &lt;- httr::GET(query_url)\nresp\n\n\n\nResponse [https://services2.arcgis.com/11XBiaBYA9Ep0yNJ/arcgis/rest/services/Bicycle_Counts/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json]\n  Date: 2022-04-27 03:26\n  Status: 200\n  Content-Type: application/json; charset=utf-8\n  Size: 579 kB\n\n\nThe response code (200) indicates a successful connection. The data comes in JSON format, which I can parse to an R list with:\n\nparsed_content &lt;- content(resp, as = \"parsed\")\nstr(parsed_content, max.level = 1)\n\nList of 6\n $ objectIdFieldName    : chr \"ObjectId\"\n $ uniqueIdField        :List of 2\n $ globalIdFieldName    : chr \"\"\n $ fields               :List of 11\n $ exceededTransferLimit: logi TRUE\n $ features             :List of 2000\n\n\nThis returned a list of 6 items. The fields item is a list of variables:\n\nfields &lt;- map_dfr(\n  parsed_content$fields,\n  # Drop NULL elements so I can convert to a tibble\n  ~ discard(.x, is.null) %&gt;% as_tibble()\n)\ngt(fields)\n\n\n\n\n\n\n\n\nname\ntype\nalias\nsqlType\nlength\n\n\n\n\nCHANNEL_ID\nesriFieldTypeString\nCHANNEL_ID\nsqlTypeNVarchar\n256\n\n\nCHANNEL_NAME\nesriFieldTypeString\nCHANNEL_NAME\nsqlTypeNVarchar\n4000\n\n\nSERIAL_NUMBER\nesriFieldTypeString\nSERIAL_NUMBER\nsqlTypeNVarchar\n4000\n\n\nSITE_NAME\nesriFieldTypeString\nSITE_NAME\nsqlTypeNVarchar\n4000\n\n\nLATITUDE\nesriFieldTypeDouble\nLATITUDE\nsqlTypeFloat\nNA\n\n\nLONGITUDE\nesriFieldTypeDouble\nLONGITUDE\nsqlTypeFloat\nNA\n\n\nINSTALLATION_DATE\nesriFieldTypeDate\nINSTALLATION_DATE\nsqlTypeTimestamp2\n8\n\n\nCOUNT_DATETIME\nesriFieldTypeDate\nCOUNT_DATETIME\nsqlTypeTimestamp2\n8\n\n\nCOUNTER_TYPE\nesriFieldTypeString\nCOUNTER_TYPE\nsqlTypeNVarchar\n4000\n\n\nCOUNTER_VALUE\nesriFieldTypeInteger\nCOUNTER_VALUE\nsqlTypeInteger\nNA\n\n\nObjectId\nesriFieldTypeOID\nObjectId\nsqlTypeInteger\nNA\n\n\n\n\n\n\n\nThe data is the features item, which itself is a list of length 2000. Here is the first element:\n\nparsed_content$features[[1]]\n\n$attributes\n$attributes$CHANNEL_ID\n[1] \"100059339\"\n\n$attributes$CHANNEL_NAME\n[1] \"Hollis St\"\n\n$attributes$SERIAL_NUMBER\n[1] \"X2H20032465\"\n\n$attributes$SITE_NAME\n[1] \"Hollis St\"\n\n$attributes$LATITUDE\n[1] 44.64799\n\n$attributes$LONGITUDE\n[1] -63.57352\n\n$attributes$INSTALLATION_DATE\n[1] 1.594166e+12\n\n$attributes$COUNT_DATETIME\n[1] 1.595966e+12\n\n$attributes$COUNTER_TYPE\n[1] \"Bicycle\"\n\n$attributes$COUNTER_VALUE\n[1] 2\n\n$attributes$ObjectId\n[1] 1\n\n\nLooks like there is another level of nesting with attributes. Compile all of these elements into a single data frame:\n\nbike_counts &lt;- map_dfr(\n  parsed_content$features,\n  ~ as_tibble(.x$attributes)\n)\nglimpse(bike_counts)\n\nRows: 2,000\nColumns: 11\n$ CHANNEL_ID        &lt;chr&gt; \"100059339\", \"100059339\", \"100059339\", \"100059339\", ‚Ä¶\n$ CHANNEL_NAME      &lt;chr&gt; \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", ‚Ä¶\n$ SERIAL_NUMBER     &lt;chr&gt; \"X2H20032465\", \"X2H20032465\", \"X2H20032465\", \"X2H200‚Ä¶\n$ SITE_NAME         &lt;chr&gt; \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", ‚Ä¶\n$ LATITUDE          &lt;dbl&gt; 44.64799, 44.64799, 44.64799, 44.64799, 44.64799, 44‚Ä¶\n$ LONGITUDE         &lt;dbl&gt; -63.57352, -63.57352, -63.57352, -63.57352, -63.5735‚Ä¶\n$ INSTALLATION_DATE &lt;dbl&gt; 1.594166e+12, 1.594166e+12, 1.594166e+12, 1.594166e+‚Ä¶\n$ COUNT_DATETIME    &lt;dbl&gt; 1.595966e+12, 1.595970e+12, 1.595974e+12, 1.595977e+‚Ä¶\n$ COUNTER_TYPE      &lt;chr&gt; \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle‚Ä¶\n$ COUNTER_VALUE     &lt;int&gt; 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 6, 4, 13, 8, 5, 6, 8, ‚Ä¶\n$ ObjectId          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1‚Ä¶\n\n\nNote that just 2000 records were returned. The exceededTransferLimit = TRUE value tells us that this is the limit of the API. I can get the total count of records by altering the original query slightly:\n\nn_records &lt;- httr::GET(paste0(query_url, \"&returnCountOnly=true\")) %&gt;%\n  content(as = \"parsed\") %&gt;%\n  unlist(use.names = FALSE)\nn_records\n\n\n\n[1] 124356\n\n\nSo to get all of the data at 2000 records per request, I‚Äôll have to make a minimum of 63 calls to the API. The API offers a ‚ÄúresultOffset‚Äù argument to get records in sequence. Make a function to get 2000 records for a given offset:\n\nget_bike_data &lt;- function(offset) {\n  # Need to prevent scientific notation, e.g. \"1e+05\" instead of \"100000\"\n  offset &lt;- format(offset, scientific = FALSE)\n  \n  parsed_content &lt;- httr::GET(paste0(query_url, \"&resultOffset=\", offset)) %&gt;%\n    content(as = \"parsed\")\n  \n  map_dfr(\n    parsed_content$features,\n    ~ as_tibble(.x$attributes)\n  ) \n}\n\nAnd combine it all into a single data frame:\n\nbike_data &lt;- map_dfr(\n  seq(0, ceiling(n_records / 2000)),\n  ~ get_bike_data(offset = .x * 2000)\n)\n\n\nglimpse(bike_data)\n\nRows: 124,356\nColumns: 11\n$ CHANNEL_ID        &lt;chr&gt; \"100059339\", \"100059339\", \"100059339\", \"100059339\", ‚Ä¶\n$ CHANNEL_NAME      &lt;chr&gt; \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", ‚Ä¶\n$ SERIAL_NUMBER     &lt;chr&gt; \"X2H20032465\", \"X2H20032465\", \"X2H20032465\", \"X2H200‚Ä¶\n$ SITE_NAME         &lt;chr&gt; \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", ‚Ä¶\n$ LATITUDE          &lt;dbl&gt; 44.64799, 44.64799, 44.64799, 44.64799, 44.64799, 44‚Ä¶\n$ LONGITUDE         &lt;dbl&gt; -63.57352, -63.57352, -63.57352, -63.57352, -63.5735‚Ä¶\n$ INSTALLATION_DATE &lt;dbl&gt; 1.594166e+12, 1.594166e+12, 1.594166e+12, 1.594166e+‚Ä¶\n$ COUNT_DATETIME    &lt;dbl&gt; 1.595966e+12, 1.595970e+12, 1.595974e+12, 1.595977e+‚Ä¶\n$ COUNTER_TYPE      &lt;chr&gt; \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle‚Ä¶\n$ COUNTER_VALUE     &lt;int&gt; 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 6, 4, 13, 8, 5, 6, 8, ‚Ä¶\n$ ObjectId          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1‚Ä¶\n\n\nThis returned 124356 records, as expected. The ObjectId should be a unique sequential identifier from 1 to 124356, which I‚Äôll check:\n\nrange(bike_data$ObjectId); n_distinct(bike_data$ObjectId)\n\n[1]      1 124356\n\n\n[1] 124356\n\n\n\nEDA and cleaning\nFirst thing I usually do with a new data set is clean the column names:\n\nbike_data &lt;- janitor::clean_names(bike_data)\nglimpse(bike_data)\n\nRows: 124,356\nColumns: 11\n$ channel_id        &lt;chr&gt; \"100059339\", \"100059339\", \"100059339\", \"100059339\", ‚Ä¶\n$ channel_name      &lt;chr&gt; \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", ‚Ä¶\n$ serial_number     &lt;chr&gt; \"X2H20032465\", \"X2H20032465\", \"X2H20032465\", \"X2H200‚Ä¶\n$ site_name         &lt;chr&gt; \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", ‚Ä¶\n$ latitude          &lt;dbl&gt; 44.64799, 44.64799, 44.64799, 44.64799, 44.64799, 44‚Ä¶\n$ longitude         &lt;dbl&gt; -63.57352, -63.57352, -63.57352, -63.57352, -63.5735‚Ä¶\n$ installation_date &lt;dbl&gt; 1.594166e+12, 1.594166e+12, 1.594166e+12, 1.594166e+‚Ä¶\n$ count_datetime    &lt;dbl&gt; 1.595966e+12, 1.595970e+12, 1.595974e+12, 1.595977e+‚Ä¶\n$ counter_type      &lt;chr&gt; \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle‚Ä¶\n$ counter_value     &lt;int&gt; 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 6, 4, 13, 8, 5, 6, 8, ‚Ä¶\n$ object_id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1‚Ä¶\n\n\nNext I want to deal with the installation_date and count_datetime variables, which are very large integers. From fields above, the data type for these variables is esriFieldTypeDate. After some digging on Google, turns out this is Unix time (the number of milliseconds since January 1, 1970; also called epoch time). With as.POSIXct(), I can supply the number of seconds and set origin = \"1970-01-01\" to get back the correct datetime objects:\n\nbike_data &lt;- bike_data %&gt;%\n  mutate(\n    across(c(installation_date, count_datetime),\n           ~ as.POSIXct(.x / 1000, tz = \"UTC\", origin = \"1970-01-01\")),\n    # These are just dates, the time of day doesn't matter\n    installation_date = as.Date(installation_date),\n    # I'll also want the date without time of day\n    count_date = as.Date(count_datetime)\n  )\n\nThese variables are unique to the sites:\n\nbike_data %&gt;%\n  count(site_name, latitude, longitude, serial_number, installation_date,\n        counter_type, name = \"n_records\") %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nsite_name\nlatitude\nlongitude\nserial_number\ninstallation_date\ncounter_type\nn_records\n\n\n\n\nDartmouth Harbourfront Greenway\n44.66436\n-63.55736\nX2H20114473\n2021-07-08\nBicycle\n13976\n\n\nHollis St\n44.64799\n-63.57352\nX2H20032465\n2020-07-08\nBicycle\n15748\n\n\nSouth Park St\n44.64194\n-63.57972\nX2H19070467\n2019-09-01\nBicycle\n46424\n\n\nVernon St\n44.64292\n-63.59154\nX2H20114470\n2020-12-09\nBicycle\n24104\n\n\nWindsor St\n44.65466\n-63.60368\nX2H20114472\n2020-12-09\nBicycle\n24104\n\n\n\n\n\n\n\nDrop serial_number and counter_type, which aren‚Äôt useful.\n\nbike_data &lt;- bike_data %&gt;% select(-serial_number, -counter_type)\n\nSites can have multiple channels:\n\nbike_data %&gt;%\n  count(site_name, channel_name, channel_id, name = \"n_records\") %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nsite_name\nchannel_name\nchannel_id\nn_records\n\n\n\n\nDartmouth Harbourfront Greenway\nDartmouth Harbourfront Greenway Northbound\n353280085\n6988\n\n\nDartmouth Harbourfront Greenway\nDartmouth Harbourfront Greenway Southbound\n353280086\n6988\n\n\nHollis St\nHollis St\n100059339\n15748\n\n\nSouth Park St\nSouth Park St Northbound\n101054257\n23212\n\n\nSouth Park St\nSouth Park St Southbound\n102054257\n23212\n\n\nVernon St\nVernon St Northbound\n353252897\n12052\n\n\nVernon St\nVernon St Southbound\n353252898\n12052\n\n\nWindsor St\nWindsor St Northbound\n353252910\n12052\n\n\nWindsor St\nWindsor St Southbound\n353252909\n12052\n\n\n\n\n\n\n\nAll but the Hollis St site has separate northbound and southbound channels.\nFor each site, check the installation_date relative to the range of count_date:\n\nbike_data %&gt;%\n  group_by(site_name, installation_date) %&gt;%\n  summarise(min_count_date = min(count_date), max_count_date = max(count_date),\n            .groups = \"drop\") %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nsite_name\ninstallation_date\nmin_count_date\nmax_count_date\n\n\n\n\nDartmouth Harbourfront Greenway\n2021-07-08\n2021-07-08\n2022-04-25\n\n\nHollis St\n2020-07-08\n2020-07-08\n2022-04-25\n\n\nSouth Park St\n2019-09-01\n2019-09-01\n2022-04-25\n\n\nVernon St\n2020-12-09\n2020-12-09\n2022-04-25\n\n\nWindsor St\n2020-12-09\n2020-12-09\n2022-04-25\n\n\n\n\n\n\n\nEverything is nicely aligned: the first data corresponds to the installation date, and the last data corresponds to the date the data were retrieved.\nPlot the position of each of the counters using the given latitude and longitude, overlaid on a map of Halifax with the ggmap package:1\n\nlibrary(ggmap)\nsite_locs &lt;- bike_data %&gt;%\n  distinct(site_name, lat = latitude, lon = longitude)\n\nmean_lat &lt;- mean(site_locs$lat)\nmean_lon &lt;- mean(site_locs$lon)\n\n\nhalifax_map &lt;- get_googlemap(c(mean_lon, mean_lat),\n                             zoom = 14, maptype = \"satellite\")\n\n\nggmap(halifax_map) +\n  geom_point(data = site_locs, size = 4,\n             aes(fill = site_name), shape = 21, color = \"white\") +\n  ggrepel::geom_label_repel(\n    data = site_locs,\n    aes(color = site_name, label = str_trunc(site_name, width = 25)),\n    box.padding = 1.0\n  ) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nFor each site and channel, get the time of day from count_datetime to determine the frequency of collection:\n\nbike_data %&gt;%\n  mutate(time_of_day = format(count_datetime, \"%H:%M:%S\")) %&gt;%\n  count(site_name, time_of_day, name = \"n_records\") %&gt;%\n  ggplot(aes(y = time_of_day, x = n_records)) +\n  geom_col() +\n  facet_wrap(~ str_trunc(site_name, 15), nrow = 1) +\n  scale_x_continuous(expand = c(0, 0), breaks = c(0, 500, 1000, 1500)) +\n  dunnr::add_facet_borders()\n\n\n\n\nEach counter reports observations at the hour mark (+1 second). There are some slight difference in the number of records due to the time of day I retrieved the data.\nI made an assumption that the count_datetime variable was in UTC timezone. I can check this assumption by looking at average counts (over the entire data set).\n\nbike_data_tod &lt;- bike_data %&gt;%\n  mutate(\n    time_of_day = format(count_datetime, \"%H:%M:%S\"),\n    # Create a dummy variable with arbitrary date so I can plot time of day\n    time_of_day = lubridate::ymd_hms(\n      paste0(\"2022-04-22 \", time_of_day)\n    )\n  )\nbike_data_tod %&gt;%\n  group_by(site_name, time_of_day) %&gt;%\n  summarise(\n    n = n(), mean_count = mean(counter_value),\n    .groups = \"drop\"\n  ) %&gt;%\n  ggplot(aes(x = time_of_day, y = mean_count)) +\n  geom_area(fill = td_colors$nice$mellow_yellow, color = \"black\") +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  scale_x_datetime(date_breaks = \"2 hours\", date_labels = \"%H\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  dunnr::add_facet_borders()\n\n\n\n\nThese peaks at around 8AM and 5PM tell me that the data is actually recorded in the local time zone (Atlantic), not UTC like I assumed. If they were in UTC time, the peaks would correspond to 11AM and 8PM locally, which would be odd times for peak cyclists.\nAny interesting trends in different channels?\n\nbike_data_tod %&gt;%\n  # Remove Hollis St, which does not have different channels\n  filter(site_name != \"Hollis St\") %&gt;%\n  mutate(channel_direction = str_extract(channel_name, \"(North|South)bound\")) %&gt;%\n  group_by(site_name, channel_direction, time_of_day) %&gt;%\n  summarise(mean_count = mean(counter_value), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = time_of_day, y = mean_count, color = channel_direction)) +\n  geom_line() +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  scale_x_datetime(date_breaks = \"2 hours\", date_labels = \"%H\") +\n  theme(legend.position = \"top\")\n\n\n\n\nVernon St and Windsor St counters have higher traffic Southbound (heading downtown) at the start of the typical workday, and higher traffic Northbound (leaving downtown) at the end of the typical workday.\nI am less interested in counts over the course of a day or by channel, and more interested in daily counts. Now that I know the count_date is correctly converted with the local time, get the sum at each site and each 24 hour day:\n\nbike_data_daily_counts &lt;- bike_data %&gt;%\n  group_by(site_name, installation_date, count_date) %&gt;%\n  summarise(\n    n_records = n(), n_bikes = sum(counter_value), .groups = \"drop\"\n  )\n\nNow plot counts per day at each site:\n\nbike_data_daily_counts %&gt;%\n  ggplot(aes(x = count_date, y = n_bikes)) +\n  geom_line() +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  dunnr::add_facet_borders()\n\n\n\n\nThe seasonal trends are very obvious from this plot. One thing that stood out to me is the big increase from 2020 to 2021 on South Park St. It may be representative of the start of the COVID pandemic, but I think it also has to do with the addition of protected bike lanes in December 2020. Before 2020, there appears to be a series of 0 counts on South Park St which may be artifacts:\n\nbike_data_daily_counts %&gt;%\n  filter(site_name == \"South Park St\", count_date &lt; \"2020-01-01\") %&gt;%\n  ggplot(aes(x = count_date, y = n_bikes)) +\n  geom_line()\n\n\n\n\nI‚Äôm almost certain this series of zeroes is not real, so I‚Äôll remove it from the data. Find the date of the first non-zero n_bikes at this site, and filter out data before then:\n\nsouth_park_min_date &lt;- bike_data_daily_counts %&gt;%\n  filter(site_name == \"South Park St\", n_bikes &gt; 0) %&gt;%\n  pull(count_date) %&gt;%\n  min()\nsouth_park_min_date\n\n[1] \"2019-11-23\"\n\nbike_data_daily_counts &lt;- bike_data_daily_counts %&gt;%\n  filter(!((site_name == \"South Park St\") & (count_date &lt; south_park_min_date)))\n\nOverlay counts by year for each site:\n\nbike_data_daily_counts %&gt;%\n  mutate(count_year = year(count_date),\n         # Replace year with 1970 so I can plot on the same scale\n         count_date = as.Date(yday(count_date), origin = \"1970-01-01\")) %&gt;%\n  ggplot(aes(x = count_date, y = n_bikes, color = factor(count_year))) +\n  geom_line(size = 1, alpha = 0.8) +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  scale_x_date(date_labels = \"%B\") +\n  dunnr::add_facet_borders() +\n  theme(legend.position = \"bottom\") +\n  labs(x = NULL, color = \"Year\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nI‚Äôm interested in day of the week effects as well:\n\nbike_data_daily_counts %&gt;%\n  mutate(day_of_week = wday(count_date, label = TRUE)) %&gt;%\n  ggplot(aes(y = day_of_week, x = n_bikes)) +\n  geom_boxplot()\n\n\n\n\nLess activity on the weekends."
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#getting-weather-data",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#getting-weather-data",
    "title": "Predicting bike ridership: getting the data",
    "section": "Getting weather data",
    "text": "Getting weather data\nTemporal data are probably the most important predictors of ridership, but I‚Äôm sure a close second is the day‚Äôs weather. I‚Äôll get this with the API provided by the Meteorological Service of Canada. I can get a list of available data sets (which they call collections) as follows:2\n\nbase_url &lt;- \"https://api.weather.gc.ca/\"\nresp &lt;- httr::GET(paste0(base_url, \"collections?f=json\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 2\n $ collections:List of 70\n $ links      :List of 3\n\n\nThe first element of collections:\n\ncollections &lt;- content_parsed$collections\nstr(collections[[1]], max.level = 2)\n\nList of 7\n $ id         : chr \"hydrometric-stations\"\n $ title      : chr \"Hydrometric Monitoring Stations\"\n $ description: chr \"A station is a site on a river or lake where water quantity (water level and flow) are collected and recorded.\"\n $ keywords   :List of 2\n  ..$ : chr \"station\"\n  ..$ : chr \"hydrometric station\"\n $ links      :List of 13\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n $ extent     :List of 2\n  ..$ spatial :List of 2\n  ..$ temporal:List of 1\n $ itemType   : chr \"feature\"\n\n\nUnlike the bicycle counts data, this nested format doesn‚Äôt lend itself well to direct conversion to a tibble:\n\nas_tibble(collections[[1]])\n\nError:\n! Tibble columns must have compatible sizes.\n‚Ä¢ Size 2: Columns `keywords` and `extent`.\n‚Ä¢ Size 13: Column `links`.\n‚Ñπ Only values of size one are recycled.\n\n\nInstead, I can use enframe() to get a two-column data frame:\n\nenframe(collections[[1]])\n\n# A tibble: 7 √ó 2\n  name        value           \n  &lt;chr&gt;       &lt;list&gt;          \n1 id          &lt;chr [1]&gt;       \n2 title       &lt;chr [1]&gt;       \n3 description &lt;chr [1]&gt;       \n4 keywords    &lt;list [2]&gt;      \n5 links       &lt;list [13]&gt;     \n6 extent      &lt;named list [2]&gt;\n7 itemType    &lt;chr [1]&gt;       \n\n\nAssuming every item in the collections list has the same structure, I‚Äôll just extract the id, title, and description:\n\ncollections_df &lt;-  map_dfr(\n  collections,\n  ~ enframe(.x) %&gt;%\n    filter(name %in% c(\"id\", \"title\", \"description\")) %&gt;%\n    pivot_wider(names_from = name, values_from = value)\n)\ngt(collections_df) %&gt;%\n  tab_options(container.height = 300, container.overflow.y = TRUE)\n\n\n\n\n\n\n\n\nid\ntitle\ndescription\n\n\n\n\nhydrometric-stations\nHydrometric Monitoring Stations\nA station is a site on a river or lake where water quantity (water level and flow) are collected and recorded.\n\n\nhydrometric-daily-mean\nDaily Mean of Water Level or Flow\nThe daily mean is the average of all unit values for a given day.\n\n\nhydrometric-monthly-mean\nMonthly Mean of Water Level or Flow\nThe monthly mean is the average of daily mean values for a given month.\n\n\nhydrometric-annual-statistics\nAnnual Maximum and Minimum Daily Water Level or Flow\nThe annual maximum and minimum daily data are the maximum and minimum daily mean values for a given year.\n\n\nhydrometric-annual-peaks\nAnnual Maximum and Minimum Instantaneous Water Level or Flow\nThe annual maximum and minimum instantaneous data are the maximum and minimum instantaneous values for a given year.\n\n\nhydrometric-realtime\nReal-time hydrometric data\nReal-time water level and flow (discharge) data collected at over 2100 hydrometric stations across Canada (last 30 days).\n\n\nclimate-normals\n1981-2010 Climate Normals\nClimate Normals are used to summarize or describe the average climatic conditions of a particular location. At the completion of each decade, Environment and Climate Change Canada updates its climate normals for as many locations and as many climatic characteristics as possible. The climate normals offered here are based on Canadian climate stations with at least 15 years of data between 1981 to 2010.\n\n\nclimate-stations\nClimate Stations\nClimate observations are derived from two sources of data. The first are Daily Climate Stations producing one or two observations per day of temperature, precipitation. The second are hourly stations that typically produce more weather elements e.g. wind or snow on ground.\n\n\nclimate-monthly\nMonthly Climate Observation Summaries\nA cross-country summary of the averages and extremes for the month, including precipitation totals, max-min temperatures, and degree days. This data is available from stations that produce daily data.\n\n\nclimate-daily\nDaily Climate Observations\nDaily climate observations are derived from two sources of data. The first are Daily Climate Stations producing one or two observations per day of temperature, precipitation. The second are hourly stations that typically produce more weather elements e.g. wind or snow on ground. Only a subset of the total stations is shown due to size limitations. The criteria for station selection are listed as below. The priorities for inclusion are as follows: (1) Station is currently operational, (2) Stations with long periods of record, (3) Stations that are co-located with the categories above and supplement the period of record\n\n\nclimate-hourly\nHourly Climate Observations\nHourly climate observations are derived from the data source HLY01. These are stations that produce hourly meteorological observations, taken each hour of the day for the hours 00h-23h, for both daily (temperature, precipitation) and non-daily elements (station pressure, relative humidity, visibility). Only a subset of the total stations are shown due to size limitations. The stations were selected based on the following criteria: (1) Stations near cities with populations greater than 10,000, (2) Stations with long periods of record of at least 30 years of data, (3) Stations selected for the 1991-2020 WMO Normals, (4) Stations with RBCN designation not selected for 1991-2020 WMO Normals, and/or (5) Stations whose sum when joined with co-located/successor stations equates to a period of record of 30 years or greater.\n\n\nahccd-stations\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Stations\nClimate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation.\n\n\nahccd-annual\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Annual\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n\n\nahccd-seasonal\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Seasonal\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n\n\nahccd-monthly\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Monthly\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n\n\nahccd-trends\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Trends\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n\n\nswob-realtime\nSurface Weather Observations\nSurface Observations measured at the automatic and manual stations of the Environment and Climate Change Canada and partners networks, either for a single station, or for the stations of specific provinces and territories (last 30 days)\n\n\nltce-stations\nVirtual Climate Stations (LTCE)\nA Virtual Climate station is the result of threading together climate data from proximate current and historical stations to construct a long term threaded data set. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. The length of the time series of virtual stations is often greater than 100 years. A Virtual Climate station is always named for an ‚ÄúArea‚Äù rather than a point, e.g. Winnipeg Area, to indicate that the data are drawn from that area(within a 20km radius from the urban center) rather than a single precise location.\n\n\nltce-temperature\nDaily Extremes of Records (LTCE) ‚Äì Temperature\nAnomalous weather resulting in Temperature and Precipitation extremes occurs almost every day somewhere in Canada. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. Virtual Climate stations correspond with the city pages of weather.gc.ca. This data provides the daily extremes of record for Temperature for each day of the year. Daily elements include: High Maximum, Low Maximum, High Minimum, Low Minimum.\n\n\nltce-precipitation\nDaily Extremes of Records (LTCE) ‚Äì Precipitation\nAnomalous weather resulting in Temperature and Precipitation extremes occurs almost every day somewhere in Canada. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. Virtual Climate stations correspond with the city pages of weather.gc.ca. This data provides the daily extremes of record for Precipitation for each day of the year. Daily elements include: Greatest Precipitation.\n\n\nltce-snowfall\nDaily Extremes of Records (LTCE) ‚Äì Snowfall\nAnomalous weather resulting in Temperature and Precipitation extremes occurs almost every day somewhere in Canada. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. Virtual Climate stations correspond with the city pages of weather.gc.ca. This data provides the daily extremes of record for Snowfall for each day of the year. Daily elements include: Greatest Snowfall.\n\n\naqhi-forecasts-realtime\nAir Quality Health Index ‚Äì Forecasts\nThe Air Quality Health Index (AQHI) is a scale designed to help quantify the quality of the air in a certain region on a scale from 1 to 10. When the amount of air pollution is very high, the number is reported as 10+. It also includes a category that describes the health risk associated with the index reading (e.g. Low, Moderate, High, or Very High Health Risk). The AQHI is calculated based on the relative risks of a combination of common air pollutants that are known to harm human health, including ground-level ozone, particulate matter, and nitrogen dioxide. The AQHI formulation captures only the short term or acute health risk (exposure of hour or days at a maximum). The formulation of the AQHI may change over time to reflect new understanding associated with air pollution health effects. The AQHI is calculated from data observed in real time, without being verified (quality control).\n\n\naqhi-observations-realtime\nAir Quality Health Index ‚Äì Observations\nThe Air Quality Health Index (AQHI) is a scale designed to help quantify the quality of the air in a certain region on a scale from 1 to 10. When the amount of air pollution is very high, the number is reported as 10+. It also includes a category that describes the health risk associated with the index reading (e.g. Low, Moderate, High, or Very High Health Risk). The AQHI is calculated based on the relative risks of a combination of common air pollutants that are known to harm human health, including ground-level ozone, particulate matter, and nitrogen dioxide. The AQHI formulation captures only the short term or acute health risk (exposure of hour or days at a maximum). The formulation of the AQHI may change over time to reflect new understanding associated with air pollution health effects. The AQHI is calculated from data observed in real time, without being verified (quality control).\n\n\nbulletins-realtime\nReal-time meteorological bulletins\nReal-time meteorological bulletins (last 140 days)\n\n\nclimate:cmip5:projected:annual:anomaly\nProjected annual anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:projected:seasonal:anomaly\nProjected seasonal anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:projected:monthly:anomaly\nProjected monthly anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:projected:annual:absolute\nProjected annual CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:projected:seasonal:absolute\nProjected seasonal CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:projected:monthly:absolute\nProjected monthly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:projected:annual:P20Y-Avg\nProjected annual anomaly for 20 years average CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:projected:seasonal:P20Y-Avg\nProjected seasonal anomaly for 20 years average CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:historical:annual:absolute\nHistorical annual CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:historical:annual:anomaly\nHistorical annual anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:historical:seasonal:absolute\nHistorical seasonal CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:historical:seasonal:anomaly\nHistorical seasonal anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:historical:monthly:absolute\nHistorical monthly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:cmip5:historical:monthly:anomaly\nHistorical monthly anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n\n\nclimate:dcs:projected:annual:anomaly\nProjected annual anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:dcs:projected:seasonal:anomaly\nProjected seasonal anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:dcs:projected:annual:absolute\nProjected annual DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:dcs:projected:seasonal:absolute\nProjected seasonal DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:dcs:projected:monthly:absolute\nProjected monthly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:dcs:projected:annual:P20Y-Avg\nProjected annual anomaly for 20 years average DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:dcs:projected:seasonal:P20Y-Avg\nProjected seasonal anomaly for 20 years average DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:dcs:historical:annual:absolute\nHistorical annual DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:dcs:historical:annual:anomaly\nHistorical annual anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:dcs:historical:seasonal:absolute\nHistorical seasonal DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:dcs:historical:seasonal:anomaly\nHistorical seasonal anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:dcs:historical:monthly:absolute\nHistorical monthly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n\n\nclimate:indices:historical\nHistorical indices\nHigh-resolution statistically downscaled climate indices relevant to climate change impacts in Canada are available at a 10 km spatial resolution and an annual temporal resolution for 1951-2100. The climate indices are based on model projections from 24 global climate models (GCMs) that participated in the Coupled Model Intercomparison Project Phase 5 (CMIP5).\n\n\nclimate:indices:projected\nProjected indices\nHigh-resolution statistically downscaled climate indices relevant to climate change impacts in Canada are available at a 10 km spatial resolution and an annual temporal resolution for 1951-2100. The climate indices are based on model projections from 24 global climate models (GCMs) that participated in the Coupled Model Intercomparison Project Phase 5 (CMIP5).\n\n\nclimate:spei-1:historical\nHistorical SPEI-1\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n\n\nclimate:spei-3:historical\nHistorical SPEI-3\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n\n\nclimate:spei-12:historical\nHistorical SPEI-12\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n\n\nclimate:spei-1:projected\nProjected SPEI-1\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n\n\nclimate:spei-3:projected\nProjected SPEI-3\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n\n\nclimate:spei-12:projected\nProjected SPEI-12\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n\n\nclimate:cangrd:historical:annual:trend\nCanGRD historical annual trend\nCANGRD data are interpolated from adjusted and homogenized climate station data (i.e., AHCCD datasets). Homogenized climate data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Annual trends of relative total precipitation change (%) for 1948-2012 based on Canadian gridded data (CANGRD) are available, at a 50km resolution across Canada. The relative trends reflect the percent change in total precipitation over a period from the baseline value (defined as the average over 1961-1990 as the reference period). Annual trends of mean surface air temperature change (degrees Celsius) for 1948-2016 based on Canadian gridded data (CANGRD) are available at a 50km resolution across Canada. Temperature trends represent the departure from a mean reference period (1961-1990).\n\n\nclimate:cangrd:historical:annual:anomaly\nCanGRD historical annual anomaly\nGridded annual mean temperature anomalies derived from daily minimum, maximum and mean surface air temperatures (degrees Celsius) and anomalies derived from daily total precipitation is available at a 50km resolution across Canada. The Canadian gridded data (CANGRD) are interpolated from homogenized temperature (i.e., AHCCD datasets). Homogenized temperatures incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the difference between the temperature for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal temperature anomalies were computed for the years 1948 to 2017. The data will continue to be updated every year. For precipitation, the Canadian gridded data (CANGRD) are interpolated from adjusted precipitation (i.e., AHCCD datasets). Adjusted precipitation data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the percentage difference between the value for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal relative precipitation anomalies were computed for the years 1948 to 2014. The data will be updated as time permits.\n\n\nclimate:cangrd:historical:monthly:anomaly\nCanGRD historical monthly anomaly\nGridded monthly mean temperature anomalies derived from daily minimum, maximum and mean surface air temperatures (degrees Celsius) and anomalies derived from daily total precipitation is available at a 50km resolution across Canada. The Canadian gridded data (CANGRD) are interpolated from homogenized temperature (i.e., AHCCD datasets). Homogenized temperatures incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the difference between the temperature for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal temperature anomalies were computed for the years 1948 to 2017. The data will continue to be updated every year. For precipitation, the Canadian gridded data (CANGRD) are interpolated from adjusted precipitation (i.e., AHCCD datasets). Adjusted precipitation data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the percentage difference between the value for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal relative precipitation anomalies were computed for the years 1948 to 2014. The data will be updated as time permits.\n\n\nclimate:cangrd:historical:seasonal:trend\nCanGRD historical seasonal trend\nCANGRD data are interpolated from adjusted and homogenized climate station data (i.e., AHCCD datasets). Homogenized climate data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation.Seasonal trends of relative total precipitation change (%) for 1948-2012 based on Canadian gridded data (CANGRD) are available, at a 50km resolution across Canada. The relative trends reflect the percent change in total precipitation over a period from the baseline value (defined as the average over 1961-1990 as the reference period). Seasonal trends of mean surface air temperature change (degrees Celsius) for 1948-2016 based on Canadian gridded data (CANGRD) are available at a 50km resolution across Canada. Temperature trends represent the departure from a mean reference period (1961-1990).\n\n\nclimate:cangrd:historical:seasonal:anomaly\nCanGRD historical seasonal anomaly\nGridded seasonal mean temperature anomalies derived from daily minimum, maximum and mean surface air temperatures (degrees Celsius) and anomalies derived from daily total precipitation is available at a 50km resolution across Canada. The Canadian gridded data (CANGRD) are interpolated from homogenized temperature (i.e., AHCCD datasets). Homogenized temperatures incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the difference between the temperature for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal temperature anomalies were computed for the years 1948 to 2017. The data will continue to be updated every year. For precipitation, the Canadian gridded data (CANGRD) are interpolated from adjusted precipitation (i.e., AHCCD datasets). Adjusted precipitation data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the percentage difference between the value for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal relative precipitation anomalies were computed for the years 1948 to 2014. The data will be updated as time permits.\n\n\nweather:rdpa:15km:24f\nRegional Deterministic Precipitation Analysis (RDPA) 24 hours accumulation at 15km\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 24 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 15 km. Data is only available for the surface level. Analysis data is made available once a day for the 24h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n\n\nweather:rdpa:15km:6f\nRegional Deterministic Precipitation Analysis (RDPA) 6 hours accumulation at 15 km\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 6 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 15 km. Data is only available for the surface level. Analysis data is made available four times a day for the 6h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n\n\nweather:rdpa:10km:24f\nRegional Deterministic Precipitation Analysis (RDPA) 24 hours accumulation\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 24 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available once a day for the 24h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n\n\nweather:rdpa:10km:6f\nRegional Deterministic Precipitation Analysis (RDPA) 6 hours accumulation\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 6 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available four times a day for the 6h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n\n\nweather:rdpa:10km:24p\nRegional Deterministic Precipitation Analysis (RDPA) 24 hours accumulation (preliminary)\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 24 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available once a day for the 24h intervals. The preliminary estimate is available approximately 1h after the end of the accumulation period.\n\n\nweather:rdpa:10km:6p\nRegional Deterministic Precipitation Analysis (RDPA) 6 hours accumulation (preliminary)\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 6 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available four times a day for 6h intervals. The preliminary estimate is available approximately 1h after the end of the accumulation period.\n\n\nweather:cansips:250km:forecast:members\nCanadian Seasonal to Inter-annual Prediction System\nThe Canadian Seasonal to Inter-annual Prediction System (CanSIPS) carries out physics calculations to arrive at probabilistic predictions of atmospheric elements from the beginning of a month out to up to 12 months into the future. Atmospheric elements include temperature, precipitation, wind speed and direction and others. This product contains raw numerical results of these calculations. Geographical coverage is global. Data is available on a grid at a horizontal resolution of 2.5 degrees and for a few selected vertical levels. Predictions and corresponding hindcast are made available monthly.\n\n\n\n\n\n\n\nThe collections I want for this project are climate-stations (to find the appropriate Halifax station) and climate-daily to get daily measurements at that station. Get climate-stations:\n\nresp &lt;- httr::GET(paste0(base_url, \"collections/climate-stations/items?f=json\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 6\n $ type          : chr \"FeatureCollection\"\n $ features      :List of 500\n $ numberMatched : int 8552\n $ numberReturned: int 500\n $ links         :List of 5\n $ timeStamp     : chr \"2022-04-27T03:27:26.475648Z\"\n\n\nBefore looking closer at the data, I can already tell I‚Äôll want to increase the limit of returned entries. From the API documentation, the maximum number is 10000, so I can get all 8552 records in one API call:\n\nresp &lt;- GET(paste0(base_url,\n                   \"collections/climate-stations/items?f=json&limit=10000\"))\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 6\n $ type          : chr \"FeatureCollection\"\n $ features      :List of 8552\n $ numberMatched : int 8552\n $ numberReturned: int 8552\n $ links         :List of 4\n $ timeStamp     : chr \"2022-04-27T03:27:27.265491Z\"\n\n\nThe data is contained in the features list:\n\nclimate_stations &lt;- content_parsed$features\nstr(climate_stations[[1]], max.level = 3)\n\nList of 4\n $ type      : chr \"Feature\"\n $ properties:List of 32\n  ..$ STN_ID                  : int 8496\n  ..$ STATION_NAME            : chr \"CARLETON SUR MER\"\n  ..$ PROV_STATE_TERR_CODE    : chr \"QC\"\n  ..$ ENG_PROV_NAME           : chr \"QUEBEC\"\n  ..$ FRE_PROV_NAME           : chr \"QU√âBEC\"\n  ..$ COUNTRY                 : chr \"CAN\"\n  ..$ LATITUDE                : int 480800000\n  ..$ LONGITUDE               : int -660700000\n  ..$ TIMEZONE                : chr \"EST\"\n  ..$ ELEVATION               : chr \"541.00\"\n  ..$ CLIMATE_IDENTIFIER      : chr \"705AA86\"\n  ..$ TC_IDENTIFIER           : NULL\n  ..$ WMO_IDENTIFIER          : NULL\n  ..$ STATION_TYPE            : chr \"N/A\"\n  ..$ NORMAL_CODE             : NULL\n  ..$ PUBLICATION_CODE        : int 1\n  ..$ DISPLAY_CODE            : int 9\n  ..$ ENG_STN_OPERATOR_ACRONYM: NULL\n  ..$ FRE_STN_OPERATOR_ACRONYM: NULL\n  ..$ ENG_STN_OPERATOR_NAME   : NULL\n  ..$ FRE_STN_OPERATOR_NAME   : NULL\n  ..$ FIRST_DATE              : chr \"1968-10-01 00:00:00\"\n  ..$ LAST_DATE               : chr \"1968-10-31 00:00:00\"\n  ..$ HLY_FIRST_DATE          : NULL\n  ..$ HLY_LAST_DATE           : NULL\n  ..$ DLY_FIRST_DATE          : chr \"1968-10-01 00:00:00\"\n  ..$ DLY_LAST_DATE           : chr \"1968-10-31 00:00:00\"\n  ..$ MLY_FIRST_DATE          : NULL\n  ..$ MLY_LAST_DATE           : NULL\n  ..$ HAS_MONTHLY_SUMMARY     : chr \"Y\"\n  ..$ HAS_NORMALS_DATA        : chr \"N\"\n  ..$ HAS_HOURLY_DATA         : chr \"N\"\n $ geometry  :List of 2\n  ..$ type       : chr \"Point\"\n  ..$ coordinates:List of 2\n  .. ..$ : num -66.1\n  .. ..$ : num 48.1\n $ id        : chr \"705AA86\"\n\n\nAfter some frustration, I found that the geometry$coordinates are the correct latitude/longitude ‚Äì those in the properties list are slightly off for some reason. Extract the data:\n\nclimate_stations &lt;- map_dfr(\n  climate_stations,\n  ~ discard(.x$properties, is.null) %&gt;% as_tibble() %&gt;%\n    mutate(lat = .x$geometry$coordinates[[2]],\n           lon = .x$geometry$coordinates[[1]])\n) %&gt;%\n  janitor::clean_names() %&gt;%\n  # Drop the incorrect latitude and longitude\n  select(-latitude, -longitude)\n\nglimpse(climate_stations)\n\nRows: 8,552\nColumns: 32\n$ stn_id                   &lt;int&gt; 8496, 9005, 10205, 6149, 6154, 6174, 6177, 61‚Ä¶\n$ station_name             &lt;chr&gt; \"CARLETON SUR MER\", \"PORT COLBORNE (AUT)\", \"K‚Ä¶\n$ prov_state_terr_code     &lt;chr&gt; \"QC\", \"ON\", \"QC\", \"NB\", \"NB\", \"NB\", \"NB\", \"NB‚Ä¶\n$ eng_prov_name            &lt;chr&gt; \"QUEBEC\", \"ONTARIO\", \"QUEBEC\", \"NEW BRUNSWICK‚Ä¶\n$ fre_prov_name            &lt;chr&gt; \"QU√âBEC\", \"ONTARIO\", \"QU√âBEC\", \"NOUVEAU-BRUNS‚Ä¶\n$ country                  &lt;chr&gt; \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CA‚Ä¶\n$ timezone                 &lt;chr&gt; \"EST\", \"EST\", \"EST\", \"AST\", \"AST\", \"AST\", \"AS‚Ä¶\n$ elevation                &lt;chr&gt; \"541.00\", \"183.50\", \"123.80\", \"152.40\", \"173.‚Ä¶\n$ climate_identifier       &lt;chr&gt; \"705AA86\", \"613F606\", \"7113382\", \"8101178\", \"‚Ä¶\n$ station_type             &lt;chr&gt; \"N/A\", \"Climate-Auto\", \"N/A\", \"N/A\", \"N/A\", \"‚Ä¶\n$ publication_code         &lt;int&gt; 1, NA, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ display_code             &lt;int&gt; 9, NA, NA, 7, 9, 5, 9, 7, 7, 9, 9, 8, 9, 9, 1‚Ä¶\n$ first_date               &lt;chr&gt; \"1968-10-01 00:00:00\", \"1992-12-02 00:00:00\",‚Ä¶\n$ last_date                &lt;chr&gt; \"1968-10-31 00:00:00\", \"2022-04-24 12:30:02\",‚Ä¶\n$ dly_first_date           &lt;chr&gt; \"1968-10-01 00:00:00\", \"1992-12-02 00:00:00\",‚Ä¶\n$ dly_last_date            &lt;chr&gt; \"1968-10-31 00:00:00\", \"2022-04-24 00:00:00\",‚Ä¶\n$ has_monthly_summary      &lt;chr&gt; \"Y\", \"Y\", \"N\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", ‚Ä¶\n$ has_normals_data         &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", ‚Ä¶\n$ has_hourly_data          &lt;chr&gt; \"N\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", ‚Ä¶\n$ lat                      &lt;dbl&gt; 48.13333, 42.86667, 60.02306, 45.93333, 47.36‚Ä¶\n$ lon                      &lt;dbl&gt; -66.11667, -79.25000, -70.00361, -64.78333, -‚Ä¶\n$ tc_identifier            &lt;chr&gt; NA, \"WPC\", \"YAS\", NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ wmo_identifier           &lt;chr&gt; NA, \"71463\", NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ eng_stn_operator_acronym &lt;chr&gt; NA, \"ECCC - MSC\", \"DND\", NA, NA, NA, NA, NA, ‚Ä¶\n$ fre_stn_operator_acronym &lt;chr&gt; NA, \"ECCC - SMC\", \"MDN\", NA, NA, NA, NA, NA, ‚Ä¶\n$ eng_stn_operator_name    &lt;chr&gt; NA, \"Environment and Climate Change Canada - ‚Ä¶\n$ fre_stn_operator_name    &lt;chr&gt; NA, \"Environnement et Changement climatique C‚Ä¶\n$ hly_first_date           &lt;chr&gt; NA, \"1994-02-01 02:00:00\", \"1992-10-21 07:00:‚Ä¶\n$ hly_last_date            &lt;chr&gt; NA, \"2022-04-24 12:30:02\", \"2015-09-10 13:00:‚Ä¶\n$ mly_first_date           &lt;chr&gt; NA, \"2006-04-01 00:00:00\", NA, \"1964-01-01 00‚Ä¶\n$ mly_last_date            &lt;chr&gt; NA, \"2006-12-01 00:00:00\", NA, \"1979-12-01 00‚Ä¶\n$ normal_code              &lt;chr&gt; NA, NA, NA, \"F\", NA, \"D\", NA, \"F\", \"F\", NA, N‚Ä¶\n\n\nNow I‚Äôll filter this list down to those in Halifax, NS using distance to the bike counter latitude/longitude means:\n\nclimate_stations_halifax &lt;- climate_stations %&gt;%\n  filter(prov_state_terr_code == \"NS\") %&gt;%\n  mutate(\n    # Compare to the mean lat/lon from the bike counters\n    diff_lat = abs(lat - mean_lat), diff_lon = abs(lon - mean_lon),\n    # Use squared distance to determine the closest points\n    diff2 = diff_lat^2 + diff_lon^2\n  ) %&gt;%\n  # Look at the top 5 for now\n  slice_min(diff2, n = 5)\n\nclimate_stations_halifax %&gt;% rmarkdown::paged_table()\n\n\n\n  \n\n\n\nVisualize the locations of the stations and bike counters:\n\nd &lt;- bind_rows(\n  site_locs %&gt;% mutate(group = \"bike counters\", label = site_name),\n  climate_stations_halifax %&gt;%\n    transmute(label = glue(\"{station_name} ({stn_id})\"),\n              lat, lon, diff2, group = \"climate stations\")\n)\n  \nggmap(halifax_map) +\n  geom_point(data = d, size = 4,\n             aes(fill = group), shape = 21, color = \"white\") +\n  ggrepel::geom_label_repel(\n    data = d,\n    aes(color = group, label = str_trunc(label, width = 25)),\n    box.padding = 2\n  ) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nHalifax Citadel is the closest to the center, but last_date is 2002-01-31 for this station, so it hasn‚Äôt been active for the past two decades. The next closest is the dockyard, which is actively being updated (last_date is 2022-04-24).\nNow with the station name (‚ÄúHALIFAX DOCKYARD‚Äù), I can request the daily climate reports:\n\nresp &lt;- GET(\n  paste0(\n    base_url,\n    \"collections/climate-daily/items?f=json&limit=10000&STATION_NAME=HALIFAX%20DOCKYARD\"\n  )\n)\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 6\n $ type          : chr \"FeatureCollection\"\n $ features      :List of 1399\n $ numberMatched : int 1399\n $ numberReturned: int 1399\n $ links         :List of 4\n $ timeStamp     : chr \"2022-04-27T03:27:56.527206Z\"\n\n\nThe features data:\n\ndaily_climate &lt;- content_parsed$features\nstr(daily_climate[[1]])\n\nList of 4\n $ id        : chr \"8202240.2021.2.11\"\n $ type      : chr \"Feature\"\n $ geometry  :List of 2\n  ..$ coordinates:List of 2\n  .. ..$ : num -63.6\n  .. ..$ : num 44.7\n  ..$ type       : chr \"Point\"\n $ properties:List of 34\n  ..$ STATION_NAME            : chr \"HALIFAX DOCKYARD\"\n  ..$ CLIMATE_IDENTIFIER      : chr \"8202240\"\n  ..$ ID                      : chr \"8202240.2021.2.11\"\n  ..$ LOCAL_DATE              : chr \"2021-02-11 00:00:00\"\n  ..$ PROVINCE_CODE           : chr \"NS\"\n  ..$ LOCAL_YEAR              : int 2021\n  ..$ LOCAL_MONTH             : int 2\n  ..$ LOCAL_DAY               : int 11\n  ..$ MEAN_TEMPERATURE        : num -7.5\n  ..$ MEAN_TEMPERATURE_FLAG   : NULL\n  ..$ MIN_TEMPERATURE         : num -9.9\n  ..$ MIN_TEMPERATURE_FLAG    : NULL\n  ..$ MAX_TEMPERATURE         : num -5.1\n  ..$ MAX_TEMPERATURE_FLAG    : NULL\n  ..$ TOTAL_PRECIPITATION     : NULL\n  ..$ TOTAL_PRECIPITATION_FLAG: NULL\n  ..$ TOTAL_RAIN              : NULL\n  ..$ TOTAL_RAIN_FLAG         : NULL\n  ..$ TOTAL_SNOW              : NULL\n  ..$ TOTAL_SNOW_FLAG         : NULL\n  ..$ SNOW_ON_GROUND          : NULL\n  ..$ SNOW_ON_GROUND_FLAG     : NULL\n  ..$ DIRECTION_MAX_GUST      : int 28\n  ..$ DIRECTION_MAX_GUST_FLAG : NULL\n  ..$ SPEED_MAX_GUST          : int 47\n  ..$ SPEED_MAX_GUST_FLAG     : NULL\n  ..$ COOLING_DEGREE_DAYS     : int 0\n  ..$ COOLING_DEGREE_DAYS_FLAG: NULL\n  ..$ HEATING_DEGREE_DAYS     : num 25.5\n  ..$ HEATING_DEGREE_DAYS_FLAG: NULL\n  ..$ MIN_REL_HUMIDITY        : int 41\n  ..$ MIN_REL_HUMIDITY_FLAG   : NULL\n  ..$ MAX_REL_HUMIDITY        : int 66\n  ..$ MAX_REL_HUMIDITY_FLAG   : NULL\n\n\nUnfortunately, this station does not report some helpful measurements, like precipitation and snowfall. I might have to expand my search to find a more informative station:\n\nclimate_stations_halifax &lt;- climate_stations %&gt;%\n  filter(prov_state_terr_code == \"NS\",\n         # Only include stations with recent data\n         last_date &gt; \"2022-04-21\") %&gt;%\n  mutate(\n    diff_lat = abs(lat - mean_lat), diff_lon = abs(lon - mean_lon),\n    diff2 = diff_lat^2 + diff_lon^2\n  ) %&gt;%\n  slice_min(diff2, n = 5)\n\nclimate_stations_halifax %&gt;% rmarkdown::paged_table()\n\n\n\n  \n\n\n\nVisualize these station locations in a zoomed out map:\n\nhrm_map &lt;- get_googlemap(c(mean_lon, mean_lat),\n                         zoom = 12, maptype = \"satellite\")\n\n\nd &lt;- bind_rows(\n  site_locs %&gt;% mutate(group = \"bike counters\", label = site_name),\n  climate_stations_halifax %&gt;%\n    transmute(label = glue(\"{station_name} ({stn_id})\"),\n              lat, lon, diff2, group = \"climate stations\")\n)\n  \nggmap(hrm_map) +\n  geom_point(data = d, size = 4,\n             aes(fill = group), shape = 21, color = \"white\") +\n  ggrepel::geom_label_repel(\n    data = d,\n    aes(color = group, label = str_trunc(label, width = 25)),\n    box.padding = 0.5, force = 1.5\n  ) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nExploring the data from these stations a bit (not shown), Halifax Windsor Park seems a reasonable choice in terms of available data.\n\nresp &lt;- GET(\n  paste0(base_url,\n         \"collections/climate-daily/items?f=json&limit=10000&STATION_NAME=\",\n         URLencode(\"HALIFAX WINDSOR PARK\"))\n)\n\n\ncontent_parsed &lt;- content(resp, as = \"parsed\")\n\ndaily_climate &lt;- map_dfr(\n  content_parsed$features,\n  ~ discard(.x$properties, is.null) %&gt;% as_tibble()\n) %&gt;%\n  janitor::clean_names()\n\nglimpse(daily_climate)\n\nRows: 1,431\nColumns: 29\n$ station_name             &lt;chr&gt; \"HALIFAX WINDSOR PARK\", \"HALIFAX WINDSOR PARK‚Ä¶\n$ climate_identifier       &lt;chr&gt; \"8202255\", \"8202255\", \"8202255\", \"8202255\", \"‚Ä¶\n$ id                       &lt;chr&gt; \"8202255.2021.2.11\", \"8202255.2021.2.12\", \"82‚Ä¶\n$ local_date               &lt;chr&gt; \"2021-02-11 00:00:00\", \"2021-02-12 00:00:00\",‚Ä¶\n$ province_code            &lt;chr&gt; \"NS\", \"NS\", \"NS\", \"NS\", \"NS\", \"NS\", \"NS\", \"NS‚Ä¶\n$ local_year               &lt;int&gt; 2021, 2021, 2021, 2021, 2021, 2018, 2018, 201‚Ä¶\n$ local_month              &lt;int&gt; 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ‚Ä¶\n$ local_day                &lt;int&gt; 11, 12, 13, 14, 15, 14, 15, 16, 17, 18, 19, 2‚Ä¶\n$ mean_temperature         &lt;dbl&gt; -8.1, -8.1, -5.2, -5.0, -1.5, 10.8, 12.1, 9.0‚Ä¶\n$ min_temperature          &lt;dbl&gt; -11.1, -13.0, -8.0, -8.9, -5.4, 5.0, 9.1, 5.4‚Ä¶\n$ max_temperature          &lt;dbl&gt; -5.2, -3.2, -2.3, -1.2, 2.4, 16.5, 15.1, 12.5‚Ä¶\n$ snow_on_ground           &lt;int&gt; 13, 12, 11, 11, 11, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ direction_max_gust       &lt;int&gt; 26, 30, NA, NA, NA, 22, 21, 33, NA, NA, NA, 2‚Ä¶\n$ speed_max_gust           &lt;int&gt; 42, 32, NA, NA, NA, 38, 48, 32, NA, NA, NA, 4‚Ä¶\n$ cooling_degree_days      &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ‚Ä¶\n$ heating_degree_days      &lt;dbl&gt; 26.1, 26.1, 23.2, 23.0, 19.5, 7.2, 5.9, 9.0, ‚Ä¶\n$ min_rel_humidity         &lt;int&gt; 39, 66, 52, 67, 40, 37, 68, 40, 38, 21, 30, 8‚Ä¶\n$ max_rel_humidity         &lt;int&gt; 68, 87, 90, 90, 83, 90, 96, 87, 97, 89, 96, 9‚Ä¶\n$ total_precipitation_flag &lt;chr&gt; NA, NA, NA, NA, \"M\", NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ total_precipitation      &lt;dbl&gt; NA, NA, NA, NA, NA, 0.0, 0.6, 0.0, 0.0, 0.0, ‚Ä¶\n$ mean_temperature_flag    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ min_temperature_flag     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ max_temperature_flag     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ direction_max_gust_flag  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ speed_max_gust_flag      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ cooling_degree_days_flag &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ heating_degree_days_flag &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ min_rel_humidity_flag    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ max_rel_humidity_flag    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n\n\n\nEDA and cleaning\nVariable summaries:\n\nskimr::skim(daily_climate)\n\n\nData summary\n\n\nName\ndaily_climate\n\n\nNumber of rows\n1431\n\n\nNumber of columns\n29\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n15\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstation_name\n0\n1.00\n20\n20\n0\n1\n0\n\n\nclimate_identifier\n0\n1.00\n7\n7\n0\n1\n0\n\n\nid\n0\n1.00\n16\n18\n0\n1431\n0\n\n\nlocal_date\n0\n1.00\n19\n19\n0\n1431\n0\n\n\nprovince_code\n0\n1.00\n2\n2\n0\n1\n0\n\n\ntotal_precipitation_flag\n1098\n0.23\n1\n1\n0\n1\n0\n\n\nmean_temperature_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmin_temperature_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmax_temperature_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\ndirection_max_gust_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nspeed_max_gust_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\ncooling_degree_days_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nheating_degree_days_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmin_rel_humidity_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmax_rel_humidity_flag\n1409\n0.02\n1\n1\n0\n1\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlocal_year\n0\n1.00\n2019.83\n1.20\n2018.0\n2019.0\n2020.0\n2021.00\n2022.0\n‚ñÖ‚ñá‚ñá‚ñá‚ñÇ\n\n\nlocal_month\n0\n1.00\n6.59\n3.44\n1.0\n4.0\n7.0\n10.00\n12.0\n‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá\n\n\nlocal_day\n0\n1.00\n15.85\n8.76\n1.0\n8.0\n16.0\n23.00\n31.0\n‚ñá‚ñá‚ñá‚ñá‚ñÜ\n\n\nmean_temperature\n19\n0.99\n8.51\n9.16\n-13.5\n1.3\n8.2\n16.30\n26.8\n‚ñÇ‚ñÜ‚ñá‚ñá‚ñÖ\n\n\nmin_temperature\n19\n0.99\n4.12\n8.95\n-17.3\n-2.1\n3.9\n11.93\n20.7\n‚ñÇ‚ñÖ‚ñá‚ñÜ‚ñÜ\n\n\nmax_temperature\n19\n0.99\n12.90\n9.66\n-11.0\n5.0\n12.9\n21.20\n34.1\n‚ñÇ‚ñá‚ñá‚ñá‚ñÉ\n\n\nsnow_on_ground\n1061\n0.26\n2.82\n4.07\n0.0\n0.0\n1.0\n3.00\n24.0\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ndirection_max_gust\n554\n0.61\n23.77\n8.16\n1.0\n21.0\n24.0\n30.00\n36.0\n‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÜ\n\n\nspeed_max_gust\n554\n0.61\n42.48\n10.34\n31.0\n35.0\n40.0\n47.00\n96.0\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\ncooling_degree_days\n19\n0.99\n0.61\n1.48\n0.0\n0.0\n0.0\n0.00\n8.8\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nheating_degree_days\n19\n0.99\n10.10\n8.32\n0.0\n1.7\n9.8\n16.70\n31.5\n‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÅ\n\n\nmin_rel_humidity\n19\n0.99\n56.63\n18.72\n15.0\n42.0\n56.0\n70.00\n100.0\n‚ñÇ‚ñá‚ñá‚ñÜ‚ñÇ\n\n\nmax_rel_humidity\n22\n0.98\n92.31\n9.40\n47.0\n88.0\n96.0\n100.00\n100.0\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñá\n\n\ntotal_precipitation\n554\n0.61\n4.16\n9.75\n0.0\n0.0\n0.0\n3.00\n102.4\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nDrop some un-needed variables:\n\ndaily_climate &lt;- daily_climate %&gt;%\n  select(-station_name, -climate_identifier, -id, -province_code)\n\nProcess the date variable:\n\ndaily_climate &lt;- daily_climate %&gt;%\n  mutate(report_date = as.POSIXct(local_date) %&gt;% as.Date()) %&gt;%\n  # Can drop these now\n  select(-local_date, -local_year, -local_month, -local_day)\n\nThere happens to be some missing days:\n\ntibble(report_date = seq.Date(as.Date(\"2018-05-14\"), as.Date(\"2022-04-22\"),\n                              by = \"days\")) %&gt;%\n  anti_join(daily_climate, by = \"report_date\") %&gt;%\n  pull(report_date)\n\n [1] \"2020-01-02\" \"2020-01-03\" \"2020-01-04\" \"2020-01-05\" \"2020-01-06\"\n [6] \"2021-01-03\" \"2021-01-04\" \"2021-01-05\" \"2021-01-06\" \"2021-01-07\"\n[11] \"2022-01-03\"\n\n\nSeems odd that all of the missing days are in January of different years.\nThere are also some missing temperature values:\n\ndaily_climate %&gt;%\n  filter(\n    is.na(mean_temperature) | is.na(min_temperature) | is.na(max_temperature)\n  ) %&gt;%\n  select(report_date, contains(\"_temperature\")) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThe report_dates range from 2018 to 2022. The flag values (*_temperature_flag) are all ‚ÄúM‚Äù, telling us what we already know: the data is missing.\nFor non-missing values, here is the trend over time:\n\ndaily_climate %&gt;%\n  filter(!is.na(mean_temperature)) %&gt;%\n  ggplot(aes(x = report_date)) +\n  geom_line(aes(y = mean_temperature), color = td_colors$nice$ruby_red)\n\n\n\n\nThe total_precipitation values:\n\ndaily_climate %&gt;%\n  count(total_precipitation, total_precipitation_flag) %&gt;%\n  arrange(desc(is.na(total_precipitation))) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThere are missing total_precipitation values with NA total_precipitation_flag, which makes me think that the flag variables are not going to be useful/reliable.\nVisualize the non-missing:\n\ndaily_climate %&gt;%\n  filter(!is.na(total_precipitation)) %&gt;%\n  ggplot(aes(x = report_date)) +\n  geom_point(aes(y = total_precipitation), color = td_colors$nice$spanish_blue)\n\n\n\n\nThe snow_on_ground values:\n\ndaily_climate %&gt;%\n  count(snow_on_ground) %&gt;%\n  arrange(desc(is.na(snow_on_ground))) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\ndaily_climate %&gt;%\n  filter(!is.na(snow_on_ground)) %&gt;%\n  ggplot(aes(x = report_date)) +\n  geom_point(aes(y = snow_on_ground), color = td_colors$nice$spanish_blue)\n\n\n\n\nThe speed_max_gust values (in km/h):\n\ndaily_climate %&gt;%\n  count(speed_max_gust, speed_max_gust_flag) %&gt;%\n  arrange(desc(is.na(speed_max_gust))) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\ndaily_climate %&gt;%\n  filter(!is.na(speed_max_gust)) %&gt;%\n  ggplot(aes(x = report_date)) +\n  geom_point(aes(y = speed_max_gust), color = td_colors$nice$emerald)"
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#combining-the-data",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#combining-the-data",
    "title": "Predicting bike ridership: getting the data",
    "section": "Combining the data",
    "text": "Combining the data\nNow I‚Äôll combine the two data sets (joining on the date), only taking the most useful variables from the climate report (temperature, precipitation, wind speed, snow):\n\nbike_counts_climate &lt;- bike_data_daily_counts %&gt;%\n  left_join(\n    daily_climate %&gt;%\n      select(report_date, mean_temperature, total_precipitation,\n             speed_max_gust, snow_on_ground),\n    by = c(\"count_date\" = \"report_date\")\n  )\nglimpse(bike_counts_climate)\n\nRows: 2,840\nColumns: 9\n$ site_name           &lt;chr&gt; \"Dartmouth Harbourfront Greenway\", \"Dartmouth Harb‚Ä¶\n$ installation_date   &lt;date&gt; 2021-07-08, 2021-07-08, 2021-07-08, 2021-07-08, 2‚Ä¶\n$ count_date          &lt;date&gt; 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2‚Ä¶\n$ n_records           &lt;int&gt; 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48‚Ä¶\n$ n_bikes             &lt;int&gt; 130, 54, 180, 245, 208, 250, 182, 106, 152, 257, 1‚Ä¶\n$ mean_temperature    &lt;dbl&gt; 18.2, 17.6, 21.0, 21.0, 20.6, 18.6, 17.7, 20.2, 20‚Ä¶\n$ total_precipitation &lt;dbl&gt; 0.6, 10.0, 0.4, 0.0, 0.0, 0.0, 0.0, 11.6, 0.2, 0.4‚Ä¶\n$ speed_max_gust      &lt;int&gt; NA, 54, 56, NA, NA, NA, NA, 32, 37, NA, NA, NA, NA‚Ä¶\n$ snow_on_ground      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n\n\nVisualize the missing climate data:\n\nbike_counts_climate %&gt;%\n  distinct(count_date, mean_temperature, total_precipitation,\n           speed_max_gust, snow_on_ground) %&gt;%\n  mutate(across(where(is.numeric), is.na)) %&gt;%\n  pivot_longer(cols = -count_date) %&gt;%\n  ggplot(aes(x = count_date, y = name)) +\n  geom_tile(aes(fill = value)) +\n  labs(y = NULL, x = NULL, fill = \"Missing\") +\n  scale_fill_manual(values = c(td_colors$nice$indigo_blue, \"gray80\")) +\n  scale_x_date(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  theme(legend.position = \"top\")\n\n\n\n\nQuite a bit of missing data, but we should have enough to make for an interesting analysis. Save the data:\n\nwrite_rds(bike_counts_climate, \"bike-ridership-data.rds\")\n\nIn my next post, I will use this data to develop and evaluate various prediction models."
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#reproducibility",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#reproducibility",
    "title": "Predicting bike ridership: getting the data",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#footnotes",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/index.html#footnotes",
    "title": "Predicting bike ridership: getting the data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWent through some trial and error to get the get_googlemap() function working here. In brief, I (1) downloaded the development version of ggmap from GitHub (remotes::install_github(\"dkahle/ggmap\")) (2) created a new project on my Google Cloud Platform (GCP) account, (3) added an API key with access to the Google Static Maps API and registered it with register_google(), and (4) had to enable billing (because my free trial had been used).‚Ü©Ô∏é\nTo build these API queries, I found this documentation to be very helpful.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html",
    "title": "Predicting bike ridership: developing a model",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(tidymodels)\nlibrary(tictoc)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#introduction",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#introduction",
    "title": "Predicting bike ridership: developing a model",
    "section": "Introduction",
    "text": "Introduction\nIn my last post, I retrieved, explored, and prepared bike counter data from Halifax, Nova Scotia. I also got some historical weather data to go along with it. Here, I will further explore the data, engineer some features, and fit and evaluate many models to predict bike ridership at different sites around the city."
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#exploratory-data-analysis",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#exploratory-data-analysis",
    "title": "Predicting bike ridership: developing a model",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nImport the data:\n\nbike_ridership &lt;- read_rds(\"bike-ridership-data.rds\")\n\n\nglimpse(bike_ridership)\n\nRows: 2,840\nColumns: 9\n$ site_name           &lt;chr&gt; \"Dartmouth Harbourfront Greenway\", \"Dartmouth Harb‚Ä¶\n$ installation_date   &lt;date&gt; 2021-07-08, 2021-07-08, 2021-07-08, 2021-07-08, 2‚Ä¶\n$ count_date          &lt;date&gt; 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2‚Ä¶\n$ n_records           &lt;int&gt; 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48‚Ä¶\n$ n_bikes             &lt;int&gt; 130, 54, 180, 245, 208, 250, 182, 106, 152, 257, 1‚Ä¶\n$ mean_temperature    &lt;dbl&gt; 18.2, 17.6, 21.0, 21.0, 20.6, 18.6, 17.7, 20.2, 20‚Ä¶\n$ total_precipitation &lt;dbl&gt; 0.6, 10.0, 0.4, 0.0, 0.0, 0.0, 0.0, 11.6, 0.2, 0.4‚Ä¶\n$ speed_max_gust      &lt;int&gt; NA, 54, 56, NA, NA, NA, NA, 32, 37, NA, NA, NA, NA‚Ä¶\n$ snow_on_ground      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n\n\nThis data consists of daily bike counts (n_bikes) over a 24 hour period (count_date) recorded at 5 sites (site_name) around the city. In the original data, counts are recorded every hour which is reflected by the n_records variable:\n\nbike_ridership %&gt;%\n  count(site_name, n_records) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nsite_name\nn_records\nn\n\n\n\n\nDartmouth Harbourfront Greenway\n8\n1\n\n\nDartmouth Harbourfront Greenway\n48\n291\n\n\nHollis St\n4\n1\n\n\nHollis St\n24\n656\n\n\nSouth Park St\n8\n1\n\n\nSouth Park St\n48\n884\n\n\nVernon St\n8\n1\n\n\nVernon St\n48\n502\n\n\nWindsor St\n8\n1\n\n\nWindsor St\n48\n502\n\n\n\n\n\n\n\nAll except the Hollis St site have two channels (northbound and southbound), which is why n_records = 48. The entries with fewer n_records reflect the time of day that the data was extracted:\n\nbike_ridership %&gt;%\n  filter(n_records &lt; 24) %&gt;%\n  select(site_name, count_date, n_records) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nsite_name\ncount_date\nn_records\n\n\n\n\nDartmouth Harbourfront Greenway\n2022-04-25\n8\n\n\nHollis St\n2022-04-25\n4\n\n\nSouth Park St\n2022-04-25\n8\n\n\nVernon St\n2022-04-25\n8\n\n\nWindsor St\n2022-04-25\n8\n\n\n\n\n\n\n\nFor this analysis, I will exclude this incomplete day:\n\nbike_ridership &lt;- bike_ridership %&gt;% filter(n_records &gt; 8)\n\nThe date ranges for each site:\n\nbike_ridership %&gt;%\n  group_by(site_name) %&gt;%\n  summarise(\n    min_date = min(count_date), max_date = max(count_date),\n    n_days = n(), .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    site_name = fct_reorder(site_name, min_date),\n    n_days_label = ifelse(site_name == levels(site_name)[1],\n                          str_c(\"n_days = \", n_days), n_days),\n    midpoint_date = min_date + n_days / 2\n  ) %&gt;%\n  ggplot(aes(y = site_name, color = site_name)) +\n  geom_linerange(aes(xmin = min_date, xmax = max_date)) +\n  geom_point(aes(x = min_date)) +\n  geom_point(aes(x = max_date)) +\n  geom_text(aes(label = n_days_label, x = midpoint_date), vjust = -0.5) +\n  scale_y_discrete(labels = ~ str_wrap(., width = 15)) +\n  labs(y = NULL, x = \"min_date -&gt; max_date\") +\n  theme(legend.position = \"none\")\n\n\n\n\nBefore any more EDA, I‚Äôll split the data into training and testing sets (and only look at the training data going forward). For time series data, there is the rsample::initial_time_split() function, which I‚Äôll use to make a 70-30 split:\n\n# Need to order by time to properly use time split\nbike_ridership &lt;- bike_ridership %&gt;% arrange(count_date, site_name)\n\nbike_split &lt;- initial_time_split(bike_ridership, prop = 0.7)\n\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\n\nbind_rows(\n  train = bike_train, test = bike_test, .id = \"data_set\"\n) %&gt;%\n  group_by(data_set, site_name) %&gt;%\n  summarise(\n    min_date = min(count_date), max_date = max(count_date),\n    n_days = n(), midpoint_date = min_date + n_days / 2,\n    .groups = \"drop\"\n  ) %&gt;%\n  ggplot(aes(y = fct_reorder(site_name, min_date), color = data_set)) +\n  geom_linerange(aes(xmin = min_date, xmax = max_date),\n                 position = position_dodge(0.2)) +\n  geom_point(aes(x = min_date), position = position_dodge(0.2)) +\n  geom_point(aes(x = max_date), position = position_dodge(0.2)) +\n  geom_text(aes(label = n_days, x = midpoint_date), vjust = -0.5,\n            position = position_dodge(0.2), show.legend = FALSE) +\n  labs(x = \"date range\", y = NULL, color = NULL)\n\n\n\n\nIt might make more sense to stratify by site_name so that there is a 70-30 split in each site.1 For now, I‚Äôm using a simpler approach to split into the first 70% and 30% of the data.\nFor each site, the distribution of daily n_bikes:\n\nbike_train %&gt;%\n  ggplot(aes(x = n_bikes, fill = site_name)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~ str_trunc(site_name, 25)) +\n  theme(legend.position = \"none\")\n\n\n\n\nThe South Park St site appears bimodal, which I noted in part 1 was likely due to the addition of protected bike lanes in 2021. This can be seen more clearly in the trend over time:\n\nbike_train %&gt;%\n  ggplot(aes(x = count_date, y = n_bikes, color = site_name)) +\n  geom_line() +\n  facet_wrap(~ site_name, ncol = 1) +\n  theme(legend.position = \"none\")\n\n\n\n\nAs you would expect for data in the same city, bike counters between sites are very highly correlated, which I can visualize:\n\nbike_train %&gt;%\n  transmute(count_date, n_bikes1 = n_bikes,\n            site_name1 = factor(str_trunc(site_name, 10))) %&gt;%\n  left_join(., rename(., n_bikes2 = n_bikes1, site_name2 = site_name1),\n            by = \"count_date\") %&gt;%\n  filter(as.numeric(site_name1) &lt; as.numeric(site_name2)) %&gt;%\n  ggplot(aes(x = n_bikes1, y = n_bikes2)) +\n  geom_point(aes(color = site_name1), alpha = 0.3) +\n  facet_grid(site_name2 ~ site_name1) +\n  theme(legend.position = \"none\") +\n  dunnr::add_facet_borders()\n\n\n\n\nThe day of the week effect looks important:\n\nbike_train %&gt;%\n  mutate(day_of_week = lubridate::wday(count_date, label = TRUE)) %&gt;%\n  ggplot(aes(x = day_of_week, y = n_bikes)) +\n  geom_jitter(aes(color = site_name), height = 0, width = 0.2, alpha = 0.3) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  facet_wrap(~ str_trunc(site_name, 30)) +\n  theme(legend.position = \"none\") +\n  dunnr::add_facet_borders()\n\n\n\n\nAnother thing to consider is holidays. I can get Canadian holidays with the timeDate package (this is how recipes::step_holiday() works):\n\nlibrary(timeDate)\ncanada_holidays &lt;-\n  timeDate::listHolidays(\n    pattern = \"^CA|^Christmas|^NewYears|Easter[Sun|Mon]|^GoodFriday|^CaRem\"\n  )\ncanada_holidays\n\n [1] \"CACanadaDay\"              \"CACivicProvincialHoliday\"\n [3] \"CALabourDay\"              \"CaRemembranceDay\"        \n [5] \"CAThanksgivingDay\"        \"CAVictoriaDay\"           \n [7] \"ChristmasDay\"             \"ChristmasEve\"            \n [9] \"EasterMonday\"             \"EasterSunday\"            \n[11] \"GoodFriday\"               \"NewYearsDay\"             \n\n\nThen get the dates for each across the years in the bike data:\n\ncanada_holiday_dates &lt;- tibble(holiday = canada_holidays) %&gt;%\n  crossing(year = 2019:2022) %&gt;%\n  mutate(\n    holiday_date = map2(\n      year, holiday,\n      ~ as.Date(timeDate::holiday(.x, .y)@Data)\n    )\n  ) %&gt;%\n  unnest(holiday_date)\n\ncanada_holiday_dates %&gt;%\n  select(holiday, holiday_date) %&gt;%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nOnly day I can think is missing from this list is Family Day (Heritage Day in Nova Scotia) which is the third Monday in February. Visualize the effect of these holidays on bike ridership by plotting n_bikes in a 2 week window around the holidays (showing just the South Park St site for this plot):\n\ncanada_holiday_dates %&gt;%\n  filter(holiday_date %in% unique(bike_train$count_date)) %&gt;%\n  mutate(\n    date_window = map(holiday_date, ~ seq.Date(.x - 7, .x + 7, by = \"1 day\"))\n  ) %&gt;%\n  unnest(date_window) %&gt;%\n  left_join(\n    bike_train, by = c(\"date_window\" = \"count_date\")\n  ) %&gt;%\n  mutate(is_holiday = holiday_date == date_window) %&gt;%\n  group_by(holiday) %&gt;%\n  mutate(day_from_holiday = as.numeric(holiday_date - date_window)) %&gt;%\n  ungroup() %&gt;%\n  filter(site_name == \"South Park St\") %&gt;%\n  ggplot(aes(x = day_from_holiday, y = n_bikes,\n             group = factor(year))) +\n  geom_line() +\n  geom_vline(xintercept = 0, lty = 2) +\n  geom_point(aes(color = factor(year))) +\n  facet_wrap(~ holiday) +\n  labs(color = \"year\") +\n  dunnr::add_facet_borders() +\n  theme(legend.position = \"top\")\n\n\n\n\nThe only holidays with a clear drop in ridership are Labour Day and the Civic Holiday (called Natal Day in NS) in 2021. Victoria Day seems to have the opposite effect. The Good Friday, Easter Sunday and Easter Monday holidays are obviously overlapping, and the n_bikes trend is a bit of a mess, but I can see an indication to the left of Good Friday that there may be a drop in ridership going into that weekend.\nOne thing to note is that the first, middle and last points correspond to the same day of the week, and the middle point in this set is usually lower than those two point, so holidays may be useful features in conjunction with day of the week.\nThe weather variables have varying levels of completeness:\n\n# Separate out the weather data\nweather_data &lt;- bike_train %&gt;%\n  distinct(count_date, mean_temperature, total_precipitation,\n           speed_max_gust, snow_on_ground)\n\nweather_data %&gt;%\n  mutate(across(where(is.numeric), is.na)) %&gt;%\n  pivot_longer(cols = -count_date) %&gt;%\n  ggplot(aes(x = count_date, y = name)) +\n  geom_tile(aes(fill = value)) +\n  labs(y = NULL, x = NULL, fill = \"Missing\") +\n  scale_fill_manual(values = c(td_colors$nice$indigo_blue, \"gray80\")) +\n  scale_x_date(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  theme(legend.position = \"top\")\n\n\n\n\nThe distributions:\n\nweather_data %&gt;%\n  pivot_longer(cols = -count_date) %&gt;%\n  filter(!is.na(value)) %&gt;%\n  ggplot(aes(x = value, fill = name)) +\n  geom_histogram(bins = 20) +\n  scale_y_continuous(expand = c(0, 0)) +\n  facet_wrap(~ name, nrow = 1, scales = \"free\") +\n  theme(legend.position = \"none\")\n\n\n\n\nFor non-missing cases, plot the pairwise relationships:\n\nweather_data %&gt;%\n  pivot_longer(cols = -count_date, names_to = \"var1\", values_to = \"val1\") %&gt;%\n  mutate(var1 = factor(var1)) %&gt;%\n  left_join(., rename(., var2 = var1, val2 = val1),\n            by = \"count_date\") %&gt;%\n  filter(!is.na(val1), !is.na(val2),\n         # Use numeric factor labels to remove duplicates\n         as.numeric(var1) &lt; as.numeric(var2)) %&gt;%\n  ggplot(aes(x = val1, y = val2, color = var1)) +\n  geom_point(alpha = 0.5) +\n  facet_grid(var2 ~ var1, scales = \"free\") +\n  theme(legend.position = \"none\") +\n  dunnr::add_facet_borders()\n\n\n\n\nThe clearest relationship to me is unsurprising: increasing mean_temperature is associated with decreasing snow_on_ground (top left plot).\nVisualize relationships with n_bikes:\n\nbike_train %&gt;%\n  pivot_longer(\n    cols = c(mean_temperature, total_precipitation,\n             speed_max_gust, snow_on_ground),\n    names_to = \"var\", values_to = \"val\"\n  ) %&gt;%\n  filter(!is.na(val)) %&gt;%\n  ggplot(aes(x = val, y = n_bikes)) +\n  geom_point(aes(color = str_trunc(site_name, 15)), alpha = 0.4) +\n  facet_wrap(~ var, nrow = 2, scales = \"free_x\") +\n  dunnr::add_facet_borders() +\n  labs(x = NULL, color = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nAll of the weather variables seem to be have a relationship with n_bikes. In terms of predictive value, mean_temperature looks like it might be the most useful, and speed_max_gust the least."
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#feature-engineering",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#feature-engineering",
    "title": "Predicting bike ridership: developing a model",
    "section": "Feature engineering",
    "text": "Feature engineering\nFrom my EDA, I decided I want try including all 4 weather variables to predict bike ridership. This will require imputation of missing values for all 4, which I‚Äôll attempt here.\nAdd some more time variables for working with the weather_data:\n\nweather_data &lt;- weather_data %&gt;%\n  mutate(count_year = lubridate::year(count_date),\n         # Day number relative to earliest date\n         count_day = as.numeric(count_date - min(count_date)),\n         # Day of the year, from 1 to 365\n         count_yday = lubridate::yday(count_date))\n\n\nTemperature\nThe mean_temperature variable is missing 3% of values. Visualize the trend over time:\n\np1 &lt;- weather_data %&gt;%\n  filter(!is.na(mean_temperature)) %&gt;%\n  ggplot(aes(x = count_date, y = mean_temperature)) +\n  geom_line(aes(color = factor(count_year))) +\n  scale_color_viridis_d(\"year\") +\n  scale_x_date(\"date\", date_breaks = \"1 year\")\np2 &lt;- weather_data %&gt;%\n  filter(!is.na(mean_temperature)) %&gt;%\n  ggplot(aes(x = count_yday, y = mean_temperature)) +\n  geom_line(aes(color = factor(count_year))) +\n  scale_color_viridis_d(\"year\") +\n  scale_x_continuous(\"day of year\", breaks = c(0, 90, 180, 270, 365)) +\n  labs(y = NULL)\np1 + p2 +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"top\")\n\n\n\n\nThe cyclic nature makes it a good candidate for smoothing splines. As a starting point, try a natural spline with 5 knots on the count_yday variable:\n\nlibrary(splines)\nlm_temperature &lt;- \n  lm(mean_temperature ~ ns(count_yday, knots = 5),\n      data = filter(weather_data, !is.na(mean_temperature)))\n\np1 +\n  geom_line(\n    data = augment(lm_temperature, newdata = weather_data),\n    aes(y = .fitted), size = 1\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\nWe can obviously do a lot better, especially at the yearly boundaries. I‚Äôll fit the data using a generalized additive model (GAM) with the mgcv package.2 For the count_yday variable (ranges from 1-365), I‚Äôll make sure that there is no discontinuity between year by using a cyclic spline (bs = \"cc\"). I‚Äôll also include a smoothing term of count_day which will capture the trend across years.\n\nlibrary(mgcv)\n\ngam_temperature &lt;-\n  gam(mean_temperature ~ s(count_yday, bs = \"cc\", k = 12) + s(count_day),\n      data = filter(weather_data, !is.na(mean_temperature)))\nplot(gam_temperature, pages = 1, shade = TRUE)\n\n\n\n\nThe left plot shows the seasonal trend within a year (note the lines would connect at count_yday = 1 and 365), and the right plot shows the increase in average temperature throughout time (across years) after accounting for the seasonal effect. Overlay the fit:\n\np1 +\n  geom_line(\n    data = augment(gam_temperature, newdata = weather_data),\n    aes(y = .fitted), size = 1\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\nI‚Äôm pretty happy with that. I‚Äôll use predictions from this GAM to impute missing daily temperatures.\n\n\nPrecipitation and snow\nThe total_precipitation variable is missing 36% of values; 76% for snow_on_ground.\nThe total_precipitation distribution:\n\np1 &lt;- weather_data %&gt;%\n  mutate(total_precipitation = replace_na(total_precipitation, -5)) %&gt;%\n  ggplot(aes(x = count_date, y = total_precipitation)) +\n  geom_point(alpha = 0.5) +\n  scale_y_continuous(breaks = c(-5, 0, 20, 40, 60),\n                     labels = c(\"missing\", 0, 20, 40, 60))\np1\n\n\n\n\nThis pattern of missing data during winter months makes me think that the total_precipitation is actually total rainfall, i.e.¬†snowfall is not counted. I‚Äôm going to impute the missing values with 0 during pre-processing, which is admittedly a poor approximation of the truth.\nThe snow_on_ground distribution:\n\np2 &lt;- weather_data %&gt;%\n  mutate(snow_on_ground = replace_na(snow_on_ground, -2)) %&gt;%\n  ggplot(aes(x = count_date, y = snow_on_ground)) +\n  geom_point(alpha = 0.5) +\n  scale_y_continuous(breaks = c(-2, 0, 10, 20),\n                     labels = c(\"missing\", 0, 10, 20))\np2\n\n\n\n\nI‚Äôll also impute zero for missing snow_on_ground, which I‚Äôm a lot more confident doing here because most of the missing values occur during non-winter months. A more careful approach might involve imputing 0 during non-winter months that I‚Äôm certain would have no snow on the ground, then modeling the winter months with something like a zero-inflated Poisson model.\n\n\nWind speed\nThe speed_max_gust variable is the daily maximum wind speed in km/h, and has 41% missing values.\n\nmean_speed &lt;- mean(weather_data$speed_max_gust, na.rm = TRUE)\n\nweather_data %&gt;%\n  mutate(\n    speed_max_gust = replace_na(speed_max_gust, 20)\n  ) %&gt;%\n  ggplot(aes(x = count_date, y = speed_max_gust)) +\n  geom_line(data = . %&gt;% filter(speed_max_gust &gt; 20)) +\n  geom_smooth(data = . %&gt;% filter(speed_max_gust &gt; 20),\n              method = \"loess\", formula = \"y ~ x\",\n              color = td_colors$nice$spanish_blue) +\n  geom_hline(yintercept = mean_speed,\n             color = td_colors$nice$opera_mauve, size = 1, lty = 2) +\n  geom_jitter(data = . %&gt;% filter(speed_max_gust == 20),\n              width = 0, alpha = 0.5) +\n  scale_y_continuous(breaks = c(mean_speed, 20, 40, 60, 80),\n                     labels = c(\"mean_speed\", \"missing\", 40, 60, 80))\n\n\n\n\nRelative to the noise, the time trends are pretty minor, and the missing data looks to be missing mostly at random. I‚Äôll just impute using the mean speed.\n\n\nLagged counts\nAs a time series data set, it would be careless to not account for past data when predicting future data, so I‚Äôll include lagged n_bikes values. Investigate the correlation in n_bikes for values lagged by 1, 2 and 3 days, and by 1 and 2 weeks (because they are the same day of the week):\n\nbike_train_lag &lt;- bike_train %&gt;%\n  arrange(site_name, count_date) %&gt;%\n  group_by(site_name) %&gt;%\n  mutate(\n    n_bikes_lag_1 = lag(n_bikes, 1),\n    n_bikes_lag_2 = lag(n_bikes, 2),\n    n_bikes_lag_3 = lag(n_bikes, 3),\n    n_bikes_lag_7 = lag(n_bikes, 7),\n    n_bikes_lag_14 = lag(n_bikes, 14)\n  )\nbike_train_lag %&gt;%\n  select(site_name, count_date, starts_with(\"n_bikes\")) %&gt;%\n  pivot_longer(cols = matches(\"n_bikes_lag\"),\n               names_to = \"lag_days\", values_to = \"n_bikes_lag\") %&gt;%\n  filter(!is.na(n_bikes_lag)) %&gt;%\n  mutate(lag_days = str_extract(lag_days, \"\\\\d+\") %&gt;% as.integer()) %&gt;%\n  group_by(site_name, lag_days) %&gt;%\n  mutate(corr_coef = cor(n_bikes, n_bikes_lag)) %&gt;%\n  ggplot(aes(x = n_bikes_lag, y = n_bikes, color = site_name)) +\n  geom_point(alpha = 0.2) +\n  geom_label(data = . %&gt;% distinct(n_bikes_lag, site_name, corr_coef),\n             aes(label = round(corr_coef, 2), x = 500, y = 200)) +\n  geom_abline(slope = 1) +\n  facet_grid(str_trunc(site_name, 8) ~ factor(lag_days)) +\n  dunnr::add_facet_borders() +\n  theme(legend.position = \"none\")\n\n\n\n\nThe 7- and 14-day lagged values are correlated just as strongly (in some cases stronger) than the other options. This is great news because I only want to include a single lag variable, and using the 14th day lag means I can forecast 14 days ahead.\nIn order to use 14-day lag in a tidymodels workflow, I need to add it to the data myself. The step_lag() function won‚Äôt allow the outcome n_bikes to be lagged, because any new data won‚Äôt have an n_bikes variable to use. See the warning in this section of the Tidy Modeling with R book. Add the n_bikes_lag_14 predictor, and exclude any values without it:\n\nbike_ridership &lt;- bike_ridership %&gt;%\n  group_by(site_name) %&gt;%\n  mutate(n_bikes_lag_14 = lag(n_bikes, 14)) %&gt;%\n  ungroup() %&gt;%\n  filter(!is.na(n_bikes_lag_14))\n\nWhile I‚Äôm at it, I‚Äôll impute the missing mean_temperature values with the seasonal GAM.\n\nbike_ridership &lt;- bike_ridership %&gt;%\n  mutate(\n    count_day = as.numeric(count_date - min(count_date)),\n    count_yday = lubridate::yday(count_date),\n  ) %&gt;%\n  bind_cols(pred = predict(gam_temperature, newdata = .)) %&gt;%\n  mutate(\n    mean_temperature = ifelse(is.na(mean_temperature), pred,\n                              mean_temperature)\n  ) %&gt;%\n  select(-count_day, -count_yday, -pred)\n\nThis means I‚Äôll have to re-split the data into training and testing (initial_time_split() isn‚Äôt random, so doesn‚Äôt require setting the seed):\n\nbike_ridership &lt;- bike_ridership %&gt;% arrange(count_date, site_name)\n\nbike_split &lt;- initial_time_split(bike_ridership, prop = 0.7)\n\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)"
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#modeling",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#modeling",
    "title": "Predicting bike ridership: developing a model",
    "section": "Modeling",
    "text": "Modeling\nRegister parallel computing:\n\nn_cores &lt;- parallel::detectCores(logical = FALSE)\nlibrary(doParallel)\ncl &lt;- makePSOCKcluster(n_cores - 1)\nregisterDoParallel(cl)\n# This extra step makes sure the parallel workers have access to the\n#  `tidyr::replace_na()` function during pre-processing, which I use later on\n# See this issue: https://github.com/tidymodels/tune/issues/364\nparallel::clusterExport(cl, c(\"replace_na\"))\n\n\nResampling and metrics\nFor re-sampling, I will use sliding_period() to break up the data into 14 months of data (chosen to give 10 resamples) for analysis and 1 month for assessment:\n\nbike_resamples &lt;-\n  sliding_period(bike_train, index = count_date,\n                 period = \"month\", lookback = 12, assess_stop = 2)\n\nVisualize the resamples:\n\nbind_rows(\n  analysis_set = map_dfr(bike_resamples$splits, analysis, .id = \"i\"),\n  assessment_set = map_dfr(bike_resamples$splits, assessment, .id = \"i\"),\n  .id = \"data_set\"\n) %&gt;%\n  mutate(i = as.integer(i)) %&gt;%\n  group_by(i, data_set) %&gt;%\n  summarise(\n    min_date = min(count_date), max_date = max(count_date),\n    n_days = n(), midpoint_date = min_date + n_days / 2,\n    .groups = \"drop\"\n  ) %&gt;%\n  ggplot(aes(y = factor(i), color = data_set)) +\n  geom_linerange(aes(xmin = min_date, xmax = max_date),\n                 position = position_dodge(0.3)) +\n  geom_point(aes(x = min_date), position = position_dodge(0.3)) +\n  geom_point(aes(x = max_date), position = position_dodge(0.3)) +\n  labs(x = \"date range\", y = NULL, color = NULL)\n\n\n\n\nI‚Äôll define a set of metrics to use here as well:\n\nbike_metrics &lt;- metric_set(rmse, rsq, mase, poisson_log_loss)\n\nThe Poisson log loss is a new one to me, that was recently added to yardstick. I‚Äôll include it just for kicks, but I will choose my final model with mase, the mean absolute scaled error, which was introduced by Hyndman and Koehler (Hyndman and Koehler 2006). The MASE involves dividing the absolute forecast error (\\(|y_i - \\hat{y}_i|\\)) by absolute naive forecast error (which involves predicting with the last observed value). The main advantage of this is that it is scale invariant. Mean absolute percentage error (MAPE) is the typical scale-invariant choice in regression problems, but the MASE avoids dividing by n_bikes = 0 (of which there are about 20 in this data set). It also has a straightforward interpretation: values greater than one indicate a worse forecast than the naive method, and values less indicate better.\n\n\nGeneralized linear models\nThe first models I will try are simple linear regression and Poisson regression, but I will test out a few different pre-processing/feature combinations. I would prefer to use negative binomial regression instead of Poisson to account for overdispersion (see aside), but it hasn‚Äôt been implemented in parsnip yet.\n\nlm_spec &lt;- linear_reg(engine = \"lm\")\nlibrary(poissonreg) # This wrapper package is required to use `poisson_reg()`\npoisson_spec &lt;- poisson_reg(engine = \"glm\")\n\n\n\nOver-dispersion in n_bikes (variance much higher than the mean):\n\nbike_train %&gt;%\n  group_by(site_name) %&gt;%\n  summarise(mean_n_bikes = mean(n_bikes), var_n_bikes = var(n_bikes))\n\n# A tibble: 5 √ó 3\n  site_name                       mean_n_bikes var_n_bikes\n  &lt;chr&gt;                                  &lt;dbl&gt;       &lt;dbl&gt;\n1 Dartmouth Harbourfront Greenway        145.        3121.\n2 Hollis St                               77.0       2180.\n3 South Park St                          174.       26594.\n4 Vernon St                              212.       18080.\n5 Windsor St                              96.3       3413.\n\n\nFor my base pre-processing, I‚Äôll include just site_name and n_bikes_lag_14:\n\nglm_recipe &lt;-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14,\n         data = bike_train) %&gt;%\n  add_role(count_date, new_role = \"date_variable\") %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nThe first extension of this recipe will include date variables. I‚Äôll add day of week as categorical, day of year as numerical (and tune a natural spline function), year as numerical and Canadian holidays as categorical.\n\nglm_recipe_date &lt;-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14,\n         data = bike_train) %&gt;%\n  add_role(count_date, new_role = \"date_variable\") %&gt;%\n  step_date(count_date, features = c(\"dow\", \"doy\", \"year\"),\n            label = TRUE, ordinal = FALSE) %&gt;%\n  step_ns(count_date_doy, deg_free = tune()) %&gt;%\n  step_holiday(count_date, holidays = canada_holidays) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nI‚Äôll consider the weather variables, with the imputations discussed in the feature engineering section, separately:\n\nglm_recipe_weather &lt;-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14 + mean_temperature +\n           total_precipitation + speed_max_gust + snow_on_ground,\n         data = bike_train) %&gt;%\n  add_role(count_date, new_role = \"date_variable\") %&gt;%\n  step_impute_mean(speed_max_gust) %&gt;%\n  # Impute these missing values with zero\n  step_mutate_at(c(total_precipitation, snow_on_ground),\n                 fn = ~ replace_na(., 0)) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nAnd lastly, all the features together:\n\nglm_recipe_date_weather &lt;-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14 + mean_temperature +\n           total_precipitation + speed_max_gust + snow_on_ground,\n         data = bike_train) %&gt;%\n  add_role(count_date, new_role = \"date_variable\") %&gt;%\n  step_date(count_date, features = c(\"dow\", \"doy\", \"year\"),\n            label = TRUE, ordinal = FALSE) %&gt;%\n  step_ns(count_date_doy, deg_free = tune()) %&gt;%\n  step_holiday(count_date, holidays = canada_holidays) %&gt;%\n  step_impute_mean(speed_max_gust) %&gt;%\n  step_mutate_at(c(total_precipitation, snow_on_ground),\n                 fn = ~ replace_na(., 0)) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nPut these pre-processing recipes and the model specifications into a workflow_set():\n\nglm_wf_set &lt;- workflow_set(\n  preproc = list(base = glm_recipe,\n                 date = glm_recipe_date, weather = glm_recipe_weather,\n                 date_weather = glm_recipe_date_weather),\n  models = list(linear_reg = lm_spec, poisson_reg = poisson_spec),\n  cross = TRUE\n)\n\nNow fit the resamples using each combination of model specification and pre-processing recipe:\n\ntic()\nglm_wf_set_res &lt;- workflow_map(\n  glm_wf_set,\n  \"tune_grid\",\n  # I'll try just a few `deg_free` in the spline term\n  grid = grid_regular(deg_free(range = c(4, 7)), levels = 4),\n  resamples = bike_resamples, metrics = bike_metrics\n)\ntoc()\n\n35.38 sec elapsed\n\n\nFor plotting the results this set of workflows, I‚Äôll use a custom plotting function with rank_results():\n\nplot_wf_set_metrics &lt;- function(wf_set_res, rank_metric = \"mase\") {\n  rank_results(wf_set_res, rank_metric = rank_metric) %&gt;%\n    mutate(preproc = str_remove(wflow_id, paste0(\"_\", model))) %&gt;%\n    ggplot(aes(x = rank, y = mean, color = model, shape = preproc)) +\n    geom_point(size = 2) +\n    geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err),\n                  width = 0.2) +\n    facet_wrap(~ .metric, scales = \"free_y\")\n  \n}\nplot_wf_set_metrics(glm_wf_set_res)\n\n\n\n\n\n\nI‚Äôm using my own function because the workflowsets::autoplot() function doesn‚Äôt distinguish preprocessor recipes using the names provided to the preproc argument ‚Äì everything just gets the name ‚Äúrecipe‚Äù:\n\nrank_results(glm_wf_set_res) %&gt;%\n  distinct(wflow_id, model, preprocessor)\n\n# A tibble: 8 √ó 3\n  wflow_id                 preprocessor model      \n  &lt;chr&gt;                    &lt;chr&gt;        &lt;chr&gt;      \n1 date_weather_poisson_reg recipe       poisson_reg\n2 date_weather_linear_reg  recipe       linear_reg \n3 weather_linear_reg       recipe       linear_reg \n4 date_linear_reg          recipe       linear_reg \n5 base_linear_reg          recipe       linear_reg \n6 date_poisson_reg         recipe       poisson_reg\n7 weather_poisson_reg      recipe       poisson_reg\n8 base_poisson_reg         recipe       poisson_reg\n\n\nMy function extracts the recipe name from wflow_id.\nAcross the board, the model will all the features (date and weather predictors; indicated by the square symbols) are best, and Poisson regression slightly outperforms linear. Here are the models ranked by MASE:3\n\nrank_results(glm_wf_set_res, select_best = TRUE, rank_metric = \"mase\") %&gt;%\n  filter(.metric == \"mase\") %&gt;%\n  select(rank, wflow_id, .config, mase = mean, std_err) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = c(mase, std_err), decimals = 3)\n\n\n\n\n\n\n\n\nrank\nwflow_id\n.config\nmase\nstd_err\n\n\n\n\n1\ndate_weather_poisson_reg\nPreprocessor3_Model1\n0.547\n0.056\n\n\n2\ndate_weather_linear_reg\nPreprocessor3_Model1\n0.577\n0.075\n\n\n3\ndate_linear_reg\nPreprocessor2_Model1\n0.604\n0.052\n\n\n4\nweather_linear_reg\nPreprocessor1_Model1\n0.622\n0.067\n\n\n5\ndate_poisson_reg\nPreprocessor2_Model1\n0.626\n0.064\n\n\n6\nbase_linear_reg\nPreprocessor1_Model1\n0.635\n0.055\n\n\n7\nweather_poisson_reg\nPreprocessor1_Model1\n0.670\n0.080\n\n\n8\nbase_poisson_reg\nPreprocessor1_Model1\n0.760\n0.055\n\n\n\n\n\n\n\nSo our best workflow has the id date_weather_poisson_reg4 with the .config ‚ÄúPreprocessor3_Model1‚Äù which refers to a specific spline degree from our tuning of the count_date_doy feature. I can check out the results of the tuning with autoplot():\n\nautoplot(glm_wf_set_res, id = \"date_weather_poisson_reg\") +\n  facet_wrap(~ .metric, nrow = 2, scales = \"free_y\")\n\n\n\n\nThe polynomial of degree 6 did best by MASE, which I‚Äôll use to finalize the workflow and fit to the full training set:\n\nglm_poisson_workflow &lt;- finalize_workflow(\n  extract_workflow(glm_wf_set_res, \"date_weather_poisson_reg\"),\n  extract_workflow_set_result(glm_wf_set_res, \"date_weather_poisson_reg\") %&gt;%\n    select_best(metric = \"mase\")\n)\nglm_poisson_fit &lt;- glm_poisson_workflow %&gt;% fit(bike_train)\n\n\nModel interpretation\nAn advantage of using a generalized linear model is that they are easy to interpret. A simple way to estimate variable important in a GLM is to look at the absolute value of the \\(t\\)-statistic (statistic in the below table) ‚Äì here are the top 5:\n\npoisson_coefs &lt;- tidy(glm_poisson_fit) %&gt;%\n  arrange(desc(abs(statistic))) %&gt;%\n  mutate(estimate = signif(estimate, 3), std.error = signif(estimate, 2),\n         statistic = round(statistic, 1), p.value = scales::pvalue(p.value))\nhead(poisson_coefs, 5) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\ntotal_precipitation\n-0.0384\n-0.038\n-84.6\n&lt;0.001\n\n\nsite_name_South.Park.St\n0.6890\n0.690\n68.6\n&lt;0.001\n\n\nsite_name_Vernon.St\n0.5490\n0.550\n57.8\n&lt;0.001\n\n\nmean_temperature\n0.0365\n0.036\n53.3\n&lt;0.001\n\n\nn_bikes_lag_14\n0.0010\n0.001\n48.2\n&lt;0.001\n\n\n\n\n\n\n\nA couple of the weather variables are among the most influential in predicting n_bikes. Here are all the weather coefficients:\n\npoisson_coefs %&gt;%\n  filter(str_detect(term, \"temperature|precip|max_gust|snow\")) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\ntotal_precipitation\n-0.03840\n-0.0380\n-84.6\n&lt;0.001\n\n\nmean_temperature\n0.03650\n0.0360\n53.3\n&lt;0.001\n\n\nsnow_on_ground\n-0.05360\n-0.0540\n-28.9\n&lt;0.001\n\n\nspeed_max_gust\n-0.00735\n-0.0074\n-19.6\n&lt;0.001\n\n\n\n\n\n\n\nSince this is a Poisson model, the link function (non-linear relationship between the outcome and the predictors) is the logarithm:\n\\[\n\\begin{align}\n\\log{n}_{\\text{bikes}} &= \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p \\\\\nn_{\\text{bikes}} &= \\text{exp}(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p) \\\\\n&= \\text{exp}(\\beta_0) \\text{exp}(\\beta_1 x_1) \\dots \\text{exp}(\\beta_p x_p) \\\\\n\\end{align}\n\\]\nSo the coefficients are interpreted as: for every one unit increase in \\(x_i\\), the expected value of \\(n_{\\text{bikes}}\\) changes by multiplicative factor of \\(\\text{exp}(\\beta_i)\\), holding all other predictors constant. Here are some plain-language interpretations of the weather coefficients:\n\nFor every 5¬∞C increase in daily average temperature (mean_temperature), the expected value of n_bikes increases by 120%.\nFor every 10mm increase in daily rain (total_precipitation), the expected value of n_bikes decreases by 68%.\nFor every 10km/h increase in maximum wind speed (speed_max_gust), the expected value of n_bikes decreases by 93%.\nFor every 5cm of snow (snow_on_ground), the expected value of n_bikes decreases by 77%.\n\nA more thorough and formal analysis of these types of relationships should involve exploration of marginal effects (with a package like marginaleffects for example), but I‚Äôll move on to other models.\n\n\n\nTree-based methods\nFor tree-based methods, I‚Äôll use similar pre-processing as with GLM, except I won‚Äôt use a spline term (trees partition the feature space to capture non-linearity):\n\ntrees_recipe &lt;-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14 + mean_temperature +\n           total_precipitation + speed_max_gust + snow_on_ground,\n         data = bike_train) %&gt;%\n  update_role(count_date, new_role = \"date_variable\") %&gt;%\n  step_date(count_date, features = c(\"dow\", \"doy\", \"year\"),\n            label = TRUE, ordinal = FALSE) %&gt;%\n  step_holiday(count_date, holidays = canada_holidays) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_impute_mean(speed_max_gust) %&gt;%\n  step_mutate_at(c(total_precipitation, snow_on_ground),\n                 fn = ~ replace_na(., 0)) %&gt;%\n  step_zv(all_predictors())\n\n# XGBoost requires dummy variables\ntrees_recipe_dummy &lt;- trees_recipe %&gt;%\n  step_dummy(all_nominal_predictors())\n\nI‚Äôll try a decision tree, a random forest, and an XGBoost model, each with hyperparameters indicated for tuning:\n\ndecision_spec &lt;-\n  decision_tree(cost_complexity = tune(), tree_depth = tune(),\n                min_n = tune()) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\nrf_spec &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;%\n  # Setting the `importance` parameter now lets me use `vip` later\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_spec &lt;- boost_tree(\n  mtry = tune(), trees = tune(), min_n = tune(),\n  tree_depth = tune(), learn_rate = tune()\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\ntrees_wf_set &lt;- workflow_set(\n  preproc = list(trees_recipe = trees_recipe,\n                 trees_recipe = trees_recipe,\n                 trees_recipe_dummy = trees_recipe_dummy),\n  models = list(rf = rf_spec, decision = decision_spec, xgb = xgb_spec),\n  cross = FALSE\n)\n\nAs a first pass, I‚Äôll let tune_grid() choose 10 parameter combinations automatically for each model (grid = 10):\n\nset.seed(225)\ntic()\ntrees_wf_set_res &lt;- workflow_map(\n  trees_wf_set,\n  \"tune_grid\",\n  grid = 10, resamples = bike_resamples, metrics = bike_metrics,\n)\ntoc()\n\n98.83 sec elapsed\n\n\nVisualize the performance of the 30 workflows, ranked by MASE:\n\n# Don't need to use my custom function defined previously because I'm\n#  not using functionally different recipes for each model\nautoplot(trees_wf_set_res, rank_metric = \"mase\")\n\n\n\n\nAs I would have expected, the decision tree models generally perform worse than random forest models (which consist of multiple decision trees). The boosted tree models (which also consist of multiple decision trees, but differ in how they are built and combined) slightly outperform random forests, which is also expected.\nFor the three different tree-based model types, I‚Äôll extract the best-performing workflows (by MASE) and fit to the full training set:\n\ndecision_workflow &lt;- finalize_workflow(\n  extract_workflow(trees_wf_set_res, \"trees_recipe_decision\"),\n  extract_workflow_set_result(trees_wf_set_res, \"trees_recipe_decision\") %&gt;%\n    select_best(metric = \"mase\")\n)\nrf_workflow &lt;- finalize_workflow(\n  extract_workflow(trees_wf_set_res, \"trees_recipe_rf\"),\n  extract_workflow_set_result(trees_wf_set_res, \"trees_recipe_rf\") %&gt;%\n    select_best(metric = \"mase\")\n)\nxgb_workflow &lt;- finalize_workflow(\n  extract_workflow(trees_wf_set_res, \"trees_recipe_dummy_xgb\"),\n  extract_workflow_set_result(trees_wf_set_res, \"trees_recipe_dummy_xgb\") %&gt;%\n    select_best(metric = \"mase\")\n)\n\ndecision_fit &lt;- decision_workflow %&gt;% fit(bike_train)\nrf_fit &lt;- rf_workflow %&gt;% fit(bike_train)\nxgb_fit &lt;- xgb_workflow %&gt;% fit(bike_train)\n\n\nModel interpretation\nDecision trees are the easiest of the three to interpret, but this decision tree is quite complicated. It has 44 leaf nodes (i.e.¬†terminal nodes) with a depth of 6. The visualization (done with the rpart.plot package) is messy and hidden below:\n\n\nDecision tree plot\n\n\nextract_fit_engine(decision_fit) %&gt;%\n  rpart.plot::rpart.plot(\n    # Using some options to try and make the tree more readable\n    fallen.leaves = FALSE, roundint = FALSE,\n    tweak = 5, type = 0, faclen = 10, clip.facs = TRUE, compress = FALSE\n  )\n\n\n\n\n\nThe nodes at the top of the tree are a good indicator of feature importance. Here, that was n_bikes_lag_14, followed by mean_temperature and total_precipitation.\nTo quantify the contribution of each feature in a tree-based model, we can calculate variable importance with the vip package. Plot the top 5 variables for all 3 models:\n\nlibrary(vip)\n\np1 &lt;- extract_fit_engine(decision_fit) %&gt;%\n  vip(num_features = 5) +\n  scale_y_continuous(NULL, expand = c(0, 0)) +\n  labs(subtitle = \"Decision tree\")\np2 &lt;- extract_fit_engine(rf_fit) %&gt;%\n  vip(num_features = 5) +\n  scale_y_continuous(NULL, expand = c(0, 0)) +\n  labs(subtitle = \"Random forest\")\np3 &lt;- extract_fit_engine(xgb_fit) %&gt;%\n  vip(num_features = 5) +\n  scale_y_continuous(NULL, expand = c(0, 0)) +\n  labs(subtitle = \"XGBoost\")\np1 + p2 + p3 +\n  plot_layout(ncol = 1)\n\n\n\n\nWe see that n_bikes_lag_14, mean_temperature, and site_name are important with all three models. Measures of time within (count_date_doy) and across (count_date_year) years are also important.\n\n\nXGBoost tuning\nSo far, I‚Äôve only considered 10 candidate sets of hyperparameters for tuning each tree-based model. Let‚Äôs try 100 with XGBoost:\n\n# Get the number of predictors so I can set max number of predictors in `mtry()`\nbike_train_baked &lt;- prep(trees_recipe_dummy) %&gt;% bake(bike_train)\n\n# `grid_latin_hypercube()` is a space-filling parameter grid design that will\n#  efficiently cover the parameter space for me\nxgb_grid &lt;- grid_latin_hypercube(\n  finalize(mtry(), select(bike_train_baked, -n_bikes)),\n  trees(), min_n(), tree_depth(), learn_rate(),\n  size = 100\n)\nxgb_grid\n\n# A tibble: 100 √ó 5\n    mtry trees min_n tree_depth learn_rate\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;\n 1    14   300    20          8   4.27e- 4\n 2    30  1560     5         12   2.17e- 5\n 3    22  1723    30          2   1.37e- 2\n 4     2   268    21          2   6.42e-10\n 5     7   780    15         12   1.09e- 5\n 6    24   642    24          2   7.51e- 5\n 7     3  1066    36          3   8.06e- 9\n 8    13   725     4          6   1.94e- 2\n 9    18   574    40          9   5.55e- 7\n10    27  1988    38         13   7.08e- 9\n# ‚Ä¶ with 90 more rows\n# ‚Ñπ Use `print(n = ...)` to see more rows\n\n\nWe have just one model and one preprocessor now, so put it into a single workflow:\n\nxgb_workflow_2 &lt;- workflow() %&gt;%\n  add_recipe(trees_recipe_dummy) %&gt;%\n  add_model(xgb_spec)\n\nAnd tune:\n\nset.seed(2081)\ntic()\nxgb_tune &lt;- tune_grid(\n  xgb_workflow_2, resamples = bike_resamples,\n  grid = xgb_grid, metrics = bike_metrics\n)\ntoc()\n\n475.8 sec elapsed\n\n\nHere are the metrics for the different candidate models:\n\nautoplot(xgb_tune)\n\n\n\n\nFinalize the workflow with the best hyperparameters and fit the full training set:\n\nxgb_workflow_2 &lt;- finalize_workflow(\n  xgb_workflow_2, select_best(xgb_tune, metric = \"mase\")\n)\n\nxgb_fit_2 &lt;- xgb_workflow_2 %&gt;% fit(bike_train)\n\n\n\n\nSupport vector machines\nLastly, I will try some support vector machine regressions with linear, polynomial and radial basis function (RBF) kernels:\n\n# New recipe for the SVMs which require a `step_normalize()`\nsvm_recipe &lt;-\n  recipe(\n    n_bikes ~ count_date + site_name + n_bikes_lag_14 + mean_temperature + \n      total_precipitation + speed_max_gust + snow_on_ground,\n    data = bike_train, \n  ) %&gt;% \n  update_role(count_date, new_role = \"date_variable\") %&gt;%\n  step_date(count_date, features = c(\"dow\", \"doy\", \"year\"),\n            label = TRUE, ordinal = FALSE) %&gt;%\n  step_holiday(count_date, holidays = canada_holidays) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_impute_mean(speed_max_gust) %&gt;%\n  step_mutate_at(c(total_precipitation, snow_on_ground),\n                 fn = ~ replace_na(., 0)) %&gt;%\n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nsvm_linear_spec &lt;- svm_linear(cost = tune(), margin = tune()) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"kernlab\")\n\nsvm_poly_spec &lt;- svm_poly(cost = tune(), margin = tune(),\n                          scale_factor = tune(), degree = tune()) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"kernlab\")\n\nsvm_rbf_spec &lt;-  svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;%\n  set_engine(\"kernlab\")\n\nsvm_wf_set &lt;- workflow_set(\n  preproc = list(svm_recipe),\n  models = list(linear = svm_linear_spec, poly = svm_poly_spec,\n                rbf = svm_rbf_spec)\n)\n\n\nset.seed(4217)\nsvm_wf_set_res &lt;- workflow_map(\n  svm_wf_set,\n  \"tune_grid\",\n  grid = 10, resamples = bike_resamples, metrics = bike_metrics\n)\n\n\nautoplot(svm_wf_set_res)\n\n\n\n\nThose are some pretty interesting (and smooth) trends. The best model is one of the svm_rbf candidates (though the next 10 are the svm_linear models). Finalize the workflows and fit:\n\nsvm_linear_wf_res &lt;-\n  extract_workflow_set_result(svm_wf_set_res, \"recipe_linear\")\nsvm_linear_wf &lt;-\n  finalize_workflow(\n    extract_workflow(svm_wf_set, \"recipe_linear\"),\n    select_best(svm_linear_wf_res, \"mase\")\n  )\nsvm_linear_fit &lt;- fit(svm_linear_wf, bike_train)\n\nsvm_poly_wf_res &lt;-\n  extract_workflow_set_result(svm_wf_set_res, \"recipe_poly\")\nsvm_poly_wf &lt;-\n  finalize_workflow(\n    extract_workflow(svm_wf_set, \"recipe_poly\"),\n    select_best(svm_poly_wf_res, \"mase\")\n  )\nsvm_poly_fit &lt;- fit(svm_poly_wf, bike_train)\n\nsvm_rbf_wf_res &lt;-\n  extract_workflow_set_result(svm_wf_set_res, \"recipe_rbf\")\nsvm_rbf_wf &lt;-\n  finalize_workflow(\n    extract_workflow(svm_wf_set, \"recipe_rbf\"),\n    select_best(svm_rbf_wf_res, \"mase\")\n  )\nsvm_rbf_fit &lt;- fit(svm_rbf_wf, bike_train)"
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#choosing-a-final-model",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#choosing-a-final-model",
    "title": "Predicting bike ridership: developing a model",
    "section": "Choosing a final model",
    "text": "Choosing a final model\nI‚Äôll choose a model by the best cross-validated MASE:\n\nbind_rows(\n  rank_results(glm_wf_set_res, rank_metric = \"mase\",\n                     select_best = TRUE) %&gt;%\n    # Only include the full pre-processing recipe\n    filter(str_detect(wflow_id, \"date_weather\")),\n  rank_results(trees_wf_set_res, rank_metric = \"mase\", select_best = TRUE),\n  rank_results(svm_wf_set_res, rank_metric = \"mase\", select_best = TRUE),\n  show_best(xgb_tune, metric = \"mase\", n = 1) %&gt;%\n    mutate(model = \"boost_tree_2\")\n) %&gt;%\n  filter(.metric == \"mase\") %&gt;%\n  select(model, mase = mean, std_err) %&gt;%\n  arrange(mase) %&gt;%\n  gt() %&gt;%\n  fmt_number(c(mase, std_err), decimals = 4)\n\n\n\n\n\n\n\n\nmodel\nmase\nstd_err\n\n\n\n\nboost_tree_2\n0.4913\n0.0503\n\n\nboost_tree\n0.4933\n0.0545\n\n\nrand_forest\n0.5099\n0.0566\n\n\nsvm_rbf\n0.5420\n0.0440\n\n\npoisson_reg\n0.5471\n0.0563\n\n\ndecision_tree\n0.5481\n0.0500\n\n\nsvm_linear\n0.5608\n0.0478\n\n\nlinear_reg\n0.5775\n0.0754\n\n\nsvm_poly\n0.6253\n0.0410\n\n\n\n\n\n\n\nXGBoost takes the top two spots, with boost_tree_2 (the more thorough tuning) being the slight winner. Perform a last_fit() and get the performance on the held-out test set:\n\nxgb_final_fit &lt;- last_fit(\n  xgb_workflow_2, split = bike_split, metrics = bike_metrics\n)\ncollect_metrics(xgb_final_fit) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nrmse\nstandard\n44.2385659\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.5415386\nPreprocessor1_Model1\n\n\nmase\nstandard\n0.7406183\nPreprocessor1_Model1\n\n\npoisson_log_loss\nstandard\nNaN\nPreprocessor1_Model1\n\n\n\n\n\n\n\nThere‚Äôs a bit of a drop-off in performance from the training metrics. Let‚Äôs dig into the predictions.\n\nExploring the predictions\nPlot the relationship between actual n_bikes and predicted:\n\nxgb_final_preds &lt;- bind_rows(\n  train = augment(extract_workflow(xgb_final_fit), bike_train),\n  test = augment(extract_workflow(xgb_final_fit), bike_test),\n  .id = \"data_set\"\n) %&gt;%\n  mutate(data_set = fct_inorder(data_set))\n\nxgb_final_preds %&gt;%\n  ggplot(aes(x = n_bikes, y = .pred)) +\n  geom_point() +\n  geom_abline(slope = 1, size = 1, color = td_colors$nice$emerald) +\n  facet_wrap(~ data_set)\n\n\n\n\nAnd here are the predictions overlaid on the data (vertical line delineates training and testing sets):\n\np &lt;- xgb_final_preds %&gt;%\n  ggplot(aes(x = count_date)) +\n  geom_line(aes(y = n_bikes, color = site_name), size = 1) +\n  geom_line(aes(y = .pred), color = \"black\") +\n  geom_vline(xintercept = min(bike_test$count_date), lty = 2) +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(breaks = seq(0, 600, 200)) +\n  expand_limits(y = 200) +\n  dunnr::add_facet_borders()\np\n\n\n\n\nTruncate the dates to look closer at the testing set performance:\n\np + coord_cartesian(xlim = as.Date(c(\"2021-07-01\", \"2022-05-01\")))\n\n\n\n\nThis looks okay to my eye, though it does look to be overfitting the training set. It‚Äôs a shame to not have more data to test a full year ‚Äì most of the test set covers winter and spring.5\nTo investigate the model performance further, I‚Äôll plot the biggest outliers in the training and testing set (and ¬±1 week on either side):\n\nworst_preds &lt;- xgb_final_preds %&gt;%\n  mutate(abs_error = abs(n_bikes - .pred)) %&gt;%\n  group_by(data_set, site_name) %&gt;%\n  slice_max(abs_error, n = 1) %&gt;%\n  ungroup() %&gt;%\n  select(data_set, site_name, count_date, n_bikes, .pred, abs_error)\n\nsite_colors &lt;- setNames(td_colors$pastel6[1:5], unique(worst_preds$site_name))\n\nworst_preds %&gt;%\n  select(data_set, site_name, count_date) %&gt;%\n  mutate(\n    worst_date = count_date,\n    count_date = map(count_date, ~ seq.Date(.x - 7, .x + 7, by = \"day\"))\n  ) %&gt;%\n  unnest(count_date) %&gt;%\n  left_join(xgb_final_preds,\n            by = c(\"data_set\", \"site_name\", \"count_date\")) %&gt;%\n  filter(!is.na(n_bikes)) %&gt;%\n  mutate(site_name = fct_inorder(site_name)) %&gt;%\n  split(.$site_name) %&gt;%\n  map(\n    ~ ggplot(., aes(x = count_date)) +\n      geom_line(aes(y = n_bikes, color = site_name)) +\n      geom_line(aes(y = .pred), color = \"black\") +\n      geom_vline(aes(xintercept = worst_date), lty = 2) +\n      facet_wrap(~ data_set, scales = \"free_x\", ncol = 2) +\n      dunnr::add_facet_borders() +\n      scale_color_manual(values = site_colors) +\n      scale_x_date(breaks = unique(.$worst_date)) +\n      expand_limits(y = 0) +\n      theme(legend.position = \"none\") +\n      labs(x = NULL, y = NULL,\n           subtitle = .$site_name[[1]])\n  ) %&gt;%\n  reduce(`+`) +\n  plot_layout(ncol = 2)\n\n\n\n\nRelative to the noise in the data, that doesn‚Äôt look too bad. The Vernon St site has a series of n_bikes = 0 in a row that I‚Äôm guessing aren‚Äôt real (see aside). It was probably an issue with the counter, or maybe some construction on the street halting traffic for that period.\n\n\nThe Vernon St site has 6 dates with n_bikes = 0 all in a row:\n\nbike_ridership %&gt;%\n  filter(site_name == \"Vernon St\", n_bikes == 0) %&gt;%\n  pull(count_date)\n\n[1] \"2021-11-13\" \"2021-11-14\" \"2021-11-15\" \"2021-11-16\" \"2021-11-17\"\n[6] \"2021-11-18\"\n\n\nI also think some of the poor predictions can be explained by missing data. For instance, there are some poor predictions around 2021-11-23, where the model is over-estimating n_bikes at a few sites. Check out the weather around that day:\n\nbike_ridership %&gt;%\n  filter(count_date &gt; \"2021-11-20\", count_date &lt; \"2021-11-26\") %&gt;%\n  distinct(count_date, mean_temperature, total_precipitation,\n           speed_max_gust, snow_on_ground) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\ncount_date\nmean_temperature\ntotal_precipitation\nspeed_max_gust\nsnow_on_ground\n\n\n\n\n2021-11-21\n3.2\nNA\nNA\n2\n\n\n2021-11-22\n11.1\n42.2\n57\n2\n\n\n2021-11-23\n6.1\nNA\n41\n1\n\n\n2021-11-24\n0.1\nNA\n43\n1\n\n\n2021-11-25\n3.7\nNA\n43\n1\n\n\n\n\n\n\n\nThis happens to be around the time of a huge rain and wind storm in Nova Scotia that knocked out power for a lot of people. This is seen in the total_precipitation on 2021-11-22, but the next day is missing data so, in my pre-processing, it was imputed as 0, which is definitely a poor approximation of the truth. If I impute a large amount of rain (let‚Äôs say 15mm) for that date, here is how the predictions change:\n\naugment(\n  extract_workflow(xgb_final_fit),\n  xgb_final_preds %&gt;%\n    filter(count_date == \"2021-11-23\") %&gt;%\n    rename(.pred_old = .pred) %&gt;%\n    mutate(total_precipitation = 15)\n) %&gt;%\n  select(site_name, n_bikes, .pred_old, .pred_new = .pred) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nsite_name\nn_bikes\n.pred_old\n.pred_new\n\n\n\n\nDartmouth Harbourfront Greenway\n30\n122.19430\n40.232090\n\n\nHollis St\n19\n79.22559\n6.992786\n\n\nSouth Park St\n59\n320.20889\n165.825897\n\n\nVernon St\n133\n165.12285\n77.777298\n\n\nWindsor St\n26\n91.21191\n12.152815\n\n\n\n\n\n\n\nThat is a much better prediction for a rainy and windy day."
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#conclusions",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#conclusions",
    "title": "Predicting bike ridership: developing a model",
    "section": "Conclusions",
    "text": "Conclusions\nIn the end, the XGBoost model was able to best predict daily bike ridership.\nIs this model useful to anyone? Maybe. Is it useful just sitting on my computer? Definitely not. So in my next post, I‚Äôll put this model into production."
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#reproducibility",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#reproducibility",
    "title": "Predicting bike ridership: developing a model",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#footnotes",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/index.html#footnotes",
    "title": "Predicting bike ridership: developing a model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSince initial_time_split() doesn‚Äôt take strata argument, this would require defining a custom split function with rsample::make_splits(). This functionality might be available in future versions of rsample (see this issue).‚Ü©Ô∏é\nCheck out this blog post by Gavin Simpson for a great walkthrough of modeling seasonal data with GAMs.‚Ü©Ô∏é\nUsing the select_best = TRUE argument means only the best model in each workflow is returned. In this case, that means the workflows with the tunable spline feature of count_date_doy will only have one entry.‚Ü©Ô∏é\nYou could make the argument that the improvement is so minor that it is worth choosing the simpler linear regression model instead.‚Ü©Ô∏é\nThis is my reminder to myself to come back in fall 2022 to see how the model performs on a full summer season.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html",
    "title": "Predicting bike ridership: deploying the model",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(httr)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#introduction",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#introduction",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Introduction",
    "text": "Introduction\nThis is the last in a series of posts about predicting bike ridership in Halifax. Previously I:\n\nretrieved and prepared bicycle counter and weather data, then\ndeveloped and evaluated different machine learning models.\n\nHere, I will walk through my steps of putting the model into ‚Äúproduction‚Äù on Google Cloud Platform (GCP):\n\ndeploying an ETL pipeline with BigQuery and Cloud Scheduler,\nautomating model training with Cloud Run and Pub/Sub,\nserving predictions with a REST API via the plumber package (try it out here), and\ndeveloping a Shiny app for visualizing predictions (try it out here).\n\nThe source code for everything, including the Dockerfiles, can be found on GitHub here."
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#creating-the-project",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#creating-the-project",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Creating the project",
    "text": "Creating the project\nBefore anything, I had to create and set up a new project on GCP that I called hfx-bike-ridership. In addition to the very detailed GCP documentation, there are lots of great resources out there to walk through all the steps, like this one. In brief: after creating the project, I had to enable billing on the project, activate various APIs, create credentials (OAuth client and service account), and install the Cloud SDK."
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#etl-pipeline",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#etl-pipeline",
    "title": "Predicting bike ridership: deploying the model",
    "section": "ETL pipeline",
    "text": "ETL pipeline\nI next set up the ETL (extract, transform, load) pipeline to automatically extract the raw data from their sources, perform some transformations, and load it into a database.\nIn BigQuery, I created the bike_counts and weather data sets:\n\n\n\nI then wrote a script etl.R that retrieves and processes bike counter data from the Halifax open data platform, and weather data from the government of Canada. Most of the code there is copied from my previous post, except for the lines at the end to upload the data to BigQuery tables:\n\nbq_auth(\"oauth-client.json\")\n\nproject &lt;- \"hfx-bike-ridership\"\n\ndaily_counts_table &lt;- bq_table(project, \"bike_counts\", \"daily_counts\")\nweather_table &lt;- bq_table(project, \"weather\", \"daily_report\")\n\nbq_table_upload(daily_counts_table,\n                value = bike_counts_daily, fields = bike_counts_daily,\n                create_disposition = \"CREATE_IF_NEEDED\",\n                write_disposition = \"WRITE_TRUNCATE\")\n\nbq_table_upload(weather_table,\n                value = climate_report_daily, fields = climate_report_daily,\n                create_disposition = \"CREATE_IF_NEEDED\",\n                write_disposition = \"WRITE_TRUNCATE\")\n\nThis uses the bigrquery package to authenticate (bq_auth()) using my OAuth credentials and upload (bq_table_upload()) the data (creates if missing, overwrites if existing) to the tables daily_counts and daily_report. Here is what BigQuery looked like after running this script:\n\n\n\nAnd the daily_counts table:\n\n\n\nPutting these data into BigQuery, as opposed to a Cloud Storage bucket for example, is convenient for quick queries when I don‚Äôt want to load the data into R, like this one to find days with zero bikes counted:\n\n\n\nI could have simply wrote the data to CSV files and uploaded them via the GCP console, but that would defeat the purpose of next step: automation. To deploy my etl.R script, I wrote a fairly simple Dockerfile:\n\nFROM rocker/tidyverse:latest\n\nRUN R -e \"install.packages(c('bigrquery', 'httr'), repos = 'http://cran.us.r-project.org')\"\n\nADD oauth-client.json /home/rstudio\nADD etl/etl.R /home/rstudio\n\nCMD Rscript /home/rstudio/etl.R\n\nExplaining how Docker works is a bit out of scope for this post1 but from top to bottom:\n\nFROM rocker/tidyverse:latest\n\nThe tidyverse Docker image provided by RStudio, which you can read more about here: https://hub.docker.com/r/rocker/tidyverse.\nThis image is a bit overkill for this simple script. If I were worried about the size and portability of my image, I would instead use the base R image https://hub.docker.com/_/r-base and install only the packages I need from tidyverse.\n\nRUN R -e \"install.packages(c('bigrquery', 'httr'), repos = 'http://cran.us.r-project.org')\"\n\nInstalls the other packages I need besides those that come with tidyverse.\nParticularly, httr for interacting with APIs, and bigrquery for BigQuery.\n\nADD oauth-client.json /home/rstudio and ADD etl/etl.R /home/rstudio\n\nAdd my GCP credentials the ETL script to the Docker container.\n\nCMD Rscript /home/rstudio/etl.R\n\nRun the script.\n\n\nI then built the image, tagged it, and pushed it to the Container Registry with these commands:\n\ndocker build -t hfx-bike-ridership-etl .\ndocker tag hfx-bike-ridership-etl gcr.io/hfx-bike-ridership/hfx-bike-ridership-etl\ndocker push gcr.io/hfx-bike-ridership/hfx-bike-ridership-etl:latest\n\nNow that it exists on GCP, I want to schedule this container to run every week through Cloud Build and Cloud Scheduler. I used the googleCloudRunner package and followed these instructions:\n\nlibrary(googleCloudRunner)\n\ncr_setup() # Define project ID and authenticate with credentials\n\nbuild &lt;- cr_build_make(\"etl/hfx-bike-ridership-etl.yaml\")\n\ncr_schedule(\n  # Schedule for every Sunday at 12am\n  schedule = \"0 0 * * SUN\",\n  name = \"etl\",\n  httpTarget = cr_schedule_http(build),\n  region = \"northamerica-northeast1\"\n)\n\nHere is how the job showed up in Cloud Scheduler:\n\n\n\nAnd that‚Äôs the ETL taken care of. I left it for a day, and checked the data on Sunday morning to confirm that the data had been updated as expected."
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#model-tuning-and-training",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#model-tuning-and-training",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Model tuning and training",
    "text": "Model tuning and training\nWith the data in place, I then created a Cloud Storage bucket to store models, and wrote the tune.R script.\n\n\ntune.R\n# Setup -------------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(bigrquery)\nlibrary(googleCloudStorageR)\nlibrary(tictoc)\nsource(\"model/preprocess.R\")\n\nn_cores &lt;- parallel::detectCores(logical = FALSE)\nlibrary(doParallel)\ncl &lt;- makePSOCKcluster(n_cores - 1)\nregisterDoParallel(cl)\n# This extra step makes sure the parallel workers have access to the\n#  `tidyr::replace_na()` function during pre-processing\nparallel::clusterExport(cl, c(\"replace_na\"))\n\n# Read data ---------------------------------------------------\n\nbq_auth(path = \"oauth-client.json\")\n\n# Define the project, dataset and a new table for this project\nproject &lt;- \"hfx-bike-ridership\"\n\ndaily_counts_table &lt;- bq_table(project, \"bike_counts\", \"daily_counts\")\nbike_data &lt;- bq_table_download(daily_counts_table)\nbike_data_updated &lt;- bq_table_meta(daily_counts_table)$lastModifiedTime %&gt;%\n  as.numeric() %&gt;%\n  # `lastModifiedTime` is in milliseconds from 1970-01-01\n  {as.POSIXct(. / 1000, origin = \"1970-01-01\")}\n\nweather_table &lt;- bq_table(project, \"weather\", \"daily_report\")\nweather_data &lt;- bq_table_download(weather_table)\nweather_data_updated &lt;- bq_table_meta(weather_table)$lastModifiedTime %&gt;%\n  as.numeric() %&gt;%\n  {as.POSIXct(. / 1000, origin = \"1970-01-01\")}\n\n# Pre-process -------------------------------------------------------------\n\nbike_data &lt;- preprocess(bike_data, weather_data)\n\n# Splitting and resampling ------------------------------------------------\n\n# For the initial time split, data is ordered by date so that the training\n#  data consists of the earliest dates across all sites\nbike_data &lt;- bike_data %&gt;% arrange(count_date, site_name)\nbike_split &lt;- initial_time_split(bike_data, prop = 0.7)\n\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\n\n# ... but once I'm done splitting the data, I want to order by site followed by\n#  date for two reasons:\n#  (1) `step_impute_roll()` looks for rows in a window (ordered)\n#  (2) the `mase` metric compares predictions to the naive prediction, which\n#      uses the previous value\nbike_train &lt;- bike_train %&gt;% arrange(count_date, site_name)\nbike_test &lt;- bike_test %&gt;% arrange(count_date, site_name)\n\nbike_resamples &lt;-\n  sliding_period(bike_train, index = count_date,\n                 period = \"month\", lookback = 13, assess_stop = 1)\n\n# For model versioning, record the splitting and resampling strategy\nsplits_resamples &lt;- tibble(\n  n_data = nrow(bike_data), n_train = nrow(bike_train), n_test = nrow(bike_test),\n  min_date_train = min(bike_train$count_date),\n  max_date_train = max(bike_train$count_date),\n  min_date_test = min(bike_test$count_date),\n  max_date_test = max(bike_test$count_date),\n  prop = 0.7, resamples = \"sliding_period\",\n  resample_params = \"lookback = 13, assess_stop = 1\"\n)\n\n# Features ------------------------------------------------------------------\n\n# Get Canadian holidays\ncanada_holidays &lt;-\n  timeDate::listHolidays(\n    pattern = \"^CA|^Christmas|^NewYears|Easter[Sun|Mon]|^GoodFriday|^CaRem\"\n  )\n\nbike_recipe &lt;-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14 +\n           mean_temperature + total_precipitation + speed_max_gust +\n           snow_on_ground,\n         data = bike_train) %&gt;%\n  update_role(count_date, new_role = \"date_variable\") %&gt;%\n  step_date(count_date, features = c(\"dow\", \"doy\", \"year\"),\n            label = TRUE, ordinal = FALSE) %&gt;%\n  step_holiday(count_date, holidays = canada_holidays) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_impute_mean(speed_max_gust) %&gt;%\n  step_mutate_at(c(total_precipitation, snow_on_ground),\n                 fn = ~ replace_na(., 0)) %&gt;%\n  # Use a rolling window to impute temperature\n  step_impute_roll(mean_temperature, statistic = mean, window = 31) %&gt;%\n  step_zv(all_predictors())\n\n\n# Model spec and workflow -----------------------------------------------------\n\nxgb_spec &lt;- boost_tree(\n  mtry = tune(), trees = tune(), min_n = tune(),\n  tree_depth = tune(), learn_rate = tune()\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nbike_xgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(bike_recipe) %&gt;%\n  add_model(xgb_spec)\n\nbike_train_baked &lt;- prep(bike_recipe) %&gt;% bake(bike_train)\n\nxgb_grid &lt;- grid_latin_hypercube(\n  finalize(mtry(), select(bike_train_baked, -n_bikes)),\n  trees(), min_n(), tree_depth(), learn_rate(),\n  size = 100\n)\n\n# Tune --------------------------------------------------------------------\n\nbike_metrics &lt;- metric_set(rmse, mae, rsq, mase)\n\nset.seed(944)\ntic()\nxgb_tune &lt;- tune_grid(\n  bike_xgb_workflow, resamples = bike_resamples,\n  grid = xgb_grid, metrics = bike_metrics\n)\ntoc()\n\n# Choose the hyperparameters by MASE\nxgb_params &lt;- select_best(xgb_tune, metric = \"mase\")\n# Also get all the metrics on the training for the chosen parameters\ntrain_metrics &lt;- xgb_params %&gt;%\n  left_join(\n    collect_metrics(xgb_tune) %&gt;%\n      select(.metric, mean, n, std_err, .config),\n    by = \".config\"\n  )\n\n# Finalize and fit to the full training set\nbike_xgb_workflow_final &lt;- finalize_workflow(bike_xgb_workflow, xgb_params)\nbike_xgb_fit &lt;- bike_xgb_workflow_final %&gt;% fit(bike_train)\n\n# Predict on the test set and get metrics\ntest_metrics &lt;- bike_xgb_fit %&gt;%\n  augment(bike_test) %&gt;%\n  bike_metrics(truth = n_bikes, estimate = .pred)\n\n# Compile the model and  info into a list\nmodel_tuned &lt;- list(\n  timestamp = Sys.time(),\n  bike_data_updated = bike_data_updated,\n  weather_data_updated = weather_data_updated,\n  splits_resamples = splits_resamples,\n  xgb_params = xgb_params,\n  train_metrics = train_metrics,\n  test_metrics = test_metrics,\n  bike_xgb_fit = bike_xgb_fit\n)\n\n# Save model and model info -----------------------------------------------\n\n# Model object\nwrite_rds(model_tuned, \"model/tune/xgb-model-tuned.rds\")\ngcs_upload_set_limit(20000000L) # 20 Mb\nmetadata &lt;- gcs_upload(\"model/tune/xgb-model-tuned.rds\",\n                       name = \"tune/xgb-model-tuned.rds\",\n                       bucket = \"hfx-bike-ridership-model\")\ntimestamp &lt;- as.POSIXct(metadata$updated,\n                        tryFormats = \"%Y-%m-%dT%H:%M:%OS\", tz = \"GMT\")\n\n# XGB hyperparameters\nxgb_params &lt;- xgb_params %&gt;%\n  mutate(timestamp = model_tuned$timestamp) %&gt;%\n  select(-.config)\nwrite_csv(xgb_params, \"model/tune/xgb-params.csv\",\n          append = TRUE, col_names = FALSE)\nparams_table &lt;- bq_table(project, \"model_info\", \"params\")\nbq_table_upload(params_table,\n                value = xgb_params, fields = xgb_params,\n                create_disposition = \"CREATE_IF_NEEDED\",\n                write_disposition = \"WRITE_APPEND\")\n\n# Model metrics\nxgb_metrics &lt;- bind_rows(\n  train = train_metrics %&gt;%\n    select(metric = .metric, value = mean, n, std_err),\n  test = test_metrics %&gt;%\n    select(metric = .metric, value = .estimate),\n  .id = \"data_set\"\n) %&gt;%\n  mutate(timestamp = model_tuned$timestamp)\nwrite_csv(xgb_metrics, \"model/tune/xgb-metrics.csv\",\n          append = TRUE, col_names = FALSE)\nmetrics_table &lt;- bq_table(project, \"model_info\", \"metrics\")\nbq_table_upload(metrics_table,\n                value = xgb_metrics, fields = xgb_metrics,\n                create_disposition = \"CREATE_IF_NEEDED\",\n                write_disposition = \"WRITE_APPEND\")\n\n# Splitting and resampling strategy\nsplits_resamples &lt;- splits_resamples %&gt;%\n  mutate(timestamp = model_tuned$timestamp)\nwrite_csv(splits_resamples, \"model/tune/splits-resamples.csv\",\n          append = TRUE, col_names = FALSE)\nsplits_resamples_table &lt;- bq_table(project, \"model_info\", \"splits_resamples\")\nbq_table_upload(splits_resamples_table,\n                value = splits_resamples, fields = splits_resamples,\n                create_disposition = \"CREATE_IF_NEEDED\",\n                write_disposition = \"WRITE_APPEND\")\n\n\nThis actual model code and choices are mostly unchanged from my last post, but in brief it: retrieves the latest data from BigQuery, splits the data into training and testing, creates resamples, engineers features, tunes the XGBoost model, finds the best hyperparameters by MASE, and saves the model (as an R object) to the bucket. I also decided to keep track of metrics with a BigQuery table:\n\n\n\nI decided to keep this part of the pipeline manual. Tuning the XGBoost model takes a while on my machine, even with parallel processing, and I‚Äôve heard enough horror stories of surprise charges from cloud services that I don‚Äôt feel like risking it. I will periodically check in on my model predictions, and only plan on re-tuning if performance degrades appreciably.\nWhat I will automate, however, is model training. Every time the data is updated (i.e.¬†Sundays at midnight), I want to train the tuned model on the full data set. The idea is pretty simple: get the data from BigQuery, the tuned model from the bucket, fit to the data and save that fit to the same bucket. The tricky bit is that I want this process to trigger only when the data is updated. It turns out that BigQuery currently doesn‚Äôt have native functionality to trigger Cloud Run, so I had to use a workaround.\nFirst, I wrote the fit.R function to work as a plumber API (these instructions were helpful):\n\n\nfit.R\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidymodels)\nlibrary(bigrquery)\nlibrary(googleCloudStorageR)\nlibrary(googleCloudRunner)\nlibrary(plumber)\nsource(\"preprocess.R\")\n\nbq_auth(path = \"oauth-client.json\")\ngcs_auth(\"oauth-client.json\")\ngcs_upload_set_limit(20000000L) # 20 Mb\n\n# This function will retrieve the latest data from BigQuery, the trained\n#  model from GCS, and fit an XGBoost model, which is saved to GCS\npub &lt;- function(message) {\n  # Define the project, dataset and a new table for this project\n  project &lt;- \"hfx-bike-ridership\"\n\n  daily_counts_table &lt;- bq_table(project, \"bike_counts\", \"daily_counts\")\n  bike_data &lt;- bq_table_download(daily_counts_table)\n  bike_data_updated &lt;- bq_table_meta(daily_counts_table)$lastModifiedTime %&gt;%\n    as.numeric() %&gt;%\n    {as.POSIXct(. / 1000, origin = \"1970-01-01\")}\n\n\n  weather_table &lt;- bq_table(project, \"weather\", \"daily_report\")\n  weather_data &lt;- bq_table_download(weather_table)\n  weather_data_updated &lt;- bq_table_meta(weather_table)$lastModifiedTime %&gt;%\n    as.numeric() %&gt;%\n    {as.POSIXct(. / 1000, origin = \"1970-01-01\")}\n\n  bike_data &lt;- preprocess(bike_data, weather_data)\n  xgb_tuned &lt;- gcs_get_object(\"tune/xgb-model-tuned.rds\",\n                              bucket = \"hfx-bike-ridership-model\",\n                              parseFunction = gcs_parse_rds)\n\n  message(\"Writing updating xgb-fit\")\n  xgb_fit &lt;- list(\n    tune_timestamp = xgb_tuned$timestamp,\n    timestamp = Sys.time(),\n    bike_data_updated = bike_data_updated,\n    weather_data_updated = weather_data_updated,\n    bike_xgb_fit = fit(xgb_tuned$bike_xgb_fit, bike_data)\n  )\n\n  f &lt;- function(input, output) write_rds(input, output)\n  metadata &lt;- gcs_upload(xgb_fit, name = \"xgb-fit.rds\",\n                         bucket = \"hfx-bike-ridership-model\",\n                         object_function = f)\n\n  return(TRUE)\n}\n\n#' Receive pub/sub message\n#' @post /pubsub\n#' @param message a pub/sub message\nfunction(message = NULL) {\n  message(\"Received message \", message)\n  googleCloudRunner::cr_plumber_pubsub(message, pub)\n}\n\n\nI wrote a Docker file to containerize the API, built the image, and pushed it to the Container Registry. I then went to Cloud Run, created a new service called hfx-bike-ridership-fit using the just-uploaded Docker image:\n\n\n\nOnce up and running, this gave me a URL from which to query the API:\n\n\n\nNext, I had to set up an internal messaging system. The steps were:\n\nI added a message(\"Finished ETL pipeline\") at the end of the etl.R script to indicate that the data was updated.\nThis message shows up in Cloud Logging, so I added a ‚Äúsink‚Äù (which is how Logging routes messages) to look for this specific log.\n\n\n\n\n\nThe destination of this sink is a Pub/Sub topic called data-updated.\nI added a subscription to this topic which pushes a POST request to the API.\n\n\n\n\n\nThe POST request triggers the model fitting code, and the new model is uploaded to the Storage bucket.\n\nThis seems like a complex workaround for a fairly simple task ‚Äì I might be missing an easier method. Also, it may have made more sense to just have the model re-train on a weekly schedule (just after the ETL pipeline), but I wanted more flexibility for ad hoc updates. Regardless, both the ETL and model training are now fully automated."
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#deployment",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#deployment",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Deployment",
    "text": "Deployment\n\nREST API\nTo serve predictions, I wrote another plumber API in the api.R script:\n\n\napi.R\n#* @apiTitle Predict bike ridership in Halifax, NS\n#* @apiDescription This API serves predictions for the daily number of bicyclists passing particular sites around Halifax, Nova Scotia. For more information, check out the [source code](https://github.com/taylordunn/hfx-bike-ridership), my [post about the data](https://tdunn.ca/posts/2022-04-27-predicting-bike-ridership-getting-the-data/), and [my post about developing the model](https://tdunn.ca/posts/2022-04-29-predicting-bike-ridership-developing-a-model/).\n#* @apiContact list(name = \"Taylor Dunn\", url = \"http://www.tdunn.ca\", email = \"t.dunn19@gmail.com\")\n#* @apiVersion 1.0\n\nlibrary(plumber)\nlibrary(dplyr)\nlibrary(tidymodels)\nlibrary(bigrquery)\nlibrary(googleCloudStorageR)\n\nbq_auth(path = \"oauth-client.json\",\n        email = \"hfx-bike-ridership@hfx-bike-ridership.iam.gserviceaccount.com\")\ngcs_auth(\"oauth-client.json\")\n\nproject &lt;- \"hfx-bike-ridership\"\n\nmessage(\"Reading data\")\ndaily_counts_table &lt;- bq_table(project, \"bike_counts\", \"daily_counts\")\nbike_data &lt;- bq_table_download(daily_counts_table)\n\nmessage(\"Loading model\")\nxgb_fit &lt;- gcs_get_object(\"xgb-fit.rds\", bucket = \"hfx-bike-ridership-model\",\n                          parseFunction = gcs_parse_rds)\n\nsite_names &lt;- c(\"Dartmouth Harbourfront Greenway\", \"Hollis St\",\n                \"South Park St\", \"Vernon St\", \"Windsor St\")\n\n#* @param count_date:str The date in YYYY-MM-DD format.\n#* @param site_name:[str] The location of the bike counter. One of \"Dartmouth Harbourfront Greenway\", \"Hollis St\", \"South Park St\", \"Vernon St\", \"Windsor St\".\n#* @param n_bikes_lag_14:[int] The number of bikes measured at the given `site_name` 14 days ago. If not provided, will attempt to impute with the actual value 14 days before `count_date`.\n#* @param mean_temperature:numeric The daily mean temperature. If not provided, will impute with the rolling mean.\n#* @param total_precipitation:numeric The daily amount of precipitation in mm. If not provided, will impute with zero.\n#* @param snow_on_ground:numeric The daily amount of snow on the ground in cm. If not provided, will impute with zero.\n#* @param speed_max_gust:numeric The daily maximum wind speed in km/h. If not provided, will impute with the mean in the training set.\n#* @get /n_bikes\nfunction(count_date, site_name = NA_character_, n_bikes_lag_14 = NA_integer_,\n         mean_temperature = NA_real_, total_precipitation = NA_real_,\n         snow_on_ground = NA_real_, speed_max_gust = NA_real_) {\n\n  # If not provided, use all `site_name`s\n  if (any(is.na(site_name))) {\n    site_name &lt;- site_names\n  } else {\n    site_name &lt;- match.arg(\n      site_name, choices = site_names, several.ok = TRUE\n    )\n  }\n\n  count_date &lt;- as.Date(count_date)\n\n  # Get the 14-day lagged bike counts for each site\n  if (!is.na(n_bikes_lag_14) & length(site_name) != length(n_bikes_lag_14)) {\n    return(list(\n      status = 400,\n      message = \"Must provide a value of `n_bikes_lag_14` for every given `site_name`.\"\n    ))\n  } else {\n    d &lt;- tibble(site_name = .env$site_name, count_date = .env$count_date,\n                count_date_lag_14 = count_date - 14,\n                n_bikes_lag_14 = .env$n_bikes_lag_14)\n\n    if (sum(is.na(d$n_bikes_lag_14)) &gt; 0) {\n      message(\"Imputing `n_bikes_lag_14`\")\n      d &lt;- d %&gt;%\n        left_join(\n          bike_data %&gt;%\n            select(site_name, count_date_lag_14 = count_date,\n                   n_bikes_lag_14_impute = n_bikes),\n          by = c(\"site_name\", \"count_date_lag_14\")\n        ) %&gt;%\n        mutate(\n          n_bikes_lag_14 = ifelse(is.na(n_bikes_lag_14),\n                                  n_bikes_lag_14_impute, n_bikes_lag_14)\n        ) %&gt;%\n        select(-n_bikes_lag_14_impute)\n\n      if (sum(is.na(d$n_bikes_lag_14)) &gt; 0) {\n        return(list(\n          status = 400,\n          message = paste0(\n            \"Could not find `n_bikes_lag_14` values on date \", count_date,\n            \" for these sites \",\n            filter(d, is.na(n_bikes_lag_14)) %&gt;% pull(site_name) %&gt;% paste(collapse = \", \"),\n            \". Please provide your own `n_bikes_lag_14`, or choose a different `count_date`.\"\n          )\n        ))\n      }\n    }\n  }\n\n  # Add weather variables\n  d &lt;- d %&gt;%\n    mutate(\n      n_bikes_lag_14 = as.numeric(n_bikes_lag_14),\n      mean_temperature = as.numeric(mean_temperature),\n      total_precipitation = as.numeric(total_precipitation),\n      snow_on_ground = as.numeric(snow_on_ground),\n      speed_max_gust = as.numeric(speed_max_gust)\n    )\n\n  augment(xgb_fit$bike_xgb_fit, d)\n}\n\n#* @get /model_info\n#* @response 200 Returns model information: timestamps of when the model was last trained (`timestamp`), the model was last tuned (`tune_timestamp`), the bicycle data was last updated (`bike_data_updated`), the weather data was last updated (`weather_data_updated`).\nfunction() {\n  list(\n    timestamp = xgb_fit$timestamp,\n    tune_timestamp = xgb_fit$tune_timestamp,\n    bike_data_updated = xgb_fit$bike_data_updated,\n    weather_data_updated = xgb_fit$weather_data_updated\n  )\n}\n\n\nThis reads in the model from the Cloud Storage bucket and the latest bike data from BigQuery. As inputs, it requires only a single date (count_date), for which it will return predictions for all 5 sites. One or more specific sites can also be provided (site_name). If the lagged values (n_bikes_lag_14) are not provided, then they will be imputed from the bike data (an error will be returned if the lagged value cannot be imputed, i.e.¬†there is no data 14 days before count_date). The weather inputs mean_temperature, total_precipitation, snow_on_ground, and speed_max_gust are imputed if not provided.\nAs with fit.R, I put this into a Docker container, pushed to Container Registry, and created a Cloud Run service hfx-bike-ridership-api using that image.\n\nUnlike the previous Cloud Run service which only accepts internal requests, this one is publicly available. For instance, you can get a prediction for n_bikes on Hollis St for May 23rd, 2022 with the following R code:\n\nbase_url &lt;- \"https://hfx-bike-ridership-api-74govvz7xq-uc.a.run.app/\"\nquery &lt;- \"n_bikes?count_date=2022-05-23&site_name=Hollis St\"\n\npaste0(base_url, query) %&gt;%\n  URLencode() %&gt;%\n  GET() %&gt;%\n  content(as = \"parsed\") %&gt;%\n  purrr::flatten()\n\n$site_name\n[1] \"Hollis St\"\n\n$count_date\n[1] \"2022-05-23\"\n\n$count_date_lag_14\n[1] \"2022-05-09\"\n\n$n_bikes_lag_14\n[1] 86\n\n$.pred\n[1] 27.9456\n\n\nA great feature of plumber is that provides an HTML interface for documenting and interacting with REST APIs. Check out this API here: https://hfx-bike-ridership-api-74govvz7xq-uc.a.run.app/docs/.\n\n\nScreenshot for posterity\n\n\n\nI also added a model_info option, which can be queried to see timestamps of when the model was last tuned and trained, and when the data were last updated:\n\nquery &lt;- \"model_info\"\npaste0(base_url, query) %&gt;%\n  URLencode() %&gt;%\n  GET() %&gt;%\n  content(as = \"parsed\") %&gt;%\n  purrr::flatten()\n\n$timestamp\n[1] \"2022-10-23 04:02:27\"\n\n$tune_timestamp\n[1] \"2022-05-20 15:23:46\"\n\n$bike_data_updated\n[1] \"2022-10-23 04:02:05\"\n\n$weather_data_updated\n[1] \"2022-10-23 04:02:08\"\n\n\n\n\nShiny dashboard\nLastly, I wrote a Shiny dashboard to visualize predictions, app.R:\n\n\napp.R\nlibrary(shiny)\nlibrary(shinydashboard)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(workflows)\nlibrary(bigrquery)\nlibrary(googleCloudStorageR)\nlibrary(DT)\nlibrary(dunnr)\nsource(\"funcs.R\")\n\n# Plotting\nlibrary(showtext)\nsysfonts::font_add_google(\"Roboto Condensed\")\nshowtext_auto()\ntheme_set(theme_td(base_size = 14))\nset_geom_fonts()\nset_palette()\n\n# Authentication to GCP\nproject &lt;- \"hfx-bike-ridership\"\nbq_auth(path = \"oauth-client.json\")\ngcs_auth(\"oauth-client.json\")\nbike_counts_table &lt;- bq_table(project, \"bike_counts\", \"daily_counts\")\nweather_table &lt;- bq_table(project, \"weather\", \"daily_report\")\n\nserver &lt;- function(input, output, session) {\n  # Import data and model ---------------------------------------------------\n  data &lt;- reactiveValues()\n  min_date &lt;- reactiveVal()\n  max_date &lt;- reactiveVal()\n  model &lt;- reactiveVal()\n\n  observe({\n    # Re-reads data every hour\n    invalidateLater(1000 * 60 * 60)\n\n    message(\"Reading data and model\")\n    bike_data_raw &lt;- bq_table_download(bike_counts_table)\n    weather_data_raw &lt;- bq_table_download(weather_table)\n    model(gcs_get_object(\"xgb-fit.rds\",\n                         bucket = \"hfx-bike-ridership-model\",\n                         parseFunction = gcs_parse_rds))\n\n    bike_data &lt;- bike_data_raw %&gt;%\n      preprocess_bike_data() %&gt;%\n      # Only include the last 14 days\n      filter(count_date &gt;= max(count_date) - 13)\n    min_date(min(bike_data$count_date))\n    max_date(max(bike_data$count_date))\n    bike_data_future &lt;- bike_data %&gt;%\n      transmute(\n        count_date = count_date + 14, site_name, n_bikes_lag_14 = n_bikes\n      )\n\n    weather_data &lt;- weather_data_raw %&gt;%\n      preprocess_weather_data() %&gt;%\n      filter(report_date &gt;= min(bike_data$count_date),\n             report_date &lt;= max(bike_data$count_date))\n    weather_data_future &lt;- weather_data %&gt;%\n      transmute(\n        report_date = report_date + 14,\n        # Impute temperature and wind speed with the mean\n        mean_temperature = round(mean(weather_data$mean_temperature,\n                                      na.rm = TRUE), 1),\n        speed_max_gust = round(mean(weather_data$speed_max_gust,\n                                    na.rm = TRUE)),\n        # Impute precipitation and snow with zero\n        total_precipitation = 0, snow_on_ground = 0\n      )\n\n    data$bike &lt;- bind_rows(bike_data, bike_data_future)\n    data$weather &lt;- bind_rows(weather_data, weather_data_future)\n  })\n\n  bike_weather_data &lt;- reactive({\n    data$bike %&gt;%\n      left_join(data$weather, by = c(\"count_date\" = \"report_date\"))\n  })\n\n  # Model info --------------------------------------------------------------\n  output$model_info_1 &lt;- renderText({\n    HTML(\n      paste(\n        \"This Shiny app visualizes predictions of the daily number of bicyclists passing various bike counter sites around Halifax, Nova Scotia, in a four-week window.\",\n        \"Check out the &lt;a href='https://github.com/taylordunn/hfx-bike-ridership'&gt;source code here&lt;/a&gt;, and &lt;a href='https://tdunn.ca/posts/2022-05-19-predicting-bike-ridership-deploying-the-model/'&gt;this write-up&lt;/a&gt; for more information.\",\n        paste0(\"&lt;br&gt;Data are updated, and the model is re-trained on a schedule: currently every Sunday at midnight AST, and sometimes manually by me. \",\n               \"The current data go up to \",\n               \"&lt;b&gt;\", max_date(), \"&lt;/b&gt;\",\n               \" as indicated by the vertical dashed line in the plots.\"),\n        \"&lt;br&gt;The locations of the sites are overlaid on a map of Halifax below:\",\n        sep = \"&lt;br&gt;\"\n      )\n    )\n  })\n\n  output$model_info_2 &lt;- renderText({\n    HTML(\n      paste(\n        \"&lt;br&gt;\",\n        \"In addition to site, other features of the model are:\",\n        paste0(\"&lt;ul&gt;\",\n               \"&lt;li&gt;date features: day of week, day of year, year, and Canadian holidays&lt;/li&gt;\",\n               \"&lt;li&gt;the number of bikes counted 14 days ago&lt;/li&gt;\",\n               \"&lt;li&gt;weather features: daily mean temperature, total precipitation, maximum gust speed, and snow on the ground\",\n               \"&lt;/ul&gt;\"),\n        \"See more information about the features and how missing data are handled &lt;a href='https://tdunn.ca/posts/2022-04-29-predicting-bike-ridership-developing-a-model/'&gt;in this post&lt;/a&gt;.\",\n        \"&lt;br&gt;\"\n      )\n    )\n  })\n\n  # Plotting helpers --------------------------------------------------------\n  scale_x &lt;- reactive({\n    scale_x_date(NULL, limits = c(min_date() - 1, max_date() + 14),\n                 breaks = seq.Date(min_date() - 1, max_date() + 14, \"7 days\"),\n                 date_labels = \"%b %d\")\n  })\n  vline &lt;- reactive({\n    geom_vline(xintercept = max_date(), lty = 2, size = 1)\n  })\n\n  # Bike predictions --------------------------------------------------------\n  output$n_bikes_plot &lt;- renderPlot({\n    workflows:::augment.workflow(model()$bike_xgb_fit,\n                                 bike_weather_data()) %&gt;%\n      ggplot(aes(x = count_date)) +\n      vline() +\n      geom_line(aes(y = .pred), color = \"black\", size = 1) +\n      geom_point(aes(y = n_bikes, fill = site_name),\n                 color = \"black\", shape = 21, size = 4) +\n      facet_wrap(~ site_name, ncol = 1) +\n      expand_limits(y = 0) +\n      labs(y = NULL) +\n      scale_x() +\n      labs(title = \"Number of bikes vs date\",\n           subtitle = \"Coloured points show actual values, black lines are predictions\") +\n      theme(legend.position = \"none\") +\n      dunnr::add_facet_borders()\n  })\n\n  # Weather data ------------------------------------------------------------\n  temperature_plot &lt;- reactive({\n    data$weather %&gt;%\n      filter(!is.na(mean_temperature)) %&gt;%\n      mutate(var = \"Mean daily temperature (celsius)\") %&gt;%\n      ggplot(aes(x = report_date, y = mean_temperature)) +\n      vline() +\n      geom_point(fill = td_colors$nice$strong_red, shape = 21, size = 4) +\n      facet_wrap(~ var) +\n      labs(y = NULL,\n           title = \"Weather vs date\",\n           subtitle = \"Use the table below to edit values for prediction\") +\n      scale_x() +\n      theme(axis.text.x = element_blank()) +\n      dunnr::add_facet_borders()\n  })\n  precipitation_plot &lt;- reactive({\n    data$weather %&gt;%\n      filter(!is.na(total_precipitation)) %&gt;%\n      mutate(var = \"Total daily precipitation (mm)\") %&gt;%\n      ggplot(aes(x = report_date, y = total_precipitation)) +\n      vline() +\n      geom_col(fill = td_colors$nice$spanish_blue, color = \"black\") +\n      facet_wrap(~ var) +\n      expand_limits(y = 5) +\n      scale_y_continuous(NULL, expand = expansion(mult = c(0, 0.05))) +\n      scale_x() +\n      theme(axis.text.x = element_blank()) +\n      dunnr::add_facet_borders()\n  })\n\n  snow_plot &lt;- reactive({\n    data$weather %&gt;%\n      filter(!is.na(snow_on_ground)) %&gt;%\n      mutate(var = \"Snow on ground (cm)\") %&gt;%\n      ggplot(aes(x = report_date, y = snow_on_ground)) +\n      vline() +\n      geom_col(fill = td_colors$nice$charcoal, color = \"black\") +\n      facet_wrap(~ var) +\n      expand_limits(y = 5) +\n      scale_y_continuous(NULL, expand = expansion(mult = c(0, 0.05))) +\n      scale_x() +\n      theme(axis.text.x = element_blank()) +\n      dunnr::add_facet_borders()\n  })\n  wind_plot &lt;- reactive({\n    data$weather %&gt;%\n      filter(!is.na(speed_max_gust)) %&gt;%\n      mutate(var = \"Maximum wind gust (km/h)\") %&gt;%\n      ggplot(aes(x = report_date, y = speed_max_gust)) +\n      vline() +\n      geom_point(fill = td_colors$nice$emerald, shape = 21, size = 4) +\n      facet_wrap(~ var) +\n      labs(y = NULL) +\n      scale_x()\n  })\n\n  output$weather_plot &lt;- renderPlot({\n    temperature_plot() +\n      precipitation_plot() +\n      snow_plot() +\n      wind_plot() +\n      plot_layout(ncol = 1)\n  })\n\n  output$weather_table &lt;- renderDataTable(\n    datatable(\n      data$weather,\n      rownames = FALSE, escape = FALSE,\n      colnames = c(\"Date\", \"Temp.\", \"Precip.\", \"Snow\", \"Wind\"),\n      editable = list(target = \"cell\", numeric = c(2, 3, 4, 5)),\n      options = list(pageLength = 7, dom = \"tp\"),\n      caption = \"Double click a cell to edit values. Plots and predictions will update automatically.\"\n    ) %&gt;%\n      DT::formatStyle(names(data$weather), lineHeight = \"80%\")\n  )\n\n  observeEvent(input$weather_table_cell_edit, {\n    row &lt;- input$weather_table_cell_edit$row\n    col &lt;- input$weather_table_cell_edit$col\n    data$weather[row, col + 1] &lt;- input$weather_table_cell_edit$value\n  })\n}\n\nui &lt;- dashboardPage(\n  skin = \"yellow\",\n  dashboardHeader(title = \"Predicting bike ridership in Halifax, NS\",\n                  titleWidth = 500),\n  dashboardSidebar(disable = TRUE),\n  dashboardBody(\n    tags$head(\n      tags$link(rel = \"stylesheet\", type = \"text/css\", href = \"custom.css\")\n    ),\n    fluidRow(\n    column(\n      width = 3,\n      box(\n        title = HTML(paste0(as.character(icon(\"info\")), \" &lt;b&gt;Info&lt;/b&gt;\")),\n        width = 12,\n        style = \"overflow-x: scroll;\",\n        uiOutput(\"model_info_1\"),\n        img(src = \"bike-counter-sites.png\",\n            style = \"width: 300px; display: block; margin-left: auto; margin-right: auto;\"),\n        uiOutput(\"model_info_2\")\n      )\n    ),\n    column(\n      width = 5,\n      box(\n        width = 12,\n        style = \"overflow-x: scroll;\",\n        plotOutput(\"n_bikes_plot\", height = \"800px\")\n      )\n    ),\n    column(\n      width = 4,\n      box(\n        width = 12,\n        style = \"overflow-x: scroll;\",\n        plotOutput(\"weather_plot\", height = \"600px\"),\n        dataTableOutput(\"weather_table\")\n      )\n    )\n  )\n  )\n)\n\nshinyApp(ui, server)\n\n\nI wrote the Docker file, pushed it to Container Registry, and deployed on Cloud Run.\nAssuming I haven‚Äôt shut it down (and that my billing information is not out of date), you can try the app here or embedded below:\n\n\n\n\nIn terms of design, I went with a three column layout with content organized into shinydashboard::box()s. The left-most column has some basic information, including the date of when the data and model were last updated. I also included a map showing the locations of the five sites:\n\n\n\nThe main interest of this dashboard is the forecasted number of bikes, so it takes the centre column:\n\nThere is a lot of data involved in this proejct, but I decided to keep this app fairly small in scope. Just the last 14 days and the next 14 days (relative to the when data/model were updated) are shown here.\nThe third column shows the most interesting predictors of the model ‚Äì the weather variables:\n\n\n\nThe 14 days to the left of the dashed line are actual values, and the 14 days to right right are imputed future values.2 The table at the bottom lists all of the visualized weather data. To add some interactivity, I decided to make this part editable:\n\nEverything is reactive to this table, so the plots will be updated immediately:\n\nand so will the predictions:\n\nThis allows me to ask questions like: how will the predicted number of bicyclists change if it downpours tomorrow?"
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#conclusion",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#conclusion",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post I used Google Cloud Platform to automate the data ETL pipeline and model training. The model was then deployed in a REST API and Shiny dashboard. When considering the full MLOps cycle, the biggest missing piece is some sort of monitoring functionality. This could include data validation (e.g.¬†trigger an alert if new data have abnormal or missing values) and model drift detection (e.g.¬†model performance is below some threshold which triggers re-tuning). But as far as personal projects go, I‚Äôm content to leave it here and re-visit it every once in a while to see how the data and predictions are holding up.\nRegardless of the value of the model, this was a great learning experience. I‚Äôd not used GCP much before this (we use AWS at my company) but it wasn‚Äôt too painful a transition between cloud services. The packages by Mark Edmondson (googleCloudStorageR, googleCloudRunner), and the accompanying documentation, were a great help.\nDocumenting my process here was important to me. I learn best by doing, and second best by seeing what others do in open source projects like this. I hope that this walkthrough and code can help others in getting their own MLOps projects up-and-running."
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#reproducibility",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#reproducibility",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-10-27\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [4eb5bf2] 2022-10-26: Added font import to style sheet\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#footnotes",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/index.html#footnotes",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor great introductions to Docker for R users, check out this by Colin Fay and this by Andrew Heiss.‚Ü©Ô∏é\nIt would be cool to impute these values with weather forecasts, but I couldn‚Äôt find a reliable/free way to get that data.‚Ü©Ô∏é"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "tdunn",
    "section": "",
    "text": "Projects\n\nA collection of my open source projects.\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n        hfx-bike-ridership\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      An end-to-end machine learning project to predict bike ridership in Halifax, Nova Scotia.\n    \n  \n  \n    \n      \n        tidytuesday-dashboard\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      A Shiny dashboard compiling tweets from the TidyTuesday project.\n    \n  \n  \n    \n      \n        canadacovid\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      An R package for the Canadian COVID-19 tracker API.\n    \n  \n  \n    \n      \n        canadacovidshiny\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      A Shiny dashboard reporting and visualizing the latest COVID-19 data in Canada.\n    \n  \n  \n    \n      \n        dunnr\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      My personal R package of commonly used functions and templates.\n    \n  \n  \n    \n      \n        gasr\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      An R package for simulating goal attainment scaling (GAS) data.\n    \n  \n  \n    \n      \n        islr-tidy\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      An Introduction to Statistical Learning translated with the tidyverse and tidymodels.\n    \n  \n\n\nNo matching items"
  }
]