[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2022\n\n\nTaylor Dunn, Harlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2022\n\n\nTaylor Dunn, Tristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nAPI\n\n\n\n\nPart 1 of predicting bike ridership in Halifax, Nova Scotia. In this post, I retrieve and explore data from two open APIs.\n\n\n\n\n\n\nApr 27, 2022\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\nordinal\n\n\nfrequentist statistics\n\n\n\n\nA theoretical and applied walkthrough of ordinal regression. Part 1: the frequentist approach with ordinal.\n\n\n\n\n\n\nMar 15, 2020\n\n\nTaylor Dunn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html",
    "title": "Ordinal regression in R: part 1",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(dunnr)\nlibrary(gt)\nlibrary(broom)\nlibrary(patchwork)\n\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()\n\nwine_red <- \"#58181F\"\nupdate_geom_defaults(\"point\", list(color = wine_red))\nupdate_geom_defaults(\"line\", list(color = wine_red))\nThe purpose of this post is to learn more about ordinal regression models (a.k.a. cumulative link, proportional odds, ordered logit models, etc.) and practice their implementation in R. This is part 1, where I’ll be taking the frequentist approach via the ordinal package. There are other options, like MASS::polr, but two features in particular drew me to ordinal: (1) it allows for random effects, and (2) it has broom::tidy methods available.\nParticularly, I’ll be following along with"
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#setup",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#setup",
    "title": "Ordinal regression in R: part 1",
    "section": "Setup",
    "text": "Setup\nImport ordinal, and the included data set wine:\n\nlibrary(ordinal)\ndata(wine)\nwine <- as_tibble(wine)\nglimpse(wine)\n\nRows: 72\nColumns: 6\n$ response <dbl> 36, 48, 47, 67, 77, 60, 83, 90, 17, 22, 14, 50, 30, 51, 90, 7…\n$ rating   <ord> 2, 3, 3, 4, 4, 4, 5, 5, 1, 2, 1, 3, 2, 3, 5, 4, 2, 3, 3, 2, 5…\n$ temp     <fct> cold, cold, cold, cold, warm, warm, warm, warm, cold, cold, c…\n$ contact  <fct> no, no, yes, yes, no, no, yes, yes, no, no, yes, yes, no, no,…\n$ bottle   <fct> 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5…\n$ judge    <fct> 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3…\n\n\nwine is a data set from Randall (1989) of wine bitterness ratings from multiple judges. The variables are as follows:\n\nOutcome:\n\nresponse: wine bitterness rating on a 0-100 scale\nrating: ordered factor with 5 levels (grouped version of response) with 1 = “least bitter” and 5 = “most bitter”\n\nTreatment factors:\n\ntemp: temperature during wine production (cold and warm)\ncontact: contact between juice and skins during wine production (no and yes)\n\nRandom effects\n\nbottle with 8 levels\njudge with 9 levels\n\n\nRelationship between response and rating:\n\nwine %>%\n  ggplot(aes(y = rating, x = response)) +\n  geom_boxplot(width = 0.5) +\n  geom_jitter(alpha = 0.5)\n\n\n\n\nNote that there is no overlap between the levels.\nThere are 72 total observations with the following ratings distribution by treatment and random effects:\n\nwine %>%\n  transmute(temp, contact, bottle, judge, rating = as.numeric(rating)) %>%\n  pivot_wider(names_from = judge, values_from = rating) %>%\n  gt() %>%\n  tab_spanner(columns = `1`:`9`, label = \"judge\") %>%\n  data_color(\n    columns = `1`:`9`,\n    colors = scales::col_numeric(\n      palette = c(\"white\", wine_red), domain = c(1, 5)\n    )\n  )\n\n\n\n\n\n  \n  \n    \n      temp\n      contact\n      bottle\n      \n        judge\n      \n    \n    \n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    cold\nno\n1\n2\n1\n2\n3\n2\n3\n1\n2\n1\n    cold\nno\n2\n3\n2\n3\n2\n3\n2\n1\n2\n2\n    cold\nyes\n3\n3\n1\n3\n3\n4\n3\n2\n2\n3\n    cold\nyes\n4\n4\n3\n2\n2\n3\n2\n2\n3\n2\n    warm\nno\n5\n4\n2\n5\n3\n3\n2\n2\n3\n3\n    warm\nno\n6\n4\n3\n5\n2\n3\n4\n3\n3\n2\n    warm\nyes\n7\n5\n5\n4\n5\n3\n5\n2\n3\n4\n    warm\nyes\n8\n5\n4\n4\n3\n3\n4\n3\n4\n4\n  \n  \n  \n\n\n\n\nSo each bottle had a particular temp and contact (2 bottles for each of the 4 combinations), and each judge rated the bitterness each bottle.\nBefore modeling, can we see a clear effect of temp and contact?\n\nwine %>%\n  count(contact, rating, temp) %>%\n  mutate(temp = fct_rev(temp)) %>%\n  ggplot(aes(x = temp, y = rating, color = temp)) +\n  geom_point(aes(group = temp, size = n)) +\n  facet_wrap(~contact, scales = \"free_x\",\n             labeller = labeller(contact = label_both)) +\n  scale_size(breaks = c(1, 2, 4, 6, 8)) +\n  add_facet_borders()\n\n\n\n\nAt a glance, it looks like the temp = warm and contact = yes is associated with higher ratings."
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#the-cumulative-link-model",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#the-cumulative-link-model",
    "title": "Ordinal regression in R: part 1",
    "section": "The cumulative link model",
    "text": "The cumulative link model\n\nTheory\nThe ordinal response \\(y_i\\) falls into response category \\(j\\) (out of \\(J\\) total) with probability \\(\\pi_{ij}\\). The cumulative probabilities are defined:\n\\[\nP(y_i \\leq j) = \\pi_{i1} + \\dots + \\pi_{ij}.\n\\]\nAs an oversimplification, suppose that each probability \\(\\pi_{ij}\\) is equal to the proportion of that response in the wine data. Then the cumulative “probability” can be visualized:\n\nwine_prop <- wine %>%\n  count(rating) %>%\n  mutate(p = n / sum(n), cumsum_p = cumsum(p))\n\n(\n  ggplot(wine_prop, aes(x = rating, y = p)) +\n    geom_col(fill = wine_red) +\n    scale_y_continuous(labels = scales::percent, expand = c(0, 0)) +\n    labs(x = \"j\", y = \"proportion\")\n) +\n  (\n    ggplot(wine_prop, aes(x = as.integer(rating), y = cumsum_p)) +\n      geom_point(size = 2) +\n      geom_line(size = 1) +\n      labs(x = \"j\", y = \"cumulative proportion\")\n  ) +\n  (\n    ggplot(wine_prop,\n        aes(x = as.integer(rating), y = log(cumsum_p) - log(1 - cumsum_p))) +\n      geom_point(size = 2) +\n      geom_line(size = 1) +\n      labs(x = \"j\", y = \"logit(cumulative proportion)\")\n  )\n\n\n\n\nWe will explore other links, but first the most common, the logit link, which is depicted in the right-most panel of the above figure:\n\\[\n\\text{logit} (P(y_i \\leq j) = \\log \\frac{P(y_i \\leq j)}{1 - P(y_i \\leq j)}\n\\]\nNote that the above function is defined for all but the last category \\(j = J\\), because \\(1 - P(Y_i \\leq J) = 1 - 1 = 0\\).\nFor the wine data, where we have \\(J\\) = 5 rating categories, we will build up to the following mixed effects model:\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - u( \\text{judge}_i) \\\\\ni &= 1, \\dots n \\; \\; \\; \\; \\; \\; j = 1, \\dots, J - 1\n\\end{align}\n\\]\nwhere \\(\\theta_j\\) is called the threshold parameter, or cutpoint, of category \\(j\\). These thresholds can also be thought of as \\(J-1\\) = 4 intercepts. Note that the fixed effect parameters \\(\\beta_1\\) and \\(\\beta_2\\) are independent of \\(j\\), so each \\(\\beta\\) has the same effect for each of the \\(J-1\\) cumulative logits. The judge effects, which are also independent of \\(j\\), are assumed normal: \\(u(\\text{judge}_i) \\sim N(0, \\sigma_u^2)\\). We are using the logit link because it is the most popular for this kind of model (and the one I am familiar with), but there are other options we will briefly explore later.\nThe subtraction of terms in the above model is new to me. The main reason seems to be for familiar interpretation: the larger the value of any independent term \\(\\beta x\\), the smaller the thresholds \\(\\theta_j\\), and therefore a larger probability of the a response falling into a category at the upper end of the scale. This way, \\(\\beta\\) has the same direction of effect as in ordinary linear regression.\nWe are essentially modeling a “chain” of logistic regressions where the binary response is “less than or equal to a certain level” vs “greater than that level”. In this case, with \\(J\\) = 5, the thresholds \\(\\theta_j\\) are capturing the adjusted log-odds of observing:\n\n\\(j\\) = 1: log-odds of rating = 1 vs. 2-5\n\\(j\\) = 2: log-odds of rating = 1-2 vs. 3-5\n\\(j\\) = 3: log-odds of rating = 1-3 vs. 4-5\n\\(j\\) = 4: log-odds of rating = 1-4 vs. 5\n\n\n\nFitting\nNow with a surface-level understanding of what is being modeled, we will fit the data using ordinal::clm (cumulative link models) and ordinal::clmm (cumulative link mixed models), and logit links.\n\nFixed effects model\nFirst, fit a simple model, by maximum likelihood, with contact as the sole predictor:\n\\[\n\\text{logit}(p(y_i \\leq j)) = \\theta_j - \\beta_2 \\text{contact}_i\n\\]\n\nclm_rating_contact <-\n  clm(\n    rating ~ contact,\n    data = wine, link = \"logit\"\n  )\nsummary(clm_rating_contact)\n\nformula: rating ~ contact\ndata:    wine\n\n link  threshold nobs logLik AIC    niter max.grad cond.H \n logit flexible  72   -99.96 209.91 5(0)  1.67e-07 1.7e+01\n\nCoefficients:\n           Estimate Std. Error z value Pr(>|z|)   \ncontactyes   1.2070     0.4499   2.683   0.0073 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -2.13933    0.48981  -4.368\n2|3  0.04257    0.32063   0.133\n3|4  1.71449    0.38637   4.437\n4|5  2.97875    0.50207   5.933\n\n\nThe model gives us \\(K - 1 = 4\\) threshold coefficients, as expected. The \\(\\beta_2\\) coefficient estimate was statistically significant (by a Wald test), and tells us that contact = yes decreases the thresholds \\(\\theta_j\\) by \\(\\beta_2\\) = 1.21 (because of the subtraction of model terms), and therefore is associated with higher ratings.\nThe condition number of the Hessian for this model is 16.98. The ordinal primer says that larger values (like > 1e4) might indicate that the model is ill-defined.\nIt is nicely illustrative to compare this model to 4 separate logistic regressions with a dichotomized response:\n\\[\n\\begin{align}\n\\text{logit} (p(y_i \\leq 1)) &= \\theta_1 + \\beta_2 \\text{contact}_i \\\\\n\\text{logit} (p(y_i \\leq 2)) &= \\theta_2 + \\beta_2 \\text{contact}_i \\\\\n\\text{logit} (p(y_i \\leq 3)) &= \\theta_3 + \\beta_2 \\text{contact}_i \\\\\n\\text{logit} (p(y_i \\leq 4)) &= \\theta_4 + \\beta_2 \\text{contact}_i\n\\end{align}\n\\]\n\nwine %>%\n  crossing(j = 1:4) %>%\n  # Create a binary (0 or 1) to indicate where rating <= j\n  mutate(rating_leq_j = as.numeric(rating) <= j) %>%\n  group_by(j) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(\n    mod = map(\n      data,\n      ~glm(rating_leq_j ~ 1 + contact,\n           data = ., family = binomial(link = \"logit\")) %>% broom::tidy()\n    )\n  ) %>%\n  unnest(mod) %>%\n  transmute(\n    j, term,\n    estimate_se = str_c(round(estimate, 2), \" (\", round(std.error, 2), \")\")\n  ) %>%\n  pivot_wider(names_from = term, values_from = estimate_se) %>%\n  left_join(\n    tidy(clm_rating_contact) %>%\n      transmute(\n        j = as.integer(substr(term, 1, 1)),\n        term = if_else(!is.na(j), \"theta_j\", term),\n        estimate_se = str_c(round(estimate, 2), \" (\", round(std.error, 2), \")\")\n      ) %>%\n      mutate(j = replace_na(j, 1)) %>%\n      spread(term, estimate_se),\n    by = \"j\"\n  ) %>%\n  ungroup() %>%\n  gt() %>%\n  tab_spanner(label = \"Logistic regression\",\n              columns = c(`(Intercept)`, contactyes.x)) %>%\n  tab_spanner(label = \"CLM\",\n              columns = c(theta_j, contactyes.y)) %>%\n  fmt_missing(columns = everything(), missing_text = \"\")\n\n\n\n\n\n  \n  \n    \n      j\n      \n        Logistic regression\n      \n      \n        CLM\n      \n    \n    \n      (Intercept)\n      contactyes.x\n      theta_j\n      contactyes.y\n    \n  \n  \n    1\n-2.08 (0.53)\n-1.48 (1.14)\n-2.14 (0.49)\n1.21 (0.45)\n    2\n0 (0.33)\n-1.1 (0.51)\n0.04 (0.32)\n\n    3\n1.82 (0.48)\n-1.37 (0.59)\n1.71 (0.39)\n\n    4\n2.83 (0.73)\n-1.01 (0.87)\n2.98 (0.5)\n\n  \n  \n  \n\n\n\n\nThe intercepts from the ordinary logistic regression correspond closely to the threshold parameters \\(\\theta_j\\) from the cumulative link model. In the fixed effect of contact (\\(\\beta_2\\)), first note the sign difference, and second notice that the estimate from CLM is about the average of the 4 estimates from logistic regression. The advantage of the CLM is seen in the small standard error in the \\(\\beta_2\\) estimate.\nTo quote the primer:\n\nThe cumulative logit model can be seen as the model that combines these four ordinary logistic regression models into a single model and therefore makes better use of the information in the data.\n\nFor the second model, we add the \\(\\beta_1 \\text{temp}_i\\) term:\n\\[\n\\text{logit}(p(y_i \\leq j)) = \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i\n\\]\n\nclm_rating_contact_temp <-\n  clm(\n    rating ~ contact + temp,\n    data = wine, link = \"logit\"\n  )\nsummary(clm_rating_contact_temp)\n\nformula: rating ~ contact + temp\ndata:    wine\n\n link  threshold nobs logLik AIC    niter max.grad cond.H \n logit flexible  72   -86.49 184.98 6(0)  4.01e-12 2.7e+01\n\nCoefficients:\n           Estimate Std. Error z value Pr(>|z|)    \ncontactyes   1.5278     0.4766   3.205  0.00135 ** \ntempwarm     2.5031     0.5287   4.735 2.19e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -1.3444     0.5171  -2.600\n2|3   1.2508     0.4379   2.857\n3|4   3.4669     0.5978   5.800\n4|5   5.0064     0.7309   6.850\n\n\nBoth fixed effects (contact = yes and temp = warm) are strongly associated with higher probability of higher ratings. The summary function provides \\(p\\)-values from Wald tests, but more accurate likelihood ratio tests can be done via the drop1 function, which evaluates each fixed effect while controlling the other:\n\ndrop1(clm_rating_contact_temp, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\nrating ~ contact + temp\n        Df    AIC    LRT  Pr(>Chi)    \n<none>     184.98                     \ncontact  1 194.03 11.043 0.0008902 ***\ntemp     1 209.91 26.928 2.112e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOr the reverse via the add1() function, which evaluates each fixed effect while ignoring the other:\n\n# Fit the null model first\nclm_rating_null <- clm(rating ~ 1, data = wine, link = \"logit\")\nadd1(clm_rating_null, scope = ~ contact + temp, test = \"Chisq\")\n\nSingle term additions\n\nModel:\nrating ~ 1\n        Df    AIC     LRT  Pr(>Chi)    \n<none>     215.44                      \ncontact  1 209.91  7.5263   0.00608 ** \ntemp     1 194.03 23.4113 1.308e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSymmetric Wald confidence intervals can be extracted with confint or with broom::tidy:\n\ntidy(clm_rating_contact_temp, conf.int = TRUE, conf.type = \"Wald\") %>%\n  ggplot(aes(y = term, x = estimate)) +\n  geom_point(size = 2) +\n  geom_linerange(size = 1, aes(xmin = conf.low, xmax = conf.high))\n\n\n\n\nIn these types of analyses, we are often interested in the odds ratios. For the two categorical fixed effects, which have two levels each, the odds ratios \\(y \\leq j\\) comparing the two levels are:\n\\[\n\\begin{align}\n\\text{OR} &= \\frac{\\gamma_j (\\text{temp} = \\text{warm})}{\\gamma_j (\\text{temp} = \\text{cold})} = \\frac{\\exp(\\theta_j - \\beta_1 - \\beta_2 \\text{contact})}{\\exp (\\theta_j - 0 - \\beta_2 \\text{contact}}) = \\exp(\\beta_1) \\\\\n\\text{OR} &= \\frac{\\gamma_j (\\text{contact} = \\text{yes})}{\\gamma_j (\\text{contact} = \\text{no})} = \\frac{\\exp(\\theta_j - \\beta_1 \\text{temp} - \\beta_2 )}{\\exp (\\theta_j - \\beta_1 \\text{temp} - 0)}) = \\exp(\\beta_2)\n\\end{align}\n\\]\nwhere we have introduced the shorthand \\(\\gamma_j = \\text{logit} (p(y \\leq j))\\). Compute those odds ratios, and their corresponding Wald 95% CIs:\n\ntidy(clm_rating_contact_temp, conf.int = T, conf.type = \"Wald\") %>%\n  transmute(\n    term, across(c(estimate, conf.low, conf.high), exp)\n  ) %>%\n  gt() %>%\n  fmt_number(c(estimate, conf.low, conf.high), decimals = 2)\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      conf.low\n      conf.high\n    \n  \n  \n    1|2\n0.26\n0.09\n0.72\n    2|3\n3.49\n1.48\n8.24\n    3|4\n32.04\n9.93\n103.39\n    4|5\n149.37\n35.65\n625.75\n    contactyes\n4.61\n1.81\n11.73\n    tempwarm\n12.22\n4.34\n34.44\n  \n  \n  \n\n\n\n\nOne last thing to check: does the data support an interaction between \\(\\text{temp}_i\\) and \\(\\text{contact}_i\\)?\n\\[\n\\text{logit}(p(y_i \\leq j)) = \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - \\beta_3 \\text{temp}_i \\text{contact}_i\n\\]\n\nclm_rating_contact_temp_inter <-\n  clm(\n    rating ~ contact * temp, data = wine, link = \"logit\"\n  )\n\n#drop1(clm_rating_contact_temp_inter, test = \"Chisq\") # this accomplishes the same thing as anova()\nanova(clm_rating_contact_temp, clm_rating_contact_temp_inter)\n\nLikelihood ratio tests of cumulative link models:\n \n                              formula:                link: threshold:\nclm_rating_contact_temp       rating ~ contact + temp logit flexible  \nclm_rating_contact_temp_inter rating ~ contact * temp logit flexible  \n\n                              no.par    AIC  logLik LR.stat df Pr(>Chisq)\nclm_rating_contact_temp            6 184.98 -86.492                      \nclm_rating_contact_temp_inter      7 186.83 -86.416  0.1514  1     0.6972\n\n\nNo, The interaction term contact:temp is not supported by the data.\n\nComparison to linear model\nConsider the following linear model which treats rating as continuous:\n\\[\ny_i = \\alpha + \\beta_1 \\text{temp}_i + \\beta_2 \\text{contact}_i + \\epsilon_i\n\\]\nwhere \\(\\epsilon_i \\sim N(0, \\sigma_{\\epsilon}^2)\\).\n\nlm_rating_contact_temp <- lm(as.numeric(rating) ~ contact + temp, data = wine)\n\nTo compare this to a CLM, we must use the probit link:\n\nclm_rating_contact_temp_probit <-\n  clm(\n    rating ~ contact + temp, data = wine, link = \"probit\"\n  )\ntidy(clm_rating_contact_temp_probit) %>%\n  filter(coef.type == \"location\") %>%\n  mutate(model = \"CLM\") %>%\n  select(-coef.type) %>%\n  bind_rows(\n    tidy(lm_rating_contact_temp) %>%\n      filter(term != \"(Intercept)\") %>%\n      # Need to divide by the residual SE here to get the right scale\n      mutate(estimate = estimate / summary(lm_rating_contact_temp)$sigma,\n             model = \"LM\")\n  ) %>%\n  group_by(model) %>%\n  gt() %>%\n  fmt_number(c(estimate, std.error, statistic), decimals = 2) %>%\n  fmt(p.value, fns = scales::pvalue)\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    \n      CLM\n    \n    contactyes\n0.87\n0.27\n3.25\n0.001\n    tempwarm\n1.50\n0.29\n5.14\n<0.001\n    \n      LM\n    \n    contactyes\n0.79\n0.20\n3.36\n0.001\n    tempwarm\n1.38\n0.20\n5.87\n<0.001\n  \n  \n  \n\n\n\n\nThe relative estimates from the linear model are lower than those from the CLM (probit link), indicating that the assumptions of the linear model are not met. In particular, the distance between thresholds is not equidistant, as we can see from differences in the CLM coefficients:\n\ndiff(coef(clm_rating_contact_temp_probit)[1:4]) %>% round(2)\n\n 2|3  3|4  4|5 \n1.51 1.31 0.90 \n\n\n\n\n\nMixed effects model\nNow that we have explored ordinal regression with just fixed effects, we will fit the following random effects model:\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - u( \\text{judge}_i) \\\\\ni &= 1, \\dots n \\; \\; \\; \\; \\; \\; j = 1, \\dots, J - 1\n\\end{align}\n\\]\nwhere the judge effects are independent of \\(j\\), and assumed normal: \\(u(\\text{judge}_i) \\sim N(0, \\sigma_u^2)\\).\nEach judge has 8 ratings each (two per combination of temp and contact). See if we can spot the judge variance in a plot of ratings:\n\nwine %>%\n  count(judge, rating) %>%\n  ggplot(aes(x = judge, y = rating)) +\n  geom_tile(aes(fill = n)) +\n  geom_text(aes(label = n), color = \"white\") +\n  scale_x_discrete(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Number of ratings by judge\")\n\n\n\n\nThere is definitely some judge-specific variability in the perception of bitterness of wine. judge 5, for instance, doesn’t stray far from rating = 3, while judge 7 didn’t consider any of the wines particularly bitter.\nFit the full model with ordinal::clmm and logit link:\n\nclmm_rating_contact_temp <-\n  clmm(\n    rating ~ temp + contact + (1|judge),\n    data = wine, link = \"logit\"\n  )\n# This is an older function, which we need to run stats::profile later\nclmm2_rating_contact_temp <-\n  clmm2(\n    rating ~ temp + contact, random = judge,\n    data = wine, link = \"logistic\"\n  )\nsummary(clmm_rating_contact_temp)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ temp + contact + (1 | judge)\ndata:    wine\n\n link  threshold nobs logLik AIC    niter    max.grad cond.H \n logit flexible  72   -81.57 177.13 332(999) 1.03e-05 2.8e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n judge  (Intercept) 1.279    1.131   \nNumber of groups:  judge 9 \n\nCoefficients:\n           Estimate Std. Error z value Pr(>|z|)    \ntempwarm     3.0630     0.5954   5.145 2.68e-07 ***\ncontactyes   1.8349     0.5125   3.580 0.000344 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -1.6237     0.6824  -2.379\n2|3   1.5134     0.6038   2.507\n3|4   4.2285     0.8090   5.227\n4|5   6.0888     0.9725   6.261\n\n\nCompare model coefficients:\n\nbind_rows(\n  CLM = tidy(clm_rating_contact_temp),\n  CLMM = tidy(clmm_rating_contact_temp),\n  .id = \"model\"\n) %>%\n  select(-coef.type) %>%\n  group_by(model) %>%\n  gt() %>%\n  fmt_number(c(estimate, std.error, statistic), decimals = 2) %>%\n  fmt(p.value, fns = scales::pvalue)\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    \n      CLM\n    \n    1|2\n−1.34\n0.52\n−2.60\n0.009\n    2|3\n1.25\n0.44\n2.86\n0.004\n    3|4\n3.47\n0.60\n5.80\n<0.001\n    4|5\n5.01\n0.73\n6.85\n<0.001\n    contactyes\n1.53\n0.48\n3.21\n0.001\n    tempwarm\n2.50\n0.53\n4.73\n<0.001\n    \n      CLMM\n    \n    1|2\n−1.62\n0.68\n−2.38\n0.017\n    2|3\n1.51\n0.60\n2.51\n0.012\n    3|4\n4.23\n0.81\n5.23\n<0.001\n    4|5\n6.09\n0.97\n6.26\n<0.001\n    tempwarm\n3.06\n0.60\n5.14\n<0.001\n    contactyes\n1.83\n0.51\n3.58\n<0.001\n  \n  \n  \n\n\n\n\nBoth fixed effect estimates \\(\\beta_1\\) and \\(\\beta_2\\) are higher in the CLMM. Use anova to compare the CLMM to the CLM:\n\nanova(clm_rating_contact_temp, clmm_rating_contact_temp)\n\nLikelihood ratio tests of cumulative link models:\n \n                         formula:                              link: threshold:\nclm_rating_contact_temp  rating ~ contact + temp               logit flexible  \nclmm_rating_contact_temp rating ~ temp + contact + (1 | judge) logit flexible  \n\n                         no.par    AIC  logLik LR.stat df Pr(>Chisq)   \nclm_rating_contact_temp       6 184.98 -86.492                         \nclmm_rating_contact_temp      7 177.13 -81.565   9.853  1   0.001696 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUnsurprisingly, the judge term makes a significant improvement to the fit. We can extract profile confidence intervals on the variance \\(\\sigma_u\\) using stats::profile:\n\nprofile(clmm2_rating_contact_temp,\n        range = c(0.1, 4), nSteps = 30, trace = 0) %>%\n  confint()\n\n          2.5 %  97.5 %\nstDev 0.5014584 2.26678\n\n\nNote that these intervals are asymmetric (\\(\\sigma_u\\) = 1.28), unlike the less accurate Wald tests. We can produce “best guess” estimates for judge effects using conditional modes:\n\ntibble(\n  judge_effect = clmm_rating_contact_temp$ranef,\n  cond_var = clmm_rating_contact_temp$condVar\n) %>%\n  mutate(\n    judge = fct_reorder(factor(1:n()), judge_effect),\n    conf.low = judge_effect - qnorm(0.975) * sqrt(cond_var),\n    conf.high = judge_effect + qnorm(0.975) * sqrt(cond_var)\n  ) %>%\n  ggplot(aes(y = judge, x = judge_effect)) +\n  geom_point(size = 2) +\n  geom_linerange(size = 1, aes(xmin = conf.low, xmax = conf.high)) +\n  theme(panel.grid.major.x = element_line(color = \"grey\"))\n\n\n\n\n\nPredictions\nThere are different ways to extract predicted probabilities. First, and most obviously, with the predict function:\n\nwine %>%\n  bind_cols(\n    pred =  predict(\n      # Have to use clmm2 for predict\n      clmm2_rating_contact_temp, newdata = wine\n    )\n  ) %>%\n  # These are predicted probabilities for the average judge, so we can\n  #  exclude the judge variable\n  distinct(rating, temp, contact, pred) %>%\n  arrange(temp, contact, rating)\n\n# A tibble: 15 × 4\n   rating temp  contact   pred\n   <ord>  <fct> <fct>    <dbl>\n 1 1      cold  no      0.165 \n 2 2      cold  no      0.655 \n 3 3      cold  no      0.166 \n 4 1      cold  yes     0.0305\n 5 2      cold  yes     0.390 \n 6 3      cold  yes     0.496 \n 7 4      cold  yes     0.0696\n 8 2      warm  no      0.166 \n 9 3      warm  no      0.587 \n10 4      warm  no      0.191 \n11 5      warm  no      0.0463\n12 2      warm  yes     0.0313\n13 3      warm  yes     0.306 \n14 4      warm  yes     0.428 \n15 5      warm  yes     0.233 \n\n\nThis only gives us predictions for rating, temp and contact values which exist in the data. There is no predicted probability for rating > 3, temp cold and contact no, for example.\nAnother way is to pre-specify which values we want to predict:\n\nnd <-\n  crossing(\n    temp = factor(c(\"cold\", \"warm\")),\n    contact = factor(c(\"no\", \"yes\")),\n    rating = factor(1:5, ordered = T)\n  )\nnd %>%\n  bind_cols(pred = predict(clmm2_rating_contact_temp, nd)) %>%\n  ggplot(aes(x = glue::glue(\"{temp}-{contact}\"), y = pred, fill = rating)) +\n  geom_col() +\n  scale_fill_td(palette = \"div5\") +\n  scale_y_continuous(expand = c(0, 0), labels = scales::percent) +\n  labs(x = \"temp-contact\", y = \"predicted probability\")\n\n\n\n\nWe can also get model-estimated cumulative probabilities by considering the model coefficients. For example, for a cold temp and contact, the cumulative probability of a bitterness rating \\(j\\) or less:\n\\[\nP(y_i \\leq j) = \\text{logit}^{-1} [\\theta_j - \\beta_2 \\text{contact}_i]\n\\]\nwhere we are considering the average judge (\\(u(\\text{judge}_i) = 0\\)). The inverse logit is \\(\\text{logit}^{-1}(x) = 1 / (1 + \\exp(-x))\\), and can be calculated with plogis as a shorthand (brms::inv_logit_scaled is another). We can subtract cumulative probabilities to get non-cumulative probabilities of a rating \\(j\\). For example, \\(j\\) = 3:\n\nplogis(clmm_rating_contact_temp$Theta[3] - clmm_rating_contact_temp$beta[2]) -\n  plogis(clmm_rating_contact_temp$Theta[2] - clmm_rating_contact_temp$beta[2])\n\ncontactyes \n 0.4960357 \n\n\nwhich matches the value calculated previously using predict.\n\n\nEstimated marginal means\nThe emmeans package provides functionality for estimating marginal mean effects of ordinal models. The package documentation also provides an example using ordinal and wine data here.\n\nlibrary(emmeans)\n\nIn the “Models supported by emmeans” document, we see the following:\n\n\n\n\n\n\n\n\n\n\nObject.class\nPackage\nGroup\nArguments/notes\n\n\n\n\n\nclm\nordinal\nO\nmode = c(\"latent\", \"linear.predictor\", \"cum.prob\", \"exc.prob\", \"prob\", \"mean.class\", \"scale\")\n\n\n\nclmm\nordinal\nO\nLike clm but no \"scale\" mode\n\n\n\n\n\n\n\n\n\n\n\n\nemmeans(clmm_rating_contact_temp,\n        specs = list(pairwise ~ temp, pairwise ~ contact), mode = \"latent\")\n\n$`emmeans of temp`\n temp emmean    SE  df asymp.LCL asymp.UCL\n cold  -1.63 0.547 Inf    -2.707    -0.562\n warm   1.43 0.532 Inf     0.387     2.470\n\nResults are averaged over the levels of: contact \nConfidence level used: 0.95 \n\n$`pairwise differences of temp`\n 1           estimate    SE  df z.ratio p.value\n cold - warm    -3.06 0.595 Inf  -5.145  <.0001\n\nResults are averaged over the levels of: contact \n\n$`emmeans of contact`\n contact emmean    SE  df asymp.LCL asymp.UCL\n no      -1.020 0.522 Inf    -2.043   0.00274\n yes      0.815 0.513 Inf    -0.191   1.82072\n\nResults are averaged over the levels of: temp \nConfidence level used: 0.95 \n\n$`pairwise differences of contact`\n 1        estimate    SE  df z.ratio p.value\n no - yes    -1.83 0.513 Inf  -3.580  0.0003\n\nResults are averaged over the levels of: temp \n\n\nThe contrast estimates are in terms of the latent (underlying unobserved) bitterness rating.\nUsing mode = \"cum.prob\" and mode = \"exc.prob“, we can get cumulative probabilities and exceedance (1 - cumulative) probabilities. For example, the probability of a rating of at least 4 for different temp:\n\nemmeans(clmm_rating_contact_temp, ~ temp,\n        mode = \"exc.prob\", at = list(cut = \"3|4\"))\n\n temp exc.prob     SE  df asymp.LCL asymp.UCL\n cold    0.049 0.0304 Inf   -0.0107     0.109\n warm    0.450 0.1084 Inf    0.2371     0.662\n\nResults are averaged over the levels of: contact \nConfidence level used: 0.95 \n\n\nmode = \"prob\" gives us probability distributions of each rating, which have a nice auto plot functionality:\n\nemmeans(clmm_rating_contact_temp,\n        ~ rating | temp, mode = \"prob\") %>%\n  plot() +\n  add_facet_borders()\n\n\n\n\n\n\nChoice of link function\nSo far, we have used the logit link (and briefly the probit link to compare estimates with linear regression). The links available to ordinal::clmm are logit, probit, cloglog, loglog, and cauchit.\nWe can fit the CLMM using all of these links and compare log-likelihoods:\n\nwine %>%\n  nest(data = everything()) %>%\n  crossing(\n    link = c(\"logit\", \"probit\", \"cloglog\", \"loglog\", \"cauchit\")\n  ) %>%\n  mutate(\n    mod = map2(\n      data, link,\n      ~clmm(\n        rating ~ 1 + contact + temp + (1|judge),\n        data = .x, link = .y\n      )\n    )\n  ) %>%\n  mutate(mod_summary = map(mod, glance)) %>%\n  unnest(mod_summary) %>%\n  select(link, logLik, AIC, BIC) %>%\n  arrange(logLik) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      link\n      logLik\n      AIC\n      BIC\n    \n  \n  \n    cauchit\n-86.83499\n187.6700\n203.6066\n    cloglog\n-82.72936\n179.4587\n195.3954\n    logit\n-81.56541\n177.1308\n193.0675\n    loglog\n-81.54137\n177.0827\n193.0194\n    probit\n-80.93061\n175.8612\n191.7979\n  \n  \n  \n\n\n\n\nThe probit model appears to be the best description of the data.\nWe can also consider the effect of “flexible” vs “equidistant” thresholds:\n\nwine %>%\n  nest(data = everything()) %>%\n  crossing(\n    link = c(\"logit\", \"probit\", \"cloglog\", \"loglog\", \"cauchit\"),\n    threshold = c(\"flexible\", \"equidistant\")\n  ) %>%\n  mutate(\n    mod = pmap(\n      list(data, link, threshold),\n      function(a, b, c) {\n        clmm(\n          rating ~ 1 + contact + temp + (1|judge),\n          data = a, link = b, threshold = c\n        )\n      }\n    )\n  ) %>%\n  #mutate(mod_summary = map(mod, glance)) %>%\n  mutate(\n    mod_summary = map(\n      mod,\n      # glance() on a clmm object returns a <logLik> variable type which\n      #  can't be bound together by unnest(), so need to convert it to numeric\n      ~glance(.x) %>% mutate(logLik = as.numeric(logLik))\n    )\n  ) %>%\n  unnest(mod_summary) %>%\n  select(link, threshold, logLik, edf, AIC, BIC) %>%\n  arrange(logLik) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      link\n      threshold\n      logLik\n      edf\n      AIC\n      BIC\n    \n  \n  \n    cauchit\nequidistant\n-87.75021\n5\n185.5004\n196.8837\n    cauchit\nflexible\n-86.83499\n7\n187.6700\n203.6066\n    loglog\nequidistant\n-84.36440\n5\n178.7288\n190.1121\n    cloglog\nequidistant\n-83.32634\n5\n176.6527\n188.0360\n    logit\nequidistant\n-83.05497\n5\n176.1099\n187.4933\n    cloglog\nflexible\n-82.72936\n7\n179.4587\n195.3954\n    probit\nequidistant\n-82.52622\n5\n175.0524\n186.4358\n    logit\nflexible\n-81.56541\n7\n177.1308\n193.0675\n    loglog\nflexible\n-81.54137\n7\n177.0827\n193.0194\n    probit\nflexible\n-80.93061\n7\n175.8612\n191.7979\n  \n  \n  \n\n\n\n\nNote the change in degrees of freedom, resulting in the equidistant probit model having the lowest BIC. In terms of log likelihood, however, flexible always outperform equidistant thresholds."
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#conclusion",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#conclusion",
    "title": "Ordinal regression in R: part 1",
    "section": "Conclusion",
    "text": "Conclusion\nThanks to detailed documentation, fitting cumulative link (mixed) models is very easy with ordinal. In this post, we first learned the theoretical basis for these models, then worked through examples using wine bitterness ratings from multiple judges.\nIn the next post, I’ll explore the Bayesian approach to ordinal regression with the brms package."
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#reproducibility",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#reproducibility",
    "title": "Ordinal regression in R: part 1",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-08-07\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [53358c7] 2022-08-06: Set fonts and trying out `renv.lock` for reproducibility\n\n\n\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html",
    "title": "Predicting bike ridership: getting the data",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(httr)\nlibrary(lubridate)\nlibrary(gt)\nlibrary(glue)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#introduction",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#introduction",
    "title": "Predicting bike ridership: getting the data",
    "section": "Introduction",
    "text": "Introduction\nIn 2016, the city of Halifax installed its first cyclist tracker on Agricola Street. Last year, the city made bike counter data available on their open data platform. As a cyclist and Haligonian, this is of course interesting to me personally. As a data scientist, this seems like a nice opportunity to work through a machine learning project end-to-end: from retrieving, exploring, and processing the data, to building and evaluating models, to producing an end product. (A REST API? A Shiny app? TBD.)\nIn this post, I get and explore data from two sources: (1) the aforementioned bike counter data from city of Halifax, and (2) historical weather data from the government of Canada."
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#getting-bicycle-count-data",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#getting-bicycle-count-data",
    "title": "Predicting bike ridership: getting the data",
    "section": "Getting bicycle count data",
    "text": "Getting bicycle count data\nThe bicycle counts were easy enough to find on Halifax’s platform: (https://catalogue-hrm.opendata.arcgis.com/datasets/45d4ecb0cb48469186e683ebc54eb188_0/explore?showTable=true). Each data set comes with a nice API explorer for constructing queries. I’ll use httr to GET the data with the basic query provided there:\n\nquery_url <- \"https://services2.arcgis.com/11XBiaBYA9Ep0yNJ/arcgis/rest/services/Bicycle_Counts/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json\"\nresp <- httr::GET(query_url)\nresp\n\n\n\nResponse [https://services2.arcgis.com/11XBiaBYA9Ep0yNJ/arcgis/rest/services/Bicycle_Counts/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json]\n  Date: 2022-04-27 03:26\n  Status: 200\n  Content-Type: application/json; charset=utf-8\n  Size: 579 kB\n\n\nThe response code (200) indicates a successful connection. The data comes in JSON format, which I can parse to an R list with:\n\nparsed_content <- content(resp, as = \"parsed\")\nstr(parsed_content, max.level = 1)\n\nList of 6\n $ objectIdFieldName    : chr \"ObjectId\"\n $ uniqueIdField        :List of 2\n $ globalIdFieldName    : chr \"\"\n $ fields               :List of 11\n $ exceededTransferLimit: logi TRUE\n $ features             :List of 2000\n\n\nThis returned a list of 6 items. The fields item is a list of variables:\n\nfields <- map_dfr(\n  parsed_content$fields,\n  # Drop NULL elements so I can convert to a tibble\n  ~ discard(.x, is.null) %>% as_tibble()\n)\ngt(fields)\n\n\n\n\n\n  \n  \n    \n      name\n      type\n      alias\n      sqlType\n      length\n    \n  \n  \n    CHANNEL_ID\nesriFieldTypeString\nCHANNEL_ID\nsqlTypeNVarchar\n256\n    CHANNEL_NAME\nesriFieldTypeString\nCHANNEL_NAME\nsqlTypeNVarchar\n4000\n    SERIAL_NUMBER\nesriFieldTypeString\nSERIAL_NUMBER\nsqlTypeNVarchar\n4000\n    SITE_NAME\nesriFieldTypeString\nSITE_NAME\nsqlTypeNVarchar\n4000\n    LATITUDE\nesriFieldTypeDouble\nLATITUDE\nsqlTypeFloat\nNA\n    LONGITUDE\nesriFieldTypeDouble\nLONGITUDE\nsqlTypeFloat\nNA\n    INSTALLATION_DATE\nesriFieldTypeDate\nINSTALLATION_DATE\nsqlTypeTimestamp2\n8\n    COUNT_DATETIME\nesriFieldTypeDate\nCOUNT_DATETIME\nsqlTypeTimestamp2\n8\n    COUNTER_TYPE\nesriFieldTypeString\nCOUNTER_TYPE\nsqlTypeNVarchar\n4000\n    COUNTER_VALUE\nesriFieldTypeInteger\nCOUNTER_VALUE\nsqlTypeInteger\nNA\n    ObjectId\nesriFieldTypeOID\nObjectId\nsqlTypeInteger\nNA\n  \n  \n  \n\n\n\n\nThe data is the features item, which itself is a list of length 2000. Here is the first element:\n\nparsed_content$features[[1]]\n\n$attributes\n$attributes$CHANNEL_ID\n[1] \"100059339\"\n\n$attributes$CHANNEL_NAME\n[1] \"Hollis St\"\n\n$attributes$SERIAL_NUMBER\n[1] \"X2H20032465\"\n\n$attributes$SITE_NAME\n[1] \"Hollis St\"\n\n$attributes$LATITUDE\n[1] 44.64799\n\n$attributes$LONGITUDE\n[1] -63.57352\n\n$attributes$INSTALLATION_DATE\n[1] 1.594166e+12\n\n$attributes$COUNT_DATETIME\n[1] 1.595966e+12\n\n$attributes$COUNTER_TYPE\n[1] \"Bicycle\"\n\n$attributes$COUNTER_VALUE\n[1] 2\n\n$attributes$ObjectId\n[1] 1\n\n\nLooks like there is another level of nesting with attributes. Compile all of these elements into a single data frame:\n\nbike_counts <- map_dfr(\n  parsed_content$features,\n  ~ as_tibble(.x$attributes)\n)\nglimpse(bike_counts)\n\nRows: 2,000\nColumns: 11\n$ CHANNEL_ID        <chr> \"100059339\", \"100059339\", \"100059339\", \"100059339\", …\n$ CHANNEL_NAME      <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ SERIAL_NUMBER     <chr> \"X2H20032465\", \"X2H20032465\", \"X2H20032465\", \"X2H200…\n$ SITE_NAME         <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ LATITUDE          <dbl> 44.64799, 44.64799, 44.64799, 44.64799, 44.64799, 44…\n$ LONGITUDE         <dbl> -63.57352, -63.57352, -63.57352, -63.57352, -63.5735…\n$ INSTALLATION_DATE <dbl> 1.594166e+12, 1.594166e+12, 1.594166e+12, 1.594166e+…\n$ COUNT_DATETIME    <dbl> 1.595966e+12, 1.595970e+12, 1.595974e+12, 1.595977e+…\n$ COUNTER_TYPE      <chr> \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle…\n$ COUNTER_VALUE     <int> 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 6, 4, 13, 8, 5, 6, 8, …\n$ ObjectId          <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n\n\nNote that just 2000 records were returned. The exceededTransferLimit = TRUE value tells us that this is the limit of the API. I can get the total count of records by altering the original query slightly:\n\nn_records <- httr::GET(paste0(query_url, \"&returnCountOnly=true\")) %>%\n  content(as = \"parsed\") %>%\n  unlist(use.names = FALSE)\nn_records\n\n\n\n[1] 124356\n\n\nSo to get all of the data at 2000 records per request, I’ll have to make a minimum of 63 calls to the API. The API offers a “resultOffset” argument to get records in sequence. Make a function to get 2000 records for a given offset:\n\nget_bike_data <- function(offset) {\n  # Need to prevent scientific notation, e.g. \"1e+05\" instead of \"100000\"\n  offset <- format(offset, scientific = FALSE)\n  \n  parsed_content <- httr::GET(paste0(query_url, \"&resultOffset=\", offset)) %>%\n    content(as = \"parsed\")\n  \n  map_dfr(\n    parsed_content$features,\n    ~ as_tibble(.x$attributes)\n  ) \n}\n\nAnd combine it all into a single data frame:\n\nbike_data <- map_dfr(\n  seq(0, ceiling(n_records / 2000)),\n  ~ get_bike_data(offset = .x * 2000)\n)\n\n\n\n\n\nglimpse(bike_data)\n\nRows: 124,356\nColumns: 11\n$ CHANNEL_ID        <chr> \"100059339\", \"100059339\", \"100059339\", \"100059339\", …\n$ CHANNEL_NAME      <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ SERIAL_NUMBER     <chr> \"X2H20032465\", \"X2H20032465\", \"X2H20032465\", \"X2H200…\n$ SITE_NAME         <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ LATITUDE          <dbl> 44.64799, 44.64799, 44.64799, 44.64799, 44.64799, 44…\n$ LONGITUDE         <dbl> -63.57352, -63.57352, -63.57352, -63.57352, -63.5735…\n$ INSTALLATION_DATE <dbl> 1.594166e+12, 1.594166e+12, 1.594166e+12, 1.594166e+…\n$ COUNT_DATETIME    <dbl> 1.595966e+12, 1.595970e+12, 1.595974e+12, 1.595977e+…\n$ COUNTER_TYPE      <chr> \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle…\n$ COUNTER_VALUE     <int> 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 6, 4, 13, 8, 5, 6, 8, …\n$ ObjectId          <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n\n\nThis returned 124356 records, as expected. The ObjectId should be a unique sequential identifier from 1 to 124356, which I’ll check:\n\nrange(bike_data$ObjectId); n_distinct(bike_data$ObjectId)\n\n[1]      1 124356\n\n\n[1] 124356\n\n\n\nEDA and cleaning\nFirst thing I usually do with a new data set is clean the column names:\n\nbike_data <- janitor::clean_names(bike_data)\nglimpse(bike_data)\n\nRows: 124,356\nColumns: 11\n$ channel_id        <chr> \"100059339\", \"100059339\", \"100059339\", \"100059339\", …\n$ channel_name      <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ serial_number     <chr> \"X2H20032465\", \"X2H20032465\", \"X2H20032465\", \"X2H200…\n$ site_name         <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ latitude          <dbl> 44.64799, 44.64799, 44.64799, 44.64799, 44.64799, 44…\n$ longitude         <dbl> -63.57352, -63.57352, -63.57352, -63.57352, -63.5735…\n$ installation_date <dbl> 1.594166e+12, 1.594166e+12, 1.594166e+12, 1.594166e+…\n$ count_datetime    <dbl> 1.595966e+12, 1.595970e+12, 1.595974e+12, 1.595977e+…\n$ counter_type      <chr> \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle…\n$ counter_value     <int> 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 6, 4, 13, 8, 5, 6, 8, …\n$ object_id         <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n\n\nNext I want to deal with the installation_date and count_datetime variables, which are very large integers. From fields above, the data type for these variables is esriFieldTypeDate. After some digging on Google, turns out this is Unix time (the number of milliseconds since January 1, 1970; also called epoch time). With as.POSIXct(), I can supply the number of seconds and set origin = \"1970-01-01\" to get back the correct datetime objects:\n\nbike_data <- bike_data %>%\n  mutate(\n    across(c(installation_date, count_datetime),\n           ~ as.POSIXct(.x / 1000, tz = \"UTC\", origin = \"1970-01-01\")),\n    # These are just dates, the time of day doesn't matter\n    installation_date = as.Date(installation_date),\n    # I'll also want the date without time of day\n    count_date = as.Date(count_datetime)\n  )\n\nThese variables are unique to the sites:\n\nbike_data %>%\n  count(site_name, latitude, longitude, serial_number, installation_date,\n        counter_type, name = \"n_records\") %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      site_name\n      latitude\n      longitude\n      serial_number\n      installation_date\n      counter_type\n      n_records\n    \n  \n  \n    Dartmouth Harbourfront Greenway\n44.66436\n-63.55736\nX2H20114473\n2021-07-08\nBicycle\n13976\n    Hollis St\n44.64799\n-63.57352\nX2H20032465\n2020-07-08\nBicycle\n15748\n    South Park St\n44.64194\n-63.57972\nX2H19070467\n2019-09-01\nBicycle\n46424\n    Vernon St\n44.64292\n-63.59154\nX2H20114470\n2020-12-09\nBicycle\n24104\n    Windsor St\n44.65466\n-63.60368\nX2H20114472\n2020-12-09\nBicycle\n24104\n  \n  \n  \n\n\n\n\nDrop serial_number and counter_type, which aren’t useful.\n\nbike_data <- bike_data %>% select(-serial_number, -counter_type)\n\nSites can have multiple channels:\n\nbike_data %>%\n  count(site_name, channel_name, channel_id, name = \"n_records\") %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      site_name\n      channel_name\n      channel_id\n      n_records\n    \n  \n  \n    Dartmouth Harbourfront Greenway\nDartmouth Harbourfront Greenway Northbound\n353280085\n6988\n    Dartmouth Harbourfront Greenway\nDartmouth Harbourfront Greenway Southbound\n353280086\n6988\n    Hollis St\nHollis St\n100059339\n15748\n    South Park St\nSouth Park St Northbound\n101054257\n23212\n    South Park St\nSouth Park St Southbound\n102054257\n23212\n    Vernon St\nVernon St Northbound\n353252897\n12052\n    Vernon St\nVernon St Southbound\n353252898\n12052\n    Windsor St\nWindsor St Northbound\n353252910\n12052\n    Windsor St\nWindsor St Southbound\n353252909\n12052\n  \n  \n  \n\n\n\n\nAll but the Hollis St site has separate northbound and southbound channels.\nFor each site, check the installation_date relative to the range of count_date:\n\nbike_data %>%\n  group_by(site_name, installation_date) %>%\n  summarise(min_count_date = min(count_date), max_count_date = max(count_date),\n            .groups = \"drop\") %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      site_name\n      installation_date\n      min_count_date\n      max_count_date\n    \n  \n  \n    Dartmouth Harbourfront Greenway\n2021-07-08\n2021-07-08\n2022-04-25\n    Hollis St\n2020-07-08\n2020-07-08\n2022-04-25\n    South Park St\n2019-09-01\n2019-09-01\n2022-04-25\n    Vernon St\n2020-12-09\n2020-12-09\n2022-04-25\n    Windsor St\n2020-12-09\n2020-12-09\n2022-04-25\n  \n  \n  \n\n\n\n\nEverything is nicely aligned: the first data corresponds to the installation date, and the last data corresponds to the date the data were retrieved.\nPlot the position of each of the counters using the given latitude and longitude, overlaid on a map of Halifax with the ggmap package:1\n\nlibrary(ggmap)\nsite_locs <- bike_data %>%\n  distinct(site_name, lat = latitude, lon = longitude)\n\nmean_lat <- mean(site_locs$lat)\nmean_lon <- mean(site_locs$lon)\n\n\nhalifax_map <- get_googlemap(c(mean_lon, mean_lat),\n                             zoom = 14, maptype = \"satellite\")\n\n\n\n\n\nggmap(halifax_map) +\n  geom_point(data = site_locs, size = 4,\n             aes(fill = site_name), shape = 21, color = \"white\") +\n  ggrepel::geom_label_repel(\n    data = site_locs,\n    aes(color = site_name, label = str_trunc(site_name, width = 25)),\n    box.padding = 1.0\n  ) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nFor each site and channel, get the time of day from count_datetime to determine the frequency of collection:\n\nbike_data %>%\n  mutate(time_of_day = format(count_datetime, \"%H:%M:%S\")) %>%\n  count(site_name, time_of_day, name = \"n_records\") %>%\n  ggplot(aes(y = time_of_day, x = n_records)) +\n  geom_col() +\n  facet_wrap(~ str_trunc(site_name, 15), nrow = 1) +\n  scale_x_continuous(expand = c(0, 0), breaks = c(0, 500, 1000, 1500)) +\n  dunnr::add_facet_borders()\n\n\n\n\nEach counter reports observations at the hour mark (+1 second). There are some slight difference in the number of records due to the time of day I retrieved the data.\nI made an assumption that the count_datetime variable was in UTC timezone. I can check this assumption by looking at average counts (over the entire data set).\n\nbike_data_tod <- bike_data %>%\n  mutate(\n    time_of_day = format(count_datetime, \"%H:%M:%S\"),\n    # Create a dummy variable with arbitrary date so I can plot time of day\n    time_of_day = lubridate::ymd_hms(\n      paste0(\"2022-04-22 \", time_of_day)\n    )\n  )\nbike_data_tod %>%\n  group_by(site_name, time_of_day) %>%\n  summarise(\n    n = n(), mean_count = mean(counter_value),\n    .groups = \"drop\"\n  ) %>%\n  ggplot(aes(x = time_of_day, y = mean_count)) +\n  geom_area(fill = td_colors$nice$mellow_yellow, color = \"black\") +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  scale_x_datetime(date_breaks = \"2 hours\", date_labels = \"%H\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  dunnr::add_facet_borders()\n\n\n\n\nThese peaks at around 8AM and 5PM tell me that the data is actually recorded in the local time zone (Atlantic), not UTC like I assumed. If they were in UTC time, the peaks would correspond to 11AM and 8PM locally, which would be odd times for peak cyclists.\nAny interesting trends in different channels?\n\nbike_data_tod %>%\n  # Remove Hollis St, which does not have different channels\n  filter(site_name != \"Hollis St\") %>%\n  mutate(channel_direction = str_extract(channel_name, \"(North|South)bound\")) %>%\n  group_by(site_name, channel_direction, time_of_day) %>%\n  summarise(mean_count = mean(counter_value), .groups = \"drop\") %>%\n  ggplot(aes(x = time_of_day, y = mean_count, color = channel_direction)) +\n  geom_line() +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  scale_x_datetime(date_breaks = \"2 hours\", date_labels = \"%H\") +\n  theme(legend.position = \"top\")\n\n\n\n\nVernon St and Windsor St counters have higher traffic Southbound (heading downtown) at the start of the typical workday, and higher traffic Northbound (leaving downtown) at the end of the typical workday.\nI am less interested in counts over the course of a day or by channel, and more interested in daily counts. Now that I know the count_date is correctly converted with the local time, get the sum at each site and each 24 hour day:\n\nbike_data_daily_counts <- bike_data %>%\n  group_by(site_name, installation_date, count_date) %>%\n  summarise(\n    n_records = n(), n_bikes = sum(counter_value), .groups = \"drop\"\n  )\n\nNow plot counts per day at each site:\n\nbike_data_daily_counts %>%\n  ggplot(aes(x = count_date, y = n_bikes)) +\n  geom_line() +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  dunnr::add_facet_borders()\n\n\n\n\nThe seasonal trends are very obvious from this plot. One thing that stood out to me is the big increase from 2020 to 2021 on South Park St. It may be representative of the start of the COVID pandemic, but I think it also has to do with the addition of protected bike lanes in December 2020. Before 2020, there appears to be a series of 0 counts on South Park St which may be artifacts:\n\nbike_data_daily_counts %>%\n  filter(site_name == \"South Park St\", count_date < \"2020-01-01\") %>%\n  ggplot(aes(x = count_date, y = n_bikes)) +\n  geom_line()\n\n\n\n\nI’m almost certain this series of zeroes is not real, so I’ll remove it from the data. Find the date of the first non-zero n_bikes at this site, and filter out data before then:\n\nsouth_park_min_date <- bike_data_daily_counts %>%\n  filter(site_name == \"South Park St\", n_bikes > 0) %>%\n  pull(count_date) %>%\n  min()\nsouth_park_min_date\n\n[1] \"2019-11-23\"\n\nbike_data_daily_counts <- bike_data_daily_counts %>%\n  filter(!((site_name == \"South Park St\") & (count_date < south_park_min_date)))\n\nOverlay counts by year for each site:\n\nbike_data_daily_counts %>%\n  mutate(count_year = year(count_date),\n         # Replace year with 1970 so I can plot on the same scale\n         count_date = as.Date(yday(count_date), origin = \"1970-01-01\")) %>%\n  ggplot(aes(x = count_date, y = n_bikes, color = factor(count_year))) +\n  geom_line(size = 1, alpha = 0.8) +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  scale_x_date(date_labels = \"%B\") +\n  dunnr::add_facet_borders() +\n  theme(legend.position = \"bottom\") +\n  labs(x = NULL, color = \"Year\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nI’m interested in day of the week effects as well:\n\nbike_data_daily_counts %>%\n  mutate(day_of_week = wday(count_date, label = TRUE)) %>%\n  ggplot(aes(y = day_of_week, x = n_bikes)) +\n  geom_boxplot()\n\n\n\n\nLess activity on the weekends."
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#getting-weather-data",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#getting-weather-data",
    "title": "Predicting bike ridership: getting the data",
    "section": "Getting weather data",
    "text": "Getting weather data\nTemporal data are probably the most important predictors of ridership, but I’m sure a close second is the day’s weather. I’ll get this with the API provided by the Meteorological Service of Canada. I can get a list of available data sets (which they call collections) as follows:2\n\nbase_url <- \"https://api.weather.gc.ca/\"\nresp <- httr::GET(paste0(base_url, \"collections?f=json\"))\n\n\n\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 2\n $ collections:List of 70\n $ links      :List of 3\n\n\nThe first element of collections:\n\ncollections <- content_parsed$collections\nstr(collections[[1]], max.level = 2)\n\nList of 7\n $ id         : chr \"hydrometric-stations\"\n $ title      : chr \"Hydrometric Monitoring Stations\"\n $ description: chr \"A station is a site on a river or lake where water quantity (water level and flow) are collected and recorded.\"\n $ keywords   :List of 2\n  ..$ : chr \"station\"\n  ..$ : chr \"hydrometric station\"\n $ links      :List of 13\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n $ extent     :List of 2\n  ..$ spatial :List of 2\n  ..$ temporal:List of 1\n $ itemType   : chr \"feature\"\n\n\nUnlike the bicycle counts data, this nested format doesn’t lend itself well to direct conversion to a tibble:\n\nas_tibble(collections[[1]])\n\nError:\n! Tibble columns must have compatible sizes.\n• Size 2: Columns `keywords` and `extent`.\n• Size 13: Column `links`.\nℹ Only values of size one are recycled.\n\n\nInstead, I can use enframe() to get a two-column data frame:\n\nenframe(collections[[1]])\n\n# A tibble: 7 × 2\n  name        value           \n  <chr>       <list>          \n1 id          <chr [1]>       \n2 title       <chr [1]>       \n3 description <chr [1]>       \n4 keywords    <list [2]>      \n5 links       <list [13]>     \n6 extent      <named list [2]>\n7 itemType    <chr [1]>       \n\n\nAssuming every item in the collections list has the same structure, I’ll just extract the id, title, and description:\n\ncollections_df <-  map_dfr(\n  collections,\n  ~ enframe(.x) %>%\n    filter(name %in% c(\"id\", \"title\", \"description\")) %>%\n    pivot_wider(names_from = name, values_from = value)\n)\ngt(collections_df) %>%\n  tab_options(container.height = 300, container.overflow.y = TRUE)\n\n\n\n\n\n  \n  \n    \n      id\n      title\n      description\n    \n  \n  \n    hydrometric-stations\nHydrometric Monitoring Stations\nA station is a site on a river or lake where water quantity (water level and flow) are collected and recorded.\n    hydrometric-daily-mean\nDaily Mean of Water Level or Flow\nThe daily mean is the average of all unit values for a given day.\n    hydrometric-monthly-mean\nMonthly Mean of Water Level or Flow\nThe monthly mean is the average of daily mean values for a given month.\n    hydrometric-annual-statistics\nAnnual Maximum and Minimum Daily Water Level or Flow\nThe annual maximum and minimum daily data are the maximum and minimum daily mean values for a given year.\n    hydrometric-annual-peaks\nAnnual Maximum and Minimum Instantaneous Water Level or Flow\nThe annual maximum and minimum instantaneous data are the maximum and minimum instantaneous values for a given year.\n    hydrometric-realtime\nReal-time hydrometric data\nReal-time water level and flow (discharge) data collected at over 2100 hydrometric stations across Canada (last 30 days).\n    climate-normals\n1981-2010 Climate Normals\nClimate Normals are used to summarize or describe the average climatic conditions of a particular location. At the completion of each decade, Environment and Climate Change Canada updates its climate normals for as many locations and as many climatic characteristics as possible. The climate normals offered here are based on Canadian climate stations with at least 15 years of data between 1981 to 2010.\n    climate-stations\nClimate Stations\nClimate observations are derived from two sources of data. The first are Daily Climate Stations producing one or two observations per day of temperature, precipitation. The second are hourly stations that typically produce more weather elements e.g. wind or snow on ground.\n    climate-monthly\nMonthly Climate Observation Summaries\nA cross-country summary of the averages and extremes for the month, including precipitation totals, max-min temperatures, and degree days. This data is available from stations that produce daily data.\n    climate-daily\nDaily Climate Observations\nDaily climate observations are derived from two sources of data. The first are Daily Climate Stations producing one or two observations per day of temperature, precipitation. The second are hourly stations that typically produce more weather elements e.g. wind or snow on ground. Only a subset of the total stations is shown due to size limitations. The criteria for station selection are listed as below. The priorities for inclusion are as follows: (1) Station is currently operational, (2) Stations with long periods of record, (3) Stations that are co-located with the categories above and supplement the period of record\n    climate-hourly\nHourly Climate Observations\nHourly climate observations are derived from the data source HLY01. These are stations that produce hourly meteorological observations, taken each hour of the day for the hours 00h-23h, for both daily (temperature, precipitation) and non-daily elements (station pressure, relative humidity, visibility). Only a subset of the total stations are shown due to size limitations. The stations were selected based on the following criteria: (1) Stations near cities with populations greater than 10,000, (2) Stations with long periods of record of at least 30 years of data, (3) Stations selected for the 1991-2020 WMO Normals, (4) Stations with RBCN designation not selected for 1991-2020 WMO Normals, and/or (5) Stations whose sum when joined with co-located/successor stations equates to a period of record of 30 years or greater.\n    ahccd-stations\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Stations\nClimate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation.\n    ahccd-annual\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Annual\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n    ahccd-seasonal\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Seasonal\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n    ahccd-monthly\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Monthly\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n    ahccd-trends\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Trends\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n    swob-realtime\nSurface Weather Observations\nSurface Observations measured at the automatic and manual stations of the Environment and Climate Change Canada and partners networks, either for a single station, or for the stations of specific provinces and territories (last 30 days)\n    ltce-stations\nVirtual Climate Stations (LTCE)\nA Virtual Climate station is the result of threading together climate data from proximate current and historical stations to construct a long term threaded data set. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. The length of the time series of virtual stations is often greater than 100 years. A Virtual Climate station is always named for an “Area” rather than a point, e.g. Winnipeg Area, to indicate that the data are drawn from that area(within a 20km radius from the urban center) rather than a single precise location.\n    ltce-temperature\nDaily Extremes of Records (LTCE) – Temperature\nAnomalous weather resulting in Temperature and Precipitation extremes occurs almost every day somewhere in Canada. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. Virtual Climate stations correspond with the city pages of weather.gc.ca. This data provides the daily extremes of record for Temperature for each day of the year. Daily elements include: High Maximum, Low Maximum, High Minimum, Low Minimum.\n    ltce-precipitation\nDaily Extremes of Records (LTCE) – Precipitation\nAnomalous weather resulting in Temperature and Precipitation extremes occurs almost every day somewhere in Canada. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. Virtual Climate stations correspond with the city pages of weather.gc.ca. This data provides the daily extremes of record for Precipitation for each day of the year. Daily elements include: Greatest Precipitation.\n    ltce-snowfall\nDaily Extremes of Records (LTCE) – Snowfall\nAnomalous weather resulting in Temperature and Precipitation extremes occurs almost every day somewhere in Canada. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. Virtual Climate stations correspond with the city pages of weather.gc.ca. This data provides the daily extremes of record for Snowfall for each day of the year. Daily elements include: Greatest Snowfall.\n    aqhi-forecasts-realtime\nAir Quality Health Index – Forecasts\nThe Air Quality Health Index (AQHI) is a scale designed to help quantify the quality of the air in a certain region on a scale from 1 to 10. When the amount of air pollution is very high, the number is reported as 10+. It also includes a category that describes the health risk associated with the index reading (e.g. Low, Moderate, High, or Very High Health Risk). The AQHI is calculated based on the relative risks of a combination of common air pollutants that are known to harm human health, including ground-level ozone, particulate matter, and nitrogen dioxide. The AQHI formulation captures only the short term or acute health risk (exposure of hour or days at a maximum). The formulation of the AQHI may change over time to reflect new understanding associated with air pollution health effects. The AQHI is calculated from data observed in real time, without being verified (quality control).\n    aqhi-observations-realtime\nAir Quality Health Index – Observations\nThe Air Quality Health Index (AQHI) is a scale designed to help quantify the quality of the air in a certain region on a scale from 1 to 10. When the amount of air pollution is very high, the number is reported as 10+. It also includes a category that describes the health risk associated with the index reading (e.g. Low, Moderate, High, or Very High Health Risk). The AQHI is calculated based on the relative risks of a combination of common air pollutants that are known to harm human health, including ground-level ozone, particulate matter, and nitrogen dioxide. The AQHI formulation captures only the short term or acute health risk (exposure of hour or days at a maximum). The formulation of the AQHI may change over time to reflect new understanding associated with air pollution health effects. The AQHI is calculated from data observed in real time, without being verified (quality control).\n    bulletins-realtime\nReal-time meteorological bulletins\nReal-time meteorological bulletins (last 140 days)\n    climate:cmip5:projected:annual:anomaly\nProjected annual anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:seasonal:anomaly\nProjected seasonal anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:monthly:anomaly\nProjected monthly anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:annual:absolute\nProjected annual CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:seasonal:absolute\nProjected seasonal CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:monthly:absolute\nProjected monthly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:annual:P20Y-Avg\nProjected annual anomaly for 20 years average CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:seasonal:P20Y-Avg\nProjected seasonal anomaly for 20 years average CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:annual:absolute\nHistorical annual CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:annual:anomaly\nHistorical annual anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:seasonal:absolute\nHistorical seasonal CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:seasonal:anomaly\nHistorical seasonal anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:monthly:absolute\nHistorical monthly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:monthly:anomaly\nHistorical monthly anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:dcs:projected:annual:anomaly\nProjected annual anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:seasonal:anomaly\nProjected seasonal anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:annual:absolute\nProjected annual DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:seasonal:absolute\nProjected seasonal DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:monthly:absolute\nProjected monthly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:annual:P20Y-Avg\nProjected annual anomaly for 20 years average DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:seasonal:P20Y-Avg\nProjected seasonal anomaly for 20 years average DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:historical:annual:absolute\nHistorical annual DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:historical:annual:anomaly\nHistorical annual anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:historical:seasonal:absolute\nHistorical seasonal DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:historical:seasonal:anomaly\nHistorical seasonal anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:historical:monthly:absolute\nHistorical monthly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:indices:historical\nHistorical indices\nHigh-resolution statistically downscaled climate indices relevant to climate change impacts in Canada are available at a 10 km spatial resolution and an annual temporal resolution for 1951-2100. The climate indices are based on model projections from 24 global climate models (GCMs) that participated in the Coupled Model Intercomparison Project Phase 5 (CMIP5).\n    climate:indices:projected\nProjected indices\nHigh-resolution statistically downscaled climate indices relevant to climate change impacts in Canada are available at a 10 km spatial resolution and an annual temporal resolution for 1951-2100. The climate indices are based on model projections from 24 global climate models (GCMs) that participated in the Coupled Model Intercomparison Project Phase 5 (CMIP5).\n    climate:spei-1:historical\nHistorical SPEI-1\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:spei-3:historical\nHistorical SPEI-3\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:spei-12:historical\nHistorical SPEI-12\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:spei-1:projected\nProjected SPEI-1\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:spei-3:projected\nProjected SPEI-3\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:spei-12:projected\nProjected SPEI-12\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:cangrd:historical:annual:trend\nCanGRD historical annual trend\nCANGRD data are interpolated from adjusted and homogenized climate station data (i.e., AHCCD datasets). Homogenized climate data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Annual trends of relative total precipitation change (%) for 1948-2012 based on Canadian gridded data (CANGRD) are available, at a 50km resolution across Canada. The relative trends reflect the percent change in total precipitation over a period from the baseline value (defined as the average over 1961-1990 as the reference period). Annual trends of mean surface air temperature change (degrees Celsius) for 1948-2016 based on Canadian gridded data (CANGRD) are available at a 50km resolution across Canada. Temperature trends represent the departure from a mean reference period (1961-1990).\n    climate:cangrd:historical:annual:anomaly\nCanGRD historical annual anomaly\nGridded annual mean temperature anomalies derived from daily minimum, maximum and mean surface air temperatures (degrees Celsius) and anomalies derived from daily total precipitation is available at a 50km resolution across Canada. The Canadian gridded data (CANGRD) are interpolated from homogenized temperature (i.e., AHCCD datasets). Homogenized temperatures incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the difference between the temperature for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal temperature anomalies were computed for the years 1948 to 2017. The data will continue to be updated every year. For precipitation, the Canadian gridded data (CANGRD) are interpolated from adjusted precipitation (i.e., AHCCD datasets). Adjusted precipitation data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the percentage difference between the value for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal relative precipitation anomalies were computed for the years 1948 to 2014. The data will be updated as time permits.\n    climate:cangrd:historical:monthly:anomaly\nCanGRD historical monthly anomaly\nGridded monthly mean temperature anomalies derived from daily minimum, maximum and mean surface air temperatures (degrees Celsius) and anomalies derived from daily total precipitation is available at a 50km resolution across Canada. The Canadian gridded data (CANGRD) are interpolated from homogenized temperature (i.e., AHCCD datasets). Homogenized temperatures incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the difference between the temperature for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal temperature anomalies were computed for the years 1948 to 2017. The data will continue to be updated every year. For precipitation, the Canadian gridded data (CANGRD) are interpolated from adjusted precipitation (i.e., AHCCD datasets). Adjusted precipitation data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the percentage difference between the value for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal relative precipitation anomalies were computed for the years 1948 to 2014. The data will be updated as time permits.\n    climate:cangrd:historical:seasonal:trend\nCanGRD historical seasonal trend\nCANGRD data are interpolated from adjusted and homogenized climate station data (i.e., AHCCD datasets). Homogenized climate data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation.Seasonal trends of relative total precipitation change (%) for 1948-2012 based on Canadian gridded data (CANGRD) are available, at a 50km resolution across Canada. The relative trends reflect the percent change in total precipitation over a period from the baseline value (defined as the average over 1961-1990 as the reference period). Seasonal trends of mean surface air temperature change (degrees Celsius) for 1948-2016 based on Canadian gridded data (CANGRD) are available at a 50km resolution across Canada. Temperature trends represent the departure from a mean reference period (1961-1990).\n    climate:cangrd:historical:seasonal:anomaly\nCanGRD historical seasonal anomaly\nGridded seasonal mean temperature anomalies derived from daily minimum, maximum and mean surface air temperatures (degrees Celsius) and anomalies derived from daily total precipitation is available at a 50km resolution across Canada. The Canadian gridded data (CANGRD) are interpolated from homogenized temperature (i.e., AHCCD datasets). Homogenized temperatures incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the difference between the temperature for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal temperature anomalies were computed for the years 1948 to 2017. The data will continue to be updated every year. For precipitation, the Canadian gridded data (CANGRD) are interpolated from adjusted precipitation (i.e., AHCCD datasets). Adjusted precipitation data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the percentage difference between the value for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal relative precipitation anomalies were computed for the years 1948 to 2014. The data will be updated as time permits.\n    weather:rdpa:15km:24f\nRegional Deterministic Precipitation Analysis (RDPA) 24 hours accumulation at 15km\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 24 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 15 km. Data is only available for the surface level. Analysis data is made available once a day for the 24h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n    weather:rdpa:15km:6f\nRegional Deterministic Precipitation Analysis (RDPA) 6 hours accumulation at 15 km\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 6 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 15 km. Data is only available for the surface level. Analysis data is made available four times a day for the 6h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n    weather:rdpa:10km:24f\nRegional Deterministic Precipitation Analysis (RDPA) 24 hours accumulation\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 24 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available once a day for the 24h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n    weather:rdpa:10km:6f\nRegional Deterministic Precipitation Analysis (RDPA) 6 hours accumulation\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 6 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available four times a day for the 6h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n    weather:rdpa:10km:24p\nRegional Deterministic Precipitation Analysis (RDPA) 24 hours accumulation (preliminary)\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 24 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available once a day for the 24h intervals. The preliminary estimate is available approximately 1h after the end of the accumulation period.\n    weather:rdpa:10km:6p\nRegional Deterministic Precipitation Analysis (RDPA) 6 hours accumulation (preliminary)\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 6 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available four times a day for 6h intervals. The preliminary estimate is available approximately 1h after the end of the accumulation period.\n    weather:cansips:250km:forecast:members\nCanadian Seasonal to Inter-annual Prediction System\nThe Canadian Seasonal to Inter-annual Prediction System (CanSIPS) carries out physics calculations to arrive at probabilistic predictions of atmospheric elements from the beginning of a month out to up to 12 months into the future. Atmospheric elements include temperature, precipitation, wind speed and direction and others. This product contains raw numerical results of these calculations. Geographical coverage is global. Data is available on a grid at a horizontal resolution of 2.5 degrees and for a few selected vertical levels. Predictions and corresponding hindcast are made available monthly.\n  \n  \n  \n\n\n\n\nThe collections I want for this project are climate-stations (to find the appropriate Halifax station) and climate-daily to get daily measurements at that station. Get climate-stations:\n\nresp <- httr::GET(paste0(base_url, \"collections/climate-stations/items?f=json\"))\n\n\n\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 6\n $ type          : chr \"FeatureCollection\"\n $ features      :List of 500\n $ numberMatched : int 8552\n $ numberReturned: int 500\n $ links         :List of 5\n $ timeStamp     : chr \"2022-04-27T03:27:26.475648Z\"\n\n\nBefore looking closer at the data, I can already tell I’ll want to increase the limit of returned entries. From the API documentation, the maximum number is 10000, so I can get all 8552 records in one API call:\n\nresp <- GET(paste0(base_url,\n                   \"collections/climate-stations/items?f=json&limit=10000\"))\n\n\n\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 6\n $ type          : chr \"FeatureCollection\"\n $ features      :List of 8552\n $ numberMatched : int 8552\n $ numberReturned: int 8552\n $ links         :List of 4\n $ timeStamp     : chr \"2022-04-27T03:27:27.265491Z\"\n\n\nThe data is contained in the features list:\n\nclimate_stations <- content_parsed$features\nstr(climate_stations[[1]], max.level = 3)\n\nList of 4\n $ type      : chr \"Feature\"\n $ properties:List of 32\n  ..$ STN_ID                  : int 8496\n  ..$ STATION_NAME            : chr \"CARLETON SUR MER\"\n  ..$ PROV_STATE_TERR_CODE    : chr \"QC\"\n  ..$ ENG_PROV_NAME           : chr \"QUEBEC\"\n  ..$ FRE_PROV_NAME           : chr \"QUÉBEC\"\n  ..$ COUNTRY                 : chr \"CAN\"\n  ..$ LATITUDE                : int 480800000\n  ..$ LONGITUDE               : int -660700000\n  ..$ TIMEZONE                : chr \"EST\"\n  ..$ ELEVATION               : chr \"541.00\"\n  ..$ CLIMATE_IDENTIFIER      : chr \"705AA86\"\n  ..$ TC_IDENTIFIER           : NULL\n  ..$ WMO_IDENTIFIER          : NULL\n  ..$ STATION_TYPE            : chr \"N/A\"\n  ..$ NORMAL_CODE             : NULL\n  ..$ PUBLICATION_CODE        : int 1\n  ..$ DISPLAY_CODE            : int 9\n  ..$ ENG_STN_OPERATOR_ACRONYM: NULL\n  ..$ FRE_STN_OPERATOR_ACRONYM: NULL\n  ..$ ENG_STN_OPERATOR_NAME   : NULL\n  ..$ FRE_STN_OPERATOR_NAME   : NULL\n  ..$ FIRST_DATE              : chr \"1968-10-01 00:00:00\"\n  ..$ LAST_DATE               : chr \"1968-10-31 00:00:00\"\n  ..$ HLY_FIRST_DATE          : NULL\n  ..$ HLY_LAST_DATE           : NULL\n  ..$ DLY_FIRST_DATE          : chr \"1968-10-01 00:00:00\"\n  ..$ DLY_LAST_DATE           : chr \"1968-10-31 00:00:00\"\n  ..$ MLY_FIRST_DATE          : NULL\n  ..$ MLY_LAST_DATE           : NULL\n  ..$ HAS_MONTHLY_SUMMARY     : chr \"Y\"\n  ..$ HAS_NORMALS_DATA        : chr \"N\"\n  ..$ HAS_HOURLY_DATA         : chr \"N\"\n $ geometry  :List of 2\n  ..$ type       : chr \"Point\"\n  ..$ coordinates:List of 2\n  .. ..$ : num -66.1\n  .. ..$ : num 48.1\n $ id        : chr \"705AA86\"\n\n\nAfter some frustration, I found that the geometry$coordinates are the correct latitude/longitude – those in the properties list are slightly off for some reason. Extract the data:\n\nclimate_stations <- map_dfr(\n  climate_stations,\n  ~ discard(.x$properties, is.null) %>% as_tibble() %>%\n    mutate(lat = .x$geometry$coordinates[[2]],\n           lon = .x$geometry$coordinates[[1]])\n) %>%\n  janitor::clean_names() %>%\n  # Drop the incorrect latitude and longitude\n  select(-latitude, -longitude)\n\nglimpse(climate_stations)\n\nRows: 8,552\nColumns: 32\n$ stn_id                   <int> 8496, 9005, 10205, 6149, 6154, 6174, 6177, 61…\n$ station_name             <chr> \"CARLETON SUR MER\", \"PORT COLBORNE (AUT)\", \"K…\n$ prov_state_terr_code     <chr> \"QC\", \"ON\", \"QC\", \"NB\", \"NB\", \"NB\", \"NB\", \"NB…\n$ eng_prov_name            <chr> \"QUEBEC\", \"ONTARIO\", \"QUEBEC\", \"NEW BRUNSWICK…\n$ fre_prov_name            <chr> \"QUÉBEC\", \"ONTARIO\", \"QUÉBEC\", \"NOUVEAU-BRUNS…\n$ country                  <chr> \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CA…\n$ timezone                 <chr> \"EST\", \"EST\", \"EST\", \"AST\", \"AST\", \"AST\", \"AS…\n$ elevation                <chr> \"541.00\", \"183.50\", \"123.80\", \"152.40\", \"173.…\n$ climate_identifier       <chr> \"705AA86\", \"613F606\", \"7113382\", \"8101178\", \"…\n$ station_type             <chr> \"N/A\", \"Climate-Auto\", \"N/A\", \"N/A\", \"N/A\", \"…\n$ publication_code         <int> 1, NA, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ display_code             <int> 9, NA, NA, 7, 9, 5, 9, 7, 7, 9, 9, 8, 9, 9, 1…\n$ first_date               <chr> \"1968-10-01 00:00:00\", \"1992-12-02 00:00:00\",…\n$ last_date                <chr> \"1968-10-31 00:00:00\", \"2022-04-24 12:30:02\",…\n$ dly_first_date           <chr> \"1968-10-01 00:00:00\", \"1992-12-02 00:00:00\",…\n$ dly_last_date            <chr> \"1968-10-31 00:00:00\", \"2022-04-24 00:00:00\",…\n$ has_monthly_summary      <chr> \"Y\", \"Y\", \"N\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", …\n$ has_normals_data         <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", …\n$ has_hourly_data          <chr> \"N\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", …\n$ lat                      <dbl> 48.13333, 42.86667, 60.02306, 45.93333, 47.36…\n$ lon                      <dbl> -66.11667, -79.25000, -70.00361, -64.78333, -…\n$ tc_identifier            <chr> NA, \"WPC\", \"YAS\", NA, NA, NA, NA, NA, NA, NA,…\n$ wmo_identifier           <chr> NA, \"71463\", NA, NA, NA, NA, NA, NA, NA, NA, …\n$ eng_stn_operator_acronym <chr> NA, \"ECCC - MSC\", \"DND\", NA, NA, NA, NA, NA, …\n$ fre_stn_operator_acronym <chr> NA, \"ECCC - SMC\", \"MDN\", NA, NA, NA, NA, NA, …\n$ eng_stn_operator_name    <chr> NA, \"Environment and Climate Change Canada - …\n$ fre_stn_operator_name    <chr> NA, \"Environnement et Changement climatique C…\n$ hly_first_date           <chr> NA, \"1994-02-01 02:00:00\", \"1992-10-21 07:00:…\n$ hly_last_date            <chr> NA, \"2022-04-24 12:30:02\", \"2015-09-10 13:00:…\n$ mly_first_date           <chr> NA, \"2006-04-01 00:00:00\", NA, \"1964-01-01 00…\n$ mly_last_date            <chr> NA, \"2006-12-01 00:00:00\", NA, \"1979-12-01 00…\n$ normal_code              <chr> NA, NA, NA, \"F\", NA, \"D\", NA, \"F\", \"F\", NA, N…\n\n\nNow I’ll filter this list down to those in Halifax, NS using distance to the bike counter latitude/longitude means:\n\nclimate_stations_halifax <- climate_stations %>%\n  filter(prov_state_terr_code == \"NS\") %>%\n  mutate(\n    # Compare to the mean lat/lon from the bike counters\n    diff_lat = abs(lat - mean_lat), diff_lon = abs(lon - mean_lon),\n    # Use squared distance to determine the closest points\n    diff2 = diff_lat^2 + diff_lon^2\n  ) %>%\n  # Look at the top 5 for now\n  slice_min(diff2, n = 5)\n\nclimate_stations_halifax %>% rmarkdown::paged_table()\n\n\n\n  \n\n\n\nVisualize the locations of the stations and bike counters:\n\nd <- bind_rows(\n  site_locs %>% mutate(group = \"bike counters\", label = site_name),\n  climate_stations_halifax %>%\n    transmute(label = glue(\"{station_name} ({stn_id})\"),\n              lat, lon, diff2, group = \"climate stations\")\n)\n  \nggmap(halifax_map) +\n  geom_point(data = d, size = 4,\n             aes(fill = group), shape = 21, color = \"white\") +\n  ggrepel::geom_label_repel(\n    data = d,\n    aes(color = group, label = str_trunc(label, width = 25)),\n    box.padding = 2\n  ) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nHalifax Citadel is the closest to the center, but last_date is 2002-01-31 for this station, so it hasn’t been active for the past two decades. The next closest is the dockyard, which is actively being updated (last_date is 2022-04-24).\nNow with the station name (“HALIFAX DOCKYARD”), I can request the daily climate reports:\n\nresp <- GET(\n  paste0(\n    base_url,\n    \"collections/climate-daily/items?f=json&limit=10000&STATION_NAME=HALIFAX%20DOCKYARD\"\n  )\n)\n\n\n\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 6\n $ type          : chr \"FeatureCollection\"\n $ features      :List of 1399\n $ numberMatched : int 1399\n $ numberReturned: int 1399\n $ links         :List of 4\n $ timeStamp     : chr \"2022-04-27T03:27:56.527206Z\"\n\n\nThe features data:\n\ndaily_climate <- content_parsed$features\nstr(daily_climate[[1]])\n\nList of 4\n $ id        : chr \"8202240.2021.2.11\"\n $ type      : chr \"Feature\"\n $ geometry  :List of 2\n  ..$ coordinates:List of 2\n  .. ..$ : num -63.6\n  .. ..$ : num 44.7\n  ..$ type       : chr \"Point\"\n $ properties:List of 34\n  ..$ STATION_NAME            : chr \"HALIFAX DOCKYARD\"\n  ..$ CLIMATE_IDENTIFIER      : chr \"8202240\"\n  ..$ ID                      : chr \"8202240.2021.2.11\"\n  ..$ LOCAL_DATE              : chr \"2021-02-11 00:00:00\"\n  ..$ PROVINCE_CODE           : chr \"NS\"\n  ..$ LOCAL_YEAR              : int 2021\n  ..$ LOCAL_MONTH             : int 2\n  ..$ LOCAL_DAY               : int 11\n  ..$ MEAN_TEMPERATURE        : num -7.5\n  ..$ MEAN_TEMPERATURE_FLAG   : NULL\n  ..$ MIN_TEMPERATURE         : num -9.9\n  ..$ MIN_TEMPERATURE_FLAG    : NULL\n  ..$ MAX_TEMPERATURE         : num -5.1\n  ..$ MAX_TEMPERATURE_FLAG    : NULL\n  ..$ TOTAL_PRECIPITATION     : NULL\n  ..$ TOTAL_PRECIPITATION_FLAG: NULL\n  ..$ TOTAL_RAIN              : NULL\n  ..$ TOTAL_RAIN_FLAG         : NULL\n  ..$ TOTAL_SNOW              : NULL\n  ..$ TOTAL_SNOW_FLAG         : NULL\n  ..$ SNOW_ON_GROUND          : NULL\n  ..$ SNOW_ON_GROUND_FLAG     : NULL\n  ..$ DIRECTION_MAX_GUST      : int 28\n  ..$ DIRECTION_MAX_GUST_FLAG : NULL\n  ..$ SPEED_MAX_GUST          : int 47\n  ..$ SPEED_MAX_GUST_FLAG     : NULL\n  ..$ COOLING_DEGREE_DAYS     : int 0\n  ..$ COOLING_DEGREE_DAYS_FLAG: NULL\n  ..$ HEATING_DEGREE_DAYS     : num 25.5\n  ..$ HEATING_DEGREE_DAYS_FLAG: NULL\n  ..$ MIN_REL_HUMIDITY        : int 41\n  ..$ MIN_REL_HUMIDITY_FLAG   : NULL\n  ..$ MAX_REL_HUMIDITY        : int 66\n  ..$ MAX_REL_HUMIDITY_FLAG   : NULL\n\n\nUnfortunately, this station does not report some helpful measurements, like precipitation and snowfall. I might have to expand my search to find a more informative station:\n\nclimate_stations_halifax <- climate_stations %>%\n  filter(prov_state_terr_code == \"NS\",\n         # Only include stations with recent data\n         last_date > \"2022-04-21\") %>%\n  mutate(\n    diff_lat = abs(lat - mean_lat), diff_lon = abs(lon - mean_lon),\n    diff2 = diff_lat^2 + diff_lon^2\n  ) %>%\n  slice_min(diff2, n = 5)\n\nclimate_stations_halifax %>% rmarkdown::paged_table()\n\n\n\n  \n\n\n\nVisualize these station locations in a zoomed out map:\n\nhrm_map <- get_googlemap(c(mean_lon, mean_lat),\n                         zoom = 12, maptype = \"satellite\")\n\n\n\n\n\nd <- bind_rows(\n  site_locs %>% mutate(group = \"bike counters\", label = site_name),\n  climate_stations_halifax %>%\n    #filter(station_name == \"HALIFAX WINDSOR PARK\")\n    transmute(label = glue(\"{station_name} ({stn_id})\"),\n              lat, lon, diff2, group = \"climate stations\")\n)\n  \nggmap(hrm_map) +\n  geom_point(data = d, size = 4,\n             aes(fill = group), shape = 21, color = \"white\") +\n  ggrepel::geom_label_repel(\n    data = d,\n    aes(color = group, label = str_trunc(label, width = 25)),\n    box.padding = 0.5, force = 1.5\n  ) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nExploring the data from these stations a bit (not shown), Halifax Windsor Park seems a reasonable choice in terms of available data.\n\nresp <- GET(\n  paste0(base_url,\n         \"collections/climate-daily/items?f=json&limit=10000&STATION_NAME=\",\n         URLencode(\"HALIFAX WINDSOR PARK\"))\n)\n\n\n\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\n\ndaily_climate <- map_dfr(\n  content_parsed$features,\n  ~ discard(.x$properties, is.null) %>% as_tibble()\n) %>%\n  janitor::clean_names()\n\nglimpse(daily_climate)\n\nRows: 1,431\nColumns: 29\n$ station_name             <chr> \"HALIFAX WINDSOR PARK\", \"HALIFAX WINDSOR PARK…\n$ climate_identifier       <chr> \"8202255\", \"8202255\", \"8202255\", \"8202255\", \"…\n$ id                       <chr> \"8202255.2021.2.11\", \"8202255.2021.2.12\", \"82…\n$ local_date               <chr> \"2021-02-11 00:00:00\", \"2021-02-12 00:00:00\",…\n$ province_code            <chr> \"NS\", \"NS\", \"NS\", \"NS\", \"NS\", \"NS\", \"NS\", \"NS…\n$ local_year               <int> 2021, 2021, 2021, 2021, 2021, 2018, 2018, 201…\n$ local_month              <int> 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, …\n$ local_day                <int> 11, 12, 13, 14, 15, 14, 15, 16, 17, 18, 19, 2…\n$ mean_temperature         <dbl> -8.1, -8.1, -5.2, -5.0, -1.5, 10.8, 12.1, 9.0…\n$ min_temperature          <dbl> -11.1, -13.0, -8.0, -8.9, -5.4, 5.0, 9.1, 5.4…\n$ max_temperature          <dbl> -5.2, -3.2, -2.3, -1.2, 2.4, 16.5, 15.1, 12.5…\n$ snow_on_ground           <int> 13, 12, 11, 11, 11, NA, NA, NA, NA, NA, NA, N…\n$ direction_max_gust       <int> 26, 30, NA, NA, NA, 22, 21, 33, NA, NA, NA, 2…\n$ speed_max_gust           <int> 42, 32, NA, NA, NA, 38, 48, 32, NA, NA, NA, 4…\n$ cooling_degree_days      <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, …\n$ heating_degree_days      <dbl> 26.1, 26.1, 23.2, 23.0, 19.5, 7.2, 5.9, 9.0, …\n$ min_rel_humidity         <int> 39, 66, 52, 67, 40, 37, 68, 40, 38, 21, 30, 8…\n$ max_rel_humidity         <int> 68, 87, 90, 90, 83, 90, 96, 87, 97, 89, 96, 9…\n$ total_precipitation_flag <chr> NA, NA, NA, NA, \"M\", NA, NA, NA, NA, NA, NA, …\n$ total_precipitation      <dbl> NA, NA, NA, NA, NA, 0.0, 0.6, 0.0, 0.0, 0.0, …\n$ mean_temperature_flag    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ min_temperature_flag     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ max_temperature_flag     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ direction_max_gust_flag  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ speed_max_gust_flag      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cooling_degree_days_flag <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ heating_degree_days_flag <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ min_rel_humidity_flag    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ max_rel_humidity_flag    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\nEDA and cleaning\nVariable summaries:\n\nskimr::skim(daily_climate)\n\n\nData summary\n\n\nName\ndaily_climate\n\n\nNumber of rows\n1431\n\n\nNumber of columns\n29\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n15\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstation_name\n0\n1.00\n20\n20\n0\n1\n0\n\n\nclimate_identifier\n0\n1.00\n7\n7\n0\n1\n0\n\n\nid\n0\n1.00\n16\n18\n0\n1431\n0\n\n\nlocal_date\n0\n1.00\n19\n19\n0\n1431\n0\n\n\nprovince_code\n0\n1.00\n2\n2\n0\n1\n0\n\n\ntotal_precipitation_flag\n1098\n0.23\n1\n1\n0\n1\n0\n\n\nmean_temperature_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmin_temperature_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmax_temperature_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\ndirection_max_gust_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nspeed_max_gust_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\ncooling_degree_days_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nheating_degree_days_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmin_rel_humidity_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmax_rel_humidity_flag\n1409\n0.02\n1\n1\n0\n1\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlocal_year\n0\n1.00\n2019.83\n1.20\n2018.0\n2019.0\n2020.0\n2021.00\n2022.0\n▅▇▇▇▂\n\n\nlocal_month\n0\n1.00\n6.59\n3.44\n1.0\n4.0\n7.0\n10.00\n12.0\n▇▅▅▅▇\n\n\nlocal_day\n0\n1.00\n15.85\n8.76\n1.0\n8.0\n16.0\n23.00\n31.0\n▇▇▇▇▆\n\n\nmean_temperature\n19\n0.99\n8.51\n9.16\n-13.5\n1.3\n8.2\n16.30\n26.8\n▂▆▇▇▅\n\n\nmin_temperature\n19\n0.99\n4.12\n8.95\n-17.3\n-2.1\n3.9\n11.93\n20.7\n▂▅▇▆▆\n\n\nmax_temperature\n19\n0.99\n12.90\n9.66\n-11.0\n5.0\n12.9\n21.20\n34.1\n▂▇▇▇▃\n\n\nsnow_on_ground\n1061\n0.26\n2.82\n4.07\n0.0\n0.0\n1.0\n3.00\n24.0\n▇▁▁▁▁\n\n\ndirection_max_gust\n554\n0.61\n23.77\n8.16\n1.0\n21.0\n24.0\n30.00\n36.0\n▂▃▅▇▆\n\n\nspeed_max_gust\n554\n0.61\n42.48\n10.34\n31.0\n35.0\n40.0\n47.00\n96.0\n▇▂▁▁▁\n\n\ncooling_degree_days\n19\n0.99\n0.61\n1.48\n0.0\n0.0\n0.0\n0.00\n8.8\n▇▁▁▁▁\n\n\nheating_degree_days\n19\n0.99\n10.10\n8.32\n0.0\n1.7\n9.8\n16.70\n31.5\n▇▅▅▃▁\n\n\nmin_rel_humidity\n19\n0.99\n56.63\n18.72\n15.0\n42.0\n56.0\n70.00\n100.0\n▂▇▇▆▂\n\n\nmax_rel_humidity\n22\n0.98\n92.31\n9.40\n47.0\n88.0\n96.0\n100.00\n100.0\n▁▁▁▂▇\n\n\ntotal_precipitation\n554\n0.61\n4.16\n9.75\n0.0\n0.0\n0.0\n3.00\n102.4\n▇▁▁▁▁\n\n\n\n\n\nDrop some un-needed variables:\n\ndaily_climate <- daily_climate %>%\n  select(-station_name, -climate_identifier, -id, -province_code)\n\nProcess the date variable:\n\ndaily_climate <- daily_climate %>%\n  mutate(report_date = as.POSIXct(local_date) %>% as.Date()) %>%\n  # Can drop these now\n  select(-local_date, -local_year, -local_month, -local_day)\n\nThere happens to be some missing days:\n\ntibble(report_date = seq.Date(as.Date(\"2018-05-14\"), as.Date(\"2022-04-22\"),\n                              by = \"days\")) %>%\n  anti_join(daily_climate, by = \"report_date\") %>%\n  pull(report_date)\n\n [1] \"2020-01-02\" \"2020-01-03\" \"2020-01-04\" \"2020-01-05\" \"2020-01-06\"\n [6] \"2021-01-03\" \"2021-01-04\" \"2021-01-05\" \"2021-01-06\" \"2021-01-07\"\n[11] \"2022-01-03\"\n\n\nSeems odd that all of the missing days are in January of different years.\nThere are also some missing temperature values:\n\ndaily_climate %>%\n  filter(\n    is.na(mean_temperature) | is.na(min_temperature) | is.na(max_temperature)\n  ) %>%\n  select(report_date, contains(\"_temperature\")) %>%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThe report_dates range from 2018 to 2022. The flag values (*_temperature_flag) are all “M”, telling us what we already know: the data is missing.\nFor non-missing values, here is the trend over time:\n\ndaily_climate %>%\n  filter(!is.na(mean_temperature)) %>%\n  ggplot(aes(x = report_date)) +\n  geom_line(aes(y = mean_temperature), color = td_colors$nice$ruby_red)\n\n\n\n\nThe total_precipitation values:\n\ndaily_climate %>%\n  count(total_precipitation, total_precipitation_flag) %>%\n  arrange(desc(is.na(total_precipitation))) %>%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThere are missing total_precipitation values with NA total_precipitation_flag, which makes me think that the flag variables are not going to be useful/reliable.\nVisualize the non-missing:\n\ndaily_climate %>%\n  filter(!is.na(total_precipitation)) %>%\n  ggplot(aes(x = report_date)) +\n  geom_point(aes(y = total_precipitation), color = td_colors$nice$spanish_blue)\n\n\n\n\nThe snow_on_ground values:\n\ndaily_climate %>%\n  count(snow_on_ground) %>%\n  arrange(desc(is.na(snow_on_ground))) %>%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\ndaily_climate %>%\n  filter(!is.na(snow_on_ground)) %>%\n  ggplot(aes(x = report_date)) +\n  geom_point(aes(y = snow_on_ground), color = td_colors$nice$spanish_blue)\n\n\n\n\nThe speed_max_gust values (in km/h):\n\ndaily_climate %>%\n  count(speed_max_gust, speed_max_gust_flag) %>%\n  arrange(desc(is.na(speed_max_gust))) %>%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\ndaily_climate %>%\n  filter(!is.na(speed_max_gust)) %>%\n  ggplot(aes(x = report_date)) +\n  geom_point(aes(y = speed_max_gust), color = td_colors$nice$emerald)"
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#combining-the-data",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#combining-the-data",
    "title": "Predicting bike ridership: getting the data",
    "section": "Combining the data",
    "text": "Combining the data\nNow I’ll combine the two data sets (joining on the date), only taking the most useful variables from the climate report (temperature, precipitation, wind speed, snow):\n\nbike_counts_climate <- bike_data_daily_counts %>%\n  left_join(\n    daily_climate %>%\n      select(report_date, mean_temperature, total_precipitation,\n             speed_max_gust, snow_on_ground),\n    by = c(\"count_date\" = \"report_date\")\n  )\nglimpse(bike_counts_climate)\n\nRows: 2,840\nColumns: 9\n$ site_name           <chr> \"Dartmouth Harbourfront Greenway\", \"Dartmouth Harb…\n$ installation_date   <date> 2021-07-08, 2021-07-08, 2021-07-08, 2021-07-08, 2…\n$ count_date          <date> 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2…\n$ n_records           <int> 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48…\n$ n_bikes             <int> 130, 54, 180, 245, 208, 250, 182, 106, 152, 257, 1…\n$ mean_temperature    <dbl> 18.2, 17.6, 21.0, 21.0, 20.6, 18.6, 17.7, 20.2, 20…\n$ total_precipitation <dbl> 0.6, 10.0, 0.4, 0.0, 0.0, 0.0, 0.0, 11.6, 0.2, 0.4…\n$ speed_max_gust      <int> NA, 54, 56, NA, NA, NA, NA, 32, 37, NA, NA, NA, NA…\n$ snow_on_ground      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nVisualize the missing climate data:\n\nbike_counts_climate %>%\n  distinct(count_date, mean_temperature, total_precipitation,\n           speed_max_gust, snow_on_ground) %>%\n  mutate(across(where(is.numeric), is.na)) %>%\n  pivot_longer(cols = -count_date) %>%\n  ggplot(aes(x = count_date, y = name)) +\n  geom_tile(aes(fill = value)) +\n  labs(y = NULL, x = NULL, fill = \"Missing\") +\n  scale_fill_manual(values = c(td_colors$nice$indigo_blue, \"gray80\")) +\n  scale_x_date(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  theme(legend.position = \"top\")\n\n\n\n\nQuite a bit of missing data, but we should have enough to make for an interesting analysis. Save the data:\n\nwrite_rds(bike_counts_climate, \"bike-ridership-data.rds\")\n\nIn my next post, I will use this data to develop and evaluate various prediction models."
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#reproducibility",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#reproducibility",
    "title": "Predicting bike ridership: getting the data",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-08-07\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [53358c7] 2022-08-06: Set fonts and trying out `renv.lock` for reproducibility\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{dunn2022,\n  author = {Taylor Dunn and Harlow Malloc},\n  title = {Post {With} {Code}},\n  date = {2022-07-12},\n  url = {https://tdunn.ca/posts/post-with-code},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTaylor Dunn, and Harlow Malloc. 2022. “Post With Code.”\nJuly 12, 2022. https://tdunn.ca/posts/post-with-code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{dunn2022,\n  author = {Taylor Dunn and Tristan O’Malley},\n  title = {Welcome {To} {My} {Blog}},\n  date = {2022-07-09},\n  url = {https://tdunn.ca/posts/welcome},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTaylor Dunn, and Tristan O’Malley. 2022. “Welcome To My\nBlog.” July 9, 2022. https://tdunn.ca/posts/welcome."
  }
]