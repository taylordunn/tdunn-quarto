[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "R\n\n\nmachine learning\n\n\ntidymodels\n\n\nXGBoost\n\n\nGoogle Cloud Platform\n\n\nDocker\n\n\nShiny\n\n\nforecasting\n\n\nMLOps\n\n\n\n\nPart 3 of predicting bike ridership in Halifax, Nova Scotia. In this post, I deploy the machine learning model on Google Cloud Platform.\n\n\n\n\n\n\nMay 19, 2022\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nmachine learning\n\n\ntidymodels\n\n\nrandom forest\n\n\nXGBoost\n\n\nsupport-vector machine\n\n\nforecasting\n\n\n\n\nPart 2 of predicting bike ridership in Halifax, Nova Scotia. In this post, I explore and tune different modeling approaches.\n\n\n\n\n\n\nApr 29, 2022\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nAPI\n\n\n\n\nPart 1 of predicting bike ridership in Halifax, Nova Scotia. In this post, I retrieve and explore data from two open APIs.\n\n\n\n\n\n\nApr 27, 2022\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\ndata visualization\n\n\n\n\nTidyTuesday 2022 Week 12: Baby names.\n\n\n\n\n\n\nMar 26, 2022\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nCOVID-19\n\n\nAPI\n\n\npackage development\n\n\nGitHub actions\n\n\ncron\n\n\n\n\nScheduling data retrieval and updating with GitHub Actions and cron.\n\n\n\n\n\n\nJan 22, 2022\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nCOVID-19\n\n\nAPI\n\n\npackage development\n\n\n\n\nCreating an R package that wraps the Canadian COVID-19 tracker API.\n\n\n\n\n\n\nDec 30, 2021\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nCOVID-19\n\n\nAPI\n\n\n\n\nAn exploration of the Canadian COVID-19 tracker API.\n\n\n\n\n\n\nDec 28, 2021\n\n\nTaylor Dunn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\nordinal\n\n\nfrequentist statistics\n\n\n\n\nA theoretical and applied walkthrough of ordinal regression. Part 1: the frequentist approach with ordinal.\n\n\n\n\n\n\nMar 15, 2020\n\n\nTaylor Dunn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html",
    "title": "Ordinal regression in R: part 1",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(dunnr)\nlibrary(gt)\nlibrary(broom)\nlibrary(patchwork)\n\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()\n\nwine_red <- \"#58181F\"\nupdate_geom_defaults(\"point\", list(color = wine_red))\nupdate_geom_defaults(\"line\", list(color = wine_red))\nThe purpose of this post is to learn more about ordinal regression models (a.k.a. cumulative link, proportional odds, ordered logit models, etc.) and practice their implementation in R. This is part 1, where I’ll be taking the frequentist approach via the ordinal package. There are other options, like MASS::polr, but two features in particular drew me to ordinal: (1) it allows for random effects, and (2) it has broom::tidy methods available.\nParticularly, I’ll be following along with"
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#setup",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#setup",
    "title": "Ordinal regression in R: part 1",
    "section": "Setup",
    "text": "Setup\nImport ordinal, and the included data set wine:\n\nlibrary(ordinal)\ndata(wine)\nwine <- as_tibble(wine)\nglimpse(wine)\n\nRows: 72\nColumns: 6\n$ response <dbl> 36, 48, 47, 67, 77, 60, 83, 90, 17, 22, 14, 50, 30, 51, 90, 7…\n$ rating   <ord> 2, 3, 3, 4, 4, 4, 5, 5, 1, 2, 1, 3, 2, 3, 5, 4, 2, 3, 3, 2, 5…\n$ temp     <fct> cold, cold, cold, cold, warm, warm, warm, warm, cold, cold, c…\n$ contact  <fct> no, no, yes, yes, no, no, yes, yes, no, no, yes, yes, no, no,…\n$ bottle   <fct> 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5…\n$ judge    <fct> 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3…\n\n\nwine is a data set from Randall (1989) of wine bitterness ratings from multiple judges. The variables are as follows:\n\nOutcome:\n\nresponse: wine bitterness rating on a 0-100 scale\nrating: ordered factor with 5 levels (grouped version of response) with 1 = “least bitter” and 5 = “most bitter”\n\nTreatment factors:\n\ntemp: temperature during wine production (cold and warm)\ncontact: contact between juice and skins during wine production (no and yes)\n\nRandom effects\n\nbottle with 8 levels\njudge with 9 levels\n\n\nRelationship between response and rating:\n\nwine %>%\n  ggplot(aes(y = rating, x = response)) +\n  geom_boxplot(width = 0.5) +\n  geom_jitter(alpha = 0.5)\n\n\n\n\nNote that there is no overlap between the levels.\nThere are 72 total observations with the following ratings distribution by treatment and random effects:\n\nwine %>%\n  transmute(temp, contact, bottle, judge, rating = as.numeric(rating)) %>%\n  pivot_wider(names_from = judge, values_from = rating) %>%\n  gt() %>%\n  tab_spanner(columns = `1`:`9`, label = \"judge\") %>%\n  data_color(\n    columns = `1`:`9`,\n    colors = scales::col_numeric(\n      palette = c(\"white\", wine_red), domain = c(1, 5)\n    )\n  )\n\n\n\n\n\n  \n  \n    \n      temp\n      contact\n      bottle\n      \n        judge\n      \n    \n    \n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    cold\nno\n1\n2\n1\n2\n3\n2\n3\n1\n2\n1\n    cold\nno\n2\n3\n2\n3\n2\n3\n2\n1\n2\n2\n    cold\nyes\n3\n3\n1\n3\n3\n4\n3\n2\n2\n3\n    cold\nyes\n4\n4\n3\n2\n2\n3\n2\n2\n3\n2\n    warm\nno\n5\n4\n2\n5\n3\n3\n2\n2\n3\n3\n    warm\nno\n6\n4\n3\n5\n2\n3\n4\n3\n3\n2\n    warm\nyes\n7\n5\n5\n4\n5\n3\n5\n2\n3\n4\n    warm\nyes\n8\n5\n4\n4\n3\n3\n4\n3\n4\n4\n  \n  \n  \n\n\n\n\nSo each bottle had a particular temp and contact (2 bottles for each of the 4 combinations), and each judge rated the bitterness each bottle.\nBefore modeling, can we see a clear effect of temp and contact?\n\nwine %>%\n  count(contact, rating, temp) %>%\n  mutate(temp = fct_rev(temp)) %>%\n  ggplot(aes(x = temp, y = rating, color = temp)) +\n  geom_point(aes(group = temp, size = n)) +\n  facet_wrap(~contact, scales = \"free_x\",\n             labeller = labeller(contact = label_both)) +\n  scale_size(breaks = c(1, 2, 4, 6, 8)) +\n  add_facet_borders()\n\n\n\n\nAt a glance, it looks like the temp = warm and contact = yes is associated with higher ratings."
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#the-cumulative-link-model",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#the-cumulative-link-model",
    "title": "Ordinal regression in R: part 1",
    "section": "The cumulative link model",
    "text": "The cumulative link model\n\nTheory\nThe ordinal response \\(y_i\\) falls into response category \\(j\\) (out of \\(J\\) total) with probability \\(\\pi_{ij}\\). The cumulative probabilities are defined:\n\\[\nP(y_i \\leq j) = \\pi_{i1} + \\dots + \\pi_{ij}.\n\\]\nAs an oversimplification, suppose that each probability \\(\\pi_{ij}\\) is equal to the proportion of that response in the wine data. Then the cumulative “probability” can be visualized:\n\nwine_prop <- wine %>%\n  count(rating) %>%\n  mutate(p = n / sum(n), cumsum_p = cumsum(p))\n\n(\n  ggplot(wine_prop, aes(x = rating, y = p)) +\n    geom_col(fill = wine_red) +\n    scale_y_continuous(labels = scales::percent, expand = c(0, 0)) +\n    labs(x = \"j\", y = \"proportion\")\n) +\n  (\n    ggplot(wine_prop, aes(x = as.integer(rating), y = cumsum_p)) +\n      geom_point(size = 2) +\n      geom_line(size = 1) +\n      labs(x = \"j\", y = \"cumulative proportion\")\n  ) +\n  (\n    ggplot(wine_prop,\n        aes(x = as.integer(rating), y = log(cumsum_p) - log(1 - cumsum_p))) +\n      geom_point(size = 2) +\n      geom_line(size = 1) +\n      labs(x = \"j\", y = \"logit(cumulative proportion)\")\n  )\n\n\n\n\nWe will explore other links, but first the most common, the logit link, which is depicted in the right-most panel of the above figure:\n\\[\n\\text{logit} (P(y_i \\leq j) = \\log \\frac{P(y_i \\leq j)}{1 - P(y_i \\leq j)}\n\\]\nNote that the above function is defined for all but the last category \\(j = J\\), because \\(1 - P(Y_i \\leq J) = 1 - 1 = 0\\).\nFor the wine data, where we have \\(J\\) = 5 rating categories, we will build up to the following mixed effects model:\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - u( \\text{judge}_i) \\\\\ni &= 1, \\dots n \\; \\; \\; \\; \\; \\; j = 1, \\dots, J - 1\n\\end{align}\n\\]\nwhere \\(\\theta_j\\) is called the threshold parameter, or cutpoint, of category \\(j\\). These thresholds can also be thought of as \\(J-1\\) = 4 intercepts. Note that the fixed effect parameters \\(\\beta_1\\) and \\(\\beta_2\\) are independent of \\(j\\), so each \\(\\beta\\) has the same effect for each of the \\(J-1\\) cumulative logits. The judge effects, which are also independent of \\(j\\), are assumed normal: \\(u(\\text{judge}_i) \\sim N(0, \\sigma_u^2)\\). We are using the logit link because it is the most popular for this kind of model (and the one I am familiar with), but there are other options we will briefly explore later.\nThe subtraction of terms in the above model is new to me. The main reason seems to be for familiar interpretation: the larger the value of any independent term \\(\\beta x\\), the smaller the thresholds \\(\\theta_j\\), and therefore a larger probability of the a response falling into a category at the upper end of the scale. This way, \\(\\beta\\) has the same direction of effect as in ordinary linear regression.\nWe are essentially modeling a “chain” of logistic regressions where the binary response is “less than or equal to a certain level” vs “greater than that level”. In this case, with \\(J\\) = 5, the thresholds \\(\\theta_j\\) are capturing the adjusted log-odds of observing:\n\n\\(j\\) = 1: log-odds of rating = 1 vs. 2-5\n\\(j\\) = 2: log-odds of rating = 1-2 vs. 3-5\n\\(j\\) = 3: log-odds of rating = 1-3 vs. 4-5\n\\(j\\) = 4: log-odds of rating = 1-4 vs. 5\n\n\n\nFitting\nNow with a surface-level understanding of what is being modeled, we will fit the data using ordinal::clm (cumulative link models) and ordinal::clmm (cumulative link mixed models), and logit links.\n\nFixed effects model\nFirst, fit a simple model, by maximum likelihood, with contact as the sole predictor:\n\\[\n\\text{logit}(p(y_i \\leq j)) = \\theta_j - \\beta_2 \\text{contact}_i\n\\]\n\nclm_rating_contact <-\n  clm(\n    rating ~ contact,\n    data = wine, link = \"logit\"\n  )\nsummary(clm_rating_contact)\n\nformula: rating ~ contact\ndata:    wine\n\n link  threshold nobs logLik AIC    niter max.grad cond.H \n logit flexible  72   -99.96 209.91 5(0)  1.67e-07 1.7e+01\n\nCoefficients:\n           Estimate Std. Error z value Pr(>|z|)   \ncontactyes   1.2070     0.4499   2.683   0.0073 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -2.13933    0.48981  -4.368\n2|3  0.04257    0.32063   0.133\n3|4  1.71449    0.38637   4.437\n4|5  2.97875    0.50207   5.933\n\n\nThe model gives us \\(K - 1 = 4\\) threshold coefficients, as expected. The \\(\\beta_2\\) coefficient estimate was statistically significant (by a Wald test), and tells us that contact = yes decreases the thresholds \\(\\theta_j\\) by \\(\\beta_2\\) = 1.21 (because of the subtraction of model terms), and therefore is associated with higher ratings.\nThe condition number of the Hessian for this model is 16.98. The ordinal primer says that larger values (like > 1e4) might indicate that the model is ill-defined.\nIt is nicely illustrative to compare this model to 4 separate logistic regressions with a dichotomized response:\n\\[\n\\begin{align}\n\\text{logit} (p(y_i \\leq 1)) &= \\theta_1 + \\beta_2 \\text{contact}_i \\\\\n\\text{logit} (p(y_i \\leq 2)) &= \\theta_2 + \\beta_2 \\text{contact}_i \\\\\n\\text{logit} (p(y_i \\leq 3)) &= \\theta_3 + \\beta_2 \\text{contact}_i \\\\\n\\text{logit} (p(y_i \\leq 4)) &= \\theta_4 + \\beta_2 \\text{contact}_i\n\\end{align}\n\\]\n\nwine %>%\n  crossing(j = 1:4) %>%\n  # Create a binary (0 or 1) to indicate where rating <= j\n  mutate(rating_leq_j = as.numeric(rating) <= j) %>%\n  group_by(j) %>%\n  nest() %>%\n  ungroup() %>%\n  mutate(\n    mod = map(\n      data,\n      ~glm(rating_leq_j ~ 1 + contact,\n           data = ., family = binomial(link = \"logit\")) %>% broom::tidy()\n    )\n  ) %>%\n  unnest(mod) %>%\n  transmute(\n    j, term,\n    estimate_se = str_c(round(estimate, 2), \" (\", round(std.error, 2), \")\")\n  ) %>%\n  pivot_wider(names_from = term, values_from = estimate_se) %>%\n  left_join(\n    tidy(clm_rating_contact) %>%\n      transmute(\n        j = as.integer(substr(term, 1, 1)),\n        term = if_else(!is.na(j), \"theta_j\", term),\n        estimate_se = str_c(round(estimate, 2), \" (\", round(std.error, 2), \")\")\n      ) %>%\n      mutate(j = replace_na(j, 1)) %>%\n      spread(term, estimate_se),\n    by = \"j\"\n  ) %>%\n  ungroup() %>%\n  gt() %>%\n  tab_spanner(label = \"Logistic regression\",\n              columns = c(`(Intercept)`, contactyes.x)) %>%\n  tab_spanner(label = \"CLM\",\n              columns = c(theta_j, contactyes.y)) %>%\n  fmt_missing(columns = everything(), missing_text = \"\")\n\n\n\n\n\n  \n  \n    \n      j\n      \n        Logistic regression\n      \n      \n        CLM\n      \n    \n    \n      (Intercept)\n      contactyes.x\n      theta_j\n      contactyes.y\n    \n  \n  \n    1\n-2.08 (0.53)\n-1.48 (1.14)\n-2.14 (0.49)\n1.21 (0.45)\n    2\n0 (0.33)\n-1.1 (0.51)\n0.04 (0.32)\n\n    3\n1.82 (0.48)\n-1.37 (0.59)\n1.71 (0.39)\n\n    4\n2.83 (0.73)\n-1.01 (0.87)\n2.98 (0.5)\n\n  \n  \n  \n\n\n\n\nThe intercepts from the ordinary logistic regression correspond closely to the threshold parameters \\(\\theta_j\\) from the cumulative link model. In the fixed effect of contact (\\(\\beta_2\\)), first note the sign difference, and second notice that the estimate from CLM is about the average of the 4 estimates from logistic regression. The advantage of the CLM is seen in the small standard error in the \\(\\beta_2\\) estimate.\nTo quote the primer:\n\nThe cumulative logit model can be seen as the model that combines these four ordinary logistic regression models into a single model and therefore makes better use of the information in the data.\n\nFor the second model, we add the \\(\\beta_1 \\text{temp}_i\\) term:\n\\[\n\\text{logit}(p(y_i \\leq j)) = \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i\n\\]\n\nclm_rating_contact_temp <-\n  clm(\n    rating ~ contact + temp,\n    data = wine, link = \"logit\"\n  )\nsummary(clm_rating_contact_temp)\n\nformula: rating ~ contact + temp\ndata:    wine\n\n link  threshold nobs logLik AIC    niter max.grad cond.H \n logit flexible  72   -86.49 184.98 6(0)  4.01e-12 2.7e+01\n\nCoefficients:\n           Estimate Std. Error z value Pr(>|z|)    \ncontactyes   1.5278     0.4766   3.205  0.00135 ** \ntempwarm     2.5031     0.5287   4.735 2.19e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -1.3444     0.5171  -2.600\n2|3   1.2508     0.4379   2.857\n3|4   3.4669     0.5978   5.800\n4|5   5.0064     0.7309   6.850\n\n\nBoth fixed effects (contact = yes and temp = warm) are strongly associated with higher probability of higher ratings. The summary function provides \\(p\\)-values from Wald tests, but more accurate likelihood ratio tests can be done via the drop1 function, which evaluates each fixed effect while controlling the other:\n\ndrop1(clm_rating_contact_temp, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\nrating ~ contact + temp\n        Df    AIC    LRT  Pr(>Chi)    \n<none>     184.98                     \ncontact  1 194.03 11.043 0.0008902 ***\ntemp     1 209.91 26.928 2.112e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOr the reverse via the add1() function, which evaluates each fixed effect while ignoring the other:\n\n# Fit the null model first\nclm_rating_null <- clm(rating ~ 1, data = wine, link = \"logit\")\nadd1(clm_rating_null, scope = ~ contact + temp, test = \"Chisq\")\n\nSingle term additions\n\nModel:\nrating ~ 1\n        Df    AIC     LRT  Pr(>Chi)    \n<none>     215.44                      \ncontact  1 209.91  7.5263   0.00608 ** \ntemp     1 194.03 23.4113 1.308e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSymmetric Wald confidence intervals can be extracted with confint or with broom::tidy:\n\ntidy(clm_rating_contact_temp, conf.int = TRUE, conf.type = \"Wald\") %>%\n  ggplot(aes(y = term, x = estimate)) +\n  geom_point(size = 2) +\n  geom_linerange(size = 1, aes(xmin = conf.low, xmax = conf.high))\n\n\n\n\nIn these types of analyses, we are often interested in the odds ratios. For the two categorical fixed effects, which have two levels each, the odds ratios \\(y \\leq j\\) comparing the two levels are:\n\\[\n\\begin{align}\n\\text{OR} &= \\frac{\\gamma_j (\\text{temp} = \\text{warm})}{\\gamma_j (\\text{temp} = \\text{cold})} = \\frac{\\exp(\\theta_j - \\beta_1 - \\beta_2 \\text{contact})}{\\exp (\\theta_j - 0 - \\beta_2 \\text{contact}}) = \\exp(\\beta_1) \\\\\n\\text{OR} &= \\frac{\\gamma_j (\\text{contact} = \\text{yes})}{\\gamma_j (\\text{contact} = \\text{no})} = \\frac{\\exp(\\theta_j - \\beta_1 \\text{temp} - \\beta_2 )}{\\exp (\\theta_j - \\beta_1 \\text{temp} - 0)}) = \\exp(\\beta_2)\n\\end{align}\n\\]\nwhere we have introduced the shorthand \\(\\gamma_j = \\text{logit} (p(y \\leq j))\\). Compute those odds ratios, and their corresponding Wald 95% CIs:\n\ntidy(clm_rating_contact_temp, conf.int = T, conf.type = \"Wald\") %>%\n  transmute(\n    term, across(c(estimate, conf.low, conf.high), exp)\n  ) %>%\n  gt() %>%\n  fmt_number(c(estimate, conf.low, conf.high), decimals = 2)\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      conf.low\n      conf.high\n    \n  \n  \n    1|2\n0.26\n0.09\n0.72\n    2|3\n3.49\n1.48\n8.24\n    3|4\n32.04\n9.93\n103.39\n    4|5\n149.37\n35.65\n625.75\n    contactyes\n4.61\n1.81\n11.73\n    tempwarm\n12.22\n4.34\n34.44\n  \n  \n  \n\n\n\n\nOne last thing to check: does the data support an interaction between \\(\\text{temp}_i\\) and \\(\\text{contact}_i\\)?\n\\[\n\\text{logit}(p(y_i \\leq j)) = \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - \\beta_3 \\text{temp}_i \\text{contact}_i\n\\]\n\nclm_rating_contact_temp_inter <-\n  clm(\n    rating ~ contact * temp, data = wine, link = \"logit\"\n  )\n\n#drop1(clm_rating_contact_temp_inter, test = \"Chisq\") # this accomplishes the same thing as anova()\nanova(clm_rating_contact_temp, clm_rating_contact_temp_inter)\n\nLikelihood ratio tests of cumulative link models:\n \n                              formula:                link: threshold:\nclm_rating_contact_temp       rating ~ contact + temp logit flexible  \nclm_rating_contact_temp_inter rating ~ contact * temp logit flexible  \n\n                              no.par    AIC  logLik LR.stat df Pr(>Chisq)\nclm_rating_contact_temp            6 184.98 -86.492                      \nclm_rating_contact_temp_inter      7 186.83 -86.416  0.1514  1     0.6972\n\n\nNo, The interaction term contact:temp is not supported by the data.\n\nComparison to linear model\nConsider the following linear model which treats rating as continuous:\n\\[\ny_i = \\alpha + \\beta_1 \\text{temp}_i + \\beta_2 \\text{contact}_i + \\epsilon_i\n\\]\nwhere \\(\\epsilon_i \\sim N(0, \\sigma_{\\epsilon}^2)\\).\n\nlm_rating_contact_temp <- lm(as.numeric(rating) ~ contact + temp, data = wine)\n\nTo compare this to a CLM, we must use the probit link:\n\nclm_rating_contact_temp_probit <-\n  clm(\n    rating ~ contact + temp, data = wine, link = \"probit\"\n  )\ntidy(clm_rating_contact_temp_probit) %>%\n  filter(coef.type == \"location\") %>%\n  mutate(model = \"CLM\") %>%\n  select(-coef.type) %>%\n  bind_rows(\n    tidy(lm_rating_contact_temp) %>%\n      filter(term != \"(Intercept)\") %>%\n      # Need to divide by the residual SE here to get the right scale\n      mutate(estimate = estimate / summary(lm_rating_contact_temp)$sigma,\n             model = \"LM\")\n  ) %>%\n  group_by(model) %>%\n  gt() %>%\n  fmt_number(c(estimate, std.error, statistic), decimals = 2) %>%\n  fmt(p.value, fns = scales::pvalue)\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    \n      CLM\n    \n    contactyes\n0.87\n0.27\n3.25\n0.001\n    tempwarm\n1.50\n0.29\n5.14\n<0.001\n    \n      LM\n    \n    contactyes\n0.79\n0.20\n3.36\n0.001\n    tempwarm\n1.38\n0.20\n5.87\n<0.001\n  \n  \n  \n\n\n\n\nThe relative estimates from the linear model are lower than those from the CLM (probit link), indicating that the assumptions of the linear model are not met. In particular, the distance between thresholds is not equidistant, as we can see from differences in the CLM coefficients:\n\ndiff(coef(clm_rating_contact_temp_probit)[1:4]) %>% round(2)\n\n 2|3  3|4  4|5 \n1.51 1.31 0.90 \n\n\n\n\n\nMixed effects model\nNow that we have explored ordinal regression with just fixed effects, we will fit the following random effects model:\n\\[\n\\begin{align}\n\\text{logit}(p(y_i \\leq j)) &= \\theta_j - \\beta_1 \\text{temp}_i - \\beta_2 \\text{contact}_i - u( \\text{judge}_i) \\\\\ni &= 1, \\dots n \\; \\; \\; \\; \\; \\; j = 1, \\dots, J - 1\n\\end{align}\n\\]\nwhere the judge effects are independent of \\(j\\), and assumed normal: \\(u(\\text{judge}_i) \\sim N(0, \\sigma_u^2)\\).\nEach judge has 8 ratings each (two per combination of temp and contact). See if we can spot the judge variance in a plot of ratings:\n\nwine %>%\n  count(judge, rating) %>%\n  ggplot(aes(x = judge, y = rating)) +\n  geom_tile(aes(fill = n)) +\n  geom_text(aes(label = n), color = \"white\") +\n  scale_x_discrete(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Number of ratings by judge\")\n\n\n\n\nThere is definitely some judge-specific variability in the perception of bitterness of wine. judge 5, for instance, doesn’t stray far from rating = 3, while judge 7 didn’t consider any of the wines particularly bitter.\nFit the full model with ordinal::clmm and logit link:\n\nclmm_rating_contact_temp <-\n  clmm(\n    rating ~ temp + contact + (1|judge),\n    data = wine, link = \"logit\"\n  )\n# This is an older function, which we need to run stats::profile later\nclmm2_rating_contact_temp <-\n  clmm2(\n    rating ~ temp + contact, random = judge,\n    data = wine, link = \"logistic\"\n  )\nsummary(clmm_rating_contact_temp)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ temp + contact + (1 | judge)\ndata:    wine\n\n link  threshold nobs logLik AIC    niter    max.grad cond.H \n logit flexible  72   -81.57 177.13 332(999) 1.03e-05 2.8e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n judge  (Intercept) 1.279    1.131   \nNumber of groups:  judge 9 \n\nCoefficients:\n           Estimate Std. Error z value Pr(>|z|)    \ntempwarm     3.0630     0.5954   5.145 2.68e-07 ***\ncontactyes   1.8349     0.5125   3.580 0.000344 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -1.6237     0.6824  -2.379\n2|3   1.5134     0.6038   2.507\n3|4   4.2285     0.8090   5.227\n4|5   6.0888     0.9725   6.261\n\n\nCompare model coefficients:\n\nbind_rows(\n  CLM = tidy(clm_rating_contact_temp),\n  CLMM = tidy(clmm_rating_contact_temp),\n  .id = \"model\"\n) %>%\n  select(-coef.type) %>%\n  group_by(model) %>%\n  gt() %>%\n  fmt_number(c(estimate, std.error, statistic), decimals = 2) %>%\n  fmt(p.value, fns = scales::pvalue)\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    \n      CLM\n    \n    1|2\n−1.34\n0.52\n−2.60\n0.009\n    2|3\n1.25\n0.44\n2.86\n0.004\n    3|4\n3.47\n0.60\n5.80\n<0.001\n    4|5\n5.01\n0.73\n6.85\n<0.001\n    contactyes\n1.53\n0.48\n3.21\n0.001\n    tempwarm\n2.50\n0.53\n4.73\n<0.001\n    \n      CLMM\n    \n    1|2\n−1.62\n0.68\n−2.38\n0.017\n    2|3\n1.51\n0.60\n2.51\n0.012\n    3|4\n4.23\n0.81\n5.23\n<0.001\n    4|5\n6.09\n0.97\n6.26\n<0.001\n    tempwarm\n3.06\n0.60\n5.14\n<0.001\n    contactyes\n1.83\n0.51\n3.58\n<0.001\n  \n  \n  \n\n\n\n\nBoth fixed effect estimates \\(\\beta_1\\) and \\(\\beta_2\\) are higher in the CLMM. Use anova to compare the CLMM to the CLM:\n\nanova(clm_rating_contact_temp, clmm_rating_contact_temp)\n\nLikelihood ratio tests of cumulative link models:\n \n                         formula:                              link: threshold:\nclm_rating_contact_temp  rating ~ contact + temp               logit flexible  \nclmm_rating_contact_temp rating ~ temp + contact + (1 | judge) logit flexible  \n\n                         no.par    AIC  logLik LR.stat df Pr(>Chisq)   \nclm_rating_contact_temp       6 184.98 -86.492                         \nclmm_rating_contact_temp      7 177.13 -81.565   9.853  1   0.001696 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUnsurprisingly, the judge term makes a significant improvement to the fit. We can extract profile confidence intervals on the variance \\(\\sigma_u\\) using stats::profile:\n\nprofile(clmm2_rating_contact_temp,\n        range = c(0.1, 4), nSteps = 30, trace = 0) %>%\n  confint()\n\n          2.5 %  97.5 %\nstDev 0.5014584 2.26678\n\n\nNote that these intervals are asymmetric (\\(\\sigma_u\\) = 1.28), unlike the less accurate Wald tests. We can produce “best guess” estimates for judge effects using conditional modes:\n\ntibble(\n  judge_effect = clmm_rating_contact_temp$ranef,\n  cond_var = clmm_rating_contact_temp$condVar\n) %>%\n  mutate(\n    judge = fct_reorder(factor(1:n()), judge_effect),\n    conf.low = judge_effect - qnorm(0.975) * sqrt(cond_var),\n    conf.high = judge_effect + qnorm(0.975) * sqrt(cond_var)\n  ) %>%\n  ggplot(aes(y = judge, x = judge_effect)) +\n  geom_point(size = 2) +\n  geom_linerange(size = 1, aes(xmin = conf.low, xmax = conf.high)) +\n  theme(panel.grid.major.x = element_line(color = \"grey\"))\n\n\n\n\n\nPredictions\nThere are different ways to extract predicted probabilities. First, and most obviously, with the predict function:\n\nwine %>%\n  bind_cols(\n    pred =  predict(\n      # Have to use clmm2 for predict\n      clmm2_rating_contact_temp, newdata = wine\n    )\n  ) %>%\n  # These are predicted probabilities for the average judge, so we can\n  #  exclude the judge variable\n  distinct(rating, temp, contact, pred) %>%\n  arrange(temp, contact, rating)\n\n# A tibble: 15 × 4\n   rating temp  contact   pred\n   <ord>  <fct> <fct>    <dbl>\n 1 1      cold  no      0.165 \n 2 2      cold  no      0.655 \n 3 3      cold  no      0.166 \n 4 1      cold  yes     0.0305\n 5 2      cold  yes     0.390 \n 6 3      cold  yes     0.496 \n 7 4      cold  yes     0.0696\n 8 2      warm  no      0.166 \n 9 3      warm  no      0.587 \n10 4      warm  no      0.191 \n11 5      warm  no      0.0463\n12 2      warm  yes     0.0313\n13 3      warm  yes     0.306 \n14 4      warm  yes     0.428 \n15 5      warm  yes     0.233 \n\n\nThis only gives us predictions for rating, temp and contact values which exist in the data. There is no predicted probability for rating > 3, temp cold and contact no, for example.\nAnother way is to pre-specify which values we want to predict:\n\nnd <-\n  crossing(\n    temp = factor(c(\"cold\", \"warm\")),\n    contact = factor(c(\"no\", \"yes\")),\n    rating = factor(1:5, ordered = T)\n  )\nnd %>%\n  bind_cols(pred = predict(clmm2_rating_contact_temp, nd)) %>%\n  ggplot(aes(x = glue::glue(\"{temp}-{contact}\"), y = pred, fill = rating)) +\n  geom_col() +\n  scale_fill_td(palette = \"div5\") +\n  scale_y_continuous(expand = c(0, 0), labels = scales::percent) +\n  labs(x = \"temp-contact\", y = \"predicted probability\")\n\n\n\n\nWe can also get model-estimated cumulative probabilities by considering the model coefficients. For example, for a cold temp and contact, the cumulative probability of a bitterness rating \\(j\\) or less:\n\\[\nP(y_i \\leq j) = \\text{logit}^{-1} [\\theta_j - \\beta_2 \\text{contact}_i]\n\\]\nwhere we are considering the average judge (\\(u(\\text{judge}_i) = 0\\)). The inverse logit is \\(\\text{logit}^{-1}(x) = 1 / (1 + \\exp(-x))\\), and can be calculated with plogis as a shorthand (brms::inv_logit_scaled is another). We can subtract cumulative probabilities to get non-cumulative probabilities of a rating \\(j\\). For example, \\(j\\) = 3:\n\nplogis(clmm_rating_contact_temp$Theta[3] - clmm_rating_contact_temp$beta[2]) -\n  plogis(clmm_rating_contact_temp$Theta[2] - clmm_rating_contact_temp$beta[2])\n\ncontactyes \n 0.4960357 \n\n\nwhich matches the value calculated previously using predict.\n\n\nEstimated marginal means\nThe emmeans package provides functionality for estimating marginal mean effects of ordinal models. The package documentation also provides an example using ordinal and wine data here.\n\nlibrary(emmeans)\n\nIn the “Models supported by emmeans” document, we see the following:\n\n\n\n\n\n\n\n\n\n\nObject.class\nPackage\nGroup\nArguments/notes\n\n\n\n\n\nclm\nordinal\nO\nmode = c(\"latent\", \"linear.predictor\", \"cum.prob\", \"exc.prob\", \"prob\", \"mean.class\", \"scale\")\n\n\n\nclmm\nordinal\nO\nLike clm but no \"scale\" mode\n\n\n\n\n\n\n\n\n\n\n\n\nemmeans(clmm_rating_contact_temp,\n        specs = list(pairwise ~ temp, pairwise ~ contact), mode = \"latent\")\n\n$`emmeans of temp`\n temp emmean    SE  df asymp.LCL asymp.UCL\n cold  -1.63 0.547 Inf    -2.707    -0.562\n warm   1.43 0.532 Inf     0.387     2.470\n\nResults are averaged over the levels of: contact \nConfidence level used: 0.95 \n\n$`pairwise differences of temp`\n 1           estimate    SE  df z.ratio p.value\n cold - warm    -3.06 0.595 Inf  -5.145  <.0001\n\nResults are averaged over the levels of: contact \n\n$`emmeans of contact`\n contact emmean    SE  df asymp.LCL asymp.UCL\n no      -1.020 0.522 Inf    -2.043   0.00274\n yes      0.815 0.513 Inf    -0.191   1.82072\n\nResults are averaged over the levels of: temp \nConfidence level used: 0.95 \n\n$`pairwise differences of contact`\n 1        estimate    SE  df z.ratio p.value\n no - yes    -1.83 0.513 Inf  -3.580  0.0003\n\nResults are averaged over the levels of: temp \n\n\nThe contrast estimates are in terms of the latent (underlying unobserved) bitterness rating.\nUsing mode = \"cum.prob\" and mode = \"exc.prob“, we can get cumulative probabilities and exceedance (1 - cumulative) probabilities. For example, the probability of a rating of at least 4 for different temp:\n\nemmeans(clmm_rating_contact_temp, ~ temp,\n        mode = \"exc.prob\", at = list(cut = \"3|4\"))\n\n temp exc.prob     SE  df asymp.LCL asymp.UCL\n cold    0.049 0.0304 Inf   -0.0107     0.109\n warm    0.450 0.1084 Inf    0.2371     0.662\n\nResults are averaged over the levels of: contact \nConfidence level used: 0.95 \n\n\nmode = \"prob\" gives us probability distributions of each rating, which have a nice auto plot functionality:\n\nemmeans(clmm_rating_contact_temp,\n        ~ rating | temp, mode = \"prob\") %>%\n  plot() +\n  add_facet_borders()\n\n\n\n\n\n\nChoice of link function\nSo far, we have used the logit link (and briefly the probit link to compare estimates with linear regression). The links available to ordinal::clmm are logit, probit, cloglog, loglog, and cauchit.\nWe can fit the CLMM using all of these links and compare log-likelihoods:\n\nwine %>%\n  nest(data = everything()) %>%\n  crossing(\n    link = c(\"logit\", \"probit\", \"cloglog\", \"loglog\", \"cauchit\")\n  ) %>%\n  mutate(\n    mod = map2(\n      data, link,\n      ~clmm(\n        rating ~ 1 + contact + temp + (1|judge),\n        data = .x, link = .y\n      )\n    )\n  ) %>%\n  mutate(mod_summary = map(mod, glance)) %>%\n  unnest(mod_summary) %>%\n  select(link, logLik, AIC, BIC) %>%\n  arrange(logLik) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      link\n      logLik\n      AIC\n      BIC\n    \n  \n  \n    cauchit\n-86.83499\n187.6700\n203.6066\n    cloglog\n-82.72936\n179.4587\n195.3954\n    logit\n-81.56541\n177.1308\n193.0675\n    loglog\n-81.54137\n177.0827\n193.0194\n    probit\n-80.93061\n175.8612\n191.7979\n  \n  \n  \n\n\n\n\nThe probit model appears to be the best description of the data.\nWe can also consider the effect of “flexible” vs “equidistant” thresholds:\n\nwine %>%\n  nest(data = everything()) %>%\n  crossing(\n    link = c(\"logit\", \"probit\", \"cloglog\", \"loglog\", \"cauchit\"),\n    threshold = c(\"flexible\", \"equidistant\")\n  ) %>%\n  mutate(\n    mod = pmap(\n      list(data, link, threshold),\n      function(a, b, c) {\n        clmm(\n          rating ~ 1 + contact + temp + (1|judge),\n          data = a, link = b, threshold = c\n        )\n      }\n    )\n  ) %>%\n  #mutate(mod_summary = map(mod, glance)) %>%\n  mutate(\n    mod_summary = map(\n      mod,\n      # glance() on a clmm object returns a <logLik> variable type which\n      #  can't be bound together by unnest(), so need to convert it to numeric\n      ~glance(.x) %>% mutate(logLik = as.numeric(logLik))\n    )\n  ) %>%\n  unnest(mod_summary) %>%\n  select(link, threshold, logLik, edf, AIC, BIC) %>%\n  arrange(logLik) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      link\n      threshold\n      logLik\n      edf\n      AIC\n      BIC\n    \n  \n  \n    cauchit\nequidistant\n-87.75021\n5\n185.5004\n196.8837\n    cauchit\nflexible\n-86.83499\n7\n187.6700\n203.6066\n    loglog\nequidistant\n-84.36440\n5\n178.7288\n190.1121\n    cloglog\nequidistant\n-83.32634\n5\n176.6527\n188.0360\n    logit\nequidistant\n-83.05497\n5\n176.1099\n187.4933\n    cloglog\nflexible\n-82.72936\n7\n179.4587\n195.3954\n    probit\nequidistant\n-82.52622\n5\n175.0524\n186.4358\n    logit\nflexible\n-81.56541\n7\n177.1308\n193.0675\n    loglog\nflexible\n-81.54137\n7\n177.0827\n193.0194\n    probit\nflexible\n-80.93061\n7\n175.8612\n191.7979\n  \n  \n  \n\n\n\n\nNote the change in degrees of freedom, resulting in the equidistant probit model having the lowest BIC. In terms of log likelihood, however, flexible always outperform equidistant thresholds."
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#conclusion",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#conclusion",
    "title": "Ordinal regression in R: part 1",
    "section": "Conclusion",
    "text": "Conclusion\nThanks to detailed documentation, fitting cumulative link (mixed) models is very easy with ordinal. In this post, we first learned the theoretical basis for these models, then worked through examples using wine bitterness ratings from multiple judges.\nIn the next post, I’ll explore the Bayesian approach to ordinal regression with the brms package."
  },
  {
    "objectID": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#reproducibility",
    "href": "posts/2020-03-15-ordinal-regression-in-r-part-1/ordinal-regression-in-r-part-1.html#reproducibility",
    "title": "Ordinal regression in R: part 1",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-08-07\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [53358c7] 2022-08-06: Set fonts and trying out `renv.lock` for reproducibility\n\n\n\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(httr)\nlibrary(gt)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#introduction",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#introduction",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Introduction",
    "text": "Introduction\nWith this post, I will explore the Canadian COVID-19 tracker API and, depending on how it goes, turn some of the code into an R package. For an introduction to working with APIs, see this vignette from the httr package.\n\n\n\n\n\n\nNote\n\n\n\nIn 2022, I ported my website from Distill to Quarto. This required me to re-run all the code in this post, so the data will go up to 2022-08-21, not 2021-12-28 (which is the date of the post)."
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#summary",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#summary",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Summary",
    "text": "Summary\nThe first data I will retrieve is the data summaries overall, by province, and by health region. To save typing it every time, the following base_url is required for all GET requests:\n\nbase_url <- \"https://api.covid19tracker.ca/\"\n\n\nOverall\nModify the URL with summary to get the latest data across all provinces:\n\napi_url <- paste0(base_url, \"summary\")\n\nSend the GET request with httr:\n\nresp <- httr::GET(api_url)\nresp\n\nResponse [https://api.covid19tracker.ca/summary]\n  Date: 2022-08-22 02:27\n  Status: 200\n  Content-Type: application/json\n  Size: 702 B\n\n\nThis returned a response object with the following structure:\n\nstr(resp, max.level = 1)\n\nList of 10\n $ url        : chr \"https://api.covid19tracker.ca/summary\"\n $ status_code: int 200\n $ headers    :List of 12\n  ..- attr(*, \"class\")= chr [1:2] \"insensitive\" \"list\"\n $ all_headers:List of 1\n $ cookies    :'data.frame':    0 obs. of  7 variables:\n $ content    : raw [1:702] 7b 22 64 61 ...\n $ date       : POSIXct[1:1], format: \"2022-08-22 02:27:30\"\n $ times      : Named num [1:6] 0 0.0477 0.0895 0.185 0.2508 ...\n  ..- attr(*, \"names\")= chr [1:6] \"redirect\" \"namelookup\" \"connect\" \"pretransfer\" ...\n $ request    :List of 7\n  ..- attr(*, \"class\")= chr \"request\"\n $ handle     :Class 'curl_handle' <externalptr> \n - attr(*, \"class\")= chr \"response\"\n\n\nThe status_code is the first thing to check:\n\nresp$status_code\n\n[1] 200\n\n\nAn HTTP status code of 200 is the standard indicator of a successful request.\nOnce confirmed successful, the content returned from the request is:\n\nhead(resp$content, 25)\n\n [1] 7b 22 64 61 74 61 22 3a 5b 7b 22 6c 61 74 65 73 74 5f 64 61 74 65 22 3a 22\n\n\nLooks like the raw data is represented in hexadecimal. The httr::content() function can parse this data:\n\ncontent_parsed <- httr::content(resp, as = \"parsed\")\nstr(content_parsed)\n\nList of 2\n $ data        :List of 1\n  ..$ :List of 23\n  .. ..$ latest_date                : chr \"2022-08-19\"\n  .. ..$ change_cases               : chr \"11044\"\n  .. ..$ change_fatalities          : chr \"98\"\n  .. ..$ change_tests               : chr \"66399\"\n  .. ..$ change_hospitalizations    : chr \"-49\"\n  .. ..$ change_criticals           : chr \"-6\"\n  .. ..$ change_recoveries          : chr \"10486\"\n  .. ..$ change_vaccinations        : chr \"111235\"\n  .. ..$ change_vaccinated          : chr \"6837\"\n  .. ..$ change_boosters_1          : chr \"13955\"\n  .. ..$ change_boosters_2          : chr \"72513\"\n  .. ..$ change_vaccines_distributed: chr \"0\"\n  .. ..$ total_cases                : chr \"4119278\"\n  .. ..$ total_fatalities           : chr \"43481\"\n  .. ..$ total_tests                : chr \"60894369\"\n  .. ..$ total_hospitalizations     : chr \"5352\"\n  .. ..$ total_criticals            : chr \"275\"\n  .. ..$ total_recoveries           : chr \"3735937\"\n  .. ..$ total_vaccinations         : chr \"88194801\"\n  .. ..$ total_vaccinated           : chr \"31530453\"\n  .. ..$ total_boosters_1           : chr \"19033545\"\n  .. ..$ total_boosters_2           : chr \"4703251\"\n  .. ..$ total_vaccines_distributed : chr \"101586018\"\n $ last_updated: chr \"2022-08-21 20:27:03\"\n\n\n\nPer the documentation of httr, it is good practice to check the mime type of the response before parsing it.\n\nhttr::http_type(resp)\n\n[1] \"application/json\"\n\n\nHere, the data is JSON, which is a type that can be parsed with httr::content (via the jsonlite package). We can also parse this data manually as follows (not run):\n\njsonlite::fromJSON(\n  content(resp, \"text\"),\n  simplifyVector = FALSE\n)\n\n\nThe returned data structure is a list of lists. data is a list with all of the summary statistics, while last_updated gives a timestamp of when the data was last updated. Put the data into a data frame:\n\nsummary_overall <- content_parsed$data %>% data.frame()\nglimpse(summary_overall)\n\nRows: 1\nColumns: 23\n$ latest_date                 <chr> \"2022-08-19\"\n$ change_cases                <chr> \"11044\"\n$ change_fatalities           <chr> \"98\"\n$ change_tests                <chr> \"66399\"\n$ change_hospitalizations     <chr> \"-49\"\n$ change_criticals            <chr> \"-6\"\n$ change_recoveries           <chr> \"10486\"\n$ change_vaccinations         <chr> \"111235\"\n$ change_vaccinated           <chr> \"6837\"\n$ change_boosters_1           <chr> \"13955\"\n$ change_boosters_2           <chr> \"72513\"\n$ change_vaccines_distributed <chr> \"0\"\n$ total_cases                 <chr> \"4119278\"\n$ total_fatalities            <chr> \"43481\"\n$ total_tests                 <chr> \"60894369\"\n$ total_hospitalizations      <chr> \"5352\"\n$ total_criticals             <chr> \"275\"\n$ total_recoveries            <chr> \"3735937\"\n$ total_vaccinations          <chr> \"88194801\"\n$ total_vaccinated            <chr> \"31530453\"\n$ total_boosters_1            <chr> \"19033545\"\n$ total_boosters_2            <chr> \"4703251\"\n$ total_vaccines_distributed  <chr> \"101586018\"\n\n\nAll of these variables are character type, and should be converted into integer and Date types:\n\nsummary_overall <- summary_overall %>%\n  mutate(\n    across(matches(\"^change|^total\"), as.integer),\n    across(matches(\"date\"), as.Date)\n  )\nglimpse(summary_overall)\n\nRows: 1\nColumns: 23\n$ latest_date                 <date> 2022-08-19\n$ change_cases                <int> 11044\n$ change_fatalities           <int> 98\n$ change_tests                <int> 66399\n$ change_hospitalizations     <int> -49\n$ change_criticals            <int> -6\n$ change_recoveries           <int> 10486\n$ change_vaccinations         <int> 111235\n$ change_vaccinated           <int> 6837\n$ change_boosters_1           <int> 13955\n$ change_boosters_2           <int> 72513\n$ change_vaccines_distributed <int> 0\n$ total_cases                 <int> 4119278\n$ total_fatalities            <int> 43481\n$ total_tests                 <int> 60894369\n$ total_hospitalizations      <int> 5352\n$ total_criticals             <int> 275\n$ total_recoveries            <int> 3735937\n$ total_vaccinations          <int> 88194801\n$ total_vaccinated            <int> 31530453\n$ total_boosters_1            <int> 19033545\n$ total_boosters_2            <int> 4703251\n$ total_vaccines_distributed  <int> 101586018\n\n\n\n\nProvince\nInstead of aggregating over all provinces, I can use /summary/split to get province-level summaries:\n\nresp <- httr::GET(paste0(base_url, \"summary/split\"))\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 2)\n\nList of 2\n $ data        :List of 13\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n  ..$ :List of 24\n $ last_updated: chr \"2022-08-21 20:27:03\"\n\n\nThe data list now has 13 lists corresponding to the 13 provinces and territories. Look at the structure of one of them:\n\nstr(content_parsed$data[[1]])\n\nList of 24\n $ province                   : chr \"ON\"\n $ date                       : chr \"2022-08-18\"\n $ change_cases               : int 10169\n $ change_fatalities          : int 56\n $ change_tests               : int 66399\n $ change_hospitalizations    : int -54\n $ change_criticals           : int -5\n $ change_recoveries          : int 10486\n $ change_vaccinations        : int 86992\n $ change_vaccinated          : int 5429\n $ change_boosters_1          : int 9964\n $ change_boosters_2          : int 59994\n $ change_vaccines_distributed: int 0\n $ total_cases                : int 1399147\n $ total_fatalities           : int 13869\n $ total_tests                : int 25220610\n $ total_hospitalizations     : int 1328\n $ total_criticals            : int 137\n $ total_recoveries           : int 1368994\n $ total_vaccinations         : int 34594350\n $ total_vaccinated           : int 12270711\n $ total_boosters_1           : int 7509601\n $ total_boosters_2           : int 2073259\n $ total_vaccines_distributed : int 38554461\n\n\nThis is the same structure as the overall summary, but with the extra variable province indicating that these numbers are specific to Ontario.\nA shortcut to compiling all of these lists into a single data frame with a row per province/territory is to use dplyr::bind_rows():\n\nsummary_province <- bind_rows(content_parsed$data)\nglimpse(summary_province)\n\nRows: 13\nColumns: 24\n$ province                    <chr> \"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", …\n$ date                        <chr> \"2022-08-18\", \"2022-08-19\", \"2022-08-11\", …\n$ change_cases                <int> 10169, 0, 0, 0, 0, 875, 0, 0, 0, 0, 0, 0, 0\n$ change_fatalities           <int> 56, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0\n$ change_tests                <int> 66399, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ change_hospitalizations     <int> -54, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0\n$ change_criticals            <int> -5, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0\n$ change_recoveries           <int> 10486, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ change_vaccinations         <int> 86992, 0, 0, 0, 0, 24243, 0, 0, 0, 0, 0, 0…\n$ change_vaccinated           <int> 5429, 0, 0, 0, 0, 1408, 0, 0, 0, 0, 0, 0, 0\n$ change_boosters_1           <int> 9964, 0, 0, 0, 0, 3991, 0, 0, 0, 0, 0, 0, 0\n$ change_boosters_2           <int> 59994, 0, 0, 0, 0, 12519, 0, 0, 0, 0, 0, 0…\n$ change_vaccines_distributed <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ total_cases                 <int> 1399147, 1167556, 106158, 70545, 146874, 3…\n$ total_fatalities            <int> 13869, 16134, 451, 438, 2079, 4037, 46, 14…\n$ total_tests                 <int> 25220610, 16733572, 1777959, 782596, 15278…\n$ total_hospitalizations      <int> 1328, 2011, 44, 22, 579, 390, 10, 147, 818…\n$ total_criticals             <int> 137, 45, 8, 1, 28, 22, 1, 5, 28, 0, 0, 0, 0\n$ total_recoveries            <int> 1368994, 1099129, 43018, 65138, 129338, 30…\n$ total_vaccinations          <int> 34594350, 20863558, 2235685, 1757172, 2863…\n$ total_vaccinated            <int> 12270711, 7195120, 842600, 661271, 1108820…\n$ total_boosters_1            <int> 7509601, 4568607, 503257, 395689, 593160, …\n$ total_boosters_2            <int> 2073259, 1565745, 0, 0, 0, 543279, 7449, 1…\n$ total_vaccines_distributed  <int> 38554461, 23092789, 2518832, 2212445, 3842…\n\n\nbind_rows() also automatically converts the numeric columns to integer, but the date column is still character:\n\nsummary_province <- summary_province %>% mutate(date = as.Date(date))\n\n\n\nHealth region\nData my be split even further by health region with summary/split/hr:\n\nresp <- httr::GET(paste0(base_url, \"summary/split/hr\"))\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 2\n $ data        :List of 92\n $ last_updated: chr \"2022-08-21 20:27:03\"\n\n\nThis data consists of 92 entries with mostly the same variables as previous summaries:\n\nstr(content_parsed$data[[1]])\n\nList of 22\n $ hr_uid                 : int 471\n $ date                   : chr \"2022-08-10\"\n $ change_cases           : NULL\n $ change_fatalities      : NULL\n $ change_tests           : NULL\n $ change_hospitalizations: NULL\n $ change_criticals       : NULL\n $ change_recoveries      : NULL\n $ change_vaccinations    : NULL\n $ change_vaccinated      : NULL\n $ change_boosters_1      : NULL\n $ change_boosters_2      : NULL\n $ total_cases            : int 0\n $ total_fatalities       : int 8\n $ total_tests            : int 66969\n $ total_hospitalizations : int 2\n $ total_criticals        : int 0\n $ total_recoveries       : int 10248\n $ total_vaccinations     : int 70534\n $ total_vaccinated       : int 33236\n $ total_boosters_1       : NULL\n $ total_boosters_2       : NULL\n\n\nThe differences are the hr_uid column in place of province, and the lack of change_vaccines_distributed and total_vaccines_distributed, presumably because these numbers aren’t available at this granularity.\n\nsummary_region <- bind_rows(content_parsed$data) %>%\n  mutate(date = as.Date(date))\nglimpse(summary_region)\n\nRows: 92\nColumns: 21\n$ hr_uid                  <int> 471, 472, 473, 474, 475, 476, 1201, 1202, 1203…\n$ date                    <date> 2022-08-10, 2022-08-10, 2022-08-10, 2022-08-1…\n$ total_cases             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ total_fatalities        <int> 8, 16, 5, 6, 3, 7, 1, 1, 5, 58, 207, 13, 16, 7…\n$ total_tests             <int> 66969, 206300, 115789, 380252, 290508, 176859,…\n$ total_hospitalizations  <int> 2, 61, 22, 171, 54, 22, NA, NA, NA, NA, 198, 7…\n$ total_criticals         <int> 0, 7, 1, 17, 5, 1, NA, NA, NA, NA, 9, 6, 0, 3,…\n$ total_recoveries        <int> 10248, 24271, 9069, 29440, 23925, 14978, 640, …\n$ total_vaccinations      <int> 70534, 319807, 201758, 513380, 430413, 273939,…\n$ total_vaccinated        <int> 33236, 152611, 98324, 249513, 208970, 133557, …\n$ total_boosters_1        <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 376228…\n$ total_boosters_2        <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ change_recoveries       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ change_vaccinated       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ change_boosters_1       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ change_boosters_2       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ change_vaccinations     <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ change_cases            <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ change_fatalities       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ change_hospitalizations <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ change_criticals        <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nhr_uid is a unique identifier for each health region. A lookup table is available through the API with regions:\n\nresp <- httr::GET(paste0(base_url, \"regions\"))\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 1\n $ data:List of 92\n\n\nThere are 92 elements, matching the 92 health regions in the summary data, with the following structure:\n\nregions <- bind_rows(content_parsed$data)\nglimpse(regions)\n\nRows: 92\nColumns: 4\n$ hr_uid   <int> 471, 472, 473, 474, 475, 476, 591, 592, 593, 594, 595, 1011, …\n$ province <chr> \"SK\", \"SK\", \"SK\", \"SK\", \"SK\", \"SK\", \"BC\", \"BC\", \"BC\", \"BC\", \"…\n$ engname  <chr> \"Far North\", \"North\", \"Central\", \"Saskatoon\", \"Regina\", \"Sout…\n$ frename  <chr> \"Far North\", \"North\", \"Central\", \"Saskatoon\", \"Regina\", \"Sout…\n\n\nAdd the health region to the summary_region data:\n\nsummary_region <- regions %>%\n  left_join(summary_region, by = \"hr_uid\")\nglimpse(summary_region)\n\nRows: 92\nColumns: 24\n$ hr_uid                  <int> 471, 472, 473, 474, 475, 476, 591, 592, 593, 5…\n$ province                <chr> \"SK\", \"SK\", \"SK\", \"SK\", \"SK\", \"SK\", \"BC\", \"BC\"…\n$ engname                 <chr> \"Far North\", \"North\", \"Central\", \"Saskatoon\", …\n$ frename                 <chr> \"Far North\", \"North\", \"Central\", \"Saskatoon\", …\n$ date                    <date> 2022-08-10, 2022-08-10, 2022-08-10, 2022-08-1…\n$ total_cases             <int> 0, 0, 0, 0, 0, 0, 67763, 167426, 77015, 37656,…\n$ total_fatalities        <int> 8, 16, 5, 6, 3, 7, 581, 1701, 944, 439, 372, 3…\n$ total_tests             <int> 66969, 206300, 115789, 380252, 290508, 176859,…\n$ total_hospitalizations  <int> 2, 61, 22, 171, 54, 22, 72, 138, 90, 66, 21, 1…\n$ total_criticals         <int> 0, 7, 1, 17, 5, 1, 6, 7, 5, 2, 2, 1, 1, 3, 0, …\n$ total_recoveries        <int> 10248, 24271, 9069, 29440, 23925, 14978, 39899…\n$ total_vaccinations      <int> 70534, 319807, 201758, 513380, 430413, 273939,…\n$ total_vaccinated        <int> 33236, 152611, 98324, 249513, 208970, 133557, …\n$ total_boosters_1        <int> NA, NA, NA, NA, NA, NA, 404650, 938141, 806464…\n$ total_boosters_2        <int> NA, NA, NA, NA, NA, NA, 97819, 162539, 124012,…\n$ change_recoveries       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ change_vaccinated       <int> NA, NA, NA, NA, NA, NA, 176, 583, 387, 171, 52…\n$ change_boosters_1       <int> NA, NA, NA, NA, NA, NA, 431, 1501, 1298, 564, …\n$ change_boosters_2       <int> NA, NA, NA, NA, NA, NA, 1740, 3710, 3636, 2733…\n$ change_vaccinations     <int> NA, NA, NA, NA, NA, NA, 2914, 7977, 7352, 4842…\n$ change_cases            <int> NA, NA, NA, NA, NA, NA, 183, 283, 197, 167, 45…\n$ change_fatalities       <int> NA, NA, NA, NA, NA, NA, 10, 11, 8, 12, 1, NA, …\n$ change_hospitalizations <int> NA, NA, NA, NA, NA, NA, 9, 21, -21, -6, 2, NA,…\n$ change_criticals        <int> NA, NA, NA, NA, NA, NA, 1, 0, 3, -6, 1, NA, NA…"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#reports",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#reports",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Reports",
    "text": "Reports\n\nOverall\nReports are much like summaries, but for every day instead of just the most recent.\n\nresp <- httr::GET(paste0(base_url, \"reports\"))\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 3\n $ province    : chr \"All\"\n $ last_updated: chr \"2022-08-21 20:27:03\"\n $ data        :List of 934\n\n\nAn additional top-level variable province defines the scope of the report. The data list here consists of 705 elements with the following structure:\n\nstr(content_parsed$data[[1]])\n\nList of 23\n $ date                       : chr \"2020-01-25\"\n $ change_cases               : int 1\n $ change_fatalities          : int 0\n $ change_tests               : int 2\n $ change_hospitalizations    : int 0\n $ change_criticals           : int 0\n $ change_recoveries          : int 0\n $ change_vaccinations        : int 0\n $ change_vaccinated          : int 0\n $ change_boosters_1          : int 0\n $ change_boosters_2          : int 0\n $ change_vaccines_distributed: int 0\n $ total_cases                : int 1\n $ total_fatalities           : int 0\n $ total_tests                : int 2\n $ total_hospitalizations     : int 0\n $ total_criticals            : int 0\n $ total_recoveries           : int 0\n $ total_vaccinations         : int 0\n $ total_vaccinated           : int 0\n $ total_boosters_1           : int 0\n $ total_boosters_2           : int 0\n $ total_vaccines_distributed : int 0\n\n\nThis first element has many zeroes, which makes sense as the date (January 25th, 2020) corresponds to the first confirmed case of COVID in Canada. The last element of this list should have today’s data:\n\nstr(content_parsed$data[[length(content_parsed$data)]])\n\nList of 23\n $ date                       : chr \"2022-08-19\"\n $ change_cases               : int 0\n $ change_fatalities          : int 0\n $ change_tests               : int 0\n $ change_hospitalizations    : int 0\n $ change_criticals           : int 0\n $ change_recoveries          : int 0\n $ change_vaccinations        : int 0\n $ change_vaccinated          : int 0\n $ change_boosters_1          : int 0\n $ change_boosters_2          : int 0\n $ change_vaccines_distributed: int 0\n $ total_cases                : int 1167556\n $ total_fatalities           : int 16134\n $ total_tests                : int 16733572\n $ total_hospitalizations     : int 2011\n $ total_criticals            : int 45\n $ total_recoveries           : int 1099129\n $ total_vaccinations         : int 20863558\n $ total_vaccinated           : int 7195120\n $ total_boosters_1           : int 4568607\n $ total_boosters_2           : int 1565745\n $ total_vaccines_distributed : int 23092789\n\n\nThe data may be bound together in the same way:\n\nreport_overall <- bind_rows(content_parsed$data) %>%\n  mutate(date = as.Date(date))\n\n\n\nProvince\nTo split data by province, the two-letter code is provided as reports/province/{code}:\n\nresp <- httr::GET(paste0(base_url, \"reports/province/ns\"))\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nreport_ns <- bind_rows(content_parsed$data) %>%\n  mutate(date = as.Date(date))\nglimpse(report_ns)\n\nRows: 930\nColumns: 23\n$ date                        <date> 2020-01-25, 2020-01-26, 2020-01-27, 2020-…\n$ change_cases                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ change_fatalities           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ change_tests                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ change_hospitalizations     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ change_criticals            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ change_recoveries           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ change_vaccinations         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ change_vaccinated           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ change_boosters_1           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ change_boosters_2           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ change_vaccines_distributed <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_cases                 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_fatalities            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_tests                 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_hospitalizations      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_criticals             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_recoveries            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_vaccinations          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_vaccinated            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_boosters_1            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_boosters_2            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_vaccines_distributed  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\n\nHealth region\nSimilarly, provide the hr_uid in reports/regions/{hr_uid} to get health region reports:\n\nresp <- httr::GET(paste0(base_url, \"reports/regions/1204\"))\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nreport_ns_central <- bind_rows(content_parsed$data) %>%\n  mutate(date = as.Date(date))\nglimpse(report_ns_central)\n\nRows: 856\nColumns: 7\n$ date              <date> 2020-01-15, 2020-01-16, 2020-01-17, 2020-01-18, 202…\n$ change_cases      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ change_fatalities <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ total_cases       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ total_fatalities  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ change_recoveries <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ total_recoveries  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\nI chose Nova Scotia central because it is where I live and, looking at this data, it clearly isn’t being updated day-to-day:\n\nreport_ns_central %>%\n  filter(date >= \"2021-12-20\", date < \"2021-12-28\") %>%\n  glimpse()\n\nRows: 8\nColumns: 7\n$ date              <date> 2021-12-20, 2021-12-21, 2021-12-22, 2021-12-23, 202…\n$ change_cases      <int> 136, 63, 0, NA, NA, NA, NA, NA\n$ change_fatalities <int> 0, 0, 0, NA, NA, NA, NA, NA\n$ total_cases       <int> 6718, 6781, 6781, 6781, 6781, 6781, 6781, 6781\n$ total_fatalities  <int> 87, 87, 87, 87, 87, 87, 87, 87\n$ change_recoveries <int> 100, 113, 0, NA, NA, NA, NA, NA\n$ total_recoveries  <int> 6231, 6344, 6344, 6344, 6344, 6344, 6344, 6344\n\n\nThere has, unfortunately, been hundreds of cases per day here recently. These numbers are reflected in the province report however:\n\nreport_ns %>%\n  filter(date >= \"2021-12-20\", date < \"2021-12-28\") %>%\n  glimpse()\n\nRows: 8\nColumns: 23\n$ date                        <date> 2021-12-20, 2021-12-21, 2021-12-22, 2021-…\n$ change_cases                <int> 485, 522, 537, 689, 611, 569, 578, 581\n$ change_fatalities           <int> 0, 0, 1, 0, 0, 0, 0, 0\n$ change_tests                <int> 34815, 0, 10254, 0, 0, 0, 0, 0\n$ change_hospitalizations     <int> 2, 1, 0, 4, 0, 0, 0, 0\n$ change_criticals            <int> 0, 1, 0, 1, 0, 0, 0, 0\n$ change_recoveries           <int> 263, 0, 0, 0, 0, 0, 0, 0\n$ change_vaccinations         <int> 13018, 8953, 10225, 7903, 0, 0, 0, 0\n$ change_vaccinated           <int> 423, 296, 366, 275, 0, 0, 0, 0\n$ change_boosters_1           <int> 9403, 7298, 8283, 6367, 0, 0, 0, 0\n$ change_boosters_2           <int> 0, 0, 0, 0, 0, 0, 0, 0\n$ change_vaccines_distributed <int> 0, 0, 0, 0, 0, 0, 0, 0\n$ total_cases                 <int> 11318, 11840, 12377, 13066, 13677, 14246, …\n$ total_fatalities            <int> 110, 110, 111, 111, 111, 111, 111, 111\n$ total_tests                 <int> 1506218, 1506218, 1516472, 1516472, 151647…\n$ total_hospitalizations      <int> 9, 10, 10, 14, 14, 14, 14, 14\n$ total_criticals             <int> 2, 3, 3, 4, 4, 4, 4, 4\n$ total_recoveries            <int> 8643, 8643, 8643, 8643, 8643, 8643, 8643, …\n$ total_vaccinations          <int> 1731205, 1740158, 1750383, 1758286, 175828…\n$ total_vaccinated            <int> 792552, 792848, 793214, 793489, 793489, 79…\n$ total_boosters_1            <int> 83071, 90369, 98652, 105019, 105019, 10501…\n$ total_boosters_2            <int> 0, 0, 0, 0, 0, 0, 0, 0\n$ total_vaccines_distributed  <int> 1950040, 1950040, 1950040, 1950040, 195004…\n\n\n\n\nParameters\nThe reports have a number of optional parameters available to alter the API request.\nThe fill_dates option fills dates with missing entries:\n\ncontent_parsed <- paste0(base_url, \"reports/regions/1204?fill_dates=false\") %>%\n  httr::GET() %>%\n  content(as = \"parsed\")\n\n\nbind_rows(content_parsed$data) %>% glimpse()\n\nRows: 939\nColumns: 8\n$ date              <chr> \"2020-01-15\", \"2020-01-16\", \"2020-01-17\", \"2020-01-1…\n$ change_cases      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ change_fatalities <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ total_cases       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ total_fatalities  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ change_recoveries <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ total_recoveries  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ fill              <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\nThe stat argument allows one to specify a single statistic to return:\n\ncontent_parsed <- paste0(base_url, \"reports/province/ns?stat=cases\") %>%\n  httr::GET() %>%\n  content(as = \"parsed\")\n\n\nbind_rows(content_parsed$data) %>% glimpse()\n\nRows: 930\nColumns: 3\n$ date         <chr> \"2020-01-25\", \"2020-01-26\", \"2020-01-27\", \"2020-01-28\", \"…\n$ change_cases <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_cases  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nThe date parameter returns a report from a single date:\n\ncontent_parsed <- paste0(base_url, \"reports/province/ab?date=2021-12-25\") %>%\n  httr::GET() %>%\n  content(as = \"parsed\")\n\n\nbind_rows(content_parsed$data) %>% glimpse()\n\nRows: 1\nColumns: 23\n$ date                        <chr> \"2021-12-25\"\n$ change_cases                <int> 2484\n$ change_fatalities           <int> 0\n$ change_tests                <int> 11479\n$ change_hospitalizations     <int> 0\n$ change_criticals            <int> 0\n$ change_recoveries           <int> 0\n$ change_vaccinations         <int> 0\n$ change_vaccinated           <int> 0\n$ change_boosters_1           <int> 0\n$ change_boosters_2           <int> 0\n$ change_vaccines_distributed <int> 0\n$ total_cases                 <int> 351199\n$ total_fatalities            <int> 3299\n$ total_tests                 <int> 6347374\n$ total_hospitalizations      <int> 318\n$ total_criticals             <int> 64\n$ total_recoveries            <int> 335047\n$ total_vaccinations          <int> 7452649\n$ total_vaccinated            <int> 3211241\n$ total_boosters_1            <int> 761153\n$ total_boosters_2            <int> 0\n$ total_vaccines_distributed  <int> 8799859\n\n\nLastly, the after and before parameters return on/after and on/before specific dates:\n\ncontent_parsed <-\n  paste0(base_url, \"reports/province/qc?after=2021-12-24&before=2021-12-26\") %>%\n  httr::GET() %>%\n  content(as = \"parsed\")\n\n\nbind_rows(content_parsed$data) %>% glimpse()\n\nRows: 3\nColumns: 23\n$ date                        <chr> \"2021-12-24\", \"2021-12-25\", \"2021-12-26\"\n$ change_cases                <int> 10031, 9206, 8231\n$ change_fatalities           <int> 2, 4, 10\n$ change_tests                <int> 55863, 53334, 44022\n$ change_hospitalizations     <int> 0, 0, 0\n$ change_criticals            <int> 0, 0, 0\n$ change_recoveries           <int> 3017, 3559, 0\n$ change_vaccinations         <int> 85039, 24435, 117\n$ change_vaccinated           <int> 2120, 638, 9\n$ change_boosters_1           <int> 78745, 22612, 98\n$ change_boosters_2           <int> 186, 124, 2\n$ change_vaccines_distributed <int> 0, 0, 0\n$ total_cases                 <int> 521126, 530332, 538563\n$ total_fatalities            <int> 11660, 11664, 11674\n$ total_tests                 <int> 14573238, 14626572, 14670594\n$ total_hospitalizations      <int> 473, 473, 473\n$ total_criticals             <int> 91, 91, 91\n$ total_recoveries            <int> 460647, 464206, 464206\n$ total_vaccinations          <int> 15034784, 15059219, 15059336\n$ total_vaccinated            <int> 6701798, 6702436, 6702445\n$ total_boosters_1            <int> 1021886, 1044498, 1044596\n$ total_boosters_2            <int> 3469, 3593, 3595\n$ total_vaccines_distributed  <int> 16179459, 16179459, 16179459\n\n\nNote how parameters can be combined as above, by separating the arguments with &."
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#vaccination-data",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#vaccination-data",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Vaccination data",
    "text": "Vaccination data\nWe have already seen the vaccination data returned by summary and report requests. The variables include:\n\nvaccinations: total doses administered\nvaccinated: total number of people with two doses\nboosters_1: total number of boosters (3rd dose) administered\nvaccines_administered: total number of doses delivered to province\n\nAt the summary level:\n\nsummary_province %>%\n  filter(province == \"NS\") %>%\n  select(date, matches(\"vacc|boost\")) %>%\n  glimpse()\n\nRows: 1\nColumns: 11\n$ date                        <date> 2022-08-11\n$ change_vaccinations         <int> 0\n$ change_vaccinated           <int> 0\n$ change_boosters_1           <int> 0\n$ change_boosters_2           <int> 0\n$ change_vaccines_distributed <int> 0\n$ total_vaccinations          <int> 2235685\n$ total_vaccinated            <int> 842600\n$ total_boosters_1            <int> 503257\n$ total_boosters_2            <int> 0\n$ total_vaccines_distributed  <int> 2518832\n\n\nAt the report level:\n\nreport_ns %>%\n  select(date, matches(\"vacc|boost\")) %>%\n  tail() %>%\n  glimpse()\n\nRows: 6\nColumns: 11\n$ date                        <date> 2022-08-06, 2022-08-07, 2022-08-08, 2022-…\n$ change_vaccinations         <int> 0, 0, 0, 0, 0, 0\n$ change_vaccinated           <int> 0, 0, 0, 0, 0, 0\n$ change_boosters_1           <int> 0, 0, 0, 0, 0, 0\n$ change_boosters_2           <int> 0, 0, 0, 0, 0, 0\n$ change_vaccines_distributed <int> 0, 0, 0, 0, 60680, 0\n$ total_vaccinations          <int> 2235685, 2235685, 2235685, 2235685, 223568…\n$ total_vaccinated            <int> 842600, 842600, 842600, 842600, 842600, 84…\n$ total_boosters_1            <int> 503257, 503257, 503257, 503257, 503257, 50…\n$ total_boosters_2            <int> 0, 0, 0, 0, 0, 0\n$ total_vaccines_distributed  <int> 2458152, 2458152, 2458152, 2458152, 251883…\n\n\n\nSubregions\nVaccination date is also available at the subregion level for certain provinces and territories. The API documentation doesn’t actually specify which provinces and territories, but I can find out by requesting the data as follows:\n\nresp <- httr::GET(paste0(base_url, \"reports/sub-regions/summary\"))\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nsubregion_vacc_summary <- bind_rows(content_parsed$data) %>%\n  mutate(date = as.Date(date))\n\nglimpse(subregion_vacc_summary)\n\nRows: 806\nColumns: 11\n$ code           <chr> \"SK001\", \"SK002\", \"SK003\", \"SK004\", \"SK005\", \"SK006\", \"…\n$ date           <date> 2022-02-06, 2022-02-06, 2022-02-06, 2022-02-06, 2022-0…\n$ total_dose_1   <int> 18733, 1959, 16606, 66614, 67900, 32682, 263867, 27576,…\n$ percent_dose_1 <chr> \"0.70048\", \"0.68282\", \"0.75147\", \"0.86337\", \"0.79184\", …\n$ source_dose_1  <chr> \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"…\n$ total_dose_2   <int> 16875, 1706, 14655, 58628, 63117, 30866, 249513, 26220,…\n$ percent_dose_2 <chr> \"0.63101\", \"0.59463\", \"0.66318\", \"0.75986\", \"0.73606\", …\n$ source_dose_2  <chr> \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"…\n$ total_dose_3   <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 367…\n$ percent_dose_3 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"0.…\n$ source_dose_3  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"pe…\n\n\nThe code labels can be retrieved via sub-regions:\n\nresp <- httr::GET(paste0(base_url, \"sub-regions\"))\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nsubregions <- bind_rows(content_parsed$data)\n\nglimpse(subregions)\n\nRows: 806\nColumns: 5\n$ code       <chr> \"AB001\", \"AB002\", \"AB003\", \"AB004\", \"AB005\", \"AB006\", \"AB00…\n$ province   <chr> \"AB\", \"AB\", \"AB\", \"AB\", \"AB\", \"AB\", \"AB\", \"AB\", \"AB\", \"AB\",…\n$ zone       <chr> \"SOUTH\", \"SOUTH\", \"SOUTH\", \"SOUTH\", \"SOUTH\", \"SOUTH\", \"SOUT…\n$ region     <chr> \"CROWSNEST PASS\", \"PINCHER CREEK\", \"FORT MACLEOD\", \"CARDSTO…\n$ population <int> 6280, 8344, 6753, 16595, 25820, 19028, 11104, 6409, 27753, …\n\n\n806 subregions, which matches the count from the summary, with the following distribution by province:\n\nsubregions %>% count(province) %>% gt()\n\n\n\n\n\n  \n  \n    \n      province\n      n\n    \n  \n  \n    AB\n132\n    MB\n79\n    NL\n38\n    NT\n30\n    ON\n514\n    SK\n13"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#vaccine-age-groups",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#vaccine-age-groups",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Vaccine age groups",
    "text": "Vaccine age groups\n\nOverall\nVaccine data by age groups is reported week-by-week, and accessed with vaccines/age-groups:\n\nresp <- httr::GET(paste0(base_url, \"vaccines/age-groups\"))\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nvaccine_age_groups <- bind_rows(content_parsed$data) %>%\n  mutate(date = as.Date(date)) %>%\n  filter(date <= \"2021-12-28\")\n\nglimpse(vaccine_age_groups)\n\nRows: 54\nColumns: 2\n$ date <date> 2020-12-19, 2020-12-26, 2021-01-02, 2021-01-09, 2021-01-16, 2021…\n$ data <chr> \"{\\\"80+\\\": {\\\"full\\\": 0, \\\"group\\\": \\\"80+\\\", \\\"partial\\\": 335, \\\"…\n\n\nThe data here is returned as an un-parsed JSON string. Per the API documentation, it has to do with shifting reporting standards across weeks:\n\ndue to reporting standard shifts overtime, the JSON string data may not be consistent across weeks. Minimal effort is taken to normalize some of this data.\n\nLook at the first element of data:\n\nvaccine_age_groups$data[[1]] %>% str_trunc(80)\n\n[1] \"{\\\"80+\\\": {\\\"full\\\": 0, \\\"group\\\": \\\"80+\\\", \\\"partial\\\": 335, \\\"atleast1\\\": 335}, \\\"0-15\\\":...\"\n\n\nParse the JSON:\n\njsonlite::fromJSON(vaccine_age_groups$data[[1]]) %>%\n  str()\n\nList of 8\n $ 80+         :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"80+\"\n  ..$ partial : int 335\n  ..$ atleast1: int 335\n $ 0-15        :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"0-15\"\n  ..$ partial : int 0\n  ..$ atleast1: int 0\n $ 16-69       :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"16-69\"\n  ..$ partial : int 11768\n  ..$ atleast1: int 11768\n $ 70-74       :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"70-74\"\n  ..$ partial : int 174\n  ..$ atleast1: int 174\n $ 75-79       :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"75-79\"\n  ..$ partial : int 85\n  ..$ atleast1: int 85\n $ unknown     :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"Unknown\"\n  ..$ partial : int 0\n  ..$ atleast1: int 0\n $ all_ages    :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"All ages\"\n  ..$ partial : int 12362\n  ..$ atleast1: int 12362\n $ not_reported:List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"Not reported\"\n  ..$ partial : int 0\n  ..$ atleast1: int 0\n\n\nTo see how the reporting has changed over time, here is the most recent age group vaccination data:\n\njsonlite::fromJSON(\n  vaccine_age_groups$data[[length(vaccine_age_groups$data)]]\n) %>%\n  str()\n\nList of 13\n $ 0-4         :List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"0-4\"\n  ..$ partial : int 276\n  ..$ atleast1: int 276\n $ 80+         :List of 4\n  ..$ full    : int 1640558\n  ..$ group   : chr \"80+\"\n  ..$ partial : int 30717\n  ..$ atleast1: int 1671275\n $ 05-11       :List of 4\n  ..$ full    : int 44794\n  ..$ group   : chr \"05-11\"\n  ..$ partial : int 1206532\n  ..$ atleast1: int 1251326\n $ 12-17       :List of 4\n  ..$ full    : int 2039683\n  ..$ group   : chr \"12-17\"\n  ..$ partial : int 111096\n  ..$ atleast1: int 2150779\n $ 18-29       :List of 4\n  ..$ full    : int 4916940\n  ..$ group   : chr \"18-29\"\n  ..$ partial : int 259662\n  ..$ atleast1: int 5176602\n $ 30-39       :List of 4\n  ..$ full    : int 4475399\n  ..$ group   : chr \"30-39\"\n  ..$ partial : int 185294\n  ..$ atleast1: int 4660693\n $ 40-49       :List of 4\n  ..$ full    : int 4250041\n  ..$ group   : chr \"40-49\"\n  ..$ partial : int 125031\n  ..$ atleast1: int 4375072\n $ 50-59       :List of 4\n  ..$ full    : int 4526830\n  ..$ group   : chr \"50-59\"\n  ..$ partial : int 104559\n  ..$ atleast1: int 4631389\n $ 60-69       :List of 4\n  ..$ full    : int 4459359\n  ..$ group   : chr \"60-69\"\n  ..$ partial : int 78044\n  ..$ atleast1: int 4537403\n $ 70-79       :List of 4\n  ..$ full    : int 2970726\n  ..$ group   : chr \"70-79\"\n  ..$ partial : int 41346\n  ..$ atleast1: int 3012072\n $ unknown     :List of 4\n  ..$ full    : int 2021\n  ..$ group   : chr \"Unknown\"\n  ..$ partial : int 804\n  ..$ atleast1: int 2825\n $ all_ages    :List of 4\n  ..$ full    : int 29326351\n  ..$ group   : chr \"All ages\"\n  ..$ partial : int 2143361\n  ..$ atleast1: int 31469712\n $ not_reported:List of 4\n  ..$ full    : int 0\n  ..$ group   : chr \"Not reported\"\n  ..$ partial : int 0\n  ..$ atleast1: int 0\n\n\nEach JSON data point can be converted to a data frame as follows:\n\njsonlite::fromJSON(vaccine_age_groups$data[[1]]) %>%\n  bind_rows(.id = \"group_code\") %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      group_code\n      full\n      group\n      partial\n      atleast1\n    \n  \n  \n    80+\n0\n80+\n335\n335\n    0-15\n0\n0-15\n0\n0\n    16-69\n0\n16-69\n11768\n11768\n    70-74\n0\n70-74\n174\n174\n    75-79\n0\n75-79\n85\n85\n    unknown\n0\nUnknown\n0\n0\n    all_ages\n0\nAll ages\n12362\n12362\n    not_reported\n0\nNot reported\n0\n0\n  \n  \n  \n\n\n\n\nUse map and unnest to apply this to each row of the data:\n\nvaccine_age_groups <- vaccine_age_groups %>%\n  mutate(\n    data = map(\n      data,\n      ~jsonlite::fromJSON(.x) %>% bind_rows(.id = \"group_code\")\n    )\n  ) %>%\n  unnest(data)\nglimpse(vaccine_age_groups)\n\nRows: 603\nColumns: 6\n$ date       <date> 2020-12-19, 2020-12-19, 2020-12-19, 2020-12-19, 2020-12-19…\n$ group_code <chr> \"80+\", \"0-15\", \"16-69\", \"70-74\", \"75-79\", \"unknown\", \"all_a…\n$ full       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ group      <chr> \"80+\", \"0-15\", \"16-69\", \"70-74\", \"75-79\", \"Unknown\", \"All a…\n$ partial    <int> 335, 0, 11768, 174, 85, 0, 12362, 0, 2229, 5, 40170, 649, 4…\n$ atleast1   <int> 335, 0, 11768, 174, 85, 0, 12362, 0, 2229, 5, 40170, 649, 4…\n\n\nThe unique groups:\n\nvaccine_age_groups %>% count(group_code, group) %>% rmarkdown::paged_table()\n\n\n\n  \n\n\n\nVisualize how the age ranges evolve over time:\n\n# Make it a function that will allow splits later\nplot_age_ranges <- function(vaccine_age_groups, split = \"overall\", ncol = 3) {\n  p <- vaccine_age_groups %>%\n    filter(str_detect(group, \"\\\\d\")) %>%\n    separate(group, into = c(\"age_min\", \"age_max\"),\n             sep = \"-\", fill = \"right\", remove = FALSE) %>%\n    mutate(\n      age_min = readr::parse_number(age_min),\n      # Set the upper range of the age to 100 (arbitrarility)\n      age_max = replace_na(age_max, replace = \"100\") %>% as.numeric(),\n      age_mid = (age_max + age_min) / 2,\n      group = fct_reorder(group, age_mid)\n    ) %>%\n    ggplot(aes(x = date, color = group)) +\n    geom_errorbar(aes(ymin = age_min, ymax = age_max)) +\n    geom_text(\n      data = . %>%\n        slice_min(date) %>%\n        mutate(age_mid = (age_max + age_min) / 2),\n      aes(label = group, y = age_mid),\n      hjust = 1, nudge_x = -3, show.legend = FALSE\n    ) +\n    geom_text(\n      data = . %>%\n        slice_max(date) %>%\n        mutate(age_mid = (age_max + age_min) / 2),\n      aes(label = group, y = age_mid),\n      hjust = 0, nudge_x = 3, show.legend = FALSE\n    ) +\n    expand_limits(x = c(min(vaccine_age_groups$date) - 10,\n                        max(vaccine_age_groups$date) + 10)) +\n    scale_color_viridis_d(end = 0.8) +\n    theme(legend.position = \"none\") +\n    labs(x = \"Date\", y = \"Age\",\n         title = \"Age ranges for weekly vaccination reports, by date\")\n  \n  if (split == \"province\") p + facet_wrap(~province, ncol = ncol)\n  else if (split == \"region\") p + facet_wrap(~hr_uid, ncol = ncol)\n  else {p}\n}\n\nplot_age_ranges(vaccine_age_groups)\n\n\n\n\nUnsurprisingly, the age ranges become more granular overtime, with the exception of 70-79 which was originally split into 70-74 and 75-79.\n\n\nProvince\nAs with the other data, adding /split to the query returns vaccination data by province:\n\ncontent_parsed <- paste0(base_url, \"vaccines/age-groups/split\") %>%\n  httr::GET() %>%\n  content(as = \"parsed\")\n\n\nvaccine_age_groups_province <- bind_rows(content_parsed$data) %>%\n  mutate(date = as.Date(date)) %>%\n  filter(date <= \"2021-12-28\")\nglimpse(vaccine_age_groups_province)\n\nRows: 1,564\nColumns: 3\n$ date     <date> 2020-12-14, 2020-12-15, 2020-12-16, 2020-12-16, 2020-12-17, …\n$ data     <chr> \"{\\\"0-4\\\": {\\\"full\\\": 0, \\\"group\\\": \\\"0-4\\\", \\\"partial\\\": 1, …\n$ province <chr> \"QC\", \"QC\", \"QC\", \"ON\", \"QC\", \"ON\", \"QC\", \"ON\", \"BC\", \"NL\", \"…\n\n\n\nvaccine_age_groups_province <- vaccine_age_groups_province %>%\n  mutate(\n    data = map(\n      data,\n      ~jsonlite::fromJSON(.x) %>% bind_rows(.id = \"group_code\")\n    )\n  ) %>%\n  unnest(data)\nglimpse(vaccine_age_groups_province)\n\nRows: 15,715\nColumns: 7\n$ date       <date> 2020-12-14, 2020-12-14, 2020-12-14, 2020-12-14, 2020-12-14…\n$ group_code <chr> \"0-4\", \"80+\", \"05-11\", \"12-17\", \"18-29\", \"30-39\", \"40-49\", …\n$ full       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ group      <chr> \"0-4\", \"80+\", \"05-11\", \"12-17\", \"18-29\", \"30-39\", \"40-49\", …\n$ partial    <int> 1, 169, 0, 0, 9, 11, 23, 22, 26, 35, 1, 328, 0, 2, 127, 216…\n$ atleast1   <int> 1, 169, 0, 0, 9, 11, 23, 22, 26, 35, 1, 328, 0, 2, 127, 216…\n$ province   <chr> \"QC\", \"QC\", \"QC\", \"QC\", \"QC\", \"QC\", \"QC\", \"QC\", \"QC\", \"QC\",…\n\n\n\nvaccine_age_groups_province %>%\n  filter(province == \"QC\") %>%\n  plot_age_ranges(split = \"province\", ncol = 1)\n\n\n\n\nA single province can also be obtained by altering the query with vaccines/age-groups/province/{code}:\n\ncontent_parsed <- paste0(base_url, \"vaccines/age-groups/province/ns\") %>%\n  httr::GET() %>%\n  content(as = \"parsed\")\n\n\nvaccine_age_groups_ns <- bind_rows(content_parsed$data) %>%\n  mutate(\n    date = as.Date(date),\n    data = map(data, ~jsonlite::fromJSON(.x) %>% bind_rows(.id = \"group_code\"))\n  ) %>%\n  unnest(data) %>%\n  filter(date <= \"2021-12-28\")\nplot_age_ranges(vaccine_age_groups_ns)\n\n\n\n\n\n\nParameters\nThis query also has the after and before parameters available:\n\ncontent_parsed <- paste0(base_url,\n                         \"vaccines/age-groups/province/ns?after=2021-11-01\") %>%\n  httr::GET() %>%\n  content(as = \"parsed\")\n\n\nglimpse(bind_rows(content_parsed$data))\n\nRows: 13\nColumns: 2\n$ date <chr> \"2021-11-06\", \"2021-11-13\", \"2021-11-20\", \"2021-11-27\", \"2021-12-…\n$ data <chr> \"{\\\"0-4\\\": {\\\"full\\\": 0, \\\"group\\\": \\\"0-4\\\", \\\"partial\\\": 0, \\\"at…\n\n\nA specific age group can also be queried with the group parameter. The value must be passed in URL encoding. For example, the 80+ range:\n\ncontent_parsed <- paste0(base_url,\n                         \"vaccines/age-groups?after=2021-11-01&group=80%2B\") %>%\n  httr::GET() %>%\n  content(as = \"parsed\")\n\n\nbind_rows(content_parsed$data) %>%\n  mutate(\n    date = as.Date(date),\n    data = map(data, ~jsonlite::fromJSON(.x) %>% bind_rows(.id = \"group_code\"))\n  ) %>%\n  unnest(data) %>%\n  filter(date <= \"2021-12-28\") %>%\n  glimpse()\n\nRows: 8\nColumns: 5\n$ date     <date> 2021-11-06, 2021-11-13, 2021-11-20, 2021-11-27, 2021-12-04, …\n$ full     <int> 1581895, 1585409, 1588815, 1592112, 1630884, 1633436, 1637238…\n$ group    <chr> \"80+\", \"80+\", \"80+\", \"80+\", \"80+\", \"80+\", \"80+\", \"80+\"\n$ partial  <int> 39515, 38628, 37810, 37087, 28833, 28621, 28542, 30717\n$ atleast1 <int> 1621410, 1624037, 1626625, 1629199, 1659717, 1662057, 1665780…\n\n\nThe utils package has a URLencode function for translating the age groups:\n\nvaccine_age_groups %>%\n  distinct(group_code) %>%\n  mutate(group_encoded = utils::URLencode(group_code, reserved = TRUE)) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      group_code\n      group_encoded\n    \n  \n  \n    80+\n80%2B\n    0-15\n0-15\n    16-69\n16-69\n    70-74\n70-74\n    75-79\n75-79\n    unknown\nunknown\n    all_ages\nall_ages\n    not_reported\nnot_reported\n    0-17\n0-17\n    18-69\n18-69\n    18-29\n18-29\n    30-39\n30-39\n    40-49\n40-49\n    50-59\n50-59\n    60-69\n60-69\n    70-79\n70-79\n    0-4\n0-4\n    05-11\n05-11\n    12-17\n12-17"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#provinces",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#provinces",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Provinces",
    "text": "Provinces\nThe API also provides a list of provinces and some population/geographical data:\n\ncontent_parsed <- paste0(base_url, \"provinces\") %>%\n  httr::GET() %>%\n  content(as = \"parsed\")\n\n\nprovinces <- bind_rows(content_parsed)\nglimpse(provinces)\n\nRows: 16\nColumns: 10\n$ id          <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\n$ code        <chr> \"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\", \"NL\"…\n$ name        <chr> \"Ontario\", \"Quebec\", \"Nova Scotia\", \"New Brunswick\", \"Mani…\n$ population  <int> 14826276, 8604495, 992055, 789225, 1383765, 5214805, 16431…\n$ area        <int> 917741, 1356128, 53338, 71450, 553556, 925186, 5660, 59167…\n$ gdp         <int> 857384, 439375, 44354, 36966, 72688, 295401, 6994, 80679, …\n$ geographic  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0\n$ data_status <chr> \"Reported\", \"Reported\", \"No report expected today\", \"Repor…\n$ updated_at  <chr> \"2022-08-22T02:10:20.000000Z\", \"2022-08-22T02:08:03.000000…\n$ density     <dbl> 16.15518540, 6.34489886, 18.59940380, 11.04583625, 2.49977…\n\n\nThe extra elements reported here are not related to any particular province/territory:\n\nprovinces %>% filter(is.na(population)) %>% glimpse()\n\nRows: 3\nColumns: 10\n$ id          <int> 14, 15, 16\n$ code        <chr> \"_RC\", \"FA\", \"NFR\"\n$ name        <chr> \"Repatriated Canadians\", \"Federal Allocation\", \"National F…\n$ population  <int> NA, NA, NA\n$ area        <int> NA, NA, NA\n$ gdp         <int> NA, NA, NA\n$ geographic  <int> 0, 0, 0\n$ data_status <chr> \"\", \"\", \"\"\n$ updated_at  <chr> NA, \"2022-03-10T20:41:40.000000Z\", \"2022-08-15T22:51:36.00…\n$ density     <dbl> NA, NA, NA\n\n\nThe geo_only parameter can be set to true to exclude these:\n\npaste0(base_url, \"provinces?geo_only=true\") %>%\n  httr::GET() %>%\n  content(as = \"parsed\") %>%\n  bind_rows() %>%\n  glimpse()\n\nRows: 13\nColumns: 10\n$ id          <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n$ code        <chr> \"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\", \"NL\"…\n$ name        <chr> \"Ontario\", \"Quebec\", \"Nova Scotia\", \"New Brunswick\", \"Mani…\n$ population  <int> 14826276, 8604495, 992055, 789225, 1383765, 5214805, 16431…\n$ area        <int> 917741, 1356128, 53338, 71450, 553556, 925186, 5660, 59167…\n$ gdp         <int> 857384, 439375, 44354, 36966, 72688, 295401, 6994, 80679, …\n$ geographic  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ data_status <chr> \"Reported\", \"Reported\", \"No report expected today\", \"Repor…\n$ updated_at  <chr> \"2022-08-22T02:10:20.000000Z\", \"2022-08-22T02:08:03.000000…\n$ density     <dbl> 16.15518540, 6.34489886, 18.59940380, 11.04583625, 2.49977…\n\n\nA helpful variable is data_status, which indicates if the daily numbers have been reported:\n\nprovinces %>%\n  select(name, data_status, updated_at) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      name\n      data_status\n      updated_at\n    \n  \n  \n    Ontario\nReported\n2022-08-22T02:10:20.000000Z\n    Quebec\nReported\n2022-08-22T02:08:03.000000Z\n    Nova Scotia\nNo report expected today\n2022-08-15T22:52:42.000000Z\n    New Brunswick\nReported\n2022-08-15T22:50:10.000000Z\n    Manitoba\nNo report expected today\n2022-08-15T23:01:29.000000Z\n    British Columbia\nReported\n2022-08-22T02:26:05.000000Z\n    Prince Edward Island\nReported\n2022-08-15T22:50:27.000000Z\n    Saskatchewan\nNo report expected today\n2022-08-15T22:49:47.000000Z\n    Alberta\nReported\n2022-08-22T02:15:30.000000Z\n    Newfoundland and Labrador\nNo report expected today\n2022-08-15T22:50:18.000000Z\n    Northwest Territories\nNo report expected today\n2022-08-15T22:50:34.000000Z\n    Yukon\nReported\n2022-08-15T22:51:21.000000Z\n    Nunavut\nNo report expected today\n2022-08-15T22:51:02.000000Z\n    Repatriated Canadians\n\nNA\n    Federal Allocation\n\n2022-03-10T20:41:40.000000Z\n    National Federal Reserve\n\n2022-08-15T22:51:36.000000Z\n  \n  \n  \n\n\n\n\ndata_status may take on the following values:\n\n\n\n\n\n\n\n\n\n\ndata_status\nMeaning\n\n\n\n\n\n\n\nWaiting for report\nThis status indicated that an update is expected to happen in the current day, but has not yet occurred.\n\n\n\n\n\nIn progress\nThis status indicates that an update is in-progress and will be completed soon. Note that when this status is indicated, some or all data may not be updated yet.\n\n\n\n\n\nReported\nWhen this status is indicated, the province has been updated with final data for the day, and the update is complete.\n\n\n\n\n\nNo report expected today\nWhen this status is indicated, the province is not expected to provide an update on the current day, and one should not be expected.\n\n\n\n\n\nCustom\nCustom statuses are used to communicate certain issues with a province’s update including delays or partial updates.\n\n\n\n\n\n\nThe density variable is population density, which is computed by dividing population by area:\n\nprovinces %>%\n  transmute(name, population, area, density,\n            density_manual = population / area) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      name\n      population\n      area\n      density\n      density_manual\n    \n  \n  \n    Ontario\n14826276\n917741\n16.15518540\n16.15518540\n    Quebec\n8604495\n1356128\n6.34489886\n6.34489886\n    Nova Scotia\n992055\n53338\n18.59940380\n18.59940380\n    New Brunswick\n789225\n71450\n11.04583625\n11.04583625\n    Manitoba\n1383765\n553556\n2.49977419\n2.49977419\n    British Columbia\n5214805\n925186\n5.63649363\n5.63649363\n    Prince Edward Island\n164318\n5660\n29.03144876\n29.03144876\n    Saskatchewan\n1179844\n591670\n1.99409130\n1.99409130\n    Alberta\n4442879\n642317\n6.91695689\n6.91695689\n    Newfoundland and Labrador\n520553\n373872\n1.39232946\n1.39232946\n    Northwest Territories\n45504\n1183085\n0.03846216\n0.03846216\n    Yukon\n42986\n474391\n0.09061302\n0.09061302\n    Nunavut\n39403\n1936113\n0.02035160\n0.02035160\n    Repatriated Canadians\nNA\nNA\nNA\nNA\n    Federal Allocation\nNA\nNA\nNA\nNA\n    National Federal Reserve\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#next-steps",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#next-steps",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Next steps",
    "text": "Next steps\nI’m impressed by the organization and accessibility of this API, and decided to write a simple R package to wrap it. In my next post, I’ll detail my steps and thought process."
  },
  {
    "objectID": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#reproducibility",
    "href": "posts/2021-12-28-canada-covid-19-data-in-r-exploring-the-api/canada-covid-19-data-in-r-exploring-the-api.html#reproducibility",
    "title": "Canada COVID-19 data in R: exploring the API",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-08-21\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [767c281] 2022-08-21: Finished converting `predicting-bike-ridership-deploying-the-model`\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(ggtext)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#tldr",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#tldr",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "TL;DR",
    "text": "TL;DR\nTo see the finished product, check out the package website and the source code. The package was published on CRAN recently, and can be downloaded with:\n\ninstall.packages(\"canadacovid\")"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#introduction",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#introduction",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Introduction",
    "text": "Introduction\nIn my previous post, I explored the Canadian COVID-19 tracker API and decided to make an API wrapper package to facilitate using it in R."
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#naming",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#naming",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Naming",
    "text": "Naming\nThe first, and some would say the hardest step for writing a package (or any piece of code for that matter) is naming it. I considered a lot of options, and in the end decided on the admittedly boring canadacovid, which I verified was available:\n\navailable::available(\"canadacovid\", browse = FALSE)"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#general-steps",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#general-steps",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "General steps",
    "text": "General steps\nWith the usethis and devtools packages, getting an R package started with best practices is very easy.\nFirst, I create the package, set it up on GitHub, and write some minor documentation:\n\nusethis::create_tidy_package(\"canadacovid\"): creates the RStudio project and opens it.\nConnect GitHub.\n\nusethis::use_git() creates the local repo.\nusethis::use_github() creates the repo on GitHub and makes an initial commit and push.\nusethis::use_tidy_github() adds some files to follow tidyverse package conventions, e.g. a template contributing.md.\nusethis::use_github_actions() configures a basic R CMD check workflow on GitHub Actions.1\nusethis::use_tidy_github_actions() sets up additional workflows for GitHub Actions.\n\nAs a tidyverse enthusiast, I always setup my packages to use magittr’s pipe operator %>% with usethis::use_pipe.\nusethis::use_testthat() sets up the testthat package and directory structure.\n\nThis step actually isn’t necessary, as it was already done by create_tidy_package.\n\nUpdate the DESCRIPTION file, particularly the Title, Description and Authors fields.\nRun devtools::document() which will update the NAMESPACE file with the magittr pipe.\n\nImmediately after pushing these setup steps to GitHub, GitHub Actions got to work but failed on two of three workflows:\n\nBoth the R CMD and the test coverage workflows failed because I haven’t written any tests yet (or functions to be tested for that matter). Which brings me to the main package development process:\n\nusethis::use_r(\"file-name\") creates the R/file-name.R file.\nAdd one or more R functions to file-name.R.\nusethis::use_package(\"dependency\") to declare any package dependencies, updating the DESCRIPTION file.\nDocument the function(s):\n\nIn RStudio, select Code -> Insert Roxygen Skeleton (or Ctrl+Alt+Shift+R on Windows).\nRun devtools::document() to generate the .Rd file and update NAMESPACE.\nOptional: check the documentation with ?func-name.\nOptional: if using examples, check them with devtools::check_examples().\n\nTry it out.\n\ndevtools::load_all() to load all package functions (Ctrl+Shift+L on Windows).\ndevtools::check() runs R CMD check on the package.\n\nWrite tests with testthat.\n\nuse_test(\"file-name\") creates the tests/testthat/test-file-name.R file, paired to R/file-name.R file.\ndevtools::test() or usethis::test_package() runs all tests.\ndevtools::test_active_file() tests just the active file.\n\nOptional: update the README.Rmd with new functionality.\n\ndevtools::build_readme() to knit.\n\nOptional: write or update a vignette to incorporate new functionality.\n\nusethis::use_vignette(\"vignette-name\") to initialize the vignette file.\ndevtools::build_vignettes() to knit the vignettes."
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#summary",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#summary",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Summary",
    "text": "Summary\nThe first function I will add retrieves the latest summary data from the API. I’ll call this file summary.R, and create it (and it’s associated test file) with:\n\nusethis::use_r(\"summary\")\nusethis::use_test(\"summary\")\n\nAnd here is the get_summary function added to R/summary.R:\n\nget_summary <- function(split = c(\"overall\", \"province\", \"region\")) {\n  split <- match.arg(split)\n  base_url <- \"https://api.covid19tracker.ca/summary\"\n  split_path <- switch(split,\n                       overall = \"\", province = \"/split\", region = \"/split/hr\")\n  url <- paste0(base_url, split_path)\n\n  resp <- httr::GET(url)\n\n  if (httr::http_error(resp)) {\n    stop(paste(\"API requested failed with code\", httr::status_code(resp)),\n         call. = FALSE)\n  }\n\n  if (httr::http_type(resp) != \"application/json\") {\n    stop(\"API did not return JSON\", call. = FALSE)\n  }\n\n  content_parsed <-\n    jsonlite::fromJSON(httr::content(resp, \"text\", encoding = \"UTF-8\"),\n                       simplifyVector = FALSE)\n\n  dplyr::bind_rows(content_parsed$data) %>%\n    dplyr::bind_cols(content_parsed[\"last_updated\"]) %>%\n    dplyr::mutate(\n      dplyr::across(tidyselect::matches(\"^change|total\"), as.integer),\n      dplyr::across(tidyselect::matches(\"date\"), as.Date),\n      last_updated = as.POSIXct(.data$last_updated)\n    )\n}\n\nSome notes:\n\nSee my previous post for a more thorough explanation of how this code is interacting with the API and processing the data.\nFor all of the imported functions, it is good practice to explicitly state the package, e.g. httr::GET.\n\nThe exception is the very last line where I use rlang to refer to the variable .data$last_updated. I find it clunky to include package names within pipe operations like that.\n\nmatch.arg is a helpful base R function which matches the split parameter to just one of the given values. It returns an error if an unexpected value is provided.\nI followed the advice from this httr vignette and turned API errors into R errors.\n\nThe httr::http_error conditional returns an error message if the GET request failed, with the resulting HTTP status code.\nThe httr::http_type conditional returns an error message if the content is not in JSON format as expected.\n\nInstead of using the as = \"parsed\" argument to httr::content, I parse the raw text directly using jsonlite::fromJSON.\n\nSee this warning from the httr documentation for the reason.\n\n\nAdd the dependencies:\n\nusethis::use_package(\"httr\", type = \"Imports\")\nusethis::use_package(\"jsonlite\", type = \"Imports\")\nusethis::use_package(\"dplyr\", type = \"Imports\")\nusethis::use_package(\"tidyselect\", type = \"Imports\")\nusethis::use_package(\"rlang\", type = \"Imports\")\n\nI’ve explicitly set the type to “Imports” (which wasn’t necessary as this is the default) to make the point that it is recommended over “Depends”:\n\nUnless there is a good reason otherwise, you should always list packages in Imports not Depends. That’s because a good package is self-contained, and minimises changes to the global environment (including the search path).\n\nNext, some Roxygen documentation:\n\n#' Get the most recent summary data\n#'\n#' Runs a GET request of summary data from the COVID-19 tracker API, and\n#' returns parsed data.\n#' Via the `split` argument, data my be \"overall\" (all provinces/territories\n#' combined), by \"province\" (one row per province/territory) or by \"region\"\n#' (one row per health region).\n#'\n#' @param split One of \"overall\", \"province\", or \"region\" to specify how the\n#'   data is split.\n#'\n#' @return A data frame containing the summary data.\n#' @export\n#'\n#' @examples\n#'\n#' get_summary()\n#' get_summary(\"province\")\n#' get_summary(\"region\")\n#'\n#' @importFrom httr GET http_error http_type content\n#' @importFrom jsonlite fromJSON\n#' @importFrom dplyr bind_rows bind_cols mutate across\n#' @importFrom tidyselect matches\n#' @importFrom rlang .data\nget_summary <- function(split = c(\"overall\", \"province\", \"region\")) {\n  ...\n\nNow I’ll edit the test-summary.R file with some simple tests (and run it here as an example):\n\nlibrary(testthat)\n\n\ntest_that(\"get_summary works\", {\n  expect_error(get_summary(split = \"provice\"), \"arg\")\n\n  summary_overall <- get_summary()\n  expect_equal(nrow(summary_overall), 1)\n  expect_equal(ncol(summary_overall), 24)\n  expect_false(any(is.na(summary_overall)))\n\n  summary_province <- get_summary(split = \"province\")\n  expect_equal(nrow(summary_province), 13)\n  expect_equal(ncol(summary_province), 25)\n  expect_false(any(is.na(summary_province)))\n  expect_setequal(summary_province$province,\n                  c(\"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\",\n                    \"NL\", \"NT\", \"YT\", \"NU\"))\n\n  summary_region <- get_summary(split = \"region\")\n  expect_equal(nrow(summary_region), 92)\n})\n\nTest passed 😸\n\n\nEverything passed successfully. Here is what was tested, from top to bottom:\n\nAn error was returned for a misspelled split argument.\nsummary_overall has the expected number of rows, columns and no values are NA.\nsummary_province has the expected number of rows, columns and no values are NA. Also the expected 13 provinces/territories are returned.\nsummary_region has the expected number of rows. The number of columns will vary, because columns with all missing (NULL) values will be dropped.\n\nNow with a documented and tested function, I do the following:\n\ndevtools::check() to run a R CMD check (also runs the testthat tests), which passes.\nUpdate the README.Rmd with installation instructions, and an example of using get_summary(). Then build with devtools::build_readme().\n\nI pushed this first iteration of the package to GitHub and, after about 10 minutes, all of the GitHub Actions workflows were successful:\n\n\nDetour: refactoring code (already)\nBefore continuing with other functions, it occurred to me that I would end up re-writing this particular block of code multiple times:\n\nresp <- httr::GET(url)\n\nif (httr::http_error(resp)) {\n  stop(paste(\"API requested failed with code\", httr::status_code(resp)),\n       call. = FALSE)\n}\n\nif (httr::http_type(resp) != \"application/json\") {\n  stop(\"API did not return JSON\", call. = FALSE)\n}\n\ncontent_parsed <-\n  jsonlite::fromJSON(httr::content(resp, \"text\", encoding = \"UTF-8\"),\n                     simplifyVector = FALSE)\n\nA quote from R for Data Science:\n\nYou should consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code).\n\nSo instead of copying and pasting this code block, I made a new function in a new file, api.R:\n\nusethis::use_r(\"api\")\nusethis::use_test(\"api\")\n\n\n#' Get content and parse it\n#'\n#' Sends a GET request to https://api.covid19tracker.ca/.\n#' If the request is successful and the returned content is JSON, formats it and\n#' returns it parsed (via `jsonlite::fromJSON`).\n#'\n#' @param url A string URL linking to the API. If it does not contain the base\n#'   \"https://api.covid19tracker.ca\", then `url` will be combined with the base\n#'   to attempt to make a valid URL (and return a warning).\n#'\n#' @return A list.\n#' @export\n#'\n#' @examples\n#'\n#' get_content_parsed(\"https://api.covid19tracker.ca/provinces\")\n#'\n#' @importFrom httr GET http_error http_type content\n#' @importFrom jsonlite fromJSON\n#' @importFrom stringr str_detect\nget_content_parsed <- function(url) {\n  base_url <- \"https://api.covid19tracker.ca\"\n  if (!stringr::str_detect(url, base_url)) {\n    url <- paste0(base_url, \"/\", url)\n    warning(\n      paste0(\"Provided URL did not include base (\", base_url, \").\\n\",\n             \"Combined URL with base for GET request: \", url)\n    )\n  }\n\n  resp <- httr::GET(url)\n\n  if (httr::http_error(resp)) {\n    stop(paste(\"API requested failed with code\", httr::status_code(resp)),\n         call. = FALSE)\n  }\n\n  if (httr::http_type(resp) != \"application/json\") {\n    stop(\"API did not return JSON\", call. = FALSE)\n  }\n\n  jsonlite::fromJSON(httr::content(resp, \"text\", encoding = \"UTF-8\"),\n                     simplifyVector = FALSE)\n}\n\nThis function expects a full url, but will add “https://api.covid19tracker.ca” if it is missing (and return a warning to say so). Some simple tests for get_content_parsed:\n\ntest_that(\"get_content_parsed works\", {\n  expect_warning(get_content_parsed(\"provinces\"), \"base\")\n  expect_error(\n    get_content_parsed(\"https://api.covid19tracker.ca/provices\"), \"API\"\n  )\n\n  provinces <- get_content_parsed(\"https://api.covid19tracker.ca/provinces\")\n  expect_true(is.list(provinces))\n  expect_equal(lengths(provinces), rep(12, 16))\n})\n\nTest passed 😀\n\n\nEverything checks out, so I replace some of the code in get_summary with a call to get_content_parsed.\nI don’t need to re-write tests for get_summary because it is functionally the same, but I do re-run those tests to make sure I didn’t break it with these changes (this is the whole point of unit testing). I then run devtools::check() and push changes to GitHub."
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#reports",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#reports",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Reports",
    "text": "Reports\nWriting the function to get reports follows the same procedure:\n\nusethis::use_r(\"reports\")\nusethis::use_test(\"reports\")\n\nThe reports are a bit more complicated because the queries can accept a few different parameters. Here is my first draft of get_reports, along with Roxygen documentation:\n\n#' Get the day-to-day reports\n#'\n#' Runs a GET request of reports data from the COVID-19 tracker API, and\n#' returns parsed data.\n#' Via the `split` argument, data may be \"overall\" (all provinces/territories\n#' combined), or by \"province\".\n#' Alternatively, provide one or more two-letter codes (e.g. \"AB\") to `province`\n#' to return reports for specific provinces, or one or more numeric `region`\n#' codes (e.g. \"1204\") to return specific health regions.\n#'\n#' @param split One of \"overall\", or \"province\" to specify how the\n#'   data is split. An \"overall\" report gives cumulative numbers across Canada.\n#'   Splitting by \"province\" returns all reports for all provinces/territories.\n#' @param province One or more province/territory codes (\"AB\", \"BC\", \"MB\", \"NB\",\n#'   \"NL\", \"NS\", \"NT\", \"NU\", \"ON\", \"PE\", \"QC\", \"SK\", \"YT\") to get reports.\n#'   Upper, lower and mixed case strings are accepted.\n#' @param region One or more health region IDs to get reports. Numeric and\n#'   character values are accepted.\n#' @param fill_dates When TRUE, the response fills in any missing dates with\n#'   blank entries.\n#' @param stat Returns only the specified statistics, e.g. \"cases\".\n#' @param date Returns reports from only the specified date.\n#' @param after Returns reports from only on or after the specified date.\n#' @param before Returns reports from only on or before the specified date.\n#'\n#' @return A data frame containing the reports data, one row per day. Includes\n#'   a `province` variable if data is split by province, and a `hr_uid` variable\n#'   if data is split by health region.\n#' @export\n#'\n#' @examples\n#'\n#' get_reports()\n#' get_reports(\"province\")\n#' get_reports(province = c(\"AB\", \"SK\"))\n#' get_reports(region = 1204)\n#' get_reports(region = c(\"472\", 1204), stat = \"cases\")\n#' @importFrom dplyr bind_rows bind_cols mutate across\n#' @importFrom tidyselect matches\n#' @importFrom rlang .data\n#' @importFrom purrr imap_chr map_dfr\n#' @importFrom tibble lst\nget_reports <- function(split = c(\"overall\", \"province\"),\n                        province = NULL, region = NULL,\n                        fill_dates = NULL, stat = NULL, date = NULL,\n                        after = NULL, before = NULL) {\n  base_url <- \"https://api.covid19tracker.ca/reports/\"\n  province_codes <- c(\n    \"AB\", \"BC\", \"MB\", \"NB\", \"NL\", \"NS\", \"NT\", \"NU\", \"ON\",\n    \"PE\", \"QC\", \"SK\", \"YT\"\n  )\n\n  split <- match.arg(split)\n  if (split == \"province\") province <- province_codes\n\n  parameters <- tibble::lst(fill_dates, stat, date, after, before)\n  # Remove NULL parameters\n  parameters <- parameters[lengths(parameters) == 1]\n  if (length(parameters) > 0) {\n    params_url <- purrr::imap_chr(parameters, ~ paste0(.y, \"=\", tolower(.x))) %>%\n      paste(collapse = \"&\")\n    params_url <- paste0(\"?\", params_url)\n  } else {\n    params_url <- \"\"\n  }\n\n  if (!is.null(province)) {\n    province <- match.arg(toupper(province), province_codes, several.ok = TRUE)\n\n    reports <- purrr::map_dfr(\n      province,\n      function(province) {\n        url <- paste0(base_url, \"province/\", province, params_url)\n        content_parsed <- get_content_parsed(url)\n\n        dplyr::bind_cols(\n          content_parsed[c(\"province\", \"last_updated\")],\n          dplyr::bind_rows(content_parsed$data)\n        )\n      }\n    )\n  } else if (!is.null(region)) {\n    reports <- purrr::map_dfr(\n      region,\n      function(region) {\n        url <- paste0(base_url, \"regions/\", region, params_url)\n        content_parsed <- get_content_parsed(url)\n\n        dplyr::bind_cols(\n          content_parsed[c(\"hr_uid\", \"last_updated\")],\n          dplyr::bind_rows(content_parsed$data)\n        )\n      }\n    )\n  } else {\n    content_parsed <- get_content_parsed(paste0(base_url, params_url))\n    reports <- dplyr::bind_cols(\n      content_parsed[\"last_updated\"],\n      dplyr::bind_rows(content_parsed$data)\n    )\n  }\n\n  reports %>%\n    dplyr::mutate(\n      dplyr::across(tidyselect::matches(\"^change|total\"), as.integer),\n      dplyr::across(tidyselect::matches(\"date\"), as.Date),\n      last_updated = as.POSIXct(.data$last_updated)\n    )\n}\n\n\nAnd the new dependencies:\n\nusethis::use_package(\"tibble\")\nusethis::use_package(\"purrr\")\n\n\nThe get_content_parsed function paid off already – I used it three times above, saving ~20 lines of code each time.\nNote that I didn’t allow split = \"region\" as an option because, if the function queries all regions, 92 separate GET requests are sent to the API in a short period. This eventually fails with HTTP code 429, indicating too many requests. This is called “rate limiting”, and the error code is a polite way of asking someone to stop spamming requests. I might alter this function in the future to send these requests slower, either with a call to Sys.sleep() or with the polite package.\nget_reports is a more complex function than get_summary, and so I wrote more tests to try to capture that complexity:\n\ntest_that(\"get_reports works\", {\n  reports_overall <- get_reports()\n  expect_equal(ncol(reports_overall), 24)\n  expect_false(any(is.na(reports_overall)))\n\n  reports_province <- get_reports(split = \"province\")\n  expect_equal(dplyr::n_distinct(reports_province$province), 13)\n  expect_equal(min(reports_province$date), min(reports_overall$date))\n  expect_equal(max(reports_province$date), max(reports_overall$date))\n\n  reports_ns_nb_nv <- get_reports(province = c(\"NS\", \"nb\", \"nU\", \"test\"))\n  expect_equal(\n    unique(reports_ns_nb_nv$province), c(\"NS\", \"NB\", \"NU\")\n  )\n  expect_equal(\n    reports_province %>% dplyr::filter(province == \"NS\"),\n    reports_ns_nb_nv %>% dplyr::filter(province == \"NS\")\n  )\n\n  expect_error(get_reports(split = \"region\"), \"arg\")\n\n  reports_592_2407_3561 <- get_reports(region = c(592, \"2407\", 3561))\n  expect_equal(\n    unique(reports_592_2407_3561$hr_uid),\n    c(592, 2407, 3561)\n  )\n\n  reports_criticals <- get_reports(split = \"province\", stat = \"criticals\")\n  expect_equal(ncol(reports_criticals), 5)\n  expect_setequal(names(reports_criticals),\n                  c(\"province\", \"last_updated\", \"date\",\n                    \"change_criticals\", \"total_criticals\"))\n\n  report_2021_07_20 <- get_reports(province = \"MB\", date = \"2021-07-20\")\n  expect_equal(report_2021_07_20$date, as.Date(\"2021-07-20\"))\n  expect_equal(report_2021_07_20$province, \"MB\")\n\n  report_date_range <- get_reports(region = 3570,\n                                   after = \"2021-10-28\", before = \"2021-11-02\")\n  expect_equal(min(report_date_range$date), as.Date(\"2021-10-28\"))\n  expect_equal(max(report_date_range$date), as.Date(\"2021-11-02\"))\n})\n\nTest passed 🥳\n\n\n\nDetour: rate limiting\nPushing the reports file and tests to GitHub resulted in failed R CMD checks due to API errors with the HTTP code 429. This is because all my testthat tests were running on 9 different platforms and flooding the API with too many requests.\nThere are more sophisticated ways to limit the request rate (e.g. using the re-written httr2 package, the vcr package, the httptest package) but I decided to use the humble Sys.sleep(). To control the delay time across all tests, I added a setup file at tests/testthat/setup.R with a very simple function:\n\nrequest_sleep <- function(seconds = 10) {\n  Sys.sleep(seconds)\n}\n\nI then called this function before every GET request in my tests so that there was at least a 10 second delay between each (I initially tried 5 seconds, but it was still too fast)."
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#vaccination-data",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#vaccination-data",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Vaccination data",
    "text": "Vaccination data\nThe vaccination data is organized a bit differently – see the API documentation or my exploration of it.\n\nSummary and reports\nThe summary and reports vaccination data is simply the same summary and reports from before, but with only the statistics related to vaccination: vaccinations, vaccinated, boosters_1, and vaccines_distributed. This function simply wraps my previous get_summary() and get_reports() functions, and returns the relevant variables:\n\n\nget_vaccination_data() documentation\n#' Get vaccination data\n#'\n#' Runs a GET request of vaccination data from the COVID-19 tracker API, and\n#' returns parsed data.\n#' Data may be returned as `type` = \"summary\" (the most recent data) or\n#' `type` = \"reports\" (day-to-day reports).\n#' Via the `split` argument, data may be \"overall\" (all provinces/territories\n#' combined), by \"province\", or by \"region\".\n#' Alternatively, provide one or more two-letter codes (e.g. \"AB\") to `province`\n#' to return reports for specific provinces, or one or more numeric `region`\n#' codes (e.g. \"1204\") to return specific health regions.\n#'\n#' @param type One of \"summary\" (most recent data) or \"reports\" (day-to-day\n#'   data).\n#' @param split One of \"overall\", \"province\", or \"region\" to specify how the\n#'   data is split. An \"overall\" summary or report gives cumulative numbers\n#'   across Canada. Splitting by \"province\" returns data for all\n#'   provinces/territories. Splitting by \"region\" is only available for\n#'   \"summary\" data, and returns data for all health regions.\n#' @param province One or more province/territory codes (\"AB\", \"BC\", \"MB\", \"NB\",\n#'   \"NL\", \"NS\", \"NT\", \"NU\", \"ON\", \"PE\", \"QC\", \"SK\", \"YT\") to get reports.\n#'   Upper, lower and mixed case strings are accepted.\n#' @param region One or more health region IDs to get reports. Numeric and\n#'   character values are accepted.\n#' @param fill_dates When TRUE, the response fills in any missing dates with\n#'   blank entries.\n#' @param stat Returns only the specified statistics, e.g. \"cases\".\n#' @param date Returns reports from only the specified date.\n#' @param after Returns reports from only on or after the specified date.\n#' @param before Returns reports from only on or before the specified date.\n#'\n#' @return A data frame containing the vaccinations data. Includes\n#'   a `province` variable if data is split by province, and a `hr_uid` variable\n#'   if data is split by health region.\n#' @export\n#'\n#' @examples\n#'\n#' get_vaccination_data()\n#' get_vaccination_data(split = \"province\")\n#' get_vaccination_data(type = \"reports\", split = \"overall\")\n#' get_vaccination_data(type = \"reports\", split = \"overall\",\n#'                      date = \"2021-12-25\")\n#' @importFrom dplyr select\n#' @importFrom tidyselect matches\n\n\n\nget_vaccination_data <- function(type = c(\"summary\", \"reports\"),\n                                 split = c(\"overall\", \"province\", \"region\"),\n                                 province = NULL, region = NULL,\n                                 fill_dates = NULL, stat = NULL, date = NULL,\n                                 after = NULL, before = NULL) {\n  type <- match.arg(type)\n  split <- match.arg(split)\n\n  if (type == \"summary\") {\n    vaccination_data <- get_summary(split)\n  } else {\n    # Getting reports for each region sends too many requests to the API\n    if (split == \"region\") {\n      stop(paste(\n        \"For `type` = 'reports', only \",\n        \"`split` = 'overall' and 'province' are available.\"\n      ))\n    }\n\n    vaccination_data <- get_reports(\n      split, province, region, fill_dates, stat,\n      date, after, before\n    )\n  }\n\n  vaccination_data %>%\n    dplyr::select(\n      tidyselect::matches(\"province|hr_uid\"),\n      tidyselect::matches(\"date|last_updated\"),\n      tidyselect::matches(\"vacc|boost\")\n    )\n}\n\nBecause this function can return both summaries and reports, I have to put in a stop() condition if reports by health region is requested, as this is not allowed by the API.\nThe tests for get_vaccination_data make use of the new request_sleep function:\n\ntest_that(\"get_vaccination_data works\", {\n  request_sleep()\n  vaccination_data_summary <- get_vaccination_data()\n  expect_equal(nrow(vaccination_data_summary), 1)\n  expect_equal(ncol(vaccination_data_summary), 12)\n\n  request_sleep()\n  vaccination_data_report <- get_vaccination_data(type = \"reports\")\n  expect_equal(ncol(vaccination_data_report), 12)\n  expect_false(any(is.na(vaccination_data_report)))\n\n  request_sleep()\n  vaccination_data_summary_region <- get_vaccination_data(split = \"region\")\n  expect_equal(nrow(vaccination_data_summary_region), 92)\n  expect_error(get_vaccination_data(type = \"reports\", split = \"region\"))\n\n  request_sleep()\n  vaccination_data_ns_pe <- get_vaccination_data(type = \"reports\",\n                                                 province = c(\"NS\", \"pe\"))\n  expect_equal(unique(vaccination_data_ns_pe$province), c(\"NS\", \"PE\"))\n\n  request_sleep()\n  expect_equal(nrow(vaccination_data_ns_pe),\n               nrow(get_reports(province = c(\"NS\", \"pe\"))))\n})\n\nTest passed 😀\n\n\n\n\nSub-regions\nIn some provinces/territories, there is additional vaccination data at the sub-region level, which requires unique requests from the API:\n\n\nget_subregion_vaccination_data() documentation\n#' Get sub-region vaccination data\n#'\n#' Runs a GET request of sub-region vaccination data from the COVID-19 tracker\n#' API, and returns parsed data.\n#' The `dates` argument specifies the time frame of the data: \"current\"\n#' (the default; latest report for each sub-region), \"recent\"\n#' (15 most recent reports for each sub-region), and \"all\" (returns all reports\n#' for one or more sub-regions specified by the `subregion_code` argument).\n#' To get a list of available sub-regions, use the function `get_subregions()`.\n#'\n#' Note that sub-region vaccination data is only for select provinces and\n#' territories. Also the percentages reported differ between percent of total\n#' population, and percent of eligible population.\n#' See the API documentation for more details:\n#' https://api.covid19tracker.ca/docs/1.0/vaccinations.\n#'\n#' @param dates One of \"current\", \"recent\", or \"all\" to specify the time frame\n#'   of the reports returned. If choosing \"all\" reports, must also provide one\n#'   or more sub-region codes.\n#' @param subregion_code One or more sub-region codes. Returns all reports for\n#'   those sub-regions (even if `dates` is not \"all\")\n#'\n#' @return A data frame with one row per sub-region report.\n#' @export\n#'\n#' @examples\n#'\n#' get_subregion_vaccination_data()\n#' get_subregion_vaccination_data(\"recent\")\n#' get_subregion_vaccination_data(\"all\", subregion_code = c(\"ON382\", \"SK007\"))\n#' @importFrom dplyr bind_cols bind_rows mutate\n#' @importFrom purrr map_dfr\n#' @importFrom tidyselect matches\n\n\n\nget_subregion_vaccination_data <- function(dates = c(\"current\", \"recent\", \"all\"),\n                                           subregion_code = NULL) {\n  dates <- match.arg(dates)\n  base_url <- \"https://api.covid19tracker.ca/reports/sub-regions/\"\n  dates_path <- switch(dates,\n    current = \"summary\",\n    recent = \"recent\",\n    all = \"\"\n  )\n  url <- paste0(base_url, dates_path)\n\n  if (is.null(subregion_code)) {\n    if (dates == \"all\") {\n      stop(\"Must specify sub-region(s) to return all vaccination reports.\")\n    }\n    content_parsed <- get_content_parsed(url)\n\n    vaccination_data <- dplyr::bind_cols(\n      content_parsed[\"last_updated\"],\n      dplyr::bind_rows(content_parsed$data)\n    )\n  } else {\n    vaccination_data <- purrr::map_dfr(\n      subregion_code,\n      function(subregion_code) {\n        url <- paste0(url, subregion_code)\n        content_parsed <- get_content_parsed(url)\n\n        dplyr::bind_cols(\n          content_parsed[\"sub_region\"],\n          dplyr::bind_rows(content_parsed$data)\n        )\n      }\n    )\n  }\n\n  vaccination_data %>%\n    dplyr::mutate(\n      dplyr::across(tidyselect::matches(\"^total\"), as.integer),\n      dplyr::across(tidyselect::matches(\"^percent\"), as.numeric),\n      dplyr::across(tidyselect::matches(\"date\"), as.Date),\n      dplyr::across(tidyselect::matches(\"last_updated\"), as.POSIXct)\n    )\n}\n\nAnother stop condition prevents sending too many requests: this time when all subregion data is requested.\n\ntest_that(\"get_subregion_vaccination_data works\", {\n  request_sleep()\n  subregion_vaccination_data_current <- get_subregion_vaccination_data()\n  expect_equal(nrow(subregion_vaccination_data_current), 806)\n  expect_equal(ncol(subregion_vaccination_data_current), 12)\n\n  request_sleep()\n  subregion_vaccination_data_recent <-\n    get_subregion_vaccination_data(dates = \"recent\")\n  expect_true(nrow(subregion_vaccination_data_recent) > 0)\n})\n\nTest passed 🌈"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#vaccine-age-groups",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#vaccine-age-groups",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Vaccine age groups",
    "text": "Vaccine age groups\nThese data are week-by-week vaccination statistics for various age groups. The request URL is /vaccines/age-groups and takes a few parameters:\n\n\nget_vaccine_age_groups() documentation\n#' Get vaccination reports by age group\n#'\n#' Runs a GET request of vaccination data by age groups from the COVID-19\n#' tracker API, and returns parsed data.\n#' Via the `split` argument, data may be \"overall\" (all provinces/territories\n#' combined), or by \"province\".\n#' Alternatively, provide one or more two-letter codes (e.g. \"AB\") to `province`\n#' to return reports for specific provinces.\n#'\n#' @param split One of \"overall\", or \"province\" to specify how the\n#'   data is split. An \"overall\" report gives cumulative numbers across Canada.\n#'   Splitting by \"province\" returns all reports for all provinces/territories.\n#' @param province One or more province/territory codes (\"AB\", \"BC\", \"MB\", \"NB\",\n#'   \"NL\", \"NS\", \"NT\", \"NU\", \"ON\", \"PE\", \"QC\", \"SK\", \"YT\") to get reports.\n#'   Upper, lower and mixed case strings are accepted.\n#' @param group A specific age group to return, for example: \"0-4\", \"05-11\",\n#'   \"30-39\", \"80+\", \"not_reported\"\n#' @param after Returns reports from only on or after the specified date,\n#'   in YYYY-MM-DD format.\n#' @param before Returns reports from only on or before the specified date,\n#'   in YYYY-MM-DD format.\n#'\n#' @return A data frame with, one row per age group per date. Includes\n#'   a `province` variable if data is split by province.\n#' @export\n#'\n#' @examples\n#'\n#' get_vaccine_age_groups()\n#' get_vaccine_age_groups(split = \"province\")\n#' get_vaccine_age_groups(province = c(\"AB\", \"SK\"))\n#' get_vaccine_age_groups(province = \"NS\", group = \"18-29\")\n#' get_vaccine_age_groups(group = \"80+\", after = \"2021-12-01\")\n#' @importFrom dplyr bind_rows bind_cols mutate across\n#' @importFrom tidyselect matches\n#' @importFrom rlang .data\n#' @importFrom purrr imap_chr map_dfr discard\n#' @importFrom tibble lst\n#' @importFrom jsonlite fromJSON\n#' @importFrom tidyr unnest\n#' @importFrom utils URLencode\n\n\n\nget_vaccine_age_groups <- function(split = c(\"overall\", \"province\"),\n                                   province = NULL,\n                                   group = NULL, before = NULL, after = NULL) {\n  base_url <- \"https://api.covid19tracker.ca/vaccines/age-groups\"\n  province_codes <- c(\n    \"AB\", \"BC\", \"MB\", \"NB\", \"NL\", \"NS\", \"NT\", \"NU\", \"ON\",\n    \"PE\", \"QC\", \"SK\", \"YT\"\n  )\n\n  split <- match.arg(split)\n  if (split == \"province\") {\n    base_url <- paste0(base_url, \"/split\")\n  } else if (!is.null(province)) {\n    province <- match.arg(toupper(province), province_codes, several.ok = TRUE)\n    base_url <- paste0(base_url, \"/province/\", province)\n  }\n\n  parameters <- tibble::lst(group, before, after)\n  # Remove NULL parameters\n  parameters <- parameters[lengths(parameters) == 1]\n  if (length(parameters) > 0) {\n    params_url <- purrr::imap_chr(\n      parameters,\n      ~ paste0(.y, \"=\", utils::URLencode(.x, reserved = TRUE))\n    ) %>%\n      paste(collapse = \"&\")\n    params_url <- paste0(\"?\", params_url)\n  } else {\n    params_url <- \"\"\n  }\n\n  purrr::map_dfr(\n    base_url,\n    function(base_url) {\n      url <- paste0(base_url, params_url)\n      content_parsed <- get_content_parsed(url)\n\n      # Because age ranges can change over time, some data returned is NULL\n      #  if the `group` param is used\n      if (!is.null(group)) {\n        # So discard NULL elements\n        content_parsed$data <- purrr::discard(content_parsed$data,\n                                              ~ is.null(.x$data))\n      }\n\n      if (!is.null(province)) {\n        dplyr::bind_cols(\n          content_parsed[\"province\"],\n          dplyr::bind_rows(content_parsed$data)\n        )\n      } else {\n        dplyr::bind_rows(content_parsed$data)\n      }\n    }\n  ) %>%\n    dplyr::mutate(\n      data = purrr::map(\n        .data$data,\n        ~jsonlite::fromJSON(.x) %>% dplyr::bind_rows(.id = \"group_code\")\n      )\n    ) %>%\n    tidyr::unnest(.data$data) %>%\n    dplyr::mutate(dplyr::across(tidyselect::matches(\"date\"), as.Date))\n}\n\n\nAnd the new dependencies:\n\nusethis::use_package(\"tidyr\")\nusethis::use_package(\"utils\")\n\n\nThere were some tricky aspects to this function, like how the content is JSON within JSON and the URL encoding for the age group parameter (see my previous post). Run some tests:\n\ntest_that(\"get_vaccine_age_groups works\", {\n  request_sleep()\n  vacc_age_overall <- get_vaccine_age_groups()\n  expect_equal(dplyr::n_distinct(vacc_age_overall$group), 19)\n\n  request_sleep()\n  vacc_age_province <- get_vaccine_age_groups(split = \"province\")\n  expect_equal(dplyr::n_distinct(vacc_age_province$province), 13)\n\n  request_sleep()\n  vacc_age_mb_nt <- get_vaccine_age_groups(province = c(\"test\", \"mB\", \"NT\"))\n  expect_setequal(unique(vacc_age_mb_nt$province), c(\"MB\", \"NT\"))\n\n  request_sleep()\n  vacc_age_ns_18_29 <- get_vaccine_age_groups(province = \"NS\", group = \"18-29\")\n  expect_setequal(unique(vacc_age_ns_18_29$province), c(\"NS\"))\n  expect_setequal(unique(vacc_age_ns_18_29$group), c(\"18-29\"))\n\n  request_sleep()\n  vacc_age_80p_date_range <-\n    get_vaccine_age_groups(group = \"80+\",\n                           after = \"2021-03-20\", before = \"2021-05-10\")\n  expect_equal(unique(vacc_age_80p_date_range$group), \"80+\")\n  expect_true(min(vacc_age_80p_date_range$date) >= \"2021-03-20\")\n  expect_true(max(vacc_age_80p_date_range$date) <= \"2021-05-10\")\n\n  request_sleep()\n  vacc_age_not_reported <- get_vaccine_age_groups(group = \"not_reported\")\n  expect_equal(unique(vacc_age_not_reported$group), \"Not reported\")\n\n  request_sleep()\n  expect_error(get_vaccine_age_groups(group = \"90+\"))\n})\n\nTest passed 🥳"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#regions-and-sub-regions",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#regions-and-sub-regions",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Regions and sub-regions",
    "text": "Regions and sub-regions\nHealth regions from get_summary and get_reports are specified by the non-descript hr_uid variable only. Also, the sub-regions from get_subregion_vaccination_data are specified by code only. Naturally, the API provides lists of regions and sub-regions, which I wrap with two new functions in the regions.R file:\n\nusethis::use_r(\"regions\")\nusethis::use_test(\"regions\")\n\n\n#' Get health regions\n#'\n#' Returns a list of health regions in the COVID-19 tracker.\n#' By default (`hr_uid` and `province` `NULL`), returns all 92 regions.\n#'\n#' @param hr_uid One or more health region UIDs (e.g. 3553) to return.\n#' @param province One or more provinces to return.\n#'\n#' @return A data frame with a row per health region.\n#' @export\n#'\n#' @examples\n#'\n#' get_regions()\n#' get_regions(hr_uid = c(\"2414\", 591))\n#' get_regions(province = c(\"ns\", \"SK\"))\n#' @importFrom dplyr bind_rows\n#' @importFrom purrr map_dfr\nget_regions <- function(hr_uid = NULL, province = NULL) {\n  base_url <- \"https://api.covid19tracker.ca/\"\n  if (!is.null(hr_uid)) {\n    url <- paste0(base_url, \"regions/\", hr_uid)\n  } else if (!is.null(province)) {\n    url <- paste0(base_url, \"province/\", province, \"/regions\")\n  } else {\n    url <- paste0(base_url, \"regions\")\n  }\n\n  purrr::map_dfr(\n    url,\n    function(url) {\n      content_parsed <- get_content_parsed(url)\n      if (!is.null(province)) {\n        dplyr::bind_rows(content_parsed)\n      } else {\n        dplyr::bind_rows(content_parsed$data)\n      }\n    }\n  )\n}\n\n#' Get sub-regions\n#'\n#' Returns a list of sub-regions in the COVID-19 tracker.\n#' By default, returns all 805 sub-regions.\n#'\n#' @param subregion_code One or more sub-regions to be returned.\n#'\n#' @return A data frame with a row per sub-region.\n#' @export\n#'\n#' @examples\n#'\n#' get_subregions()\n#' get_subregions(\"AB001\")\n#' get_subregions(c(\"SK003\", \"SK005\"))\n#' @importFrom dplyr bind_rows\n#' @importFrom purrr map_dfr\nget_subregions <- function(subregion_code = NULL) {\n  base_url <- \"https://api.covid19tracker.ca/sub-regions\"\n\n  if (is.null(subregion_code)) {\n    url <- base_url\n  } else {\n    url <- paste0(base_url, \"/\", subregion_code)\n  }\n\n  purrr::map_dfr(\n    url,\n    function(url) {\n      content_parsed <- get_content_parsed(url)\n      dplyr::bind_rows(content_parsed$data)\n    }\n  )\n}\n\nNot very complicated, and neither are the corresponding tests:\n\ntest_that(\"get_regions works\", {\n  request_sleep()\n  regions <- get_regions()\n  expect_true(\"hr_uid\" %in% names(regions))\n  expect_equal(nrow(regions), 92)\n  expect_equal(dplyr::n_distinct(regions$province), 13)\n\n  request_sleep()\n  regions_2418_3534 <- get_regions(hr_uid = c(2418, \"3534\"))\n  expect_equal(regions_2418_3534$hr_uid, c(2418, 3534))\n\n  request_sleep()\n  regions_mb_bc <- get_regions(province = c(\"mb\", \"bC\"))\n  expect_equal(unique(regions_mb_bc$province), c(\"MB\", \"BC\"))\n})\n\nTest passed 😸\n\ntest_that(\"get_subregions works\", {\n  request_sleep()\n  subregions <- get_subregions()\n  expect_equal(nrow(subregions), 806)\n  expect_equal(dplyr::n_distinct(subregions$province), 6)\n\n  request_sleep()\n  subregions_3 <- get_subregions(c(\"ON322\", \"SK010\", \"MB029\", \"test\"))\n  expect_setequal(unique(subregions_3$province), c(\"ON\", \"SK\", \"MB\"))\n})\n\nTest passed 😸"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#provinces",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#provinces",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Provinces",
    "text": "Provinces\nThe last function I added to the package is get_provinces, which has some population/geographical data, as well as a data_status variable indicating if a province’s daily numbers have been updated:\n\n#' Get provinces and territories\n#'\n#' @param geo_only Logical, indicating if only provinces/territories should be\n#'   returned. If FALSE, also returned non-geographic entities like\n#'   Repatriated Canadians and the Federal Allocation for vaccinations.\n#'\n#' @return A data frame with a row per province/territory.\n#' @export\n#'\n#' @examples\n#'\n#' get_provinces()\n#' get_provinces(geo_only = FALSE)\n#' @importFrom dplyr bind_rows mutate\n#' @importFrom rlang .data\nget_provinces <- function(geo_only = TRUE) {\n  base_url <- \"https://api.covid19tracker.ca/provinces\"\n  if (geo_only) {\n    api_params <- \"?geo_only=true\"\n  } else {\n    api_params <- \"\"\n  }\n  url <- paste0(base_url, api_params)\n\n  content_parsed <- get_content_parsed(url)\n\n  dplyr::bind_rows(content_parsed) %>%\n    dplyr::mutate(\n      # Use logical type instead of 0/1\n      geographic = .data$geographic == 1,\n      updated_at = as.POSIXct(.data$updated_at)\n    )\n}\n\n\ntest_that(\"get_provinces\", {\n  request_sleep()\n  provinces <- get_provinces()\n  expect_equal(nrow(provinces), 13)\n  expect_equal(ncol(provinces), 10)\n  expect_setequal(provinces$code,\n                  c(\"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\",\n                    \"NL\", \"NT\", \"YT\", \"NU\"))\n\n  request_sleep()\n  provinces_geo_false <- get_provinces(geo_only = FALSE)\n\n  expect_equal(nrow(provinces_geo_false), 16)\n  expect_equal(ncol(provinces_geo_false), 10)\n  expect_setequal(provinces_geo_false$code,\n                  c(\"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\",\n                    \"NL\", \"NT\", \"YT\", \"NU\",\n                    \"_RC\", \"FA\", \"NFR\"))\n\n})\n\nTest passed 🥇"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#an-example-use-case",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#an-example-use-case",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "An example use case",
    "text": "An example use case\nNow with the package up-and-running on GitHub, I’ll install it with:\n\nremotes::install_github(\"taylordunn/canadacovid\")\n\nGet cases per day across Canada, and compute a 7-day rolling average:\n\nreports_overall <- canadacovid::get_reports() %>%\n  mutate(\n    date, change_cases,\n    change_cases_rolling_avg = zoo::rollapply(change_cases, 7, mean,\n                                              align = \"right\", fill = NA)\n  )\n\n\n\n\nMake a plot of the cases:\n\ncanada_red <- \"#FF0000\"\ncanada_red_desat <- \"#bf3f3f\"\np_title <- glue::glue(\n  \"COVID-19 cases reported per day in Canada (with \",\n  \"<span style='color:{canada_red}'>7-day rolling average</span>)\"\n)\n\nreports_overall %>%\n  ggplot(aes(x = date, y = change_cases)) +\n  geom_col(aes(y = change_cases), fill = canada_red, alpha = 0.5, width = 1) +\n  geom_line(aes(y = change_cases_rolling_avg),\n            color = canada_red, size = 2) +\n  scale_y_continuous(expand = c(0, 0), labels = scales::comma) +\n  labs(y = \"Cases\", x = \"Date\", title = p_title) +\n  theme(plot.title = ggtext::element_markdown())"
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#hex-sticker",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#hex-sticker",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Hex sticker",
    "text": "Hex sticker\nTo represent the package, I’ll quickly put together a hex sticker based on the above plot. I’m no artist, so I’ll use the hexSticker package to make it.\nFirst iteration of the plot:\n\n# Use data from before the omicron peak, which (sadly) extends the y axis a lot\nd <- reports_overall %>%\n  filter(date < \"2021-12-01\", !is.na(change_cases_rolling_avg))\np <- d %>%\n  ggplot(aes(x = date, y = change_cases_rolling_avg)) +\n  geom_ribbon(aes(ymin = 0, ymax = change_cases_rolling_avg),\n              fill = canada_red, alpha = 0.5) +\n  geom_line(color = canada_red, size = 1.5) +\n  dunnr::remove_axis(\"y\") +\n  labs(x = NULL)\np\n\n\n\n\nFor some extra visual interest, I’ll mark the peaks in the plot. My lazy approach for finding those days is to first roughly group the data into waves:\n\nwaves <- as.Date(c(\"2020-03-01\", \"2020-07-01\", \"2021-03-01\",\n                   \"2021-07-15\", \"2021-11-01\"))\np + geom_vline(xintercept = waves)\n\n\n\n\nThen find the maximum case count in each wave:\n\npeaks <- d %>%\n  mutate(wave = cut(date, breaks = waves, labels = paste0(\"wave \", 1:4))) %>%\n  filter(!is.na(wave)) %>%\n  group_by(wave) %>%\n  filter(change_cases_rolling_avg == max(change_cases_rolling_avg)) %>%\n  ungroup() %>%\n  select(wave, date, change_cases, change_cases_rolling_avg)\npeaks\n\n# A tibble: 4 × 4\n  wave   date       change_cases change_cases_rolling_avg\n  <fct>  <date>            <int>                    <dbl>\n1 wave 1 2020-05-03         2794                    1795.\n2 wave 2 2021-01-10         8324                    8260.\n3 wave 3 2021-04-17         8866                    8730.\n4 wave 4 2021-09-17         5061                    4445.\n\n\nNow add these as white horizontal lines on the plot:\n\np <- p +\n  geom_vline(xintercept = peaks$date, color = \"white\", size = 1) +\n  # Re-draw the line so that it appears over the white lines\n  geom_line(color = canada_red, size = 1.5)\np\n\n\n\n\nRemove un-needed elements from the plot:\n\np <- p +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme(\n    plot.margin = margin(t = 0, r = 0, b = 0, l = 0),\n    axis.text.x = element_blank(),\n    axis.line.x = element_line(size = 1),\n    axis.ticks.x = element_blank()\n  )\np\n\n\n\n\nFinally, put it into a hexSticker and save the image:\n\n# Save the figure separately so I can control the size\nggsave(\"hex-plot.png\", plot = p, width = 10, height = 5, dpi = 500)\n\nlibrary(hexSticker)\nsysfonts::font_add_google(\"Inter\")\nshowtext::showtext_auto()\n\ncanada_red_a0.5 <- grDevices::adjustcolor(canada_red, alpha.f = 0.5)\n\nhexSticker::sticker(\n  subplot = \"hex-plot.png\", s_width = 0.7, s_x = 1.0, s_y = 0.8,\n  package = \"canadacovid\",\n  p_family = \"Inter\", p_size = 30, p_color = canada_red,\n  h_fill = \"white\", h_color = canada_red_a0.5,\n  filename = \"canadacovid-sticker.png\", dpi = 500\n)\n\n\n\n\n\n\nNot bad. Not great."
  },
  {
    "objectID": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#reproducibility",
    "href": "posts/2021-12-30-canada-covid-19-data-in-r-creating-a-package/canada-covid-19-data-in-r-creating-a-package.html#reproducibility",
    "title": "Canada COVID-19 data in R: creating a package",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-08-22\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [0b35ed3] 2022-08-22: Added `tidytuesday-2022-week-12` post\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#introduction",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#introduction",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Introduction",
    "text": "Introduction\nThis is part 3 of working with Canadian COVID-19 data via the tracker API. In the previous post, I detailed the development of the canadacovid package which was recently published on CRAN. Here, I will set up GitHub Actions to periodically download data from the API. Much of what I do here was learned from Simon Couch’s great tutorial on the subject and this bookdown project “GitHub Actions with R”.\n\n\n\n\n\n\nNote\n\n\n\nSince writing this post, I’ve put this data pipeline to use with a Shiny dashboard reporting and visualizing the latest Canadian COVID-19 numbers. Check out the dashboard here, and the source code here."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#the-goal",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#the-goal",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "The goal",
    "text": "The goal\nI want a scheduled task that periodically (every hour?) runs a script to check the API for updated COVID-19 data (overall numbers and by province). If there is updated data, then store it on GitHub. I also want to keep the API requests to a minimum if possible."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#making-it-an-r-package",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#making-it-an-r-package",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Making it an R package",
    "text": "Making it an R package\nThe R script to accomplish this will fairly simple, but it is essential to be very explicit about assumptions when running code remotely. I could use something like renv or a Docker container, but the best way to declare minimal dependencies for a piece of R code is to use a package. I’ll call it canadacoviddata and make it quickly with usethis:\n\nusethis::create_package(\"canadacoviddata\")\nusethis::use_git()\nusethis::use_github()\n\nThis sets up the necessary files and folder structure, and initializes the repository on GitHub for me. A couple more commands I usually run for R packages:\n\nusethis::use_mit_license(\"Taylor Dunn\")\nusethis::use_pipe() # Use the `%>%` pipe from `magittr`\n\nI know ahead of time two packages I will definitely want for downloading the data (my own canadacovid) and wrangling it (dplyr), so I add them as dependencies:\n\nusethis::use_dev_package(\"canadacovid\") # use_dev_package() uses GitHub version\nusethis::use_package(\"dplyr\")\n\nI then run devtools::document() and push the changes to GitHub."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#getting-the-data",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#getting-the-data",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Getting the data",
    "text": "Getting the data\nThe first data I want is the provinces table:\n\nprovinces <- canadacovid::get_provinces()\nglimpse(provinces)\n\nRows: 13\nColumns: 10\n$ id          <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n$ code        <chr> \"ON\", \"QC\", \"NS\", \"NB\", \"MB\", \"BC\", \"PE\", \"SK\", \"AB\", \"NL\"…\n$ name        <chr> \"Ontario\", \"Quebec\", \"Nova Scotia\", \"New Brunswick\", \"Mani…\n$ population  <int> 14826276, 8604495, 992055, 789225, 1383765, 5214805, 16431…\n$ area        <int> 917741, 1356128, 53338, 71450, 553556, 925186, 5660, 59167…\n$ gdp         <int> 857384, 439375, 44354, 36966, 72688, 295401, 6994, 80679, …\n$ geographic  <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ data_status <chr> \"Reported\", \"Reported\", \"No report expected today\", \"Repor…\n$ updated_at  <dttm> 2022-08-21 20:10:20, 2022-08-22 19:31:12, 2022-08-15 16:52…\n$ density     <dbl> 16.15518540, 6.34489886, 18.59940380, 11.04583625, 2.4997…\n\n\nAdd the script R/download-data.R which will hold all the functions:\n\nusethis::use_r(\"download-data\")\n\nI also need a place to store the data. In an R package, the main options are the data and data-raw folders. Files in data are “internal” will be automatically loaded upon loading the package (library(canadacoviddata)), while those in data-raw are external but are available to users via system.file(\"extdata\", \"provinces\", package = \"canadacoviddata\"). See the data chapter of the R Packages book for more information. I’ll go with data-raw:\n\ndir.create(\"data-raw\")\n\nA very simple function to download and save the data to the data-raw/ folder could look like this:\n\ndownload_provinces <- function() {\n  canadacovid::get_provinces() %>%\n    saveRDS(file = paste0(\"data-raw/provinces.rds\"))\n}\n\nAnd there is nothing wrong with this function, but I’m going to use a package I’ve been meaning to try: pins."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#storing-data-with-pins",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#storing-data-with-pins",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Storing data with pins",
    "text": "Storing data with pins\npins allows me to store R objects remotely (on boards), and retrieve and update that data when necessary. For example, create a temporary board (that will be deleted once the R session ends):\n\nlibrary(pins)\n\nboard <- board_temp()\nboard\n\nPin board <pins_board_folder>\nPath: 'C:/Users/tdunn/AppData/Local/Temp/RtmpIJIJGE/pins-1c0c673840ba'\nCache size: 0\n\n\nThen save provinces to the board:\n\nboard %>% pin_write(provinces, \"provinces\", type = \"rds\")\n\nCreating new version '20220823T022623Z-d509b'\nWriting to pin 'provinces'\n\n\nThen retrieve it:\n\nboard %>% pin_read(\"provinces\")\n\n# A tibble: 13 × 10\n      id code  name    popul…¹   area    gdp geogr…² data_…³ updated_at         \n   <int> <chr> <chr>     <int>  <int>  <int> <lgl>   <chr>   <dttm>             \n 1     1 ON    Ontario  1.48e7 9.18e5 857384 TRUE    Report… 2022-08-21 20:10:20\n 2     2 QC    Quebec   8.60e6 1.36e6 439375 TRUE    Report… 2022-08-22 19:31:12\n 3     3 NS    Nova S…  9.92e5 5.33e4  44354 TRUE    No rep… 2022-08-15 16:52:42\n 4     4 NB    New Br…  7.89e5 7.14e4  36966 TRUE    Report… 2022-08-15 16:50:10\n 5     5 MB    Manito…  1.38e6 5.54e5  72688 TRUE    No rep… 2022-08-15 17:01:29\n 6     6 BC    Britis…  5.21e6 9.25e5 295401 TRUE    Report… 2022-08-21 20:26:05\n 7     7 PE    Prince…  1.64e5 5.66e3   6994 TRUE    Report… 2022-08-15 16:50:27\n 8     8 SK    Saskat…  1.18e6 5.92e5  80679 TRUE    No rep… 2022-08-21 20:30:30\n 9     9 AB    Alberta  4.44e6 6.42e5 344812 TRUE    Report… 2022-08-21 20:15:30\n10    10 NL    Newfou…  5.21e5 3.74e5  33241 TRUE    No rep… 2022-08-15 16:50:18\n11    11 NT    Northw…  4.55e4 1.18e6   4730 TRUE    No rep… 2022-08-15 16:50:34\n12    12 YT    Yukon    4.30e4 4.74e5   3046 TRUE    Report… 2022-08-15 16:51:21\n13    13 NU    Nunavut  3.94e4 1.94e6   3421 TRUE    No rep… 2022-08-15 16:51:02\n# … with 1 more variable: density <dbl>, and abbreviated variable names\n#   ¹​population, ²​geographic, ³​data_status\n# ℹ Use `colnames()` to see all variable names\n\n\nUsing a pins board to store data has a few advantages, like versioning and caching to avoid excessive computations and downloads. Another nice feature is that I can easily get metadata, like when the data was created:\n\nboard %>% pin_meta(\"provinces\")\n\nList of 11\n $ file       : chr \"provinces.rds\"\n $ file_size  : 'fs_bytes' int 858\n $ pin_hash   : chr \"d509becae087b8ea\"\n $ type       : chr \"rds\"\n $ title      : chr \"provinces: a pinned 13 x 10 data frame\"\n $ description: NULL\n $ created    : POSIXct[1:1], format: \"2022-08-22 22:26:00\"\n $ api_version: num 1\n $ user       : list()\n $ name       : chr \"provinces\"\n $ local      :List of 3\n  ..$ dir    : 'fs_path' chr \"C:/Users/tdunn/AppData/Local/Temp/RtmpIJIJGE/pins-1c0c673840ba/provinces/20220823T022623Z-d509b\"\n  ..$ url    : NULL\n  ..$ version: chr \"20220823T022623Z-d509b\"\n\n\npins has numerous options for storing boards, including RStudio Connect, Amazon S3, and Google Cloud Platform. I want to keep this package and the data in the same repository, so I’ll register a board on this GitHub repository. Unfortunately, I have to use the legacy pins API for this task, because GitHub boards haven’t been implemented in the modern API as of me writing this:1\n\nboard <- board_register_github(\n  name = \"github\", repo = \"taylordunn/canadacoviddata\", path = \"data-raw\"\n)\n\nNow write the provinces data:\n\npins::pin(provinces, name = \"provinces\", board = \"github\")\n\nThe data get immediately pushed to the GitHub repository (under the data-raw/provinces/ directory) in both CSV and RDS formats:\n\nTo incorporate this into the package, I’ll first add pins as a dependency:\n\nusethis::use_package(\"pins\")\n\nThen add a function to register_github_board()2 and re-write download_provinces(). The R/download-data.R script now looks like this (with some added roxygen documentation):\n\n#' Register the pins board\n#'\n#' The `pins::board_register_github()` function requires a GitHub personal\n#' access token be available through the environment variable `GITHUB_PAT`.\n#'\n#' @export\n#' @importFrom pins board_register_github\nregister_github_board <- function() {\n  pins::board_register_github(\n    name = \"github\", repo = \"taylordunn/canadacoviddata\", path = \"data-raw\",\n    token = Sys.getenv(\"GITHUB_PAT\")\n  )\n}\n\n#' Retrieve and pin the provinces data\n#'\n#' Retrieves the `provinces` data from the Canadian COVID-19 tracker API\n#' and uploads it to the given `pins` board.\n#'\n#' @param board The name of the `pins` board to write the data.\n#'\n#' @export\n#' @importFrom canadacovid get_provinces\n#' @importFrom pins pin\ndownload_provinces <- function(board = \"github\") {\n  canadacovid::get_provinces() %>%\n    pins::pin(name = \"provinces\", board = board)\n}"
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#github-actions-workflow",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#github-actions-workflow",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "GitHub Actions workflow",
    "text": "GitHub Actions workflow\nNow that the functions are in place, I need to tell GitHub when and how to use them. For setting up GitHub actions, I first add the folders and files:\n\nCreated the .github/workflows/ directory.\nAdded ^\\\\.github$ to .Rbuildignore (because it does not need to be part of the installed package).\nAdded the empty .github/workflows/update-data.yaml file.\n\nAt the top of the update-data.yaml file, I need to define the frequency at which the workflow is run. I think I want data to be updated every hour at minute 0. The cron expression to specify this schedule looks like this:\n\non:\n  schedule:\n    - cron: \"0 * * * *\"\n\nFrom left to right, the \"0 * * * *\" string corresponds to:\n\n0: at minute 0 of the hour.\n*: every hour.\n*: every day.\n*: every month.\n*: every day of the week.\n\nDefining the jobs was mostly copy and paste:\n\njobs:\n  update-data:\n    runs-on: ${{ matrix.config.os }}\n\n    name: ${{ matrix.config.os }} (${{ matrix.config.r }})\n\n    strategy:\n      fail-fast: false\n      matrix:\n        config:\n          - {os: ubuntu-latest, r: 'release'}\n\n    env:\n      R_REMOTES_NO_ERRORS_FROM_WARNINGS: true\n      RSPM: ${{ matrix.config.rspm }}\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - uses: actions/checkout@v2\n\n      - uses: r-lib/actions/setup-r@master\n        with:\n          r-version: ${{ matrix.config.r }}\n          http-user-agent: ${{ matrix.config.http-user-agent }}\n\n      - uses: r-lib/actions/setup-pandoc@master\n\n      - name: Query dependencies\n        run: |\n          install.packages(\"remotes\")\n          install.packages(\"sessioninfo\")\n          install.packages(\"devtools\")\n          saveRDS(remotes::dev_package_deps(dependencies = TRUE), \".github/depends.rds\", version = 2)\n          writeLines(sprintf(\"R-%i.%i\", getRversion()$major, getRversion()$minor), \".github/r-version\")\n        shell: Rscript {0}\n\n      - name: Cache R packages\n        uses: actions/cache@v1\n        with:\n          path: ${{ env.R_LIBS_USER }}\n          key: ${{ runner.os }}-${{ hashFiles('.github/r-version') }}-1-${{ hashFiles('.github/depends.rds') }}\n          restore-keys: ${{ runner.os }}-${{ hashFiles('.github/r-version') }}-1-\n\n      - name: Install dependencies\n        run: |\n          remotes::install_deps(dependencies = TRUE)\n        shell: Rscript {0}\n\n      - name: Update data\n        run: |\n          devtools::load_all(\".\")\n          register_github_board()\n          download_provinces()\n        shell: Rscript {0}\n\nThe interesting bits, from top to bottom:\n\nSpecify that the job will be run on the latest release version of Ubuntu.\nAdd some environment variables like my GitHub PAT\n\nNote that I don’t need to add a PAT manually. At the start of each workflow run, GitHub automatically creates a unique PAT secret for authentication.\n\nInstall R.\nInstall the remotes and sessioninfo packages for downloading and managing dependencies, and the devtools package for load_all().\nInstall the dependencies for the canadacoviddata package (as defined in the DESCRIPTION file).\nCache R packages for future workflow runs.\nRun the R code that updates the data.\n\nThe R code to download the provinces data is simply three lines:\n\ndevtools::load_all(\".\") # Loads the package functions, kind of like `source()`\nregister_github_board()\ndownload_provinces()\n\nI pushed the workflow to GitHub and patiently waited about 20 minutes for the hour mark (probably should have made the workflow more frequent for quicker development/iteration) et voila:\n\nFailure. The error at the bottom tells me that the pins package was not found. It definitely should have been installed because it is explicitly listed under Imports of the DESCRIPTION file, so something must have gone wrong upstream. Digging into the logs, I found that the errors began with installing the curl package:\n\nAfter some Googling, I found that I could install the missing liburl library on the Ubuntu runner by adding the following step in the workflow YAML (before “Query dependencies”):\n\n      - name: Install curl headers\n        run: sudo apt-get install libcurl4-openssl-dev\n\nAnother problem with the workflow was that the R packages were not being cached as expected. It didn’t cause the workflow to fail, but it was taking ~13 minutes per run. This was the warning returned in the cache step:\n\nI found this GitHub issue and response from the authors, and the solution to update the version of the cache action:\n\n      - name: Cache R packages\n        uses: actions/cache@v2\n\nThis cut down the workflow run time to ~8 minutes."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#adding-functionality",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#adding-functionality",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Adding functionality",
    "text": "Adding functionality\nA list of provinces isn’t exactly the point of this post, which is to continuously retrieve COVID-19 data. The reason I started with provinces is for the updated_at variable:\n\nprovinces %>% pull(updated_at, name = code)\n\n                       ON                        QC                        NS \n\"2022-08-21 20:10:20 CST\" \"2022-08-22 19:31:12 CST\" \"2022-08-15 16:52:42 CST\" \n                       NB                        MB                        BC \n\"2022-08-15 16:50:10 CST\" \"2022-08-15 17:01:29 CST\" \"2022-08-21 20:26:05 CST\" \n                       PE                        SK                        AB \n\"2022-08-15 16:50:27 CST\" \"2022-08-21 20:30:30 CST\" \"2022-08-21 20:15:30 CST\" \n                       NL                        NT                        YT \n\"2022-08-15 16:50:18 CST\" \"2022-08-15 16:50:34 CST\" \"2022-08-15 16:51:21 CST\" \n                       NU \n\"2022-08-15 16:51:02 CST\" \n\n\nThis timestamp tells me when the province/territory last reported their COVID-19 data. By comparing new and old timestamps, I can query the API only when there is updated data, and avoid excessive requests. Here is the re-written download_provinces():\n\ndownload_provinces <- function(board = \"github\") {\n  old_provinces <- pins::pin_get(\"provinces\", board = board)\n  new_provinces <- canadacovid::get_provinces()\n\n  updated_provinces <- new_provinces %>%\n    dplyr::anti_join(old_provinces, by = c(\"name\", \"updated_at\"))\n\n  if (nrow(updated_provinces) > 0) {\n    pins::pin(new_provinces, name = \"provinces\", board = board)\n  }\n  return(updated_provinces$code)\n}\n\nIn addition to saving provinces to the pins board, this function now returns a list of provinces which have been updated since the last workflow run. Then a new function takes the list of provinces, retrieves the latest reports from the API, and writes it to the pins board:\n\ndownload_reports <- function(provinces_codes, board = \"github\") {\n  for (prov in provinces_codes) {\n    if (prov == \"overall\") {\n      new_report <- canadacovid::get_reports(\"overall\")\n    } else {\n      new_report <- canadacovid::get_reports(province = prov)\n    }\n    \n    new_report <- new_report %>%\n      dplyr::mutate(\n        change_active = .data$change_cases - .data$change_recoveries -\n          .data$change_fatalities,\n        total_active = .data$total_cases - .data$total_recoveries -\n          .data$total_fatalities,\n        positivity_rate = .data$change_cases / .data$change_tests\n      )\n    \n    pins::pin(new_report,\n              name = paste0(\"reports_\", tolower(prov)), board = board)\n  }\n}\n\nI also compute some extra variables here that I am interested in: change_active (estimated change in active cases), total_active (estimated total cases), and positivity_rate (percentage of tests which were postivie for COVID).\nThen to incorporate the new functionality, I update the workflow script:\n\n      - name: Update data\n        run: |\n          devtools::load_all(\".\")\n          register_github_board()\n          updated_provinces <- download_provinces()\n          if (length(updated_provinces) > 0) {\n            download_reports(updated_provinces)\n            download_reports(\"overall\")\n          }\n        shell: Rscript {0}\n\nAfter letting this run for a while, here is how the data-raw folder on the GitHub repo looks:\n\nNote how the age of the files is different between provinces/territories (“3 hours ago”, “9 hours ago”, etc), which shows that the selective data retrieval is working."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#conclusion",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#conclusion",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Conclusion",
    "text": "Conclusion\nThanks to some great R packages and online resources, it wasn’t too hard to set up a simple ETL (extract, transform, load) pipeline that periodically runs with GitHub actions.\nTo see the full version of the workflow, check it out on GitHub here."
  },
  {
    "objectID": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#reproducibility",
    "href": "posts/2022-01-22-canada-covid-19-data-in-r-scheduling-api-queries/canada-covid-19-data-in-r-scheduling-api-queries.html#reproducibility",
    "title": "Canada COVID-19 data in R: scheduling API queries",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-08-22\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [0b35ed3] 2022-08-22: Added `tidytuesday-2022-week-12` post\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html",
    "title": "Analyzing US baby names over the years",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(gt)\nlibrary(rmarkdown)\nlibrary(patchwork)\nlibrary(ggtext)\nlibrary(glue)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td_minimal())\nset_geom_fonts()\nset_palette()\n\nsex_pal <- c(\"M\" = \"#8ecefd\", \"F\" = \"#f88b9d\")"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#load-the-data",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#load-the-data",
    "title": "Analyzing US baby names over the years",
    "section": "Load the data",
    "text": "Load the data\n\ntt <- tidytuesdayR::tt_load(\"2022-03-22\")\n\n--- Compiling #TidyTuesday Information for 2022-03-22 ----\n\n\n--- There are 8 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 8: `applicants.csv`\n    Downloading file 2 of 8: `babynames.csv`\n    Downloading file 3 of 8: `births.csv`\n    Downloading file 4 of 8: `lifetables.csv`\n    Downloading file 5 of 8: `maorinames.csv`\n    Downloading file 6 of 8: `nz_births.csv`\n    Downloading file 7 of 8: `nz_lifetables.csv`\n    Downloading file 8 of 8: `nz_names.csv`\n\n\n--- Download complete ---\n\n\nThere are 8 data frames this week. For this post, I’ll be exploring babynames, which are baby name counts by year and sex in the US:\n\nbabynames <- tt$babynames\nglimpse(babynames)\n\nRows: 1,924,665\nColumns: 5\n$ year <dbl> 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n$ sex  <chr> \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n$ name <chr> \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n$ n    <dbl> 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n$ prop <dbl> 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#data-exploration",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#data-exploration",
    "title": "Analyzing US baby names over the years",
    "section": "Data exploration",
    "text": "Data exploration\nThe data span the following years:\n\nc(min(babynames$year), max(babynames$year))\n\n[1] 1880 2017\n\n\nThe prop variable is presumably the proportion of babies with that name and sex in a given year:\n\nbabynames %>%\n  group_by(sex, year) %>%\n  mutate(n_total = sum(n)) %>%\n  ungroup() %>%\n  mutate(prop_manual = n / n_total) %>%\n  select(prop, prop_manual)\n\n# A tibble: 1,924,665 × 2\n     prop prop_manual\n    <dbl>       <dbl>\n 1 0.0724      0.0776\n 2 0.0267      0.0286\n 3 0.0205      0.0220\n 4 0.0199      0.0213\n 5 0.0179      0.0192\n 6 0.0162      0.0173\n 7 0.0151      0.0162\n 8 0.0145      0.0155\n 9 0.0135      0.0145\n10 0.0132      0.0142\n# … with 1,924,655 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNot quite – my manually calculated prop_manual is slightly higher. It may be due to un-documented names not being counted in the babynames data frame. The applicants data has the number of social security number applications. Is this the denominator?\n\nbabynames %>%\n  left_join(tt$applicants, by = c(\"year\", \"sex\")) %>%\n  mutate(prop_manual = n / n_all) %>%\n  select(prop, prop_manual)\n\n# A tibble: 1,924,665 × 2\n     prop prop_manual\n    <dbl>       <dbl>\n 1 0.0724      0.0724\n 2 0.0267      0.0267\n 3 0.0205      0.0205\n 4 0.0199      0.0199\n 5 0.0179      0.0179\n 6 0.0162      0.0162\n 7 0.0151      0.0151\n 8 0.0145      0.0145\n 9 0.0135      0.0135\n10 0.0132      0.0132\n# … with 1,924,655 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThat looks right.\nNot much more to do in terms of data exploration or cleaning here. I’ll jump right into the analysis."
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#name-creativity-over-the-years",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#name-creativity-over-the-years",
    "title": "Analyzing US baby names over the years",
    "section": "Name creativity over the years",
    "text": "Name creativity over the years\nThe number of unique names by year and sex:\n\nd <- babynames %>% count(year, sex, name = \"n_names\")\nd_max <- d %>% group_by(sex) %>% filter(n_names == max(n_names))\n\nd %>%\n  ggplot(aes(x = year, y = n_names, color = sex)) +\n  geom_line(size = 1) +\n  geom_linerange(\n    data = d_max,\n    aes(ymin = 0, ymax = n_names, xmin = year, xmax = year, color = sex),\n    lty = 2, size = 1, show.legend = FALSE\n  ) +\n  geom_text(\n    data = d_max,\n    aes(x = year, y = c(16000, 5000), label = year),\n    hjust = 0, nudge_x = 1, show.legend = FALSE\n  ) +\n  labs(title = \"Number of unique names\", y = NULL, color = NULL) +\n  scale_color_manual(values = sex_pal) +\n  theme(legend.position = c(0.2, 0.7))\n\n\n\n\nThere is an dip in the number of unique names after peaking in 2007 and 2008 – I’m assuming this is related to a decrease in overall births following the financial crisis. If we normalize by the number of births, does this trend still appear?\n\nd <- babynames %>%\n  group_by(year, sex) %>%\n  summarise(n_names = n(), n_births = sum(n),\n            names_per_1000_births = 1000 * n_names / n_births,\n            .groups = \"drop\")\nd_max <- d %>%\n  group_by(sex) %>%\n  filter(names_per_1000_births == max(names_per_1000_births))\n\nd %>%\n  ggplot(aes(x = year, y = names_per_1000_births, color = sex)) +\n  geom_line(size = 1) +\n  geom_linerange(\n    data = d_max,\n    aes(ymin = 0, ymax = names_per_1000_births,\n        xmin = year, xmax = year, color = sex),\n    lty = 2, size = 1, show.legend = FALSE\n  ) +\n  geom_text(\n    data = d_max,\n    aes(x = year, y = c(2.5, 3.5), label = year),\n    hjust = 0, nudge_x = 1, show.legend = FALSE\n  ) +\n  labs(title = \"Number of unique names / 1000 births\",\n       y = NULL, color = NULL) +\n  scale_color_manual(values = sex_pal) +\n  theme(legend.position = c(0.5, 0.7))\n\n\n\n\nCreativity for male names peaked around 1900, and is on the rise again. Creativity for female names has been higher than male names since ~1920, and peaked in 2010.\nAnother way to think about creativity is the proportion of babies being given the most popular names in a given year:\n\nd <- babynames %>%\n  group_by(year, sex) %>%\n  mutate(name_rank = rank(-n)) %>%\n  ungroup() %>%\n  # Try a few different cut-offs\n  crossing(top_x = c(10, 25, 50, 100, 200)) %>%\n  mutate(top_name = name_rank <= top_x)  %>%\n  group_by(year, sex, top_x) %>%\n  summarise(p_top = sum(n[top_name]) / sum(n), .groups = \"drop\") \nd %>%\n  mutate(top_x = fct_reorder(paste0(\"Top \", top_x, \" names\"), top_x)) %>%\n  ggplot(aes(x = year, y = p_top, color = sex)) +\n  geom_line(size = 1) +\n  facet_wrap(~ top_x) +\n  scale_color_manual(values = sex_pal) +\n  theme(legend.position = c(0.8, 0.2))\n\n\n\n\nOut of these, I prefer the top 50 visualization:\n\nd %>%\n  filter(top_x == 50) %>%\n  ggplot(aes(x = year, y = p_top, color = sex)) +\n  geom_line(size = 1) +\n  scale_y_continuous(NULL, limits = c(0, 1), labels = scales::percent) +\n  scale_color_manual(values = sex_pal) +\n  labs(\n    x = NULL,\n    title = paste0(\"Baby <span style='color:#8ecefd'>boy</span> and \",\n                   \"<span style='color:#f88b9d'>girl</span> names \",\n                   \"are getting more creative over time\"),\n    subtitle = \"Proportion of names in the top 50 most common versus year\"\n  ) +\n  theme(legend.position = \"none\", plot.title = element_markdown())"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#gender-neutral-names",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#gender-neutral-names",
    "title": "Analyzing US baby names over the years",
    "section": "Gender-neutral names",
    "text": "Gender-neutral names\nBefore getting into gender-neutral names, I’m curious which are the least gender-neutral in this data?\n\nd <- babynames %>%\n  group_by(name, sex) %>%\n  summarise(n = sum(n), .groups = \"drop\") %>%\n  complete(name, sex, fill = list(n = 0)) %>%\n  pivot_wider(names_from = sex, values_from = n) %>%\n  rename(female = `F`, male = M) %>%\n  mutate(total = female + male) %>%\n  arrange(desc(total)) %>%\n  filter(female == 0 | male == 0) %>%\n  select(-total) %>%\n  pivot_longer(cols = -name, names_to = \"sex\", values_to = \"count\") %>%\n  filter(count > 0) %>%\n  mutate(name = fct_reorder(name, count))\n\nd %>%\n  group_by(sex) %>%\n  slice_max(count, n = 10) %>%\n  ggplot(aes(y = name, x = count, fill = sex)) +\n  geom_col() +\n  facet_wrap(~ sex, nrow = 1, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"#f88b9d\", \"#8ecefd\")) +\n  labs(\n    y = NULL, x = NULL,\n    title = paste0(\"Top 10 most common \",\n                   \"<span style='color:#f88b9d'>100% female</span> and \",\n                   \"<span style='color:#8ecefd'>100% male</span> names\"),\n    subtitle = \"US baby names from 1880 to 2017\"\n  ) +\n  theme(legend.position = \"none\", plot.title = element_markdown(),\n        panel.grid.major.y = element_blank(),\n        strip.background = element_blank(), strip.text = element_blank())\n\n\n\n\nDetermining the most popular gender-neutral names is not as straightforward. I’ll include names with 40-60% of males/females (over all years), and order by top 10 frequency:\n\nd <- babynames %>%\n  group_by(name) %>%\n  summarise(n_total = sum(n), p_female = sum(n[sex == \"F\"]) / n_total)\nd %>%\n  filter(p_female > 0.4, p_female < 0.6) %>%\n  slice_max(n_total, n = 9) %>%\n  left_join(babynames, by = \"name\") %>%\n  mutate(\n    name = fct_reorder(name, n_total),\n    name_label = ifelse(\n      n_total == max(n_total),\n      glue(\"{name} ({scales::percent(p_female, 0.1)} female)\"),\n      glue(\"{name} ({scales::percent(p_female, 0.1)})\")\n    ) %>%\n      fct_reorder(desc(n_total))\n  ) %>%\n  ggplot(aes(x = year, y = n)) +\n  geom_area(aes(fill = sex)) +\n  facet_wrap(~ name_label, scales = \"free_y\", nrow = 3) +\n  scale_fill_manual(values = sex_pal) +\n  labs(\n    y = \"Births\", x = NULL, fill = NULL,\n    title = \"The most frequent gender-neutral names (40-60% male/female)\"\n  ) +\n  theme(legend.position = c(0.1, 0.9))\n\n\n\n\nNext, I’d like to visualize how the number of gender-neutral names changed over time. To do this, I’ll compute the ratio of females to males for each name in a given year (excluding names which have 0 females or 0 males):\n\nd <- babynames %>%\n  group_by(year, name) %>%\n  # Keep only names with both sexes\n  filter(\"F\" %in% sex, \"M\" %in% sex) %>%\n  summarise(ratio_female = n[sex == \"F\"] / n[sex == \"M\"], .groups = \"drop\")\nglimpse(d)\n\nRows: 168,381\nColumns: 3\n$ year         <dbl> 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 188…\n$ name         <chr> \"Addie\", \"Allie\", \"Alma\", \"Alpha\", \"Alva\", \"Anna\", \"Annie…\n$ ratio_female <dbl> 3.425000e+01, 3.387097e+00, 1.978571e+01, 4.333333e+00, 2…\n\n\nThen I’ll plot each ratio as a point and use the ggpointdensity package to color by the density of points:\n\n# layout=\"l-body-outset\", fig.width=6, fig.height=4}\nd %>%\n  ggplot(aes(x = year, y = ratio_female)) +\n  ggpointdensity::geom_pointdensity(size = 3, adjust = 1, alpha = 0.2,\n                                    show.legend = FALSE) +\n  dunnr::scale_color_td(palette = \"div5\", type = \"continuous\") +\n  scale_y_log10(breaks = 10^seq(-2, 2),\n                labels = c(\"1:100\", \"1:10\", \"1:1\", \"10:1\", \"100:1\")) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"The amount of gender-neutral names is increasing\",\n    subtitle = \"Each point represents the ratio of females:males for a single name in a single year.\\nColor indicates the density of points.\",\n    caption = paste(\"data: {babynames} R package\",\n                    \"plot: Taylor Dunn\", sep =  \" | \")\n  ) +\n  theme(plot.subtitle = element_text(size = 10))"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#my-name",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#my-name",
    "title": "Analyzing US baby names over the years",
    "section": "My name",
    "text": "My name\nSelfishly, I of course want to explore my name and its different variations.\nFirst, here are the names which start with “Tayl” or “Tail” and have an “r” in them:\n\nbabynames_taylor <- babynames %>%\n  filter(str_detect(name, \"Tayl|Tail\") & str_detect(name, \"r\")) %>%\n  group_by(name) %>%\n  mutate(n_total = sum(n)) %>%\n  ungroup()\nbabynames_taylor %>%\n  distinct(name, n_total) %>%\n  arrange(desc(n_total)) %>%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nUnsurprised to see “Taylor” as the most common spelling. I can’t say I’ve met anyone with a double-name like “Taylorann” or “Taylormarie” before.\nI’ll take the top 6 from these and plot births over time:\n\nbabynames_taylor %>%\n  distinct(name, n_total) %>%\n  slice_max(n_total, n = 6) %>%\n  left_join(babynames_taylor %>% select(-n_total), by = \"name\") %>%\n  group_by(name, sex) %>%\n  mutate(n_cumsum = cumsum(n)) %>%\n  ungroup() %>%\n  mutate(\n    name_label = glue(\"{name} (n = {n_total})\") %>% fct_reorder(desc(n_total))\n  ) %>%\n  ggplot(aes(x = year, y = n, fill = sex)) +\n  geom_area() +\n  scale_fill_manual(values = sex_pal) +\n  facet_wrap(~ name_label, scales = \"free_y\") +\n  coord_cartesian(xlim = c(1960, 2020)) +\n  labs(y = \"Births\", x = \"Year\", fill = NULL,\n       title = \"Top 6 variations of 'Taylor'\") +\n  theme(legend.position = c(0.1, 0.9))\n\n\n\n\nNow just focusing on “Taylor”, which year was the name most popular for each sex?\n\nbabynames_taylor %>%\n  group_by(sex) %>%\n  filter(n == max(n)) %>%\n  ungroup() %>%\n  select(sex, year, n) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      sex\n      year\n      n\n    \n  \n  \n    M\n1992\n8240\n    F\n1993\n21266\n  \n  \n  \n\n\n\n\nThe most popular year for male Taylors happens to be the year I was born: 1992. Female Taylors peaked in frequency the next year.\nLastly, a plot of the proportion and number of Taylors by sex over time:\n\nd <- babynames_taylor %>%\n  filter(name == \"Taylor\", year >= 1960) %>%\n  select(year, sex, n) %>%\n  complete(year, sex, fill = list(n = 0)) %>%\n  group_by(year) %>%\n  mutate(p_female = n[sex == \"F\"] / sum(n)) %>%\n  ungroup()\np_title <- glue(\n  \"<span style='color:{sex_pal[1]}'>Male</span> and \",\n  \"<span style='color:{sex_pal[2]}'>Female</span> Taylors over time\",\n)\np1 <- d %>%\n  distinct(year, p_female) %>%\n  ggplot(aes(x = year)) +\n  geom_ribbon(aes(ymin = 0, ymax = p_female), fill = sex_pal[\"F\"]) +\n  geom_ribbon(aes(ymin = p_female, ymax = 1.0), fill = sex_pal[\"M\"]) +\n  geom_line(aes(y = p_female), color = \"white\", size = 1) +\n  geom_hline(yintercept = 0.5, lty = 2, color = \"white\", size = 1) +\n  scale_y_continuous(labels = scales::percent,\n                     expand = c(0, 0)) +\n  scale_x_continuous(breaks = NULL, expand = c(0, 0)) +\n  labs(x = NULL, y = \"Proportion by sex\", title = p_title) +\n  theme(plot.title = element_markdown())\np2 <- d %>%\n  ggplot(aes(x = year, y = n, fill = sex)) +\n  geom_area() +\n  scale_fill_manual(values = sex_pal) +\n  scale_x_continuous(\"Year\", breaks = seq(1960, 2020, 10), expand = c(0, 0)) +\n  theme(legend.position = \"none\", panel.grid.major.x = element_blank()) +\n  labs(y = \"Number of births\")\n\np1 / p2"
  },
  {
    "objectID": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#reproducibility",
    "href": "posts/2022-03-26-tidytuesday-2022-week-12/tidytuesday-2022-week-12.html#reproducibility",
    "title": "Analyzing US baby names over the years",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-08-21\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [767c281] 2022-08-21: Finished converting `predicting-bike-ridership-deploying-the-model`\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html",
    "title": "Predicting bike ridership: getting the data",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(httr)\nlibrary(lubridate)\nlibrary(gt)\nlibrary(glue)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#introduction",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#introduction",
    "title": "Predicting bike ridership: getting the data",
    "section": "Introduction",
    "text": "Introduction\nIn 2016, the city of Halifax installed its first cyclist tracker on Agricola Street. Last year, the city made bike counter data available on their open data platform. As a cyclist and Haligonian, this is of course interesting to me personally. As a data scientist, this seems like a nice opportunity to work through a machine learning project end-to-end: from retrieving, exploring, and processing the data, to building and evaluating models, to producing an end product. (A REST API? A Shiny app? TBD.)\nIn this post, I get and explore data from two sources: (1) the aforementioned bike counter data from city of Halifax, and (2) historical weather data from the government of Canada."
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#getting-bicycle-count-data",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#getting-bicycle-count-data",
    "title": "Predicting bike ridership: getting the data",
    "section": "Getting bicycle count data",
    "text": "Getting bicycle count data\nThe bicycle counts were easy enough to find on Halifax’s platform: (https://catalogue-hrm.opendata.arcgis.com/datasets/45d4ecb0cb48469186e683ebc54eb188_0/explore?showTable=true). Each data set comes with a nice API explorer for constructing queries. I’ll use httr to GET the data with the basic query provided there:\n\nquery_url <- \"https://services2.arcgis.com/11XBiaBYA9Ep0yNJ/arcgis/rest/services/Bicycle_Counts/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json\"\nresp <- httr::GET(query_url)\nresp\n\n\n\nResponse [https://services2.arcgis.com/11XBiaBYA9Ep0yNJ/arcgis/rest/services/Bicycle_Counts/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json]\n  Date: 2022-04-27 03:26\n  Status: 200\n  Content-Type: application/json; charset=utf-8\n  Size: 579 kB\n\n\nThe response code (200) indicates a successful connection. The data comes in JSON format, which I can parse to an R list with:\n\nparsed_content <- content(resp, as = \"parsed\")\nstr(parsed_content, max.level = 1)\n\nList of 6\n $ objectIdFieldName    : chr \"ObjectId\"\n $ uniqueIdField        :List of 2\n $ globalIdFieldName    : chr \"\"\n $ fields               :List of 11\n $ exceededTransferLimit: logi TRUE\n $ features             :List of 2000\n\n\nThis returned a list of 6 items. The fields item is a list of variables:\n\nfields <- map_dfr(\n  parsed_content$fields,\n  # Drop NULL elements so I can convert to a tibble\n  ~ discard(.x, is.null) %>% as_tibble()\n)\ngt(fields)\n\n\n\n\n\n  \n  \n    \n      name\n      type\n      alias\n      sqlType\n      length\n    \n  \n  \n    CHANNEL_ID\nesriFieldTypeString\nCHANNEL_ID\nsqlTypeNVarchar\n256\n    CHANNEL_NAME\nesriFieldTypeString\nCHANNEL_NAME\nsqlTypeNVarchar\n4000\n    SERIAL_NUMBER\nesriFieldTypeString\nSERIAL_NUMBER\nsqlTypeNVarchar\n4000\n    SITE_NAME\nesriFieldTypeString\nSITE_NAME\nsqlTypeNVarchar\n4000\n    LATITUDE\nesriFieldTypeDouble\nLATITUDE\nsqlTypeFloat\nNA\n    LONGITUDE\nesriFieldTypeDouble\nLONGITUDE\nsqlTypeFloat\nNA\n    INSTALLATION_DATE\nesriFieldTypeDate\nINSTALLATION_DATE\nsqlTypeTimestamp2\n8\n    COUNT_DATETIME\nesriFieldTypeDate\nCOUNT_DATETIME\nsqlTypeTimestamp2\n8\n    COUNTER_TYPE\nesriFieldTypeString\nCOUNTER_TYPE\nsqlTypeNVarchar\n4000\n    COUNTER_VALUE\nesriFieldTypeInteger\nCOUNTER_VALUE\nsqlTypeInteger\nNA\n    ObjectId\nesriFieldTypeOID\nObjectId\nsqlTypeInteger\nNA\n  \n  \n  \n\n\n\n\nThe data is the features item, which itself is a list of length 2000. Here is the first element:\n\nparsed_content$features[[1]]\n\n$attributes\n$attributes$CHANNEL_ID\n[1] \"100059339\"\n\n$attributes$CHANNEL_NAME\n[1] \"Hollis St\"\n\n$attributes$SERIAL_NUMBER\n[1] \"X2H20032465\"\n\n$attributes$SITE_NAME\n[1] \"Hollis St\"\n\n$attributes$LATITUDE\n[1] 44.64799\n\n$attributes$LONGITUDE\n[1] -63.57352\n\n$attributes$INSTALLATION_DATE\n[1] 1.594166e+12\n\n$attributes$COUNT_DATETIME\n[1] 1.595966e+12\n\n$attributes$COUNTER_TYPE\n[1] \"Bicycle\"\n\n$attributes$COUNTER_VALUE\n[1] 2\n\n$attributes$ObjectId\n[1] 1\n\n\nLooks like there is another level of nesting with attributes. Compile all of these elements into a single data frame:\n\nbike_counts <- map_dfr(\n  parsed_content$features,\n  ~ as_tibble(.x$attributes)\n)\nglimpse(bike_counts)\n\nRows: 2,000\nColumns: 11\n$ CHANNEL_ID        <chr> \"100059339\", \"100059339\", \"100059339\", \"100059339\", …\n$ CHANNEL_NAME      <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ SERIAL_NUMBER     <chr> \"X2H20032465\", \"X2H20032465\", \"X2H20032465\", \"X2H200…\n$ SITE_NAME         <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ LATITUDE          <dbl> 44.64799, 44.64799, 44.64799, 44.64799, 44.64799, 44…\n$ LONGITUDE         <dbl> -63.57352, -63.57352, -63.57352, -63.57352, -63.5735…\n$ INSTALLATION_DATE <dbl> 1.594166e+12, 1.594166e+12, 1.594166e+12, 1.594166e+…\n$ COUNT_DATETIME    <dbl> 1.595966e+12, 1.595970e+12, 1.595974e+12, 1.595977e+…\n$ COUNTER_TYPE      <chr> \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle…\n$ COUNTER_VALUE     <int> 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 6, 4, 13, 8, 5, 6, 8, …\n$ ObjectId          <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n\n\nNote that just 2000 records were returned. The exceededTransferLimit = TRUE value tells us that this is the limit of the API. I can get the total count of records by altering the original query slightly:\n\nn_records <- httr::GET(paste0(query_url, \"&returnCountOnly=true\")) %>%\n  content(as = \"parsed\") %>%\n  unlist(use.names = FALSE)\nn_records\n\n\n\n[1] 124356\n\n\nSo to get all of the data at 2000 records per request, I’ll have to make a minimum of 63 calls to the API. The API offers a “resultOffset” argument to get records in sequence. Make a function to get 2000 records for a given offset:\n\nget_bike_data <- function(offset) {\n  # Need to prevent scientific notation, e.g. \"1e+05\" instead of \"100000\"\n  offset <- format(offset, scientific = FALSE)\n  \n  parsed_content <- httr::GET(paste0(query_url, \"&resultOffset=\", offset)) %>%\n    content(as = \"parsed\")\n  \n  map_dfr(\n    parsed_content$features,\n    ~ as_tibble(.x$attributes)\n  ) \n}\n\nAnd combine it all into a single data frame:\n\nbike_data <- map_dfr(\n  seq(0, ceiling(n_records / 2000)),\n  ~ get_bike_data(offset = .x * 2000)\n)\n\n\n\n\n\nglimpse(bike_data)\n\nRows: 124,356\nColumns: 11\n$ CHANNEL_ID        <chr> \"100059339\", \"100059339\", \"100059339\", \"100059339\", …\n$ CHANNEL_NAME      <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ SERIAL_NUMBER     <chr> \"X2H20032465\", \"X2H20032465\", \"X2H20032465\", \"X2H200…\n$ SITE_NAME         <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ LATITUDE          <dbl> 44.64799, 44.64799, 44.64799, 44.64799, 44.64799, 44…\n$ LONGITUDE         <dbl> -63.57352, -63.57352, -63.57352, -63.57352, -63.5735…\n$ INSTALLATION_DATE <dbl> 1.594166e+12, 1.594166e+12, 1.594166e+12, 1.594166e+…\n$ COUNT_DATETIME    <dbl> 1.595966e+12, 1.595970e+12, 1.595974e+12, 1.595977e+…\n$ COUNTER_TYPE      <chr> \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle…\n$ COUNTER_VALUE     <int> 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 6, 4, 13, 8, 5, 6, 8, …\n$ ObjectId          <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n\n\nThis returned 124356 records, as expected. The ObjectId should be a unique sequential identifier from 1 to 124356, which I’ll check:\n\nrange(bike_data$ObjectId); n_distinct(bike_data$ObjectId)\n\n[1]      1 124356\n\n\n[1] 124356\n\n\n\nEDA and cleaning\nFirst thing I usually do with a new data set is clean the column names:\n\nbike_data <- janitor::clean_names(bike_data)\nglimpse(bike_data)\n\nRows: 124,356\nColumns: 11\n$ channel_id        <chr> \"100059339\", \"100059339\", \"100059339\", \"100059339\", …\n$ channel_name      <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ serial_number     <chr> \"X2H20032465\", \"X2H20032465\", \"X2H20032465\", \"X2H200…\n$ site_name         <chr> \"Hollis St\", \"Hollis St\", \"Hollis St\", \"Hollis St\", …\n$ latitude          <dbl> 44.64799, 44.64799, 44.64799, 44.64799, 44.64799, 44…\n$ longitude         <dbl> -63.57352, -63.57352, -63.57352, -63.57352, -63.5735…\n$ installation_date <dbl> 1.594166e+12, 1.594166e+12, 1.594166e+12, 1.594166e+…\n$ count_datetime    <dbl> 1.595966e+12, 1.595970e+12, 1.595974e+12, 1.595977e+…\n$ counter_type      <chr> \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle\", \"Bicycle…\n$ counter_value     <int> 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 6, 4, 13, 8, 5, 6, 8, …\n$ object_id         <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n\n\nNext I want to deal with the installation_date and count_datetime variables, which are very large integers. From fields above, the data type for these variables is esriFieldTypeDate. After some digging on Google, turns out this is Unix time (the number of milliseconds since January 1, 1970; also called epoch time). With as.POSIXct(), I can supply the number of seconds and set origin = \"1970-01-01\" to get back the correct datetime objects:\n\nbike_data <- bike_data %>%\n  mutate(\n    across(c(installation_date, count_datetime),\n           ~ as.POSIXct(.x / 1000, tz = \"UTC\", origin = \"1970-01-01\")),\n    # These are just dates, the time of day doesn't matter\n    installation_date = as.Date(installation_date),\n    # I'll also want the date without time of day\n    count_date = as.Date(count_datetime)\n  )\n\nThese variables are unique to the sites:\n\nbike_data %>%\n  count(site_name, latitude, longitude, serial_number, installation_date,\n        counter_type, name = \"n_records\") %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      site_name\n      latitude\n      longitude\n      serial_number\n      installation_date\n      counter_type\n      n_records\n    \n  \n  \n    Dartmouth Harbourfront Greenway\n44.66436\n-63.55736\nX2H20114473\n2021-07-08\nBicycle\n13976\n    Hollis St\n44.64799\n-63.57352\nX2H20032465\n2020-07-08\nBicycle\n15748\n    South Park St\n44.64194\n-63.57972\nX2H19070467\n2019-09-01\nBicycle\n46424\n    Vernon St\n44.64292\n-63.59154\nX2H20114470\n2020-12-09\nBicycle\n24104\n    Windsor St\n44.65466\n-63.60368\nX2H20114472\n2020-12-09\nBicycle\n24104\n  \n  \n  \n\n\n\n\nDrop serial_number and counter_type, which aren’t useful.\n\nbike_data <- bike_data %>% select(-serial_number, -counter_type)\n\nSites can have multiple channels:\n\nbike_data %>%\n  count(site_name, channel_name, channel_id, name = \"n_records\") %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      site_name\n      channel_name\n      channel_id\n      n_records\n    \n  \n  \n    Dartmouth Harbourfront Greenway\nDartmouth Harbourfront Greenway Northbound\n353280085\n6988\n    Dartmouth Harbourfront Greenway\nDartmouth Harbourfront Greenway Southbound\n353280086\n6988\n    Hollis St\nHollis St\n100059339\n15748\n    South Park St\nSouth Park St Northbound\n101054257\n23212\n    South Park St\nSouth Park St Southbound\n102054257\n23212\n    Vernon St\nVernon St Northbound\n353252897\n12052\n    Vernon St\nVernon St Southbound\n353252898\n12052\n    Windsor St\nWindsor St Northbound\n353252910\n12052\n    Windsor St\nWindsor St Southbound\n353252909\n12052\n  \n  \n  \n\n\n\n\nAll but the Hollis St site has separate northbound and southbound channels.\nFor each site, check the installation_date relative to the range of count_date:\n\nbike_data %>%\n  group_by(site_name, installation_date) %>%\n  summarise(min_count_date = min(count_date), max_count_date = max(count_date),\n            .groups = \"drop\") %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      site_name\n      installation_date\n      min_count_date\n      max_count_date\n    \n  \n  \n    Dartmouth Harbourfront Greenway\n2021-07-08\n2021-07-08\n2022-04-25\n    Hollis St\n2020-07-08\n2020-07-08\n2022-04-25\n    South Park St\n2019-09-01\n2019-09-01\n2022-04-25\n    Vernon St\n2020-12-09\n2020-12-09\n2022-04-25\n    Windsor St\n2020-12-09\n2020-12-09\n2022-04-25\n  \n  \n  \n\n\n\n\nEverything is nicely aligned: the first data corresponds to the installation date, and the last data corresponds to the date the data were retrieved.\nPlot the position of each of the counters using the given latitude and longitude, overlaid on a map of Halifax with the ggmap package:1\n\nlibrary(ggmap)\nsite_locs <- bike_data %>%\n  distinct(site_name, lat = latitude, lon = longitude)\n\nmean_lat <- mean(site_locs$lat)\nmean_lon <- mean(site_locs$lon)\n\n\nhalifax_map <- get_googlemap(c(mean_lon, mean_lat),\n                             zoom = 14, maptype = \"satellite\")\n\n\n\n\n\nggmap(halifax_map) +\n  geom_point(data = site_locs, size = 4,\n             aes(fill = site_name), shape = 21, color = \"white\") +\n  ggrepel::geom_label_repel(\n    data = site_locs,\n    aes(color = site_name, label = str_trunc(site_name, width = 25)),\n    box.padding = 1.0\n  ) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nFor each site and channel, get the time of day from count_datetime to determine the frequency of collection:\n\nbike_data %>%\n  mutate(time_of_day = format(count_datetime, \"%H:%M:%S\")) %>%\n  count(site_name, time_of_day, name = \"n_records\") %>%\n  ggplot(aes(y = time_of_day, x = n_records)) +\n  geom_col() +\n  facet_wrap(~ str_trunc(site_name, 15), nrow = 1) +\n  scale_x_continuous(expand = c(0, 0), breaks = c(0, 500, 1000, 1500)) +\n  dunnr::add_facet_borders()\n\n\n\n\nEach counter reports observations at the hour mark (+1 second). There are some slight difference in the number of records due to the time of day I retrieved the data.\nI made an assumption that the count_datetime variable was in UTC timezone. I can check this assumption by looking at average counts (over the entire data set).\n\nbike_data_tod <- bike_data %>%\n  mutate(\n    time_of_day = format(count_datetime, \"%H:%M:%S\"),\n    # Create a dummy variable with arbitrary date so I can plot time of day\n    time_of_day = lubridate::ymd_hms(\n      paste0(\"2022-04-22 \", time_of_day)\n    )\n  )\nbike_data_tod %>%\n  group_by(site_name, time_of_day) %>%\n  summarise(\n    n = n(), mean_count = mean(counter_value),\n    .groups = \"drop\"\n  ) %>%\n  ggplot(aes(x = time_of_day, y = mean_count)) +\n  geom_area(fill = td_colors$nice$mellow_yellow, color = \"black\") +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  scale_x_datetime(date_breaks = \"2 hours\", date_labels = \"%H\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  dunnr::add_facet_borders()\n\n\n\n\nThese peaks at around 8AM and 5PM tell me that the data is actually recorded in the local time zone (Atlantic), not UTC like I assumed. If they were in UTC time, the peaks would correspond to 11AM and 8PM locally, which would be odd times for peak cyclists.\nAny interesting trends in different channels?\n\nbike_data_tod %>%\n  # Remove Hollis St, which does not have different channels\n  filter(site_name != \"Hollis St\") %>%\n  mutate(channel_direction = str_extract(channel_name, \"(North|South)bound\")) %>%\n  group_by(site_name, channel_direction, time_of_day) %>%\n  summarise(mean_count = mean(counter_value), .groups = \"drop\") %>%\n  ggplot(aes(x = time_of_day, y = mean_count, color = channel_direction)) +\n  geom_line() +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  scale_x_datetime(date_breaks = \"2 hours\", date_labels = \"%H\") +\n  theme(legend.position = \"top\")\n\n\n\n\nVernon St and Windsor St counters have higher traffic Southbound (heading downtown) at the start of the typical workday, and higher traffic Northbound (leaving downtown) at the end of the typical workday.\nI am less interested in counts over the course of a day or by channel, and more interested in daily counts. Now that I know the count_date is correctly converted with the local time, get the sum at each site and each 24 hour day:\n\nbike_data_daily_counts <- bike_data %>%\n  group_by(site_name, installation_date, count_date) %>%\n  summarise(\n    n_records = n(), n_bikes = sum(counter_value), .groups = \"drop\"\n  )\n\nNow plot counts per day at each site:\n\nbike_data_daily_counts %>%\n  ggplot(aes(x = count_date, y = n_bikes)) +\n  geom_line() +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  dunnr::add_facet_borders()\n\n\n\n\nThe seasonal trends are very obvious from this plot. One thing that stood out to me is the big increase from 2020 to 2021 on South Park St. It may be representative of the start of the COVID pandemic, but I think it also has to do with the addition of protected bike lanes in December 2020. Before 2020, there appears to be a series of 0 counts on South Park St which may be artifacts:\n\nbike_data_daily_counts %>%\n  filter(site_name == \"South Park St\", count_date < \"2020-01-01\") %>%\n  ggplot(aes(x = count_date, y = n_bikes)) +\n  geom_line()\n\n\n\n\nI’m almost certain this series of zeroes is not real, so I’ll remove it from the data. Find the date of the first non-zero n_bikes at this site, and filter out data before then:\n\nsouth_park_min_date <- bike_data_daily_counts %>%\n  filter(site_name == \"South Park St\", n_bikes > 0) %>%\n  pull(count_date) %>%\n  min()\nsouth_park_min_date\n\n[1] \"2019-11-23\"\n\nbike_data_daily_counts <- bike_data_daily_counts %>%\n  filter(!((site_name == \"South Park St\") & (count_date < south_park_min_date)))\n\nOverlay counts by year for each site:\n\nbike_data_daily_counts %>%\n  mutate(count_year = year(count_date),\n         # Replace year with 1970 so I can plot on the same scale\n         count_date = as.Date(yday(count_date), origin = \"1970-01-01\")) %>%\n  ggplot(aes(x = count_date, y = n_bikes, color = factor(count_year))) +\n  geom_line(size = 1, alpha = 0.8) +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  scale_x_date(date_labels = \"%B\") +\n  dunnr::add_facet_borders() +\n  theme(legend.position = \"bottom\") +\n  labs(x = NULL, color = \"Year\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nI’m interested in day of the week effects as well:\n\nbike_data_daily_counts %>%\n  mutate(day_of_week = wday(count_date, label = TRUE)) %>%\n  ggplot(aes(y = day_of_week, x = n_bikes)) +\n  geom_boxplot()\n\n\n\n\nLess activity on the weekends."
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#getting-weather-data",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#getting-weather-data",
    "title": "Predicting bike ridership: getting the data",
    "section": "Getting weather data",
    "text": "Getting weather data\nTemporal data are probably the most important predictors of ridership, but I’m sure a close second is the day’s weather. I’ll get this with the API provided by the Meteorological Service of Canada. I can get a list of available data sets (which they call collections) as follows:2\n\nbase_url <- \"https://api.weather.gc.ca/\"\nresp <- httr::GET(paste0(base_url, \"collections?f=json\"))\n\n\n\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 2\n $ collections:List of 70\n $ links      :List of 3\n\n\nThe first element of collections:\n\ncollections <- content_parsed$collections\nstr(collections[[1]], max.level = 2)\n\nList of 7\n $ id         : chr \"hydrometric-stations\"\n $ title      : chr \"Hydrometric Monitoring Stations\"\n $ description: chr \"A station is a site on a river or lake where water quantity (water level and flow) are collected and recorded.\"\n $ keywords   :List of 2\n  ..$ : chr \"station\"\n  ..$ : chr \"hydrometric station\"\n $ links      :List of 13\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 5\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n  ..$ :List of 4\n $ extent     :List of 2\n  ..$ spatial :List of 2\n  ..$ temporal:List of 1\n $ itemType   : chr \"feature\"\n\n\nUnlike the bicycle counts data, this nested format doesn’t lend itself well to direct conversion to a tibble:\n\nas_tibble(collections[[1]])\n\nError:\n! Tibble columns must have compatible sizes.\n• Size 2: Columns `keywords` and `extent`.\n• Size 13: Column `links`.\nℹ Only values of size one are recycled.\n\n\nInstead, I can use enframe() to get a two-column data frame:\n\nenframe(collections[[1]])\n\n# A tibble: 7 × 2\n  name        value           \n  <chr>       <list>          \n1 id          <chr [1]>       \n2 title       <chr [1]>       \n3 description <chr [1]>       \n4 keywords    <list [2]>      \n5 links       <list [13]>     \n6 extent      <named list [2]>\n7 itemType    <chr [1]>       \n\n\nAssuming every item in the collections list has the same structure, I’ll just extract the id, title, and description:\n\ncollections_df <-  map_dfr(\n  collections,\n  ~ enframe(.x) %>%\n    filter(name %in% c(\"id\", \"title\", \"description\")) %>%\n    pivot_wider(names_from = name, values_from = value)\n)\ngt(collections_df) %>%\n  tab_options(container.height = 300, container.overflow.y = TRUE)\n\n\n\n\n\n  \n  \n    \n      id\n      title\n      description\n    \n  \n  \n    hydrometric-stations\nHydrometric Monitoring Stations\nA station is a site on a river or lake where water quantity (water level and flow) are collected and recorded.\n    hydrometric-daily-mean\nDaily Mean of Water Level or Flow\nThe daily mean is the average of all unit values for a given day.\n    hydrometric-monthly-mean\nMonthly Mean of Water Level or Flow\nThe monthly mean is the average of daily mean values for a given month.\n    hydrometric-annual-statistics\nAnnual Maximum and Minimum Daily Water Level or Flow\nThe annual maximum and minimum daily data are the maximum and minimum daily mean values for a given year.\n    hydrometric-annual-peaks\nAnnual Maximum and Minimum Instantaneous Water Level or Flow\nThe annual maximum and minimum instantaneous data are the maximum and minimum instantaneous values for a given year.\n    hydrometric-realtime\nReal-time hydrometric data\nReal-time water level and flow (discharge) data collected at over 2100 hydrometric stations across Canada (last 30 days).\n    climate-normals\n1981-2010 Climate Normals\nClimate Normals are used to summarize or describe the average climatic conditions of a particular location. At the completion of each decade, Environment and Climate Change Canada updates its climate normals for as many locations and as many climatic characteristics as possible. The climate normals offered here are based on Canadian climate stations with at least 15 years of data between 1981 to 2010.\n    climate-stations\nClimate Stations\nClimate observations are derived from two sources of data. The first are Daily Climate Stations producing one or two observations per day of temperature, precipitation. The second are hourly stations that typically produce more weather elements e.g. wind or snow on ground.\n    climate-monthly\nMonthly Climate Observation Summaries\nA cross-country summary of the averages and extremes for the month, including precipitation totals, max-min temperatures, and degree days. This data is available from stations that produce daily data.\n    climate-daily\nDaily Climate Observations\nDaily climate observations are derived from two sources of data. The first are Daily Climate Stations producing one or two observations per day of temperature, precipitation. The second are hourly stations that typically produce more weather elements e.g. wind or snow on ground. Only a subset of the total stations is shown due to size limitations. The criteria for station selection are listed as below. The priorities for inclusion are as follows: (1) Station is currently operational, (2) Stations with long periods of record, (3) Stations that are co-located with the categories above and supplement the period of record\n    climate-hourly\nHourly Climate Observations\nHourly climate observations are derived from the data source HLY01. These are stations that produce hourly meteorological observations, taken each hour of the day for the hours 00h-23h, for both daily (temperature, precipitation) and non-daily elements (station pressure, relative humidity, visibility). Only a subset of the total stations are shown due to size limitations. The stations were selected based on the following criteria: (1) Stations near cities with populations greater than 10,000, (2) Stations with long periods of record of at least 30 years of data, (3) Stations selected for the 1991-2020 WMO Normals, (4) Stations with RBCN designation not selected for 1991-2020 WMO Normals, and/or (5) Stations whose sum when joined with co-located/successor stations equates to a period of record of 30 years or greater.\n    ahccd-stations\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Stations\nClimate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation.\n    ahccd-annual\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Annual\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n    ahccd-seasonal\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Seasonal\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n    ahccd-monthly\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Monthly\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n    ahccd-trends\nAdjusted and Homogenized Canadian Climate Data (AHCCD) Trends\nAdjusted and Homogenized Canadian Climate Data (AHCCD) are climate station datasets that incorporate adjustments (derived from statistical procedures) to the original historical station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Data are provided for temperature, precipitation, pressure and wind speed. Station trend data are provided when available. Trends are calculated using the Theil-Sen method using the station's full period of available data. The availability of trends will vary by station; if more than 5 consecutive years are missing data or more than 10% of the data within the time series is missing, a trend was not calculated.\n    swob-realtime\nSurface Weather Observations\nSurface Observations measured at the automatic and manual stations of the Environment and Climate Change Canada and partners networks, either for a single station, or for the stations of specific provinces and territories (last 30 days)\n    ltce-stations\nVirtual Climate Stations (LTCE)\nA Virtual Climate station is the result of threading together climate data from proximate current and historical stations to construct a long term threaded data set. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. The length of the time series of virtual stations is often greater than 100 years. A Virtual Climate station is always named for an “Area” rather than a point, e.g. Winnipeg Area, to indicate that the data are drawn from that area(within a 20km radius from the urban center) rather than a single precise location.\n    ltce-temperature\nDaily Extremes of Records (LTCE) – Temperature\nAnomalous weather resulting in Temperature and Precipitation extremes occurs almost every day somewhere in Canada. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. Virtual Climate stations correspond with the city pages of weather.gc.ca. This data provides the daily extremes of record for Temperature for each day of the year. Daily elements include: High Maximum, Low Maximum, High Minimum, Low Minimum.\n    ltce-precipitation\nDaily Extremes of Records (LTCE) – Precipitation\nAnomalous weather resulting in Temperature and Precipitation extremes occurs almost every day somewhere in Canada. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. Virtual Climate stations correspond with the city pages of weather.gc.ca. This data provides the daily extremes of record for Precipitation for each day of the year. Daily elements include: Greatest Precipitation.\n    ltce-snowfall\nDaily Extremes of Records (LTCE) – Snowfall\nAnomalous weather resulting in Temperature and Precipitation extremes occurs almost every day somewhere in Canada. For the purpose of identifying and tabulating daily extremes of record for temperature, precipitation and snowfall, the Meteorological Service of Canada has threaded or put together data from closely related stations to compile a long time series of data for about 750 locations in Canada to monitor for record-breaking weather. Virtual Climate stations correspond with the city pages of weather.gc.ca. This data provides the daily extremes of record for Snowfall for each day of the year. Daily elements include: Greatest Snowfall.\n    aqhi-forecasts-realtime\nAir Quality Health Index – Forecasts\nThe Air Quality Health Index (AQHI) is a scale designed to help quantify the quality of the air in a certain region on a scale from 1 to 10. When the amount of air pollution is very high, the number is reported as 10+. It also includes a category that describes the health risk associated with the index reading (e.g. Low, Moderate, High, or Very High Health Risk). The AQHI is calculated based on the relative risks of a combination of common air pollutants that are known to harm human health, including ground-level ozone, particulate matter, and nitrogen dioxide. The AQHI formulation captures only the short term or acute health risk (exposure of hour or days at a maximum). The formulation of the AQHI may change over time to reflect new understanding associated with air pollution health effects. The AQHI is calculated from data observed in real time, without being verified (quality control).\n    aqhi-observations-realtime\nAir Quality Health Index – Observations\nThe Air Quality Health Index (AQHI) is a scale designed to help quantify the quality of the air in a certain region on a scale from 1 to 10. When the amount of air pollution is very high, the number is reported as 10+. It also includes a category that describes the health risk associated with the index reading (e.g. Low, Moderate, High, or Very High Health Risk). The AQHI is calculated based on the relative risks of a combination of common air pollutants that are known to harm human health, including ground-level ozone, particulate matter, and nitrogen dioxide. The AQHI formulation captures only the short term or acute health risk (exposure of hour or days at a maximum). The formulation of the AQHI may change over time to reflect new understanding associated with air pollution health effects. The AQHI is calculated from data observed in real time, without being verified (quality control).\n    bulletins-realtime\nReal-time meteorological bulletins\nReal-time meteorological bulletins (last 140 days)\n    climate:cmip5:projected:annual:anomaly\nProjected annual anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:seasonal:anomaly\nProjected seasonal anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:monthly:anomaly\nProjected monthly anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:annual:absolute\nProjected annual CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:seasonal:absolute\nProjected seasonal CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:monthly:absolute\nProjected monthly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:annual:P20Y-Avg\nProjected annual anomaly for 20 years average CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:projected:seasonal:P20Y-Avg\nProjected seasonal anomaly for 20 years average CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:annual:absolute\nHistorical annual CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:annual:anomaly\nHistorical annual anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:seasonal:absolute\nHistorical seasonal CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:seasonal:anomaly\nHistorical seasonal anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:monthly:absolute\nHistorical monthly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:cmip5:historical:monthly:anomaly\nHistorical monthly anomaly CMIP5\nThe Global climate model scenarios dataset is based on an ensemble of global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). Multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios at a 1x1 degree grid resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. A range of percentiles across the multi-model ensembles are available for download.\n    climate:dcs:projected:annual:anomaly\nProjected annual anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:seasonal:anomaly\nProjected seasonal anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:annual:absolute\nProjected annual DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:seasonal:absolute\nProjected seasonal DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:monthly:absolute\nProjected monthly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:annual:P20Y-Avg\nProjected annual anomaly for 20 years average DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:projected:seasonal:P20Y-Avg\nProjected seasonal anomaly for 20 years average DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:historical:annual:absolute\nHistorical annual DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:historical:annual:anomaly\nHistorical annual anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:historical:seasonal:absolute\nHistorical seasonal DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:historical:seasonal:anomaly\nHistorical seasonal anomaly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:dcs:historical:monthly:absolute\nHistorical monthly DCS\nThe statistically downscaled climate scenarios dataset provides multi-model ensembles of modelled output (actual value) and projected change (anomaly) are available for historical simulations and three emission scenarios, RCP2.6, RCP4.5, RCP8.5, at a 10km resolution. Projected changes are expressed as anomalies with respect to the reference period of 1986-2005. Downscaled data are based on global climate model projections from the Coupled Model Intercomparison Project Phase 5 (CMIP5). A range of percentiles across the multi-model ensemble are available for download.\n    climate:indices:historical\nHistorical indices\nHigh-resolution statistically downscaled climate indices relevant to climate change impacts in Canada are available at a 10 km spatial resolution and an annual temporal resolution for 1951-2100. The climate indices are based on model projections from 24 global climate models (GCMs) that participated in the Coupled Model Intercomparison Project Phase 5 (CMIP5).\n    climate:indices:projected\nProjected indices\nHigh-resolution statistically downscaled climate indices relevant to climate change impacts in Canada are available at a 10 km spatial resolution and an annual temporal resolution for 1951-2100. The climate indices are based on model projections from 24 global climate models (GCMs) that participated in the Coupled Model Intercomparison Project Phase 5 (CMIP5).\n    climate:spei-1:historical\nHistorical SPEI-1\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:spei-3:historical\nHistorical SPEI-3\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:spei-12:historical\nHistorical SPEI-12\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:spei-1:projected\nProjected SPEI-1\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:spei-3:projected\nProjected SPEI-3\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:spei-12:projected\nProjected SPEI-12\nThe Standardized Precipitation Evapotranspiration Index (SPEI) is computed similarly to the SPI. The main difference is that SPI assesses precipitation variance, while SPEI also considers demand from evapotranspiration which is subtracted from any precipitation accumulation prior to assessment. Unlike the SPI, the SPEI captures the main impact of increased temperatures on water demand.\n    climate:cangrd:historical:annual:trend\nCanGRD historical annual trend\nCANGRD data are interpolated from adjusted and homogenized climate station data (i.e., AHCCD datasets). Homogenized climate data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. Annual trends of relative total precipitation change (%) for 1948-2012 based on Canadian gridded data (CANGRD) are available, at a 50km resolution across Canada. The relative trends reflect the percent change in total precipitation over a period from the baseline value (defined as the average over 1961-1990 as the reference period). Annual trends of mean surface air temperature change (degrees Celsius) for 1948-2016 based on Canadian gridded data (CANGRD) are available at a 50km resolution across Canada. Temperature trends represent the departure from a mean reference period (1961-1990).\n    climate:cangrd:historical:annual:anomaly\nCanGRD historical annual anomaly\nGridded annual mean temperature anomalies derived from daily minimum, maximum and mean surface air temperatures (degrees Celsius) and anomalies derived from daily total precipitation is available at a 50km resolution across Canada. The Canadian gridded data (CANGRD) are interpolated from homogenized temperature (i.e., AHCCD datasets). Homogenized temperatures incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the difference between the temperature for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal temperature anomalies were computed for the years 1948 to 2017. The data will continue to be updated every year. For precipitation, the Canadian gridded data (CANGRD) are interpolated from adjusted precipitation (i.e., AHCCD datasets). Adjusted precipitation data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the percentage difference between the value for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal relative precipitation anomalies were computed for the years 1948 to 2014. The data will be updated as time permits.\n    climate:cangrd:historical:monthly:anomaly\nCanGRD historical monthly anomaly\nGridded monthly mean temperature anomalies derived from daily minimum, maximum and mean surface air temperatures (degrees Celsius) and anomalies derived from daily total precipitation is available at a 50km resolution across Canada. The Canadian gridded data (CANGRD) are interpolated from homogenized temperature (i.e., AHCCD datasets). Homogenized temperatures incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the difference between the temperature for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal temperature anomalies were computed for the years 1948 to 2017. The data will continue to be updated every year. For precipitation, the Canadian gridded data (CANGRD) are interpolated from adjusted precipitation (i.e., AHCCD datasets). Adjusted precipitation data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the percentage difference between the value for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal relative precipitation anomalies were computed for the years 1948 to 2014. The data will be updated as time permits.\n    climate:cangrd:historical:seasonal:trend\nCanGRD historical seasonal trend\nCANGRD data are interpolated from adjusted and homogenized climate station data (i.e., AHCCD datasets). Homogenized climate data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation.Seasonal trends of relative total precipitation change (%) for 1948-2012 based on Canadian gridded data (CANGRD) are available, at a 50km resolution across Canada. The relative trends reflect the percent change in total precipitation over a period from the baseline value (defined as the average over 1961-1990 as the reference period). Seasonal trends of mean surface air temperature change (degrees Celsius) for 1948-2016 based on Canadian gridded data (CANGRD) are available at a 50km resolution across Canada. Temperature trends represent the departure from a mean reference period (1961-1990).\n    climate:cangrd:historical:seasonal:anomaly\nCanGRD historical seasonal anomaly\nGridded seasonal mean temperature anomalies derived from daily minimum, maximum and mean surface air temperatures (degrees Celsius) and anomalies derived from daily total precipitation is available at a 50km resolution across Canada. The Canadian gridded data (CANGRD) are interpolated from homogenized temperature (i.e., AHCCD datasets). Homogenized temperatures incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the difference between the temperature for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal temperature anomalies were computed for the years 1948 to 2017. The data will continue to be updated every year. For precipitation, the Canadian gridded data (CANGRD) are interpolated from adjusted precipitation (i.e., AHCCD datasets). Adjusted precipitation data incorporate adjustments to the original station data to account for discontinuities from non-climatic factors, such as instrument changes or station relocation. The anomalies are the percentage difference between the value for a given year or season and a baseline value (defined as the average over 1961-1990 as the reference period). The yearly and seasonal relative precipitation anomalies were computed for the years 1948 to 2014. The data will be updated as time permits.\n    weather:rdpa:15km:24f\nRegional Deterministic Precipitation Analysis (RDPA) 24 hours accumulation at 15km\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 24 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 15 km. Data is only available for the surface level. Analysis data is made available once a day for the 24h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n    weather:rdpa:15km:6f\nRegional Deterministic Precipitation Analysis (RDPA) 6 hours accumulation at 15 km\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 6 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 15 km. Data is only available for the surface level. Analysis data is made available four times a day for the 6h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n    weather:rdpa:10km:24f\nRegional Deterministic Precipitation Analysis (RDPA) 24 hours accumulation\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 24 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available once a day for the 24h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n    weather:rdpa:10km:6f\nRegional Deterministic Precipitation Analysis (RDPA) 6 hours accumulation\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 6 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available four times a day for the 6h intervals. A preliminary estimate is available approximately 1h after the end of the accumulation period, and revised 6h after in order to assimilate gauge data arriving later.\n    weather:rdpa:10km:24p\nRegional Deterministic Precipitation Analysis (RDPA) 24 hours accumulation (preliminary)\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 24 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available once a day for the 24h intervals. The preliminary estimate is available approximately 1h after the end of the accumulation period.\n    weather:rdpa:10km:6p\nRegional Deterministic Precipitation Analysis (RDPA) 6 hours accumulation (preliminary)\nThe Regional Deterministic Precipitation Analysis (RDPA) produces a best estimate of the amount of precipitation that occurred over recent past periods of 6 hours. The estimate integrates data from in situ precipitation gauge measurements, weather radar and numerical weather prediction models. Geographic coverage is North America (Canada, United States and Mexico). Data is available at horizontal resolution of 10 km. Data is only available for the surface level. Analysis data is made available four times a day for 6h intervals. The preliminary estimate is available approximately 1h after the end of the accumulation period.\n    weather:cansips:250km:forecast:members\nCanadian Seasonal to Inter-annual Prediction System\nThe Canadian Seasonal to Inter-annual Prediction System (CanSIPS) carries out physics calculations to arrive at probabilistic predictions of atmospheric elements from the beginning of a month out to up to 12 months into the future. Atmospheric elements include temperature, precipitation, wind speed and direction and others. This product contains raw numerical results of these calculations. Geographical coverage is global. Data is available on a grid at a horizontal resolution of 2.5 degrees and for a few selected vertical levels. Predictions and corresponding hindcast are made available monthly.\n  \n  \n  \n\n\n\n\nThe collections I want for this project are climate-stations (to find the appropriate Halifax station) and climate-daily to get daily measurements at that station. Get climate-stations:\n\nresp <- httr::GET(paste0(base_url, \"collections/climate-stations/items?f=json\"))\n\n\n\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 6\n $ type          : chr \"FeatureCollection\"\n $ features      :List of 500\n $ numberMatched : int 8552\n $ numberReturned: int 500\n $ links         :List of 5\n $ timeStamp     : chr \"2022-04-27T03:27:26.475648Z\"\n\n\nBefore looking closer at the data, I can already tell I’ll want to increase the limit of returned entries. From the API documentation, the maximum number is 10000, so I can get all 8552 records in one API call:\n\nresp <- GET(paste0(base_url,\n                   \"collections/climate-stations/items?f=json&limit=10000\"))\n\n\n\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 6\n $ type          : chr \"FeatureCollection\"\n $ features      :List of 8552\n $ numberMatched : int 8552\n $ numberReturned: int 8552\n $ links         :List of 4\n $ timeStamp     : chr \"2022-04-27T03:27:27.265491Z\"\n\n\nThe data is contained in the features list:\n\nclimate_stations <- content_parsed$features\nstr(climate_stations[[1]], max.level = 3)\n\nList of 4\n $ type      : chr \"Feature\"\n $ properties:List of 32\n  ..$ STN_ID                  : int 8496\n  ..$ STATION_NAME            : chr \"CARLETON SUR MER\"\n  ..$ PROV_STATE_TERR_CODE    : chr \"QC\"\n  ..$ ENG_PROV_NAME           : chr \"QUEBEC\"\n  ..$ FRE_PROV_NAME           : chr \"QUÉBEC\"\n  ..$ COUNTRY                 : chr \"CAN\"\n  ..$ LATITUDE                : int 480800000\n  ..$ LONGITUDE               : int -660700000\n  ..$ TIMEZONE                : chr \"EST\"\n  ..$ ELEVATION               : chr \"541.00\"\n  ..$ CLIMATE_IDENTIFIER      : chr \"705AA86\"\n  ..$ TC_IDENTIFIER           : NULL\n  ..$ WMO_IDENTIFIER          : NULL\n  ..$ STATION_TYPE            : chr \"N/A\"\n  ..$ NORMAL_CODE             : NULL\n  ..$ PUBLICATION_CODE        : int 1\n  ..$ DISPLAY_CODE            : int 9\n  ..$ ENG_STN_OPERATOR_ACRONYM: NULL\n  ..$ FRE_STN_OPERATOR_ACRONYM: NULL\n  ..$ ENG_STN_OPERATOR_NAME   : NULL\n  ..$ FRE_STN_OPERATOR_NAME   : NULL\n  ..$ FIRST_DATE              : chr \"1968-10-01 00:00:00\"\n  ..$ LAST_DATE               : chr \"1968-10-31 00:00:00\"\n  ..$ HLY_FIRST_DATE          : NULL\n  ..$ HLY_LAST_DATE           : NULL\n  ..$ DLY_FIRST_DATE          : chr \"1968-10-01 00:00:00\"\n  ..$ DLY_LAST_DATE           : chr \"1968-10-31 00:00:00\"\n  ..$ MLY_FIRST_DATE          : NULL\n  ..$ MLY_LAST_DATE           : NULL\n  ..$ HAS_MONTHLY_SUMMARY     : chr \"Y\"\n  ..$ HAS_NORMALS_DATA        : chr \"N\"\n  ..$ HAS_HOURLY_DATA         : chr \"N\"\n $ geometry  :List of 2\n  ..$ type       : chr \"Point\"\n  ..$ coordinates:List of 2\n  .. ..$ : num -66.1\n  .. ..$ : num 48.1\n $ id        : chr \"705AA86\"\n\n\nAfter some frustration, I found that the geometry$coordinates are the correct latitude/longitude – those in the properties list are slightly off for some reason. Extract the data:\n\nclimate_stations <- map_dfr(\n  climate_stations,\n  ~ discard(.x$properties, is.null) %>% as_tibble() %>%\n    mutate(lat = .x$geometry$coordinates[[2]],\n           lon = .x$geometry$coordinates[[1]])\n) %>%\n  janitor::clean_names() %>%\n  # Drop the incorrect latitude and longitude\n  select(-latitude, -longitude)\n\nglimpse(climate_stations)\n\nRows: 8,552\nColumns: 32\n$ stn_id                   <int> 8496, 9005, 10205, 6149, 6154, 6174, 6177, 61…\n$ station_name             <chr> \"CARLETON SUR MER\", \"PORT COLBORNE (AUT)\", \"K…\n$ prov_state_terr_code     <chr> \"QC\", \"ON\", \"QC\", \"NB\", \"NB\", \"NB\", \"NB\", \"NB…\n$ eng_prov_name            <chr> \"QUEBEC\", \"ONTARIO\", \"QUEBEC\", \"NEW BRUNSWICK…\n$ fre_prov_name            <chr> \"QUÉBEC\", \"ONTARIO\", \"QUÉBEC\", \"NOUVEAU-BRUNS…\n$ country                  <chr> \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CA…\n$ timezone                 <chr> \"EST\", \"EST\", \"EST\", \"AST\", \"AST\", \"AST\", \"AS…\n$ elevation                <chr> \"541.00\", \"183.50\", \"123.80\", \"152.40\", \"173.…\n$ climate_identifier       <chr> \"705AA86\", \"613F606\", \"7113382\", \"8101178\", \"…\n$ station_type             <chr> \"N/A\", \"Climate-Auto\", \"N/A\", \"N/A\", \"N/A\", \"…\n$ publication_code         <int> 1, NA, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ display_code             <int> 9, NA, NA, 7, 9, 5, 9, 7, 7, 9, 9, 8, 9, 9, 1…\n$ first_date               <chr> \"1968-10-01 00:00:00\", \"1992-12-02 00:00:00\",…\n$ last_date                <chr> \"1968-10-31 00:00:00\", \"2022-04-24 12:30:02\",…\n$ dly_first_date           <chr> \"1968-10-01 00:00:00\", \"1992-12-02 00:00:00\",…\n$ dly_last_date            <chr> \"1968-10-31 00:00:00\", \"2022-04-24 00:00:00\",…\n$ has_monthly_summary      <chr> \"Y\", \"Y\", \"N\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", …\n$ has_normals_data         <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", …\n$ has_hourly_data          <chr> \"N\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", …\n$ lat                      <dbl> 48.13333, 42.86667, 60.02306, 45.93333, 47.36…\n$ lon                      <dbl> -66.11667, -79.25000, -70.00361, -64.78333, -…\n$ tc_identifier            <chr> NA, \"WPC\", \"YAS\", NA, NA, NA, NA, NA, NA, NA,…\n$ wmo_identifier           <chr> NA, \"71463\", NA, NA, NA, NA, NA, NA, NA, NA, …\n$ eng_stn_operator_acronym <chr> NA, \"ECCC - MSC\", \"DND\", NA, NA, NA, NA, NA, …\n$ fre_stn_operator_acronym <chr> NA, \"ECCC - SMC\", \"MDN\", NA, NA, NA, NA, NA, …\n$ eng_stn_operator_name    <chr> NA, \"Environment and Climate Change Canada - …\n$ fre_stn_operator_name    <chr> NA, \"Environnement et Changement climatique C…\n$ hly_first_date           <chr> NA, \"1994-02-01 02:00:00\", \"1992-10-21 07:00:…\n$ hly_last_date            <chr> NA, \"2022-04-24 12:30:02\", \"2015-09-10 13:00:…\n$ mly_first_date           <chr> NA, \"2006-04-01 00:00:00\", NA, \"1964-01-01 00…\n$ mly_last_date            <chr> NA, \"2006-12-01 00:00:00\", NA, \"1979-12-01 00…\n$ normal_code              <chr> NA, NA, NA, \"F\", NA, \"D\", NA, \"F\", \"F\", NA, N…\n\n\nNow I’ll filter this list down to those in Halifax, NS using distance to the bike counter latitude/longitude means:\n\nclimate_stations_halifax <- climate_stations %>%\n  filter(prov_state_terr_code == \"NS\") %>%\n  mutate(\n    # Compare to the mean lat/lon from the bike counters\n    diff_lat = abs(lat - mean_lat), diff_lon = abs(lon - mean_lon),\n    # Use squared distance to determine the closest points\n    diff2 = diff_lat^2 + diff_lon^2\n  ) %>%\n  # Look at the top 5 for now\n  slice_min(diff2, n = 5)\n\nclimate_stations_halifax %>% rmarkdown::paged_table()\n\n\n\n  \n\n\n\nVisualize the locations of the stations and bike counters:\n\nd <- bind_rows(\n  site_locs %>% mutate(group = \"bike counters\", label = site_name),\n  climate_stations_halifax %>%\n    transmute(label = glue(\"{station_name} ({stn_id})\"),\n              lat, lon, diff2, group = \"climate stations\")\n)\n  \nggmap(halifax_map) +\n  geom_point(data = d, size = 4,\n             aes(fill = group), shape = 21, color = \"white\") +\n  ggrepel::geom_label_repel(\n    data = d,\n    aes(color = group, label = str_trunc(label, width = 25)),\n    box.padding = 2\n  ) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nHalifax Citadel is the closest to the center, but last_date is 2002-01-31 for this station, so it hasn’t been active for the past two decades. The next closest is the dockyard, which is actively being updated (last_date is 2022-04-24).\nNow with the station name (“HALIFAX DOCKYARD”), I can request the daily climate reports:\n\nresp <- GET(\n  paste0(\n    base_url,\n    \"collections/climate-daily/items?f=json&limit=10000&STATION_NAME=HALIFAX%20DOCKYARD\"\n  )\n)\n\n\n\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\nstr(content_parsed, max.level = 1)\n\nList of 6\n $ type          : chr \"FeatureCollection\"\n $ features      :List of 1399\n $ numberMatched : int 1399\n $ numberReturned: int 1399\n $ links         :List of 4\n $ timeStamp     : chr \"2022-04-27T03:27:56.527206Z\"\n\n\nThe features data:\n\ndaily_climate <- content_parsed$features\nstr(daily_climate[[1]])\n\nList of 4\n $ id        : chr \"8202240.2021.2.11\"\n $ type      : chr \"Feature\"\n $ geometry  :List of 2\n  ..$ coordinates:List of 2\n  .. ..$ : num -63.6\n  .. ..$ : num 44.7\n  ..$ type       : chr \"Point\"\n $ properties:List of 34\n  ..$ STATION_NAME            : chr \"HALIFAX DOCKYARD\"\n  ..$ CLIMATE_IDENTIFIER      : chr \"8202240\"\n  ..$ ID                      : chr \"8202240.2021.2.11\"\n  ..$ LOCAL_DATE              : chr \"2021-02-11 00:00:00\"\n  ..$ PROVINCE_CODE           : chr \"NS\"\n  ..$ LOCAL_YEAR              : int 2021\n  ..$ LOCAL_MONTH             : int 2\n  ..$ LOCAL_DAY               : int 11\n  ..$ MEAN_TEMPERATURE        : num -7.5\n  ..$ MEAN_TEMPERATURE_FLAG   : NULL\n  ..$ MIN_TEMPERATURE         : num -9.9\n  ..$ MIN_TEMPERATURE_FLAG    : NULL\n  ..$ MAX_TEMPERATURE         : num -5.1\n  ..$ MAX_TEMPERATURE_FLAG    : NULL\n  ..$ TOTAL_PRECIPITATION     : NULL\n  ..$ TOTAL_PRECIPITATION_FLAG: NULL\n  ..$ TOTAL_RAIN              : NULL\n  ..$ TOTAL_RAIN_FLAG         : NULL\n  ..$ TOTAL_SNOW              : NULL\n  ..$ TOTAL_SNOW_FLAG         : NULL\n  ..$ SNOW_ON_GROUND          : NULL\n  ..$ SNOW_ON_GROUND_FLAG     : NULL\n  ..$ DIRECTION_MAX_GUST      : int 28\n  ..$ DIRECTION_MAX_GUST_FLAG : NULL\n  ..$ SPEED_MAX_GUST          : int 47\n  ..$ SPEED_MAX_GUST_FLAG     : NULL\n  ..$ COOLING_DEGREE_DAYS     : int 0\n  ..$ COOLING_DEGREE_DAYS_FLAG: NULL\n  ..$ HEATING_DEGREE_DAYS     : num 25.5\n  ..$ HEATING_DEGREE_DAYS_FLAG: NULL\n  ..$ MIN_REL_HUMIDITY        : int 41\n  ..$ MIN_REL_HUMIDITY_FLAG   : NULL\n  ..$ MAX_REL_HUMIDITY        : int 66\n  ..$ MAX_REL_HUMIDITY_FLAG   : NULL\n\n\nUnfortunately, this station does not report some helpful measurements, like precipitation and snowfall. I might have to expand my search to find a more informative station:\n\nclimate_stations_halifax <- climate_stations %>%\n  filter(prov_state_terr_code == \"NS\",\n         # Only include stations with recent data\n         last_date > \"2022-04-21\") %>%\n  mutate(\n    diff_lat = abs(lat - mean_lat), diff_lon = abs(lon - mean_lon),\n    diff2 = diff_lat^2 + diff_lon^2\n  ) %>%\n  slice_min(diff2, n = 5)\n\nclimate_stations_halifax %>% rmarkdown::paged_table()\n\n\n\n  \n\n\n\nVisualize these station locations in a zoomed out map:\n\nhrm_map <- get_googlemap(c(mean_lon, mean_lat),\n                         zoom = 12, maptype = \"satellite\")\n\n\n\n\n\nd <- bind_rows(\n  site_locs %>% mutate(group = \"bike counters\", label = site_name),\n  climate_stations_halifax %>%\n    #filter(station_name == \"HALIFAX WINDSOR PARK\")\n    transmute(label = glue(\"{station_name} ({stn_id})\"),\n              lat, lon, diff2, group = \"climate stations\")\n)\n  \nggmap(hrm_map) +\n  geom_point(data = d, size = 4,\n             aes(fill = group), shape = 21, color = \"white\") +\n  ggrepel::geom_label_repel(\n    data = d,\n    aes(color = group, label = str_trunc(label, width = 25)),\n    box.padding = 0.5, force = 1.5\n  ) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nExploring the data from these stations a bit (not shown), Halifax Windsor Park seems a reasonable choice in terms of available data.\n\nresp <- GET(\n  paste0(base_url,\n         \"collections/climate-daily/items?f=json&limit=10000&STATION_NAME=\",\n         URLencode(\"HALIFAX WINDSOR PARK\"))\n)\n\n\n\n\n\ncontent_parsed <- content(resp, as = \"parsed\")\n\ndaily_climate <- map_dfr(\n  content_parsed$features,\n  ~ discard(.x$properties, is.null) %>% as_tibble()\n) %>%\n  janitor::clean_names()\n\nglimpse(daily_climate)\n\nRows: 1,431\nColumns: 29\n$ station_name             <chr> \"HALIFAX WINDSOR PARK\", \"HALIFAX WINDSOR PARK…\n$ climate_identifier       <chr> \"8202255\", \"8202255\", \"8202255\", \"8202255\", \"…\n$ id                       <chr> \"8202255.2021.2.11\", \"8202255.2021.2.12\", \"82…\n$ local_date               <chr> \"2021-02-11 00:00:00\", \"2021-02-12 00:00:00\",…\n$ province_code            <chr> \"NS\", \"NS\", \"NS\", \"NS\", \"NS\", \"NS\", \"NS\", \"NS…\n$ local_year               <int> 2021, 2021, 2021, 2021, 2021, 2018, 2018, 201…\n$ local_month              <int> 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, …\n$ local_day                <int> 11, 12, 13, 14, 15, 14, 15, 16, 17, 18, 19, 2…\n$ mean_temperature         <dbl> -8.1, -8.1, -5.2, -5.0, -1.5, 10.8, 12.1, 9.0…\n$ min_temperature          <dbl> -11.1, -13.0, -8.0, -8.9, -5.4, 5.0, 9.1, 5.4…\n$ max_temperature          <dbl> -5.2, -3.2, -2.3, -1.2, 2.4, 16.5, 15.1, 12.5…\n$ snow_on_ground           <int> 13, 12, 11, 11, 11, NA, NA, NA, NA, NA, NA, N…\n$ direction_max_gust       <int> 26, 30, NA, NA, NA, 22, 21, 33, NA, NA, NA, 2…\n$ speed_max_gust           <int> 42, 32, NA, NA, NA, 38, 48, 32, NA, NA, NA, 4…\n$ cooling_degree_days      <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, …\n$ heating_degree_days      <dbl> 26.1, 26.1, 23.2, 23.0, 19.5, 7.2, 5.9, 9.0, …\n$ min_rel_humidity         <int> 39, 66, 52, 67, 40, 37, 68, 40, 38, 21, 30, 8…\n$ max_rel_humidity         <int> 68, 87, 90, 90, 83, 90, 96, 87, 97, 89, 96, 9…\n$ total_precipitation_flag <chr> NA, NA, NA, NA, \"M\", NA, NA, NA, NA, NA, NA, …\n$ total_precipitation      <dbl> NA, NA, NA, NA, NA, 0.0, 0.6, 0.0, 0.0, 0.0, …\n$ mean_temperature_flag    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ min_temperature_flag     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ max_temperature_flag     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ direction_max_gust_flag  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ speed_max_gust_flag      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cooling_degree_days_flag <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ heating_degree_days_flag <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ min_rel_humidity_flag    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ max_rel_humidity_flag    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\nEDA and cleaning\nVariable summaries:\n\nskimr::skim(daily_climate)\n\n\nData summary\n\n\nName\ndaily_climate\n\n\nNumber of rows\n1431\n\n\nNumber of columns\n29\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n15\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstation_name\n0\n1.00\n20\n20\n0\n1\n0\n\n\nclimate_identifier\n0\n1.00\n7\n7\n0\n1\n0\n\n\nid\n0\n1.00\n16\n18\n0\n1431\n0\n\n\nlocal_date\n0\n1.00\n19\n19\n0\n1431\n0\n\n\nprovince_code\n0\n1.00\n2\n2\n0\n1\n0\n\n\ntotal_precipitation_flag\n1098\n0.23\n1\n1\n0\n1\n0\n\n\nmean_temperature_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmin_temperature_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmax_temperature_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\ndirection_max_gust_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nspeed_max_gust_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\ncooling_degree_days_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nheating_degree_days_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmin_rel_humidity_flag\n1412\n0.01\n1\n1\n0\n1\n0\n\n\nmax_rel_humidity_flag\n1409\n0.02\n1\n1\n0\n1\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlocal_year\n0\n1.00\n2019.83\n1.20\n2018.0\n2019.0\n2020.0\n2021.00\n2022.0\n▅▇▇▇▂\n\n\nlocal_month\n0\n1.00\n6.59\n3.44\n1.0\n4.0\n7.0\n10.00\n12.0\n▇▅▅▅▇\n\n\nlocal_day\n0\n1.00\n15.85\n8.76\n1.0\n8.0\n16.0\n23.00\n31.0\n▇▇▇▇▆\n\n\nmean_temperature\n19\n0.99\n8.51\n9.16\n-13.5\n1.3\n8.2\n16.30\n26.8\n▂▆▇▇▅\n\n\nmin_temperature\n19\n0.99\n4.12\n8.95\n-17.3\n-2.1\n3.9\n11.93\n20.7\n▂▅▇▆▆\n\n\nmax_temperature\n19\n0.99\n12.90\n9.66\n-11.0\n5.0\n12.9\n21.20\n34.1\n▂▇▇▇▃\n\n\nsnow_on_ground\n1061\n0.26\n2.82\n4.07\n0.0\n0.0\n1.0\n3.00\n24.0\n▇▁▁▁▁\n\n\ndirection_max_gust\n554\n0.61\n23.77\n8.16\n1.0\n21.0\n24.0\n30.00\n36.0\n▂▃▅▇▆\n\n\nspeed_max_gust\n554\n0.61\n42.48\n10.34\n31.0\n35.0\n40.0\n47.00\n96.0\n▇▂▁▁▁\n\n\ncooling_degree_days\n19\n0.99\n0.61\n1.48\n0.0\n0.0\n0.0\n0.00\n8.8\n▇▁▁▁▁\n\n\nheating_degree_days\n19\n0.99\n10.10\n8.32\n0.0\n1.7\n9.8\n16.70\n31.5\n▇▅▅▃▁\n\n\nmin_rel_humidity\n19\n0.99\n56.63\n18.72\n15.0\n42.0\n56.0\n70.00\n100.0\n▂▇▇▆▂\n\n\nmax_rel_humidity\n22\n0.98\n92.31\n9.40\n47.0\n88.0\n96.0\n100.00\n100.0\n▁▁▁▂▇\n\n\ntotal_precipitation\n554\n0.61\n4.16\n9.75\n0.0\n0.0\n0.0\n3.00\n102.4\n▇▁▁▁▁\n\n\n\n\n\nDrop some un-needed variables:\n\ndaily_climate <- daily_climate %>%\n  select(-station_name, -climate_identifier, -id, -province_code)\n\nProcess the date variable:\n\ndaily_climate <- daily_climate %>%\n  mutate(report_date = as.POSIXct(local_date) %>% as.Date()) %>%\n  # Can drop these now\n  select(-local_date, -local_year, -local_month, -local_day)\n\nThere happens to be some missing days:\n\ntibble(report_date = seq.Date(as.Date(\"2018-05-14\"), as.Date(\"2022-04-22\"),\n                              by = \"days\")) %>%\n  anti_join(daily_climate, by = \"report_date\") %>%\n  pull(report_date)\n\n [1] \"2020-01-02\" \"2020-01-03\" \"2020-01-04\" \"2020-01-05\" \"2020-01-06\"\n [6] \"2021-01-03\" \"2021-01-04\" \"2021-01-05\" \"2021-01-06\" \"2021-01-07\"\n[11] \"2022-01-03\"\n\n\nSeems odd that all of the missing days are in January of different years.\nThere are also some missing temperature values:\n\ndaily_climate %>%\n  filter(\n    is.na(mean_temperature) | is.na(min_temperature) | is.na(max_temperature)\n  ) %>%\n  select(report_date, contains(\"_temperature\")) %>%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThe report_dates range from 2018 to 2022. The flag values (*_temperature_flag) are all “M”, telling us what we already know: the data is missing.\nFor non-missing values, here is the trend over time:\n\ndaily_climate %>%\n  filter(!is.na(mean_temperature)) %>%\n  ggplot(aes(x = report_date)) +\n  geom_line(aes(y = mean_temperature), color = td_colors$nice$ruby_red)\n\n\n\n\nThe total_precipitation values:\n\ndaily_climate %>%\n  count(total_precipitation, total_precipitation_flag) %>%\n  arrange(desc(is.na(total_precipitation))) %>%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThere are missing total_precipitation values with NA total_precipitation_flag, which makes me think that the flag variables are not going to be useful/reliable.\nVisualize the non-missing:\n\ndaily_climate %>%\n  filter(!is.na(total_precipitation)) %>%\n  ggplot(aes(x = report_date)) +\n  geom_point(aes(y = total_precipitation), color = td_colors$nice$spanish_blue)\n\n\n\n\nThe snow_on_ground values:\n\ndaily_climate %>%\n  count(snow_on_ground) %>%\n  arrange(desc(is.na(snow_on_ground))) %>%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\ndaily_climate %>%\n  filter(!is.na(snow_on_ground)) %>%\n  ggplot(aes(x = report_date)) +\n  geom_point(aes(y = snow_on_ground), color = td_colors$nice$spanish_blue)\n\n\n\n\nThe speed_max_gust values (in km/h):\n\ndaily_climate %>%\n  count(speed_max_gust, speed_max_gust_flag) %>%\n  arrange(desc(is.na(speed_max_gust))) %>%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\ndaily_climate %>%\n  filter(!is.na(speed_max_gust)) %>%\n  ggplot(aes(x = report_date)) +\n  geom_point(aes(y = speed_max_gust), color = td_colors$nice$emerald)"
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#combining-the-data",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#combining-the-data",
    "title": "Predicting bike ridership: getting the data",
    "section": "Combining the data",
    "text": "Combining the data\nNow I’ll combine the two data sets (joining on the date), only taking the most useful variables from the climate report (temperature, precipitation, wind speed, snow):\n\nbike_counts_climate <- bike_data_daily_counts %>%\n  left_join(\n    daily_climate %>%\n      select(report_date, mean_temperature, total_precipitation,\n             speed_max_gust, snow_on_ground),\n    by = c(\"count_date\" = \"report_date\")\n  )\nglimpse(bike_counts_climate)\n\nRows: 2,840\nColumns: 9\n$ site_name           <chr> \"Dartmouth Harbourfront Greenway\", \"Dartmouth Harb…\n$ installation_date   <date> 2021-07-08, 2021-07-08, 2021-07-08, 2021-07-08, 2…\n$ count_date          <date> 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2…\n$ n_records           <int> 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48…\n$ n_bikes             <int> 130, 54, 180, 245, 208, 250, 182, 106, 152, 257, 1…\n$ mean_temperature    <dbl> 18.2, 17.6, 21.0, 21.0, 20.6, 18.6, 17.7, 20.2, 20…\n$ total_precipitation <dbl> 0.6, 10.0, 0.4, 0.0, 0.0, 0.0, 0.0, 11.6, 0.2, 0.4…\n$ speed_max_gust      <int> NA, 54, 56, NA, NA, NA, NA, 32, 37, NA, NA, NA, NA…\n$ snow_on_ground      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nVisualize the missing climate data:\n\nbike_counts_climate %>%\n  distinct(count_date, mean_temperature, total_precipitation,\n           speed_max_gust, snow_on_ground) %>%\n  mutate(across(where(is.numeric), is.na)) %>%\n  pivot_longer(cols = -count_date) %>%\n  ggplot(aes(x = count_date, y = name)) +\n  geom_tile(aes(fill = value)) +\n  labs(y = NULL, x = NULL, fill = \"Missing\") +\n  scale_fill_manual(values = c(td_colors$nice$indigo_blue, \"gray80\")) +\n  scale_x_date(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  theme(legend.position = \"top\")\n\n\n\n\nQuite a bit of missing data, but we should have enough to make for an interesting analysis. Save the data:\n\nwrite_rds(bike_counts_climate, \"bike-ridership-data.rds\")\n\nIn my next post, I will use this data to develop and evaluate various prediction models."
  },
  {
    "objectID": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#reproducibility",
    "href": "posts/2022-04-27-predicting-bike-ridership-getting-the-data/predicting-bike-ridership-getting-the-data.html#reproducibility",
    "title": "Predicting bike ridership: getting the data",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-08-07\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [53358c7] 2022-08-06: Set fonts and trying out `renv.lock` for reproducibility\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html",
    "title": "Predicting bike ridership: developing a model",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(tidymodels)\nlibrary(tictoc)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#introduction",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#introduction",
    "title": "Predicting bike ridership: developing a model",
    "section": "Introduction",
    "text": "Introduction\nIn my last post, I retrieved, explored, and prepared bike counter data from Halifax, Nova Scotia. I also got some historical weather data to go along with it. Here, I will further explore the data, engineer some features, and fit and evaluate many models to predict bike ridership at different sites around the city."
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#exploratory-data-analysis",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#exploratory-data-analysis",
    "title": "Predicting bike ridership: developing a model",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nImport the data:\n\nbike_ridership <- read_rds(\"bike-ridership-data.rds\")\n\n\nglimpse(bike_ridership)\n\nRows: 2,840\nColumns: 9\n$ site_name           <chr> \"Dartmouth Harbourfront Greenway\", \"Dartmouth Harb…\n$ installation_date   <date> 2021-07-08, 2021-07-08, 2021-07-08, 2021-07-08, 2…\n$ count_date          <date> 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2…\n$ n_records           <int> 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48…\n$ n_bikes             <int> 130, 54, 180, 245, 208, 250, 182, 106, 152, 257, 1…\n$ mean_temperature    <dbl> 18.2, 17.6, 21.0, 21.0, 20.6, 18.6, 17.7, 20.2, 20…\n$ total_precipitation <dbl> 0.6, 10.0, 0.4, 0.0, 0.0, 0.0, 0.0, 11.6, 0.2, 0.4…\n$ speed_max_gust      <int> NA, 54, 56, NA, NA, NA, NA, 32, 37, NA, NA, NA, NA…\n$ snow_on_ground      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nThis data consists of daily bike counts (n_bikes) over a 24 hour period (count_date) recorded at 5 sites (site_name) around the city. In the original data, counts are recorded every hour which is reflected by the n_records variable:\n\nbike_ridership %>%\n  count(site_name, n_records) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      site_name\n      n_records\n      n\n    \n  \n  \n    Dartmouth Harbourfront Greenway\n8\n1\n    Dartmouth Harbourfront Greenway\n48\n291\n    Hollis St\n4\n1\n    Hollis St\n24\n656\n    South Park St\n8\n1\n    South Park St\n48\n884\n    Vernon St\n8\n1\n    Vernon St\n48\n502\n    Windsor St\n8\n1\n    Windsor St\n48\n502\n  \n  \n  \n\n\n\n\nAll except the Hollis St site have two channels (northbound and southbound), which is why n_records = 48. The entries with fewer n_records reflect the time of day that the data was extracted:\n\nbike_ridership %>%\n  filter(n_records < 24) %>%\n  select(site_name, count_date, n_records) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      site_name\n      count_date\n      n_records\n    \n  \n  \n    Dartmouth Harbourfront Greenway\n2022-04-25\n8\n    Hollis St\n2022-04-25\n4\n    South Park St\n2022-04-25\n8\n    Vernon St\n2022-04-25\n8\n    Windsor St\n2022-04-25\n8\n  \n  \n  \n\n\n\n\nFor this analysis, I will exclude this incomplete day:\n\nbike_ridership <- bike_ridership %>% filter(n_records > 8)\n\nThe date ranges for each site:\n\nbike_ridership %>%\n  group_by(site_name) %>%\n  summarise(\n    min_date = min(count_date), max_date = max(count_date),\n    n_days = n(), .groups = \"drop\"\n  ) %>%\n  mutate(\n    site_name = fct_reorder(site_name, min_date),\n    n_days_label = ifelse(site_name == levels(site_name)[1],\n                          str_c(\"n_days = \", n_days), n_days),\n    midpoint_date = min_date + n_days / 2\n  ) %>%\n  ggplot(aes(y = site_name, color = site_name)) +\n  geom_linerange(aes(xmin = min_date, xmax = max_date)) +\n  geom_point(aes(x = min_date)) +\n  geom_point(aes(x = max_date)) +\n  geom_text(aes(label = n_days_label, x = midpoint_date), vjust = -0.5) +\n  scale_y_discrete(labels = ~ str_wrap(., width = 15)) +\n  labs(y = NULL, x = \"min_date -> max_date\") +\n  theme(legend.position = \"none\")\n\n\n\n\nBefore any more EDA, I’ll split the data into training and testing sets (and only look at the training data going forward). For time series data, there is the rsample::initial_time_split() function, which I’ll use to make a 70-30 split:\n\n# Need to order by time to properly use time split\nbike_ridership <- bike_ridership %>% arrange(count_date, site_name)\n\nbike_split <- initial_time_split(bike_ridership, prop = 0.7)\n\nbike_train <- training(bike_split)\nbike_test <- testing(bike_split)\n\nbind_rows(\n  train = bike_train, test = bike_test, .id = \"data_set\"\n) %>%\n  group_by(data_set, site_name) %>%\n  summarise(\n    min_date = min(count_date), max_date = max(count_date),\n    n_days = n(), midpoint_date = min_date + n_days / 2,\n    .groups = \"drop\"\n  ) %>%\n  ggplot(aes(y = fct_reorder(site_name, min_date), color = data_set)) +\n  geom_linerange(aes(xmin = min_date, xmax = max_date),\n                 position = position_dodge(0.2)) +\n  geom_point(aes(x = min_date), position = position_dodge(0.2)) +\n  geom_point(aes(x = max_date), position = position_dodge(0.2)) +\n  geom_text(aes(label = n_days, x = midpoint_date), vjust = -0.5,\n            position = position_dodge(0.2), show.legend = FALSE) +\n  labs(x = \"date range\", y = NULL, color = NULL)\n\n\n\n\nIt might make more sense to stratify by site_name so that there is a 70-30 split in each site.1 For now, I’m using a simpler approach to split into the first 70% and 30% of the data.\nFor each site, the distribution of daily n_bikes:\n\nbike_train %>%\n  ggplot(aes(x = n_bikes, fill = site_name)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~ str_trunc(site_name, 25)) +\n  theme(legend.position = \"none\")\n\n\n\n\nThe South Park St site appears bimodal, which I noted in part 1 was likely due to the addition of protected bike lanes in 2021. This can be seen more clearly in the trend over time:\n\nbike_train %>%\n  ggplot(aes(x = count_date, y = n_bikes, color = site_name)) +\n  geom_line() +\n  facet_wrap(~ site_name, ncol = 1) +\n  theme(legend.position = \"none\")\n\n\n\n\nAs you would expect for data in the same city, bike counters between sites are very highly correlated, which I can visualize:\n\nbike_train %>%\n  transmute(count_date, n_bikes1 = n_bikes,\n            site_name1 = factor(str_trunc(site_name, 10))) %>%\n  left_join(., rename(., n_bikes2 = n_bikes1, site_name2 = site_name1),\n            by = \"count_date\") %>%\n  filter(as.numeric(site_name1) < as.numeric(site_name2)) %>%\n  ggplot(aes(x = n_bikes1, y = n_bikes2)) +\n  geom_point(aes(color = site_name1), alpha = 0.3) +\n  facet_grid(site_name2 ~ site_name1) +\n  theme(legend.position = \"none\") +\n  dunnr::add_facet_borders()\n\n\n\n\nThe day of the week effect looks important:\n\nbike_train %>%\n  mutate(day_of_week = lubridate::wday(count_date, label = TRUE)) %>%\n  ggplot(aes(x = day_of_week, y = n_bikes)) +\n  geom_jitter(aes(color = site_name), height = 0, width = 0.2, alpha = 0.3) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  facet_wrap(~ str_trunc(site_name, 30)) +\n  theme(legend.position = \"none\") +\n  dunnr::add_facet_borders()\n\n\n\n\nAnother thing to consider is holidays. I can get Canadian holidays with the timeDate package (this is how recipes::step_holiday() works):\n\nlibrary(timeDate)\ncanada_holidays <-\n  timeDate::listHolidays(\n    pattern = \"^CA|^Christmas|^NewYears|Easter[Sun|Mon]|^GoodFriday|^CaRem\"\n  )\ncanada_holidays\n\n [1] \"CACanadaDay\"              \"CACivicProvincialHoliday\"\n [3] \"CALabourDay\"              \"CaRemembranceDay\"        \n [5] \"CAThanksgivingDay\"        \"CAVictoriaDay\"           \n [7] \"ChristmasDay\"             \"ChristmasEve\"            \n [9] \"EasterMonday\"             \"EasterSunday\"            \n[11] \"GoodFriday\"               \"NewYearsDay\"             \n\n\nThen get the dates for each across the years in the bike data:\n\ncanada_holiday_dates <- tibble(holiday = canada_holidays) %>%\n  crossing(year = 2019:2022) %>%\n  mutate(\n    holiday_date = map2(\n      year, holiday,\n      ~ as.Date(timeDate::holiday(.x, .y)@Data)\n    )\n  ) %>%\n  unnest(holiday_date)\n\ncanada_holiday_dates %>%\n  select(holiday, holiday_date) %>%\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\nOnly day I can think is missing from this list is Family Day (Heritage Day in Nova Scotia) which is the third Monday in February. Visualize the effect of these holidays on bike ridership by plotting n_bikes in a 2 week window around the holidays (showing just the South Park St site for this plot):\n\ncanada_holiday_dates %>%\n  filter(holiday_date %in% unique(bike_train$count_date)) %>%\n  mutate(\n    date_window = map(holiday_date, ~ seq.Date(.x - 7, .x + 7, by = \"1 day\"))\n  ) %>%\n  unnest(date_window) %>%\n  left_join(\n    bike_train, by = c(\"date_window\" = \"count_date\")\n  ) %>%\n  mutate(is_holiday = holiday_date == date_window) %>%\n  group_by(holiday) %>%\n  mutate(day_from_holiday = as.numeric(holiday_date - date_window)) %>%\n  ungroup() %>%\n  filter(site_name == \"South Park St\") %>%\n  ggplot(aes(x = day_from_holiday, y = n_bikes,\n             group = factor(year))) +\n  geom_line() +\n  geom_vline(xintercept = 0, lty = 2) +\n  geom_point(aes(color = factor(year))) +\n  facet_wrap(~ holiday) +\n  labs(color = \"year\") +\n  dunnr::add_facet_borders() +\n  theme(legend.position = \"top\")\n\n\n\n\nThe only holidays with a clear drop in ridership are Labour Day and the Civic Holiday (called Natal Day in NS) in 2021. Victoria Day seems to have the opposite effect. The Good Friday, Easter Sunday and Easter Monday holidays are obviously overlapping, and the n_bikes trend is a bit of a mess, but I can see an indication to the left of Good Friday that there may be a drop in ridership going into that weekend.\nOne thing to note is that the first, middle and last points correspond to the same day of the week, and the middle point in this set is usually lower than those two point, so holidays may be useful features in conjunction with day of the week.\nThe weather variables have varying levels of completeness:\n\n# Separate out the weather data\nweather_data <- bike_train %>%\n  distinct(count_date, mean_temperature, total_precipitation,\n           speed_max_gust, snow_on_ground)\n\nweather_data %>%\n  mutate(across(where(is.numeric), is.na)) %>%\n  pivot_longer(cols = -count_date) %>%\n  ggplot(aes(x = count_date, y = name)) +\n  geom_tile(aes(fill = value)) +\n  labs(y = NULL, x = NULL, fill = \"Missing\") +\n  scale_fill_manual(values = c(td_colors$nice$indigo_blue, \"gray80\")) +\n  scale_x_date(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  theme(legend.position = \"top\")\n\n\n\n\nThe distributions:\n\nweather_data %>%\n  pivot_longer(cols = -count_date) %>%\n  filter(!is.na(value)) %>%\n  ggplot(aes(x = value, fill = name)) +\n  geom_histogram(bins = 20) +\n  scale_y_continuous(expand = c(0, 0)) +\n  facet_wrap(~ name, nrow = 1, scales = \"free\") +\n  theme(legend.position = \"none\")\n\n\n\n\nFor non-missing cases, plot the pairwise relationships:\n\nweather_data %>%\n  pivot_longer(cols = -count_date, names_to = \"var1\", values_to = \"val1\") %>%\n  mutate(var1 = factor(var1)) %>%\n  left_join(., rename(., var2 = var1, val2 = val1),\n            by = \"count_date\") %>%\n  filter(!is.na(val1), !is.na(val2),\n         # Use numeric factor labels to remove duplicates\n         as.numeric(var1) < as.numeric(var2)) %>%\n  ggplot(aes(x = val1, y = val2, color = var1)) +\n  geom_point(alpha = 0.5) +\n  facet_grid(var2 ~ var1, scales = \"free\") +\n  theme(legend.position = \"none\") +\n  dunnr::add_facet_borders()\n\n\n\n\nThe clearest relationship to me is unsurprising: increasing mean_temperature is associated with decreasing snow_on_ground (top left plot).\nVisualize relationships with n_bikes:\n\nbike_train %>%\n  pivot_longer(\n    cols = c(mean_temperature, total_precipitation,\n             speed_max_gust, snow_on_ground),\n    names_to = \"var\", values_to = \"val\"\n  ) %>%\n  filter(!is.na(val)) %>%\n  ggplot(aes(x = val, y = n_bikes)) +\n  geom_point(aes(color = str_trunc(site_name, 15)), alpha = 0.4) +\n  facet_wrap(~ var, nrow = 2, scales = \"free_x\") +\n  dunnr::add_facet_borders() +\n  labs(x = NULL, color = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nAll of the weather variables seem to be have a relationship with n_bikes. In terms of predictive value, mean_temperature looks like it might be the most useful, and speed_max_gust the least."
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#feature-engineering",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#feature-engineering",
    "title": "Predicting bike ridership: developing a model",
    "section": "Feature engineering",
    "text": "Feature engineering\nFrom my EDA, I decided I want try including all 4 weather variables to predict bike ridership. This will require imputation of missing values for all 4, which I’ll attempt here.\nAdd some more time variables for working with the weather_data:\n\nweather_data <- weather_data %>%\n  mutate(count_year = lubridate::year(count_date),\n         # Day number relative to earliest date\n         count_day = as.numeric(count_date - min(count_date)),\n         # Day of the year, from 1 to 365\n         count_yday = lubridate::yday(count_date))\n\n\nTemperature\nThe mean_temperature variable is missing 3% of values. Visualize the trend over time:\n\np1 <- weather_data %>%\n  filter(!is.na(mean_temperature)) %>%\n  ggplot(aes(x = count_date, y = mean_temperature)) +\n  geom_line(aes(color = factor(count_year))) +\n  scale_color_viridis_d(\"year\") +\n  scale_x_date(\"date\", date_breaks = \"1 year\")\np2 <- weather_data %>%\n  filter(!is.na(mean_temperature)) %>%\n  ggplot(aes(x = count_yday, y = mean_temperature)) +\n  geom_line(aes(color = factor(count_year))) +\n  scale_color_viridis_d(\"year\") +\n  scale_x_continuous(\"day of year\", breaks = c(0, 90, 180, 270, 365)) +\n  labs(y = NULL)\np1 + p2 +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"top\")\n\n\n\n\nThe cyclic nature makes it a good candidate for smoothing splines. As a starting point, try a natural spline with 5 knots on the count_yday variable:\n\nlibrary(splines)\nlm_temperature <- \n  lm(mean_temperature ~ ns(count_yday, knots = 5),\n      data = filter(weather_data, !is.na(mean_temperature)))\n\np1 +\n  geom_line(\n    data = augment(lm_temperature, newdata = weather_data),\n    aes(y = .fitted), size = 1\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\nWe can obviously do a lot better, especially at the yearly boundaries. I’ll fit the data using a generalized additive model (GAM) with the mgcv package.2 For the count_yday variable (ranges from 1-365), I’ll make sure that there is no discontinuity between year by using a cyclic spline (bs = \"cc\"). I’ll also include a smoothing term of count_day which will capture the trend across years.\n\nlibrary(mgcv)\n\ngam_temperature <-\n  gam(mean_temperature ~ s(count_yday, bs = \"cc\", k = 12) + s(count_day),\n      data = filter(weather_data, !is.na(mean_temperature)))\nplot(gam_temperature, pages = 1, shade = TRUE)\n\n\n\n\nThe left plot shows the seasonal trend within a year (note the lines would connect at count_yday = 1 and 365), and the right plot shows the increase in average temperature throughout time (across years) after accounting for the seasonal effect. Overlay the fit:\n\np1 +\n  geom_line(\n    data = augment(gam_temperature, newdata = weather_data),\n    aes(y = .fitted), size = 1\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\nI’m pretty happy with that. I’ll use predictions from this GAM to impute missing daily temperatures.\n\n\nPrecipitation and snow\nThe total_precipitation variable is missing 36% of values; 76% for snow_on_ground.\nThe total_precipitation distribution:\n\np1 <- weather_data %>%\n  mutate(total_precipitation = replace_na(total_precipitation, -5)) %>%\n  ggplot(aes(x = count_date, y = total_precipitation)) +\n  geom_point(alpha = 0.5) +\n  scale_y_continuous(breaks = c(-5, 0, 20, 40, 60),\n                     labels = c(\"missing\", 0, 20, 40, 60))\np1\n\n\n\n\nThis pattern of missing data during winter months makes me think that the total_precipitation is actually total rainfall, i.e. snowfall is not counted. I’m going to impute the missing values with 0 during pre-processing, which is admittedly a poor approximation of the truth.\nThe snow_on_ground distribution:\n\np2 <- weather_data %>%\n  mutate(snow_on_ground = replace_na(snow_on_ground, -2)) %>%\n  ggplot(aes(x = count_date, y = snow_on_ground)) +\n  geom_point(alpha = 0.5) +\n  scale_y_continuous(breaks = c(-2, 0, 10, 20),\n                     labels = c(\"missing\", 0, 10, 20))\np2\n\n\n\n\nI’ll also impute zero for missing snow_on_ground, which I’m a lot more confident doing here because most of the missing values occur during non-winter months. A more careful approach might involve imputing 0 during non-winter months that I’m certain would have no snow on the ground, then modeling the winter months with something like a zero-inflated Poisson model.\n\n\nWind speed\nThe speed_max_gust variable is the daily maximum wind speed in km/h, and has 41% missing values.\n\nmean_speed <- mean(weather_data$speed_max_gust, na.rm = TRUE)\n\nweather_data %>%\n  mutate(\n    speed_max_gust = replace_na(speed_max_gust, 20)\n  ) %>%\n  ggplot(aes(x = count_date, y = speed_max_gust)) +\n  geom_line(data = . %>% filter(speed_max_gust > 20)) +\n  geom_smooth(data = . %>% filter(speed_max_gust > 20),\n              method = \"loess\", formula = \"y ~ x\",\n              color = td_colors$nice$spanish_blue) +\n  geom_hline(yintercept = mean_speed,\n             color = td_colors$nice$opera_mauve, size = 1, lty = 2) +\n  geom_jitter(data = . %>% filter(speed_max_gust == 20),\n              width = 0, alpha = 0.5) +\n  scale_y_continuous(breaks = c(mean_speed, 20, 40, 60, 80),\n                     labels = c(\"mean_speed\", \"missing\", 40, 60, 80))\n\n\n\n\nRelative to the noise, the time trends are pretty minor, and the missing data looks to be missing mostly at random. I’ll just impute using the mean speed.\n\n\nLagged counts\nAs a time series data set, it would be careless to not account for past data when predicting future data, so I’ll include lagged n_bikes values. Investigate the correlation in n_bikes for values lagged by 1, 2 and 3 days, and by 1 and 2 weeks (because they are the same day of the week):\n\nbike_train_lag <- bike_train %>%\n  arrange(site_name, count_date) %>%\n  group_by(site_name) %>%\n  mutate(\n    n_bikes_lag_1 = lag(n_bikes, 1),\n    n_bikes_lag_2 = lag(n_bikes, 2),\n    n_bikes_lag_3 = lag(n_bikes, 3),\n    n_bikes_lag_7 = lag(n_bikes, 7),\n    n_bikes_lag_14 = lag(n_bikes, 14)\n  )\nbike_train_lag %>%\n  select(site_name, count_date, starts_with(\"n_bikes\")) %>%\n  pivot_longer(cols = matches(\"n_bikes_lag\"),\n               names_to = \"lag_days\", values_to = \"n_bikes_lag\") %>%\n  filter(!is.na(n_bikes_lag)) %>%\n  mutate(lag_days = str_extract(lag_days, \"\\\\d+\") %>% as.integer()) %>%\n  group_by(site_name, lag_days) %>%\n  mutate(corr_coef = cor(n_bikes, n_bikes_lag)) %>%\n  ggplot(aes(x = n_bikes_lag, y = n_bikes, color = site_name)) +\n  geom_point(alpha = 0.2) +\n  geom_label(data = . %>% distinct(n_bikes_lag, site_name, corr_coef),\n             aes(label = round(corr_coef, 2), x = 500, y = 200)) +\n  geom_abline(slope = 1) +\n  facet_grid(str_trunc(site_name, 8) ~ factor(lag_days)) +\n  dunnr::add_facet_borders() +\n  theme(legend.position = \"none\")\n\n\n\n\nThe 7- and 14-day lagged values are correlated just as strongly (in some cases stronger) than the other options. This is great news because I only want to include a single lag variable, and using the 14th day lag means I can forecast 14 days ahead.\nIn order to use 14-day lag in a tidymodels workflow, I need to add it to the data myself. The step_lag() function won’t allow the outcome n_bikes to be lagged, because any new data won’t have an n_bikes variable to use. See the warning in this section of the Tidy Modeling with R book. Add the n_bikes_lag_14 predictor, and exclude any values without it:\n\nbike_ridership <- bike_ridership %>%\n  group_by(site_name) %>%\n  mutate(n_bikes_lag_14 = lag(n_bikes, 14)) %>%\n  ungroup() %>%\n  filter(!is.na(n_bikes_lag_14))\n\nWhile I’m at it, I’ll impute the missing mean_temperature values with the seasonal GAM.\n\nbike_ridership <- bike_ridership %>%\n  mutate(\n    count_day = as.numeric(count_date - min(count_date)),\n    count_yday = lubridate::yday(count_date),\n  ) %>%\n  bind_cols(pred = predict(gam_temperature, newdata = .)) %>%\n  mutate(\n    mean_temperature = ifelse(is.na(mean_temperature), pred,\n                              mean_temperature)\n  ) %>%\n  select(-count_day, -count_yday, -pred)\n\nThis means I’ll have to re-split the data into training and testing (initial_time_split() isn’t random, so doesn’t require setting the seed):\n\nbike_ridership <- bike_ridership %>% arrange(count_date, site_name)\n\nbike_split <- initial_time_split(bike_ridership, prop = 0.7)\n\nbike_train <- training(bike_split)\nbike_test <- testing(bike_split)"
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#modeling",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#modeling",
    "title": "Predicting bike ridership: developing a model",
    "section": "Modeling",
    "text": "Modeling\nRegister parallel computing:\n\nn_cores <- parallel::detectCores(logical = FALSE)\nlibrary(doParallel)\ncl <- makePSOCKcluster(n_cores - 1)\nregisterDoParallel(cl)\n# This extra step makes sure the parallel workers have access to the\n#  `tidyr::replace_na()` function during pre-processing, which I use later on\n# See this issue: https://github.com/tidymodels/tune/issues/364\nparallel::clusterExport(cl, c(\"replace_na\"))\n\n\nResampling and metrics\nFor re-sampling, I will use sliding_period() to break up the data into 14 months of data (chosen to give 10 resamples) for analysis and 1 month for assessment:\n\nbike_resamples <-\n  sliding_period(bike_train, index = count_date,\n                 period = \"month\", lookback = 12, assess_stop = 2)\n\nVisualize the resamples:\n\nbind_rows(\n  analysis_set = map_dfr(bike_resamples$splits, analysis, .id = \"i\"),\n  assessment_set = map_dfr(bike_resamples$splits, assessment, .id = \"i\"),\n  .id = \"data_set\"\n) %>%\n  mutate(i = as.integer(i)) %>%\n  group_by(i, data_set) %>%\n  summarise(\n    min_date = min(count_date), max_date = max(count_date),\n    n_days = n(), midpoint_date = min_date + n_days / 2,\n    .groups = \"drop\"\n  ) %>%\n  ggplot(aes(y = factor(i), color = data_set)) +\n  geom_linerange(aes(xmin = min_date, xmax = max_date),\n                 position = position_dodge(0.3)) +\n  geom_point(aes(x = min_date), position = position_dodge(0.3)) +\n  geom_point(aes(x = max_date), position = position_dodge(0.3)) +\n  labs(x = \"date range\", y = NULL, color = NULL)\n\n\n\n\nI’ll define a set of metrics to use here as well:\n\nbike_metrics <- metric_set(rmse, rsq, mase, poisson_log_loss)\n\nThe Poisson log loss is a new one to me, that was recently added to yardstick. I’ll include it just for kicks, but I will choose my final model with mase, the mean absolute scaled error, which was introduced by Hyndman and Koehler (Hyndman and Koehler 2006). The MASE involves dividing the absolute forecast error (\\(|y_i - \\hat{y}_i|\\)) by absolute naive forecast error (which involves predicting with the last observed value). The main advantage of this is that it is scale invariant. Mean absolute percentage error (MAPE) is the typical scale-invariant choice in regression problems, but the MASE avoids dividing by n_bikes = 0 (of which there are about 20 in this data set). It also has a straightforward interpretation: values greater than one indicate a worse forecast than the naive method, and values less indicate better.\n\n\nGeneralized linear models\nThe first models I will try are simple linear regression and Poisson regression, but I will test out a few different pre-processing/feature combinations. I would prefer to use negative binomial regression instead of Poisson to account for overdispersion (see aside), but it hasn’t been implemented in parsnip yet.\n\nlm_spec <- linear_reg(engine = \"lm\")\nlibrary(poissonreg) # This wrapper package is required to use `poisson_reg()`\npoisson_spec <- poisson_reg(engine = \"glm\")\n\n\n\nOver-dispersion in n_bikes (variance much higher than the mean):\n\nbike_train %>%\n  group_by(site_name) %>%\n  summarise(mean_n_bikes = mean(n_bikes), var_n_bikes = var(n_bikes))\n\n# A tibble: 5 × 3\n  site_name                       mean_n_bikes var_n_bikes\n  <chr>                                  <dbl>       <dbl>\n1 Dartmouth Harbourfront Greenway        145.        3121.\n2 Hollis St                               77.0       2180.\n3 South Park St                          174.       26594.\n4 Vernon St                              212.       18080.\n5 Windsor St                              96.3       3413.\n\n\nFor my base pre-processing, I’ll include just site_name and n_bikes_lag_14:\n\nglm_recipe <-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14,\n         data = bike_train) %>%\n  add_role(count_date, new_role = \"date_variable\") %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\nThe first extension of this recipe will include date variables. I’ll add day of week as categorical, day of year as numerical (and tune a natural spline function), year as numerical and Canadian holidays as categorical.\n\nglm_recipe_date <-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14,\n         data = bike_train) %>%\n  add_role(count_date, new_role = \"date_variable\") %>%\n  step_date(count_date, features = c(\"dow\", \"doy\", \"year\"),\n            label = TRUE, ordinal = FALSE) %>%\n  step_ns(count_date_doy, deg_free = tune()) %>%\n  step_holiday(count_date, holidays = canada_holidays) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\nI’ll consider the weather variables, with the imputations discussed in the feature engineering section, separately:\n\nglm_recipe_weather <-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14 + mean_temperature +\n           total_precipitation + speed_max_gust + snow_on_ground,\n         data = bike_train) %>%\n  add_role(count_date, new_role = \"date_variable\") %>%\n  step_impute_mean(speed_max_gust) %>%\n  # Impute these missing values with zero\n  step_mutate_at(c(total_precipitation, snow_on_ground),\n                 fn = ~ replace_na(., 0)) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\nAnd lastly, all the features together:\n\nglm_recipe_date_weather <-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14 + mean_temperature +\n           total_precipitation + speed_max_gust + snow_on_ground,\n         data = bike_train) %>%\n  add_role(count_date, new_role = \"date_variable\") %>%\n  step_date(count_date, features = c(\"dow\", \"doy\", \"year\"),\n            label = TRUE, ordinal = FALSE) %>%\n  step_ns(count_date_doy, deg_free = tune()) %>%\n  step_holiday(count_date, holidays = canada_holidays) %>%\n  step_impute_mean(speed_max_gust) %>%\n  step_mutate_at(c(total_precipitation, snow_on_ground),\n                 fn = ~ replace_na(., 0)) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\nPut these pre-processing recipes and the model specifications into a workflow_set():\n\nglm_wf_set <- workflow_set(\n  preproc = list(base = glm_recipe,\n                 date = glm_recipe_date, weather = glm_recipe_weather,\n                 date_weather = glm_recipe_date_weather),\n  models = list(linear_reg = lm_spec, poisson_reg = poisson_spec),\n  cross = TRUE\n)\n\nNow fit the resamples using each combination of model specification and pre-processing recipe:\n\ntic()\nglm_wf_set_res <- workflow_map(\n  glm_wf_set,\n  \"tune_grid\",\n  # I'll try just a few `deg_free` in the spline term\n  grid = grid_regular(deg_free(range = c(4, 7)), levels = 4),\n  resamples = bike_resamples, metrics = bike_metrics\n)\ntoc()\n\n39.89 sec elapsed\n\n\nFor plotting the results this set of workflows, I’ll use a custom plotting function with rank_results():\n\nplot_wf_set_metrics <- function(wf_set_res, rank_metric = \"mase\") {\n  rank_results(wf_set_res, rank_metric = rank_metric) %>%\n    mutate(preproc = str_remove(wflow_id, paste0(\"_\", model))) %>%\n    ggplot(aes(x = rank, y = mean, color = model, shape = preproc)) +\n    geom_point(size = 2) +\n    geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err),\n                  width = 0.2) +\n    facet_wrap(~ .metric, scales = \"free_y\")\n  \n}\nplot_wf_set_metrics(glm_wf_set_res)\n\n\n\n\n\n\nI’m using my own function because the workflowsets::autoplot() function doesn’t distinguish preprocessor recipes using the names provided to the preproc argument – everything just gets the name “recipe”:\n\nrank_results(glm_wf_set_res) %>%\n  distinct(wflow_id, model, preprocessor)\n\n# A tibble: 8 × 3\n  wflow_id                 preprocessor model      \n  <chr>                    <chr>        <chr>      \n1 date_weather_poisson_reg recipe       poisson_reg\n2 date_weather_linear_reg  recipe       linear_reg \n3 weather_linear_reg       recipe       linear_reg \n4 date_linear_reg          recipe       linear_reg \n5 base_linear_reg          recipe       linear_reg \n6 date_poisson_reg         recipe       poisson_reg\n7 weather_poisson_reg      recipe       poisson_reg\n8 base_poisson_reg         recipe       poisson_reg\n\n\nMy function extracts the recipe name from wflow_id.\nAcross the board, the model will all the features (date and weather predictors; indicated by the square symbols) are best, and Poisson regression slightly outperforms linear. Here are the models ranked by MASE:3\n\nrank_results(glm_wf_set_res, select_best = TRUE, rank_metric = \"mase\") %>%\n  filter(.metric == \"mase\") %>%\n  select(rank, wflow_id, .config, mase = mean, std_err) %>%\n  gt() %>%\n  fmt_number(columns = c(mase, std_err), decimals = 3)\n\n\n\n\n\n  \n  \n    \n      rank\n      wflow_id\n      .config\n      mase\n      std_err\n    \n  \n  \n    1\ndate_weather_poisson_reg\nPreprocessor3_Model1\n0.547\n0.056\n    2\ndate_weather_linear_reg\nPreprocessor3_Model1\n0.577\n0.075\n    3\ndate_linear_reg\nPreprocessor2_Model1\n0.604\n0.052\n    4\nweather_linear_reg\nPreprocessor1_Model1\n0.622\n0.067\n    5\ndate_poisson_reg\nPreprocessor2_Model1\n0.626\n0.064\n    6\nbase_linear_reg\nPreprocessor1_Model1\n0.635\n0.055\n    7\nweather_poisson_reg\nPreprocessor1_Model1\n0.670\n0.080\n    8\nbase_poisson_reg\nPreprocessor1_Model1\n0.760\n0.055\n  \n  \n  \n\n\n\n\nSo our best workflow has the id date_weather_poisson_reg4 with the .config “Preprocessor3_Model1” which refers to a specific spline degree from our tuning of the count_date_doy feature. I can check out the results of the tuning with autoplot():\n\nautoplot(glm_wf_set_res, id = \"date_weather_poisson_reg\") +\n  facet_wrap(~ .metric, nrow = 2, scales = \"free_y\")\n\n\n\n\nThe polynomial of degree 6 did best by MASE, which I’ll use to finalize the workflow and fit to the full training set:\n\nglm_poisson_workflow <- finalize_workflow(\n  extract_workflow(glm_wf_set_res, \"date_weather_poisson_reg\"),\n  extract_workflow_set_result(glm_wf_set_res, \"date_weather_poisson_reg\") %>%\n    select_best(metric = \"mase\")\n)\nglm_poisson_fit <- glm_poisson_workflow %>% fit(bike_train)\n\n\nModel interpretation\nAn advantage of using a generalized linear model is that they are easy to interpret. A simple way to estimate variable important in a GLM is to look at the absolute value of the \\(t\\)-statistic (statistic in the below table) – here are the top 5:\n\npoisson_coefs <- tidy(glm_poisson_fit) %>%\n  arrange(desc(abs(statistic))) %>%\n  mutate(estimate = signif(estimate, 3), std.error = signif(estimate, 2),\n         statistic = round(statistic, 1), p.value = scales::pvalue(p.value))\nhead(poisson_coefs, 5) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    total_precipitation\n-0.0384\n-0.038\n-84.6\n<0.001\n    site_name_South.Park.St\n0.6890\n0.690\n68.6\n<0.001\n    site_name_Vernon.St\n0.5490\n0.550\n57.8\n<0.001\n    mean_temperature\n0.0365\n0.036\n53.3\n<0.001\n    n_bikes_lag_14\n0.0010\n0.001\n48.2\n<0.001\n  \n  \n  \n\n\n\n\nA couple of the weather variables are among the most influential in predicting n_bikes. Here are all the weather coefficients:\n\npoisson_coefs %>%\n  filter(str_detect(term, \"temperature|precip|max_gust|snow\")) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    total_precipitation\n-0.03840\n-0.0380\n-84.6\n<0.001\n    mean_temperature\n0.03650\n0.0360\n53.3\n<0.001\n    snow_on_ground\n-0.05360\n-0.0540\n-28.9\n<0.001\n    speed_max_gust\n-0.00735\n-0.0074\n-19.6\n<0.001\n  \n  \n  \n\n\n\n\nSince this is a Poisson model, the link function (non-linear relationship between the outcome and the predictors) is the logarithm:\n\\[\n\\begin{align}\n\\log{n}_{\\text{bikes}} &= \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p \\\\\nn_{\\text{bikes}} &= \\text{exp}(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p) \\\\\n&= \\text{exp}(\\beta_0) \\text{exp}(\\beta_1 x_1) \\dots \\text{exp}(\\beta_p x_p) \\\\\n\\end{align}\n\\]\nSo the coefficients are interpreted as: for every one unit increase in \\(x_i\\), the expected value of \\(n_{\\text{bikes}}\\) changes by multiplicative factor of \\(\\text{exp}(\\beta_i)\\), holding all other predictors constant. Here are some plain-language interpretations of the weather coefficients:\n\nFor every 5°C increase in daily average temperature (mean_temperature), the expected value of n_bikes increases by 120%.\nFor every 10mm increase in daily rain (total_precipitation), the expected value of n_bikes decreases by 68%.\nFor every 10km/h increase in maximum wind speed (speed_max_gust), the expected value of n_bikes decreases by 93%.\nFor every 5cm of snow (snow_on_ground), the expected value of n_bikes decreases by 77%.\n\nA more thorough and formal analysis of these types of relationships should involve exploration of marginal effects (with a package like marginaleffects for example), but I’ll move on to other models.\n\n\n\nTree-based methods\nFor tree-based methods, I’ll use similar pre-processing as with GLM, except I won’t use a spline term (trees partition the feature space to capture non-linearity):\n\ntrees_recipe <-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14 + mean_temperature +\n           total_precipitation + speed_max_gust + snow_on_ground,\n         data = bike_train) %>%\n  update_role(count_date, new_role = \"date_variable\") %>%\n  step_date(count_date, features = c(\"dow\", \"doy\", \"year\"),\n            label = TRUE, ordinal = FALSE) %>%\n  step_holiday(count_date, holidays = canada_holidays) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_impute_mean(speed_max_gust) %>%\n  step_mutate_at(c(total_precipitation, snow_on_ground),\n                 fn = ~ replace_na(., 0)) %>%\n  step_zv(all_predictors())\n\n# XGBoost requires dummy variables\ntrees_recipe_dummy <- trees_recipe %>%\n  step_dummy(all_nominal_predictors())\n\nI’ll try a decision tree, a random forest, and an XGBoost model, each with hyperparameters indicated for tuning:\n\ndecision_spec <-\n  decision_tree(cost_complexity = tune(), tree_depth = tune(),\n                min_n = tune()) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\nrf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%\n  # Setting the `importance` parameter now lets me use `vip` later\n  set_engine(\"ranger\", importance = \"permutation\") %>%\n  set_mode(\"regression\")\n\nxgb_spec <- boost_tree(\n  mtry = tune(), trees = tune(), min_n = tune(),\n  tree_depth = tune(), learn_rate = tune()\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\ntrees_wf_set <- workflow_set(\n  preproc = list(trees_recipe = trees_recipe,\n                 trees_recipe = trees_recipe,\n                 trees_recipe_dummy = trees_recipe_dummy),\n  models = list(rf = rf_spec, decision = decision_spec, xgb = xgb_spec),\n  cross = FALSE\n)\n\nAs a first pass, I’ll let tune_grid() choose 10 parameter combinations automatically for each model (grid = 10):\n\nset.seed(225)\ntic()\ntrees_wf_set_res <- workflow_map(\n  trees_wf_set,\n  \"tune_grid\",\n  grid = 10, resamples = bike_resamples, metrics = bike_metrics,\n)\ntoc()\n\n100.33 sec elapsed\n\n\nVisualize the performance of the 30 workflows, ranked by MASE:\n\n# Don't need to use my custom function defined previously because I'm\n#  not using functionally different recipes for each model\nautoplot(trees_wf_set_res, rank_metric = \"mase\")\n\n\n\n\nAs I would have expected, the decision tree models generally perform worse than random forest models (which consist of multiple decision trees). The boosted tree models (which also consist of multiple decision trees, but differ in how they are built and combined) slightly outperform random forests, which is also expected.\nFor the three different tree-based model types, I’ll extract the best-performing workflows (by MASE) and fit to the full training set:\n\ndecision_workflow <- finalize_workflow(\n  extract_workflow(trees_wf_set_res, \"trees_recipe_decision\"),\n  extract_workflow_set_result(trees_wf_set_res, \"trees_recipe_decision\") %>%\n    select_best(metric = \"mase\")\n)\nrf_workflow <- finalize_workflow(\n  extract_workflow(trees_wf_set_res, \"trees_recipe_rf\"),\n  extract_workflow_set_result(trees_wf_set_res, \"trees_recipe_rf\") %>%\n    select_best(metric = \"mase\")\n)\nxgb_workflow <- finalize_workflow(\n  extract_workflow(trees_wf_set_res, \"trees_recipe_dummy_xgb\"),\n  extract_workflow_set_result(trees_wf_set_res, \"trees_recipe_dummy_xgb\") %>%\n    select_best(metric = \"mase\")\n)\n\ndecision_fit <- decision_workflow %>% fit(bike_train)\nrf_fit <- rf_workflow %>% fit(bike_train)\nxgb_fit <- xgb_workflow %>% fit(bike_train)\n\n\nModel interpretation\nDecision trees are the easiest of the three to interpret, but this decision tree is quite complicated. It has 44 leaf nodes (i.e. terminal nodes) with a depth of 6. The visualization (done with the rpart.plot package) is messy and hidden below:\n\n\nDecision tree plot\n\n\nextract_fit_engine(decision_fit) %>%\n  rpart.plot::rpart.plot(\n    # Using some options to try and make the tree more readable\n    fallen.leaves = FALSE, roundint = FALSE,\n    tweak = 5, type = 0, faclen = 10, clip.facs = TRUE, compress = FALSE\n  )\n\n\n\n\n\nThe nodes at the top of the tree are a good indicator of feature importance. Here, that was n_bikes_lag_14, followed by mean_temperature and total_precipitation.\nTo quantify the contribution of each feature in a tree-based model, we can calculate variable importance with the vip package. Plot the top 5 variables for all 3 models:\n\nlibrary(vip)\n\np1 <- extract_fit_engine(decision_fit) %>%\n  vip(num_features = 5) +\n  scale_y_continuous(NULL, expand = c(0, 0)) +\n  labs(subtitle = \"Decision tree\")\np2 <- extract_fit_engine(rf_fit) %>%\n  vip(num_features = 5) +\n  scale_y_continuous(NULL, expand = c(0, 0)) +\n  labs(subtitle = \"Random forest\")\np3 <- extract_fit_engine(xgb_fit) %>%\n  vip(num_features = 5) +\n  scale_y_continuous(NULL, expand = c(0, 0)) +\n  labs(subtitle = \"XGBoost\")\np1 + p2 + p3 +\n  plot_layout(ncol = 1)\n\n\n\n\nWe see that n_bikes_lag_14, mean_temperature, and site_name are important with all three models. Measures of time within (count_date_doy) and across (count_date_year) years are also important.\n\n\nXGBoost tuning\nSo far, I’ve only considered 10 candidate sets of hyperparameters for tuning each tree-based model. Let’s try 100 with XGBoost:\n\n# Get the number of predictors so I can set max number of predictors in `mtry()`\nbike_train_baked <- prep(trees_recipe_dummy) %>% bake(bike_train)\n\n# `grid_latin_hypercube()` is a space-filling parameter grid design that will\n#  efficiently cover the parameter space for me\nxgb_grid <- grid_latin_hypercube(\n  finalize(mtry(), select(bike_train_baked, -n_bikes)),\n  trees(), min_n(), tree_depth(), learn_rate(),\n  size = 100\n)\nxgb_grid\n\n# A tibble: 100 × 5\n    mtry trees min_n tree_depth learn_rate\n   <int> <int> <int>      <int>      <dbl>\n 1    14   300    20          8   4.27e- 4\n 2    30  1560     5         12   2.17e- 5\n 3    22  1723    30          2   1.37e- 2\n 4     2   268    21          2   6.42e-10\n 5     7   780    15         12   1.09e- 5\n 6    24   642    24          2   7.51e- 5\n 7     3  1066    36          3   8.06e- 9\n 8    13   725     4          6   1.94e- 2\n 9    18   574    40          9   5.55e- 7\n10    27  1988    38         13   7.08e- 9\n# … with 90 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWe have just one model and one preprocessor now, so put it into a single workflow:\n\nxgb_workflow_2 <- workflow() %>%\n  add_recipe(trees_recipe_dummy) %>%\n  add_model(xgb_spec)\n\nAnd tune:\n\nset.seed(2081)\ntic()\nxgb_tune <- tune_grid(\n  xgb_workflow_2, resamples = bike_resamples,\n  grid = xgb_grid, metrics = bike_metrics\n)\ntoc()\n\n580.23 sec elapsed\n\n\nHere are the metrics for the different candidate models:\n\nautoplot(xgb_tune)\n\n\n\n\nFinalize the workflow with the best hyperparameters and fit the full training set:\n\nxgb_workflow_2 <- finalize_workflow(\n  xgb_workflow_2, select_best(xgb_tune, metric = \"mase\")\n)\n\nxgb_fit_2 <- xgb_workflow_2 %>% fit(bike_train)\n\n\n\n\nSupport vector machines\nLastly, I will try some support vector machine regressions with linear, polynomial and radial basis function (RBF) kernels:\n\n# New recipe for the SVMs which require a `step_normalize()`\nsvm_recipe <-\n  recipe(\n    n_bikes ~ count_date + site_name + n_bikes_lag_14 + mean_temperature + \n      total_precipitation + speed_max_gust + snow_on_ground,\n    data = bike_train, \n  ) %>% \n  update_role(count_date, new_role = \"date_variable\") %>%\n  step_date(count_date, features = c(\"dow\", \"doy\", \"year\"),\n            label = TRUE, ordinal = FALSE) %>%\n  step_holiday(count_date, holidays = canada_holidays) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_impute_mean(speed_max_gust) %>%\n  step_mutate_at(c(total_precipitation, snow_on_ground),\n                 fn = ~ replace_na(., 0)) %>%\n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors())\n\nsvm_linear_spec <- svm_linear(cost = tune(), margin = tune()) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"kernlab\")\n\nsvm_poly_spec <- svm_poly(cost = tune(), margin = tune(),\n                          scale_factor = tune(), degree = tune()) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"kernlab\")\n\nsvm_rbf_spec <-  svm_rbf(cost = tune(), rbf_sigma = tune()) %>% \n  set_mode(\"regression\") %>%\n  set_engine(\"kernlab\")\n\nsvm_wf_set <- workflow_set(\n  preproc = list(svm_recipe),\n  models = list(linear = svm_linear_spec, poly = svm_poly_spec,\n                rbf = svm_rbf_spec)\n)\n\n\nset.seed(4217)\nsvm_wf_set_res <- workflow_map(\n  svm_wf_set,\n  \"tune_grid\",\n  grid = 10, resamples = bike_resamples, metrics = bike_metrics\n)\n\n\nautoplot(svm_wf_set_res)\n\n\n\n\nThose are some pretty interesting (and smooth) trends. The best model is one of the svm_rbf candidates (though the next 10 are the svm_linear models). Finalize the workflows and fit:\n\nsvm_linear_wf_res <-\n  extract_workflow_set_result(svm_wf_set_res, \"recipe_linear\")\nsvm_linear_wf <-\n  finalize_workflow(\n    extract_workflow(svm_wf_set, \"recipe_linear\"),\n    select_best(svm_linear_wf_res, \"mase\")\n  )\nsvm_linear_fit <- fit(svm_linear_wf, bike_train)\n\nsvm_poly_wf_res <-\n  extract_workflow_set_result(svm_wf_set_res, \"recipe_poly\")\nsvm_poly_wf <-\n  finalize_workflow(\n    extract_workflow(svm_wf_set, \"recipe_poly\"),\n    select_best(svm_poly_wf_res, \"mase\")\n  )\nsvm_poly_fit <- fit(svm_poly_wf, bike_train)\n\nsvm_rbf_wf_res <-\n  extract_workflow_set_result(svm_wf_set_res, \"recipe_rbf\")\nsvm_rbf_wf <-\n  finalize_workflow(\n    extract_workflow(svm_wf_set, \"recipe_rbf\"),\n    select_best(svm_rbf_wf_res, \"mase\")\n  )\nsvm_rbf_fit <- fit(svm_rbf_wf, bike_train)"
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#choosing-a-final-model",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#choosing-a-final-model",
    "title": "Predicting bike ridership: developing a model",
    "section": "Choosing a final model",
    "text": "Choosing a final model\nI’ll choose a model by the best cross-validated MASE:\n\nbind_rows(\n  rank_results(glm_wf_set_res, rank_metric = \"mase\",\n                     select_best = TRUE) %>%\n    # Only include the full pre-processing recipe\n    filter(str_detect(wflow_id, \"date_weather\")),\n  rank_results(trees_wf_set_res, rank_metric = \"mase\", select_best = TRUE),\n  rank_results(svm_wf_set_res, rank_metric = \"mase\", select_best = TRUE),\n  show_best(xgb_tune, metric = \"mase\", n = 1) %>%\n    mutate(model = \"boost_tree_2\")\n) %>%\n  filter(.metric == \"mase\") %>%\n  select(model, mase = mean, std_err) %>%\n  arrange(mase) %>%\n  gt() %>%\n  fmt_number(c(mase, std_err), decimals = 4)\n\n\n\n\n\n  \n  \n    \n      model\n      mase\n      std_err\n    \n  \n  \n    boost_tree_2\n0.4913\n0.0503\n    boost_tree\n0.4933\n0.0545\n    rand_forest\n0.5099\n0.0566\n    svm_rbf\n0.5420\n0.0440\n    poisson_reg\n0.5471\n0.0563\n    decision_tree\n0.5481\n0.0500\n    svm_linear\n0.5608\n0.0478\n    linear_reg\n0.5775\n0.0754\n    svm_poly\n0.6253\n0.0410\n  \n  \n  \n\n\n\n\nXGBoost takes the top two spots, with boost_tree_2 (the more thorough tuning) being the slight winner. Perform a last_fit() and get the performance on the held-out test set:\n\nxgb_final_fit <- last_fit(\n  xgb_workflow_2, split = bike_split, metrics = bike_metrics\n)\ncollect_metrics(xgb_final_fit) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      .metric\n      .estimator\n      .estimate\n      .config\n    \n  \n  \n    rmse\nstandard\n42.8027205\nPreprocessor1_Model1\n    rsq\nstandard\n0.5487906\nPreprocessor1_Model1\n    mase\nstandard\n0.7146084\nPreprocessor1_Model1\n    poisson_log_loss\nstandard\nNaN\nPreprocessor1_Model1\n  \n  \n  \n\n\n\n\nThere’s a bit of a drop-off in performance from the training metrics. Let’s dig into the predictions.\n\nExploring the predictions\nPlot the relationship between actual n_bikes and predicted:\n\nxgb_final_preds <- bind_rows(\n  train = augment(extract_workflow(xgb_final_fit), bike_train),\n  test = augment(extract_workflow(xgb_final_fit), bike_test),\n  .id = \"data_set\"\n) %>%\n  mutate(data_set = fct_inorder(data_set))\n\nxgb_final_preds %>%\n  ggplot(aes(x = n_bikes, y = .pred)) +\n  geom_point() +\n  geom_abline(slope = 1, size = 1, color = td_colors$nice$emerald) +\n  facet_wrap(~ data_set)\n\n\n\n\nAnd here are the predictions overlaid on the data (vertical line delineates training and testing sets):\n\np <- xgb_final_preds %>%\n  ggplot(aes(x = count_date)) +\n  geom_line(aes(y = n_bikes, color = site_name), size = 1) +\n  geom_line(aes(y = .pred), color = \"black\") +\n  geom_vline(xintercept = min(bike_test$count_date), lty = 2) +\n  facet_wrap(~ site_name, ncol = 1, scales = \"free_y\") +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(breaks = seq(0, 600, 200)) +\n  expand_limits(y = 200) +\n  dunnr::add_facet_borders()\np\n\n\n\n\nTruncate the dates to look closer at the testing set performance:\n\np + coord_cartesian(xlim = as.Date(c(\"2021-07-01\", \"2022-05-01\")))\n\n\n\n\nThis looks okay to my eye, though it does look to be overfitting the training set. It’s a shame to not have more data to test a full year – most of the test set covers winter and spring.5\nTo investigate the model performance further, I’ll plot the biggest outliers in the training and testing set (and ±1 week on either side):\n\nworst_preds <- xgb_final_preds %>%\n  mutate(abs_error = abs(n_bikes - .pred)) %>%\n  group_by(data_set, site_name) %>%\n  slice_max(abs_error, n = 1) %>%\n  ungroup() %>%\n  select(data_set, site_name, count_date, n_bikes, .pred, abs_error)\n\nsite_colors <- setNames(td_colors$pastel6[1:5], unique(worst_preds$site_name))\n\nworst_preds %>%\n  select(data_set, site_name, count_date) %>%\n  mutate(\n    worst_date = count_date,\n    count_date = map(count_date, ~ seq.Date(.x - 7, .x + 7, by = \"day\"))\n  ) %>%\n  unnest(count_date) %>%\n  left_join(xgb_final_preds,\n            by = c(\"data_set\", \"site_name\", \"count_date\")) %>%\n  filter(!is.na(n_bikes)) %>%\n  mutate(site_name = fct_inorder(site_name)) %>%\n  split(.$site_name) %>%\n  map(\n    ~ ggplot(., aes(x = count_date)) +\n      geom_line(aes(y = n_bikes, color = site_name)) +\n      geom_line(aes(y = .pred), color = \"black\") +\n      geom_vline(aes(xintercept = worst_date), lty = 2) +\n      facet_wrap(~ data_set, scales = \"free_x\", ncol = 2) +\n      dunnr::add_facet_borders() +\n      scale_color_manual(values = site_colors) +\n      scale_x_date(breaks = unique(.$worst_date)) +\n      expand_limits(y = 0) +\n      theme(legend.position = \"none\") +\n      labs(x = NULL, y = NULL,\n           subtitle = .$site_name[[1]])\n  ) %>%\n  reduce(`+`) +\n  plot_layout(ncol = 2)\n\n\n\n\nRelative to the noise in the data, that doesn’t look too bad. The Vernon St site has a series of n_bikes = 0 in a row that I’m guessing aren’t real (see aside). It was probably an issue with the counter, or maybe some construction on the street halting traffic for that period.\n\n\nThe Vernon St site has 6 dates with n_bikes = 0 all in a row:\n\nbike_ridership %>%\n  filter(site_name == \"Vernon St\", n_bikes == 0) %>%\n  pull(count_date)\n\n[1] \"2021-11-13\" \"2021-11-14\" \"2021-11-15\" \"2021-11-16\" \"2021-11-17\"\n[6] \"2021-11-18\"\n\n\nI also think some of the poor predictions can be explained by missing data. For instance, there are some poor predictions around 2021-11-23, where the model is over-estimating n_bikes at a few sites. Check out the weather around that day:\n\nbike_ridership %>%\n  filter(count_date > \"2021-11-20\", count_date < \"2021-11-26\") %>%\n  distinct(count_date, mean_temperature, total_precipitation,\n           speed_max_gust, snow_on_ground) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      count_date\n      mean_temperature\n      total_precipitation\n      speed_max_gust\n      snow_on_ground\n    \n  \n  \n    2021-11-21\n3.2\nNA\nNA\n2\n    2021-11-22\n11.1\n42.2\n57\n2\n    2021-11-23\n6.1\nNA\n41\n1\n    2021-11-24\n0.1\nNA\n43\n1\n    2021-11-25\n3.7\nNA\n43\n1\n  \n  \n  \n\n\n\n\nThis happens to be around the time of a huge rain and wind storm in Nova Scotia that knocked out power for a lot of people. This is seen in the total_precipitation on 2021-11-22, but the next day is missing data so, in my pre-processing, it was imputed as 0, which is definitely a poor approximation of the truth. If I impute a large amount of rain (let’s say 15mm) for that date, here is how the predictions change:\n\naugment(\n  extract_workflow(xgb_final_fit),\n  xgb_final_preds %>%\n    filter(count_date == \"2021-11-23\") %>%\n    rename(.pred_old = .pred) %>%\n    mutate(total_precipitation = 15)\n) %>%\n  select(site_name, n_bikes, .pred_old, .pred_new = .pred) %>%\n  gt()\n\n\n\n\n\n  \n  \n    \n      site_name\n      n_bikes\n      .pred_old\n      .pred_new\n    \n  \n  \n    Dartmouth Harbourfront Greenway\n30\n137.51662\n59.39198\n    Hollis St\n19\n85.97115\n21.11964\n    South Park St\n59\n315.21432\n187.37488\n    Vernon St\n133\n169.32935\n85.16235\n    Windsor St\n26\n95.60495\n23.89534\n  \n  \n  \n\n\n\n\nThat is a much better prediction for a rainy and windy day."
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#conclusions",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#conclusions",
    "title": "Predicting bike ridership: developing a model",
    "section": "Conclusions",
    "text": "Conclusions\nIn the end, the XGBoost model was able to best predict daily bike ridership.\nIs this model useful to anyone? Maybe. Is it useful just sitting on my computer? Definitely not. So in my next post, I’ll put this model into production."
  },
  {
    "objectID": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#reproducibility",
    "href": "posts/2022-04-29-predicting-bike-ridership-developing-a-model/predicting-bike-ridership-developing-a-model.html#reproducibility",
    "title": "Predicting bike ridership: developing a model",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-08-21\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [cff964f] 2022-08-14: Converted the `predicting-bike-ridership-developing-a-model` post\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html",
    "title": "Predicting bike ridership: deploying the model",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(httr)\n\nlibrary(dunnr)\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\ntheme_set(theme_td())\nset_geom_fonts()\nset_palette()"
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#introduction",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#introduction",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Introduction",
    "text": "Introduction\nThis is the last in a series of posts about predicting bike ridership in Halifax. Previously I:\n\nretrieved and prepared bicycle counter and weather data, then\ndeveloped and evaluated different machine learning models.\n\nHere, I will walk through my steps of putting the model into “production” on Google Cloud Platform (GCP):\n\ndeploying an ETL pipeline with BigQuery and Cloud Scheduler,\nautomating model training with Cloud Run and Pub/Sub,\nserving predictions with a REST API via the plumber package (try it out here), and\ndeveloping a Shiny app for visualizing predictions (try it out here).\n\nThe source code for everything, including the Dockerfiles, can be found on GitHub here."
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#creating-the-project",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#creating-the-project",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Creating the project",
    "text": "Creating the project\nBefore anything, I had to create and set up a new project on GCP that I called hfx-bike-ridership. In addition to the very detailed GCP documentation, there are lots of great resources out there to walk through all the steps, like this one. In brief: after creating the project, I had to enable billing on the project, activate various APIs, create credentials (OAuth client and service account), and install the Cloud SDK."
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#etl-pipeline",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#etl-pipeline",
    "title": "Predicting bike ridership: deploying the model",
    "section": "ETL pipeline",
    "text": "ETL pipeline\nI next set up the ETL (extract, transform, load) pipeline to automatically extract the raw data from their sources, perform some transformations, and load it into a database.\nIn BigQuery, I created the bike_counts and weather data sets:\n\n\n\nI then wrote a script etl.R that retrieves and processes bike counter data from the Halifax open data platform, and weather data from the government of Canada. Most of the code there is copied from my previous post, except for the lines at the end to upload the data to BigQuery tables:\n\nbq_auth(\"oauth-client.json\")\n\nproject <- \"hfx-bike-ridership\"\n\ndaily_counts_table <- bq_table(project, \"bike_counts\", \"daily_counts\")\nweather_table <- bq_table(project, \"weather\", \"daily_report\")\n\nbq_table_upload(daily_counts_table,\n                value = bike_counts_daily, fields = bike_counts_daily,\n                create_disposition = \"CREATE_IF_NEEDED\",\n                write_disposition = \"WRITE_TRUNCATE\")\n\nbq_table_upload(weather_table,\n                value = climate_report_daily, fields = climate_report_daily,\n                create_disposition = \"CREATE_IF_NEEDED\",\n                write_disposition = \"WRITE_TRUNCATE\")\n\nThis uses the bigrquery package to authenticate (bq_auth()) using my OAuth credentials and upload (bq_table_upload()) the data (creates if missing, overwrites if existing) to the tables daily_counts and daily_report. Here is what BigQuery looked like after running this script:\n\n\n\nAnd the daily_counts table:\n\n\n\nPutting these data into BigQuery, as opposed to a Cloud Storage bucket for example, is convenient for quick queries when I don’t want to load the data into R, like this one to find days with zero bikes counted:\n\n\n\nI could have simply wrote the data to CSV files and uploaded them via the GCP console, but that would defeat the purpose of next step: automation. To deploy my etl.R script, I wrote a fairly simple Dockerfile:\n\nFROM rocker/tidyverse:latest\n\nRUN R -e \"install.packages(c('bigrquery', 'httr'), repos = 'http://cran.us.r-project.org')\"\n\nADD oauth-client.json /home/rstudio\nADD etl/etl.R /home/rstudio\n\nCMD Rscript /home/rstudio/etl.R\n\nExplaining how Docker works is a bit out of scope for this post1 but from top to bottom:\n\nFROM rocker/tidyverse:latest\n\nThe tidyverse Docker image provided by RStudio, which you can read more about here: https://hub.docker.com/r/rocker/tidyverse.\nThis image is a bit overkill for this simple script. If I were worried about the size and portability of my image, I would instead use the base R image https://hub.docker.com/_/r-base and install only the packages I need from tidyverse.\n\nRUN R -e \"install.packages(c('bigrquery', 'httr'), repos = 'http://cran.us.r-project.org')\"\n\nInstalls the other packages I need besides those that come with tidyverse.\nParticularly, httr for interacting with APIs, and bigrquery for BigQuery.\n\nADD oauth-client.json /home/rstudio and ADD etl/etl.R /home/rstudio\n\nAdd my GCP credentials the ETL script to the Docker container.\n\nCMD Rscript /home/rstudio/etl.R\n\nRun the script.\n\n\nI then built the image, tagged it, and pushed it to the Container Registry with these commands:\n\ndocker build -t hfx-bike-ridership-etl .\ndocker tag hfx-bike-ridership-etl gcr.io/hfx-bike-ridership/hfx-bike-ridership-etl\ndocker push gcr.io/hfx-bike-ridership/hfx-bike-ridership-etl:latest\n\nNow that it exists on GCP, I want to schedule this container to run every week through Cloud Build and Cloud Scheduler. I used the googleCloudRunner package and followed these instructions:\n\nlibrary(googleCloudRunner)\n\ncr_setup() # Define project ID and authenticate with credentials\n\nbuild <- cr_build_make(\"etl/hfx-bike-ridership-etl.yaml\")\n\ncr_schedule(\n  # Schedule for every Sunday at 12am\n  schedule = \"0 0 * * SUN\",\n  name = \"etl\",\n  httpTarget = cr_schedule_http(build),\n  region = \"northamerica-northeast1\"\n)\n\nHere is how the job showed up in Cloud Scheduler:\n\n\n\nAnd that’s the ETL taken care of. I left it for a day, and checked the data on Sunday morning to confirm that the data had been updated as expected."
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#model-tuning-and-training",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#model-tuning-and-training",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Model tuning and training",
    "text": "Model tuning and training\nWith the data in place, I then created a Cloud Storage bucket to store models, and wrote the tune.R script.\n\n\ntune.R\n# Setup -------------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(bigrquery)\nlibrary(googleCloudStorageR)\nlibrary(tictoc)\nsource(\"model/preprocess.R\")\n\nn_cores <- parallel::detectCores(logical = FALSE)\nlibrary(doParallel)\ncl <- makePSOCKcluster(n_cores - 1)\nregisterDoParallel(cl)\n# This extra step makes sure the parallel workers have access to the\n#  `tidyr::replace_na()` function during pre-processing\nparallel::clusterExport(cl, c(\"replace_na\"))\n\n# Read data ---------------------------------------------------\n\nbq_auth(path = \"oauth-client.json\")\n\n# Define the project, dataset and a new table for this project\nproject <- \"hfx-bike-ridership\"\n\ndaily_counts_table <- bq_table(project, \"bike_counts\", \"daily_counts\")\nbike_data <- bq_table_download(daily_counts_table)\nbike_data_updated <- bq_table_meta(daily_counts_table)$lastModifiedTime %>%\n  as.numeric() %>%\n  # `lastModifiedTime` is in milliseconds from 1970-01-01\n  {as.POSIXct(. / 1000, origin = \"1970-01-01\")}\n\nweather_table <- bq_table(project, \"weather\", \"daily_report\")\nweather_data <- bq_table_download(weather_table)\nweather_data_updated <- bq_table_meta(weather_table)$lastModifiedTime %>%\n  as.numeric() %>%\n  {as.POSIXct(. / 1000, origin = \"1970-01-01\")}\n\n# Pre-process -------------------------------------------------------------\n\nbike_data <- preprocess(bike_data, weather_data)\n\n# Splitting and resampling ------------------------------------------------\n\n# For the initial time split, data is ordered by date so that the training\n#  data consists of the earliest dates across all sites\nbike_data <- bike_data %>% arrange(count_date, site_name)\nbike_split <- initial_time_split(bike_data, prop = 0.7)\n\nbike_train <- training(bike_split)\nbike_test <- testing(bike_split)\n\n# ... but once I'm done splitting the data, I want to order by site followed by\n#  date for two reasons:\n#  (1) `step_impute_roll()` looks for rows in a window (ordered)\n#  (2) the `mase` metric compares predictions to the naive prediction, which\n#      uses the previous value\nbike_train <- bike_train %>% arrange(count_date, site_name)\nbike_test <- bike_test %>% arrange(count_date, site_name)\n\nbike_resamples <-\n  sliding_period(bike_train, index = count_date,\n                 period = \"month\", lookback = 13, assess_stop = 1)\n\n# For model versioning, record the splitting and resampling strategy\nsplits_resamples <- tibble(\n  n_data = nrow(bike_data), n_train = nrow(bike_train), n_test = nrow(bike_test),\n  min_date_train = min(bike_train$count_date),\n  max_date_train = max(bike_train$count_date),\n  min_date_test = min(bike_test$count_date),\n  max_date_test = max(bike_test$count_date),\n  prop = 0.7, resamples = \"sliding_period\",\n  resample_params = \"lookback = 13, assess_stop = 1\"\n)\n\n# Features ------------------------------------------------------------------\n\n# Get Canadian holidays\ncanada_holidays <-\n  timeDate::listHolidays(\n    pattern = \"^CA|^Christmas|^NewYears|Easter[Sun|Mon]|^GoodFriday|^CaRem\"\n  )\n\nbike_recipe <-\n  recipe(n_bikes ~ count_date + site_name + n_bikes_lag_14 +\n           mean_temperature + total_precipitation + speed_max_gust +\n           snow_on_ground,\n         data = bike_train) %>%\n  update_role(count_date, new_role = \"date_variable\") %>%\n  step_date(count_date, features = c(\"dow\", \"doy\", \"year\"),\n            label = TRUE, ordinal = FALSE) %>%\n  step_holiday(count_date, holidays = canada_holidays) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_impute_mean(speed_max_gust) %>%\n  step_mutate_at(c(total_precipitation, snow_on_ground),\n                 fn = ~ replace_na(., 0)) %>%\n  # Use a rolling window to impute temperature\n  step_impute_roll(mean_temperature, statistic = mean, window = 31) %>%\n  step_zv(all_predictors())\n\n\n# Model spec and workflow -----------------------------------------------------\n\nxgb_spec <- boost_tree(\n  mtry = tune(), trees = tune(), min_n = tune(),\n  tree_depth = tune(), learn_rate = tune()\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\nbike_xgb_workflow <- workflow() %>%\n  add_recipe(bike_recipe) %>%\n  add_model(xgb_spec)\n\nbike_train_baked <- prep(bike_recipe) %>% bake(bike_train)\n\nxgb_grid <- grid_latin_hypercube(\n  finalize(mtry(), select(bike_train_baked, -n_bikes)),\n  trees(), min_n(), tree_depth(), learn_rate(),\n  size = 100\n)\n\n# Tune --------------------------------------------------------------------\n\nbike_metrics <- metric_set(rmse, mae, rsq, mase)\n\nset.seed(944)\ntic()\nxgb_tune <- tune_grid(\n  bike_xgb_workflow, resamples = bike_resamples,\n  grid = xgb_grid, metrics = bike_metrics\n)\ntoc()\n\n# Choose the hyperparameters by MASE\nxgb_params <- select_best(xgb_tune, metric = \"mase\")\n# Also get all the metrics on the training for the chosen parameters\ntrain_metrics <- xgb_params %>%\n  left_join(\n    collect_metrics(xgb_tune) %>%\n      select(.metric, mean, n, std_err, .config),\n    by = \".config\"\n  )\n\n# Finalize and fit to the full training set\nbike_xgb_workflow_final <- finalize_workflow(bike_xgb_workflow, xgb_params)\nbike_xgb_fit <- bike_xgb_workflow_final %>% fit(bike_train)\n\n# Predict on the test set and get metrics\ntest_metrics <- bike_xgb_fit %>%\n  augment(bike_test) %>%\n  bike_metrics(truth = n_bikes, estimate = .pred)\n\n# Compile the model and  info into a list\nmodel_tuned <- list(\n  timestamp = Sys.time(),\n  bike_data_updated = bike_data_updated,\n  weather_data_updated = weather_data_updated,\n  splits_resamples = splits_resamples,\n  xgb_params = xgb_params,\n  train_metrics = train_metrics,\n  test_metrics = test_metrics,\n  bike_xgb_fit = bike_xgb_fit\n)\n\n# Save model and model info -----------------------------------------------\n\n# Model object\nwrite_rds(model_tuned, \"model/tune/xgb-model-tuned.rds\")\ngcs_upload_set_limit(20000000L) # 20 Mb\nmetadata <- gcs_upload(\"model/tune/xgb-model-tuned.rds\",\n                       name = \"tune/xgb-model-tuned.rds\",\n                       bucket = \"hfx-bike-ridership-model\")\ntimestamp <- as.POSIXct(metadata$updated,\n                        tryFormats = \"%Y-%m-%dT%H:%M:%OS\", tz = \"GMT\")\n\n# XGB hyperparameters\nxgb_params <- xgb_params %>%\n  mutate(timestamp = model_tuned$timestamp) %>%\n  select(-.config)\nwrite_csv(xgb_params, \"model/tune/xgb-params.csv\",\n          append = TRUE, col_names = FALSE)\nparams_table <- bq_table(project, \"model_info\", \"params\")\nbq_table_upload(params_table,\n                value = xgb_params, fields = xgb_params,\n                create_disposition = \"CREATE_IF_NEEDED\",\n                write_disposition = \"WRITE_APPEND\")\n\n# Model metrics\nxgb_metrics <- bind_rows(\n  train = train_metrics %>%\n    select(metric = .metric, value = mean, n, std_err),\n  test = test_metrics %>%\n    select(metric = .metric, value = .estimate),\n  .id = \"data_set\"\n) %>%\n  mutate(timestamp = model_tuned$timestamp)\nwrite_csv(xgb_metrics, \"model/tune/xgb-metrics.csv\",\n          append = TRUE, col_names = FALSE)\nmetrics_table <- bq_table(project, \"model_info\", \"metrics\")\nbq_table_upload(metrics_table,\n                value = xgb_metrics, fields = xgb_metrics,\n                create_disposition = \"CREATE_IF_NEEDED\",\n                write_disposition = \"WRITE_APPEND\")\n\n# Splitting and resampling strategy\nsplits_resamples <- splits_resamples %>%\n  mutate(timestamp = model_tuned$timestamp)\nwrite_csv(splits_resamples, \"model/tune/splits-resamples.csv\",\n          append = TRUE, col_names = FALSE)\nsplits_resamples_table <- bq_table(project, \"model_info\", \"splits_resamples\")\nbq_table_upload(splits_resamples_table,\n                value = splits_resamples, fields = splits_resamples,\n                create_disposition = \"CREATE_IF_NEEDED\",\n                write_disposition = \"WRITE_APPEND\")\n\n\nThis actual model code and choices are mostly unchanged from my last post, but in brief it: retrieves the latest data from BigQuery, splits the data into training and testing, creates resamples, engineers features, tunes the XGBoost model, finds the best hyperparameters by MASE, and saves the model (as an R object) to the bucket. I also decided to keep track of metrics with a BigQuery table:\n\n\n\nI decided to keep this part of the pipeline manual. Tuning the XGBoost model takes a while on my machine, even with parallel processing, and I’ve heard enough horror stories of surprise charges from cloud services that I don’t feel like risking it. I will periodically check in on my model predictions, and only plan on re-tuning if performance degrades appreciably.\nWhat I will automate, however, is model training. Every time the data is updated (i.e. Sundays at midnight), I want to train the tuned model on the full data set. The idea is pretty simple: get the data from BigQuery, the tuned model from the bucket, fit to the data and save that fit to the same bucket. The tricky bit is that I want this process to trigger only when the data is updated. It turns out that BigQuery currently doesn’t have native functionality to trigger Cloud Run, so I had to use a workaround.\nFirst, I wrote the fit.R function to work as a plumber API (these instructions were helpful):\n\n\nfit.R\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidymodels)\nlibrary(bigrquery)\nlibrary(googleCloudStorageR)\nlibrary(googleCloudRunner)\nlibrary(plumber)\nsource(\"preprocess.R\")\n\nbq_auth(path = \"oauth-client.json\")\ngcs_auth(\"oauth-client.json\")\ngcs_upload_set_limit(20000000L) # 20 Mb\n\n# This function will retrieve the latest data from BigQuery, the trained\n#  model from GCS, and fit an XGBoost model, which is saved to GCS\npub <- function(message) {\n  # Define the project, dataset and a new table for this project\n  project <- \"hfx-bike-ridership\"\n\n  daily_counts_table <- bq_table(project, \"bike_counts\", \"daily_counts\")\n  bike_data <- bq_table_download(daily_counts_table)\n  bike_data_updated <- bq_table_meta(daily_counts_table)$lastModifiedTime %>%\n    as.numeric() %>%\n    {as.POSIXct(. / 1000, origin = \"1970-01-01\")}\n\n\n  weather_table <- bq_table(project, \"weather\", \"daily_report\")\n  weather_data <- bq_table_download(weather_table)\n  weather_data_updated <- bq_table_meta(weather_table)$lastModifiedTime %>%\n    as.numeric() %>%\n    {as.POSIXct(. / 1000, origin = \"1970-01-01\")}\n\n  bike_data <- preprocess(bike_data, weather_data)\n  xgb_tuned <- gcs_get_object(\"tune/xgb-model-tuned.rds\",\n                              bucket = \"hfx-bike-ridership-model\",\n                              parseFunction = gcs_parse_rds)\n\n  message(\"Writing updating xgb-fit\")\n  xgb_fit <- list(\n    tune_timestamp = xgb_tuned$timestamp,\n    timestamp = Sys.time(),\n    bike_data_updated = bike_data_updated,\n    weather_data_updated = weather_data_updated,\n    bike_xgb_fit = fit(xgb_tuned$bike_xgb_fit, bike_data)\n  )\n\n  f <- function(input, output) write_rds(input, output)\n  metadata <- gcs_upload(xgb_fit, name = \"xgb-fit.rds\",\n                         bucket = \"hfx-bike-ridership-model\",\n                         object_function = f)\n\n  return(TRUE)\n}\n\n#' Receive pub/sub message\n#' @post /pubsub\n#' @param message a pub/sub message\nfunction(message = NULL) {\n  message(\"Received message \", message)\n  googleCloudRunner::cr_plumber_pubsub(message, pub)\n}\n\n\nI wrote a Docker file to containerize the API, built the image, and pushed it to the Container Registry. I then went to Cloud Run, created a new service called hfx-bike-ridership-fit using the just-uploaded Docker image:\n\n\n\nOnce up and running, this gave me a URL from which to query the API:\n\n\n\nNext, I had to set up an internal messaging system. The steps were:\n\nI added a message(\"Finished ETL pipeline\") at the end of the etl.R script to indicate that the data was updated.\nThis message shows up in Cloud Logging, so I added a “sink” (which is how Logging routes messages) to look for this specific log.\n\n\n\n\n\nThe destination of this sink is a Pub/Sub topic called data-updated.\nI added a subscription to this topic which pushes a POST request to the API.\n\n\n\n\n\nThe POST request triggers the model fitting code, and the new model is uploaded to the Storage bucket.\n\nThis seems like a complex workaround for a fairly simple task – I might be missing an easier method. Also, it may have made more sense to just have the model re-train on a weekly schedule (just after the ETL pipeline), but I wanted more flexibility for ad hoc updates. Regardless, both the ETL and model training are now fully automated."
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#deployment",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#deployment",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Deployment",
    "text": "Deployment\n\nREST API\nTo serve predictions, I wrote another plumber API in the api.R script:\n\n\napi.R\n#* @apiTitle Predict bike ridership in Halifax, NS\n#* @apiDescription This API serves predictions for the daily number of bicyclists passing particular sites around Halifax, Nova Scotia. For more information, check out the [source code](https://github.com/taylordunn/hfx-bike-ridership), my [post about the data](https://tdunn.ca/posts/2022-04-27-predicting-bike-ridership-getting-the-data/), and [my post about developing the model](https://tdunn.ca/posts/2022-04-29-predicting-bike-ridership-developing-a-model/).\n#* @apiContact list(name = \"Taylor Dunn\", url = \"http://www.tdunn.ca\", email = \"t.dunn19@gmail.com\")\n#* @apiVersion 1.0\n\nlibrary(plumber)\nlibrary(dplyr)\nlibrary(tidymodels)\nlibrary(bigrquery)\nlibrary(googleCloudStorageR)\n\nbq_auth(path = \"oauth-client.json\",\n        email = \"hfx-bike-ridership@hfx-bike-ridership.iam.gserviceaccount.com\")\ngcs_auth(\"oauth-client.json\")\n\nproject <- \"hfx-bike-ridership\"\n\nmessage(\"Reading data\")\ndaily_counts_table <- bq_table(project, \"bike_counts\", \"daily_counts\")\nbike_data <- bq_table_download(daily_counts_table)\n\nmessage(\"Loading model\")\nxgb_fit <- gcs_get_object(\"xgb-fit.rds\", bucket = \"hfx-bike-ridership-model\",\n                          parseFunction = gcs_parse_rds)\n\nsite_names <- c(\"Dartmouth Harbourfront Greenway\", \"Hollis St\",\n                \"South Park St\", \"Vernon St\", \"Windsor St\")\n\n#* @param count_date:str The date in YYYY-MM-DD format.\n#* @param site_name:[str] The location of the bike counter. One of \"Dartmouth Harbourfront Greenway\", \"Hollis St\", \"South Park St\", \"Vernon St\", \"Windsor St\".\n#* @param n_bikes_lag_14:[int] The number of bikes measured at the given `site_name` 14 days ago. If not provided, will attempt to impute with the actual value 14 days before `count_date`.\n#* @param mean_temperature:numeric The daily mean temperature. If not provided, will impute with the rolling mean.\n#* @param total_precipitation:numeric The daily amount of precipitation in mm. If not provided, will impute with zero.\n#* @param snow_on_ground:numeric The daily amount of snow on the ground in cm. If not provided, will impute with zero.\n#* @param speed_max_gust:numeric The daily maximum wind speed in km/h. If not provided, will impute with the mean in the training set.\n#* @get /n_bikes\nfunction(count_date, site_name = NA_character_, n_bikes_lag_14 = NA_integer_,\n         mean_temperature = NA_real_, total_precipitation = NA_real_,\n         snow_on_ground = NA_real_, speed_max_gust = NA_real_) {\n\n  # If not provided, use all `site_name`s\n  if (any(is.na(site_name))) {\n    site_name <- site_names\n  } else {\n    site_name <- match.arg(\n      site_name, choices = site_names, several.ok = TRUE\n    )\n  }\n\n  count_date <- as.Date(count_date)\n\n  # Get the 14-day lagged bike counts for each site\n  if (!is.na(n_bikes_lag_14) & length(site_name) != length(n_bikes_lag_14)) {\n    return(list(\n      status = 400,\n      message = \"Must provide a value of `n_bikes_lag_14` for every given `site_name`.\"\n    ))\n  } else {\n    d <- tibble(site_name = .env$site_name, count_date = .env$count_date,\n                count_date_lag_14 = count_date - 14,\n                n_bikes_lag_14 = .env$n_bikes_lag_14)\n\n    if (sum(is.na(d$n_bikes_lag_14)) > 0) {\n      message(\"Imputing `n_bikes_lag_14`\")\n      d <- d %>%\n        left_join(\n          bike_data %>%\n            select(site_name, count_date_lag_14 = count_date,\n                   n_bikes_lag_14_impute = n_bikes),\n          by = c(\"site_name\", \"count_date_lag_14\")\n        ) %>%\n        mutate(\n          n_bikes_lag_14 = ifelse(is.na(n_bikes_lag_14),\n                                  n_bikes_lag_14_impute, n_bikes_lag_14)\n        ) %>%\n        select(-n_bikes_lag_14_impute)\n\n      if (sum(is.na(d$n_bikes_lag_14)) > 0) {\n        return(list(\n          status = 400,\n          message = paste0(\n            \"Could not find `n_bikes_lag_14` values on date \", count_date,\n            \" for these sites \",\n            filter(d, is.na(n_bikes_lag_14)) %>% pull(site_name) %>% paste(collapse = \", \"),\n            \". Please provide your own `n_bikes_lag_14`, or choose a different `count_date`.\"\n          )\n        ))\n      }\n    }\n  }\n\n  # Add weather variables\n  d <- d %>%\n    mutate(\n      n_bikes_lag_14 = as.numeric(n_bikes_lag_14),\n      mean_temperature = as.numeric(mean_temperature),\n      total_precipitation = as.numeric(total_precipitation),\n      snow_on_ground = as.numeric(snow_on_ground),\n      speed_max_gust = as.numeric(speed_max_gust)\n    )\n\n  augment(xgb_fit$bike_xgb_fit, d)\n}\n\n#* @get /model_info\n#* @response 200 Returns model information: timestamps of when the model was last trained (`timestamp`), the model was last tuned (`tune_timestamp`), the bicycle data was last updated (`bike_data_updated`), the weather data was last updated (`weather_data_updated`).\nfunction() {\n  list(\n    timestamp = xgb_fit$timestamp,\n    tune_timestamp = xgb_fit$tune_timestamp,\n    bike_data_updated = xgb_fit$bike_data_updated,\n    weather_data_updated = xgb_fit$weather_data_updated\n  )\n}\n\n\nThis reads in the model from the Cloud Storage bucket and the latest bike data from BigQuery. As inputs, it requires only a single date (count_date), for which it will return predictions for all 5 sites. One or more specific sites can also be provided (site_name). If the lagged values (n_bikes_lag_14) are not provided, then they will be imputed from the bike data (an error will be returned if the lagged value cannot be imputed, i.e. there is no data 14 days before count_date). The weather inputs mean_temperature, total_precipitation, snow_on_ground, and speed_max_gust are imputed if not provided.\nAs with fit.R, I put this into a Docker container, pushed to Container Registry, and created a Cloud Run service hfx-bike-ridership-api using that image.\n\nUnlike the previous Cloud Run service which only accepts internal requests, this one is publicly available. For instance, you can get a prediction for n_bikes on Hollis St for May 23rd, 2022 with the following R code:\n\nbase_url <- \"https://hfx-bike-ridership-api-74govvz7xq-uc.a.run.app/\"\nquery <- \"n_bikes?count_date=2022-05-23&site_name=Hollis St\"\n\npaste0(base_url, query) %>%\n  URLencode() %>%\n  GET() %>%\n  content(as = \"parsed\") %>%\n  purrr::flatten()\n\n$site_name\n[1] \"Hollis St\"\n\n$count_date\n[1] \"2022-05-23\"\n\n$count_date_lag_14\n[1] \"2022-05-09\"\n\n$n_bikes_lag_14\n[1] 86\n\n$.pred\n[1] 31.7664\n\n\nA great feature of plumber is that provides an HTML interface for documenting and interacting with REST APIs. Check out this API here: https://hfx-bike-ridership-api-74govvz7xq-uc.a.run.app/docs/.\n\n\nScreenshot for posterity\n\n\n\nI also added a model_info option, which can be queried to see timestamps of when the model was last tuned and trained, and when the data were last updated:\n\nquery <- \"model_info\"\npaste0(base_url, query) %>%\n  URLencode() %>%\n  GET() %>%\n  content(as = \"parsed\") %>%\n  purrr::flatten()\n\n$timestamp\n[1] \"2022-08-21 04:02:28\"\n\n$tune_timestamp\n[1] \"2022-05-20 15:23:46\"\n\n$bike_data_updated\n[1] \"2022-08-21 04:02:06\"\n\n$weather_data_updated\n[1] \"2022-08-21 04:02:10\"\n\n\n\n\nShiny dashboard\nLastly, I wrote a Shiny dashboard to visualize predictions, app.R:\n\n\napp.R\nlibrary(shiny)\nlibrary(shinydashboard)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(workflows)\nlibrary(bigrquery)\nlibrary(googleCloudStorageR)\nlibrary(DT)\nlibrary(dunnr)\nsource(\"funcs.R\")\n\n# Plotting\nlibrary(showtext)\nsysfonts::font_add_google(\"Roboto Condensed\")\nshowtext_auto()\ntheme_set(theme_td(base_size = 14))\nset_geom_fonts()\nset_palette()\n\n# Authentication to GCP\nproject <- \"hfx-bike-ridership\"\nbq_auth(path = \"oauth-client.json\")\ngcs_auth(\"oauth-client.json\")\nbike_counts_table <- bq_table(project, \"bike_counts\", \"daily_counts\")\nweather_table <- bq_table(project, \"weather\", \"daily_report\")\n\nserver <- function(input, output, session) {\n  # Import data and model ---------------------------------------------------\n  data <- reactiveValues()\n  min_date <- reactiveVal()\n  max_date <- reactiveVal()\n  model <- reactiveVal()\n\n  observe({\n    # Re-reads data every hour\n    invalidateLater(1000 * 60 * 60)\n\n    message(\"Reading data and model\")\n    bike_data_raw <- bq_table_download(bike_counts_table)\n    weather_data_raw <- bq_table_download(weather_table)\n    model(gcs_get_object(\"xgb-fit.rds\",\n                         bucket = \"hfx-bike-ridership-model\",\n                         parseFunction = gcs_parse_rds))\n\n    bike_data <- bike_data_raw %>%\n      preprocess_bike_data() %>%\n      # Only include the last 14 days\n      filter(count_date >= max(count_date) - 13)\n    min_date(min(bike_data$count_date))\n    max_date(max(bike_data$count_date))\n    bike_data_future <- bike_data %>%\n      transmute(\n        count_date = count_date + 14, site_name, n_bikes_lag_14 = n_bikes\n      )\n\n    weather_data <- weather_data_raw %>%\n      preprocess_weather_data() %>%\n      filter(report_date >= min(bike_data$count_date),\n             report_date <= max(bike_data$count_date))\n    weather_data_future <- weather_data %>%\n      transmute(\n        report_date = report_date + 14,\n        # Impute temperature and wind speed with the mean\n        mean_temperature = round(mean(weather_data$mean_temperature,\n                                      na.rm = TRUE), 1),\n        speed_max_gust = round(mean(weather_data$speed_max_gust,\n                                    na.rm = TRUE)),\n        # Impute precipitation and snow with zero\n        total_precipitation = 0, snow_on_ground = 0\n      )\n\n    data$bike <- bind_rows(bike_data, bike_data_future)\n    data$weather <- bind_rows(weather_data, weather_data_future)\n  })\n\n  bike_weather_data <- reactive({\n    data$bike %>%\n      left_join(data$weather, by = c(\"count_date\" = \"report_date\"))\n  })\n\n  # Model info --------------------------------------------------------------\n  output$model_info_1 <- renderText({\n    HTML(\n      paste(\n        \"This Shiny app visualizes predictions of the daily number of bicyclists passing various bike counter sites around Halifax, Nova Scotia, in a four-week window.\",\n        \"Check out the <a href='https://github.com/taylordunn/hfx-bike-ridership'>source code here</a>, and <a href='https://tdunn.ca/posts/2022-05-19-predicting-bike-ridership-deploying-the-model/'>this write-up</a> for more information.\",\n        paste0(\"<br>Data are updated, and the model is re-trained on a schedule: currently every Sunday at midnight AST, and sometimes manually by me. \",\n               \"The current data go up to \",\n               \"<b>\", max_date(), \"</b>\",\n               \" as indicated by the vertical dashed line in the plots.\"),\n        \"<br>The locations of the sites are overlaid on a map of Halifax below:\",\n        sep = \"<br>\"\n      )\n    )\n  })\n\n  output$model_info_2 <- renderText({\n    HTML(\n      paste(\n        \"<br>\",\n        \"In addition to site, other features of the model are:\",\n        paste0(\"<ul>\",\n               \"<li>date features: day of week, day of year, year, and Canadian holidays</li>\",\n               \"<li>the number of bikes counted 14 days ago</li>\",\n               \"<li>weather features: daily mean temperature, total precipitation, maximum gust speed, and snow on the ground\",\n               \"</ul>\"),\n        \"See more information about the features and how missing data are handled <a href='https://tdunn.ca/posts/2022-04-29-predicting-bike-ridership-developing-a-model/'>in this post</a>.\",\n        \"<br>\"\n      )\n    )\n  })\n\n  # Plotting helpers --------------------------------------------------------\n  scale_x <- reactive({\n    scale_x_date(NULL, limits = c(min_date() - 1, max_date() + 14),\n                 breaks = seq.Date(min_date() - 1, max_date() + 14, \"7 days\"),\n                 date_labels = \"%b %d\")\n  })\n  vline <- reactive({\n    geom_vline(xintercept = max_date(), lty = 2, size = 1)\n  })\n\n  # Bike predictions --------------------------------------------------------\n  output$n_bikes_plot <- renderPlot({\n    workflows:::augment.workflow(model()$bike_xgb_fit,\n                                 bike_weather_data()) %>%\n      ggplot(aes(x = count_date)) +\n      vline() +\n      geom_line(aes(y = .pred), color = \"black\", size = 1) +\n      geom_point(aes(y = n_bikes, fill = site_name),\n                 color = \"black\", shape = 21, size = 4) +\n      facet_wrap(~ site_name, ncol = 1) +\n      expand_limits(y = 0) +\n      labs(y = NULL) +\n      scale_x() +\n      labs(title = \"Number of bikes vs date\",\n           subtitle = \"Coloured points show actual values, black lines are predictions\") +\n      theme(legend.position = \"none\") +\n      dunnr::add_facet_borders()\n  })\n\n  # Weather data ------------------------------------------------------------\n  temperature_plot <- reactive({\n    data$weather %>%\n      filter(!is.na(mean_temperature)) %>%\n      mutate(var = \"Mean daily temperature (celsius)\") %>%\n      ggplot(aes(x = report_date, y = mean_temperature)) +\n      vline() +\n      geom_point(fill = td_colors$nice$strong_red, shape = 21, size = 4) +\n      facet_wrap(~ var) +\n      labs(y = NULL,\n           title = \"Weather vs date\",\n           subtitle = \"Use the table below to edit values for prediction\") +\n      scale_x() +\n      theme(axis.text.x = element_blank()) +\n      dunnr::add_facet_borders()\n  })\n  precipitation_plot <- reactive({\n    data$weather %>%\n      filter(!is.na(total_precipitation)) %>%\n      mutate(var = \"Total daily precipitation (mm)\") %>%\n      ggplot(aes(x = report_date, y = total_precipitation)) +\n      vline() +\n      geom_col(fill = td_colors$nice$spanish_blue, color = \"black\") +\n      facet_wrap(~ var) +\n      expand_limits(y = 5) +\n      scale_y_continuous(NULL, expand = expansion(mult = c(0, 0.05))) +\n      scale_x() +\n      theme(axis.text.x = element_blank()) +\n      dunnr::add_facet_borders()\n  })\n\n  snow_plot <- reactive({\n    data$weather %>%\n      filter(!is.na(snow_on_ground)) %>%\n      mutate(var = \"Snow on ground (cm)\") %>%\n      ggplot(aes(x = report_date, y = snow_on_ground)) +\n      vline() +\n      geom_col(fill = td_colors$nice$charcoal, color = \"black\") +\n      facet_wrap(~ var) +\n      expand_limits(y = 5) +\n      scale_y_continuous(NULL, expand = expansion(mult = c(0, 0.05))) +\n      scale_x() +\n      theme(axis.text.x = element_blank()) +\n      dunnr::add_facet_borders()\n  })\n  wind_plot <- reactive({\n    data$weather %>%\n      filter(!is.na(speed_max_gust)) %>%\n      mutate(var = \"Maximum wind gust (km/h)\") %>%\n      ggplot(aes(x = report_date, y = speed_max_gust)) +\n      vline() +\n      geom_point(fill = td_colors$nice$emerald, shape = 21, size = 4) +\n      facet_wrap(~ var) +\n      labs(y = NULL) +\n      scale_x()\n  })\n\n  output$weather_plot <- renderPlot({\n    temperature_plot() +\n      precipitation_plot() +\n      snow_plot() +\n      wind_plot() +\n      plot_layout(ncol = 1)\n  })\n\n  output$weather_table <- renderDataTable(\n    datatable(\n      data$weather,\n      rownames = FALSE, escape = FALSE,\n      colnames = c(\"Date\", \"Temp.\", \"Precip.\", \"Snow\", \"Wind\"),\n      editable = list(target = \"cell\", numeric = c(2, 3, 4, 5)),\n      options = list(pageLength = 7, dom = \"tp\"),\n      caption = \"Double click a cell to edit values. Plots and predictions will update automatically.\"\n    ) %>%\n      DT::formatStyle(names(data$weather), lineHeight = \"80%\")\n  )\n\n  observeEvent(input$weather_table_cell_edit, {\n    row <- input$weather_table_cell_edit$row\n    col <- input$weather_table_cell_edit$col\n    data$weather[row, col + 1] <- input$weather_table_cell_edit$value\n  })\n}\n\nui <- dashboardPage(\n  skin = \"yellow\",\n  dashboardHeader(title = \"Predicting bike ridership in Halifax, NS\",\n                  titleWidth = 500),\n  dashboardSidebar(disable = TRUE),\n  dashboardBody(\n    tags$head(\n      tags$link(rel = \"stylesheet\", type = \"text/css\", href = \"custom.css\")\n    ),\n    fluidRow(\n    column(\n      width = 3,\n      box(\n        title = HTML(paste0(as.character(icon(\"info\")), \" <b>Info</b>\")),\n        width = 12,\n        style = \"overflow-x: scroll;\",\n        uiOutput(\"model_info_1\"),\n        img(src = \"bike-counter-sites.png\",\n            style = \"width: 300px; display: block; margin-left: auto; margin-right: auto;\"),\n        uiOutput(\"model_info_2\")\n      )\n    ),\n    column(\n      width = 5,\n      box(\n        width = 12,\n        style = \"overflow-x: scroll;\",\n        plotOutput(\"n_bikes_plot\", height = \"800px\")\n      )\n    ),\n    column(\n      width = 4,\n      box(\n        width = 12,\n        style = \"overflow-x: scroll;\",\n        plotOutput(\"weather_plot\", height = \"600px\"),\n        dataTableOutput(\"weather_table\")\n      )\n    )\n  )\n  )\n)\n\nshinyApp(ui, server)\n\n\nI wrote the Docker file, pushed it to Container Registry, and deployed on Cloud Run.\nAssuming I haven’t shut it down (and that my billing information is not out of date), you can try the app here or embedded below:\n\n\n\n\nIn terms of design, I went with a three column layout with content organized into shinydashboard::box()s. The left-most column has some basic information, including the date of when the data and model were last updated. I also included a map showing the locations of the five sites:\n\n\n\nThe main interest of this dashboard is the forecasted number of bikes, so it takes the centre column:\n\nThere is a lot of data involved in this proejct, but I decided to keep this app fairly small in scope. Just the last 14 days and the next 14 days (relative to the when data/model were updated) are shown here.\nThe third column shows the most interesting predictors of the model – the weather variables:\n\n\n\nThe 14 days to the left of the dashed line are actual values, and the 14 days to right right are imputed future values.2 The table at the bottom lists all of the visualized weather data. To add some interactivity, I decided to make this part editable:\n\nEverything is reactive to this table, so the plots will be updated immediately:\n\nand so will the predictions:\n\nThis allows me to ask questions like: how will the predicted number of bicyclists change if it downpours tomorrow?"
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#conclusion",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#conclusion",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post I used Google Cloud Platform to automate the data ETL pipeline and model training. The model was then deployed in a REST API and Shiny dashboard. When considering the full MLOps cycle, the biggest missing piece is some sort of monitoring functionality. This could include data validation (e.g. trigger an alert if new data have abnormal or missing values) and model drift detection (e.g. model performance is below some threshold which triggers re-tuning). But as far as personal projects go, I’m content to leave it here and re-visit it every once in a while to see how the data and predictions are holding up.\nRegardless of the value of the model, this was a great learning experience. I’d not used GCP much before this (we use AWS at my company) but it wasn’t too painful a transition between cloud services. The packages by Mark Edmondson (googleCloudStorageR, googleCloudRunner), and the accompanying documentation, were a great help.\nDocumenting my process here was important to me. I learn best by doing, and second best by seeing what others do in open source projects like this. I hope that this walkthrough and code can help others in getting their own MLOps projects up-and-running."
  },
  {
    "objectID": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#reproducibility",
    "href": "posts/2022-05-19-predicting-bike-ridership-deploying-the-model/predicting-bike-ridership-deploying-the-model.html#reproducibility",
    "title": "Predicting bike ridership: deploying the model",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\nSession info\n\n\n\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_Canada.utf8\n ctype    English_Canada.utf8\n tz       America/Curacao\n date     2022-08-21\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n\n\n\n  \n\n\n\n\n\n\nGit repository\n\n\n\nLocal:    main C:/Users/tdunn/Documents/tdunn-quarto\nRemote:   main @ origin (https://github.com/taylordunn/tdunn-quarto.git)\nHead:     [767c281] 2022-08-21: Finished converting `predicting-bike-ridership-deploying-the-model`\n\n\n\nSource code, R environment"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "tdunn",
    "section": "",
    "text": "hfx-bike-ridership\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      An end-to-end machine learning project to predict bike ridership in Halifax, Nova Scotia.\n    \n  \n  \n    \n      \n        tidytuesday-dashboard\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      A Shiny dashboard compiling tweets from the TidyTuesday project.\n    \n  \n  \n    \n      \n        canadacovid\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      An R package for the Canadian COVID-19 tracker API.\n    \n  \n  \n    \n      \n        canadacovidshiny\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      A Shiny dashboard reporting and visualizing the latest COVID-19 data in Canada.\n    \n  \n  \n    \n      \n        dunnr\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      My personal R package of commonly used functions and templates.\n    \n  \n  \n    \n      \n        gasr\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      An R package for simulating goal attainment scaling (GAS) data.\n    \n  \n  \n    \n      \n        islr-tidy\n      \n      \n        \n      \n    \n    \n      \n    \n    \n      An Introduction to Statistical Learning translated with the tidyverse and tidymodels.\n    \n  \n\n\nNo matching items"
  }
]